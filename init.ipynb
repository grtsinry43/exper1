{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2025-04-07T07:26:30.701302Z",
     "start_time": "2025-04-07T07:26:30.471291Z"
    }
   },
   "source": [
    "from mindspore import context\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import mindspore.dataset as ds\n",
    "\n",
    "context.set_context(mode=context.GRAPH_MODE,\n",
    "                    device_target=\"CPU\")  # Windows version, set to use CPU for graph calculation\n",
    "train_data_path = \"./data/train\"\n",
    "test_data_path = \"./data/test\"\n",
    "mnist_ds = ds.MnistDataset(train_data_path)  # Load training dataset\n",
    "print('The type of mnist_ds:', type(mnist_ds))\n",
    "print(\"Number of pictures contained in the mnist_ds：\", mnist_ds.get_dataset_size())  # 60000 pictures in total\n",
    "\n",
    "dic_ds = mnist_ds.create_dict_iterator()  # Convert dataset to dictionary type\n",
    "item = dic_ds.__next__()\n",
    "img = item[\"image\"].asnumpy()\n",
    "label = item[\"label\"].asnumpy()\n",
    "\n",
    "print(\"The item of mnist_ds:\",\n",
    "      item.keys())  # Take a single data to view the data structure, including two keys, image and label\n",
    "print(\"Tensor of image in item:\", img.shape)  # View the tensor of image (28,28,1)\n",
    "print(\"The label of item:\", label)\n",
    "\n",
    "plt.imshow(np.squeeze(img))\n",
    "plt.title(\"number:%s\" % item[\"label\"])\n",
    "plt.show()"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] ME(7418:129632434239040,MainProcess):2025-04-07-15:26:30.473.000 [mindspore/context.py:1335] For 'context.set_context', the parameter 'device_target' will be deprecated and removed in a future version. Please use the api mindspore.set_device() instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The type of mnist_ds: <class 'mindspore.dataset.engine.datasets_vision.MnistDataset'>\n",
      "Number of pictures contained in the mnist_ds： 60000\n",
      "The item of mnist_ds: dict_keys(['image', 'label'])\n",
      "Tensor of image in item: (28, 28, 1)\n",
      "The label of item: 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "ae5c545dc5326151",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T07:26:30.716537Z",
     "start_time": "2025-04-07T07:26:30.712594Z"
    }
   },
   "source": [
    "import mindspore.dataset.vision.c_transforms as CV\n",
    "import mindspore.dataset.transforms.c_transforms as C\n",
    "from mindspore.dataset.vision import Inter\n",
    "from mindspore.common import dtype as mstype\n",
    "\n",
    "\n",
    "def create_dataset(data_path, batch_size=32, repeat_size=1,\n",
    "                   num_parallel_workers=1):\n",
    "    \"\"\" create dataset for train or test\n",
    "    Args:\n",
    "        data_path: Data path\n",
    "        batch_size: The number of data records in each group\n",
    "        repeat_size: The number of replicated data records\n",
    "        num_parallel_workers: The number of parallel workers\n",
    "    \"\"\"\n",
    "    # define dataset\n",
    "    mnist_ds = ds.MnistDataset(data_path)\n",
    "\n",
    "    # Define some parameters needed for data enhancement and rough justification\n",
    "    resize_height, resize_width = 32, 32\n",
    "    rescale = 1.0 / 255.0\n",
    "    shift = 0.0\n",
    "    rescale_nml = 1 / 0.3081\n",
    "    shift_nml = -1 * 0.1307 / 0.3081\n",
    "\n",
    "    # According to the parameters, generate the corresponding data enhancement method\n",
    "    resize_op = CV.Resize((resize_height, resize_width),\n",
    "                          interpolation=Inter.LINEAR)  # Resize images to (32, 32) by bilinear interpolation\n",
    "    rescale_nml_op = CV.Rescale(rescale_nml, shift_nml)  # normalize images\n",
    "    rescale_op = CV.Rescale(rescale, shift)  # rescale images\n",
    "    hwc2chw_op = CV.HWC2CHW()  # change shape from (height, width, channel) to (channel, height, width) to fit network.\n",
    "    type_cast_op = C.TypeCast(mstype.int32)  # change data type of label to int32 to fit network\n",
    "\n",
    "    # Using map () to apply operations to a dataset\n",
    "    mnist_ds = mnist_ds.map(input_columns=\"label\", operations=type_cast_op, num_parallel_workers=num_parallel_workers)\n",
    "    mnist_ds = mnist_ds.map(input_columns=\"image\", operations=resize_op, num_parallel_workers=num_parallel_workers)\n",
    "    mnist_ds = mnist_ds.map(input_columns=\"image\", operations=rescale_op, num_parallel_workers=num_parallel_workers)\n",
    "    mnist_ds = mnist_ds.map(input_columns=\"image\", operations=rescale_nml_op, num_parallel_workers=num_parallel_workers)\n",
    "    mnist_ds = mnist_ds.map(input_columns=\"image\", operations=hwc2chw_op, num_parallel_workers=num_parallel_workers)\n",
    "    # Process the generated dataset\n",
    "    buffer_size = 10000\n",
    "    mnist_ds = mnist_ds.shuffle(buffer_size=buffer_size)  # 10000 as in LeNet train script\n",
    "    mnist_ds = mnist_ds.batch(batch_size, drop_remainder=True)\n",
    "    mnist_ds = mnist_ds.repeat(repeat_size)\n",
    "\n",
    "    return mnist_ds"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "id": "fac95f1215bf418a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T07:26:30.768333Z",
     "start_time": "2025-04-07T07:26:30.763095Z"
    }
   },
   "source": [
    "datas = create_dataset(train_data_path)  # Process the train dataset\n",
    "print('Number of groups in the dataset:', datas.get_dataset_size())  # Number of query dataset groups"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] ME(7418:129632434239040,MainProcess):2025-04-07-15:26:30.764.000 [mindspore/dataset/core/validator_helpers.py:744] 'Resize' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Resize' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(7418:129632434239040,MainProcess):2025-04-07-15:26:30.764.000 [mindspore/dataset/core/validator_helpers.py:744] 'Rescale' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Rescale' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(7418:129632434239040,MainProcess):2025-04-07-15:26:30.764.000 [mindspore/dataset/core/validator_helpers.py:744] 'Rescale' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Rescale' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(7418:129632434239040,MainProcess):2025-04-07-15:26:30.765.000 [mindspore/dataset/core/validator_helpers.py:744] 'HWC2CHW' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'HWC2CHW' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(7418:129632434239040,MainProcess):2025-04-07-15:26:30.765.000 [mindspore/dataset/core/validator_helpers.py:744] 'TypeCast' from mindspore.dataset.transforms.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'TypeCast' from mindspore.dataset.transforms instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of groups in the dataset: 1875\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "id": "10ccb9f7c27abf33",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T07:26:31.035189Z",
     "start_time": "2025-04-07T07:26:30.812136Z"
    }
   },
   "source": [
    "data = datas.create_dict_iterator().__next__()  # Take a set of datasets\n",
    "print(data.keys())\n",
    "images = data[\"image\"].asnumpy()  # Take out the image data in this dataset\n",
    "labels = data[\"label\"].asnumpy()  # Take out the label (subscript) of this data set\n",
    "print('Tensor of image:', images.shape)  # Query the tensor of images in each dataset (32,1,32,32)\n",
    "print('labels:', labels)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['image', 'label'])\n",
      "Tensor of image: (32, 1, 32, 32)\n",
      "labels: [2 5 9 0 4 6 9 6 1 7 7 8 5 3 0 0 5 5 5 6 5 0 7 9 9 8 6 7 9 7 4 6]\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "id": "305ec7daaacd26d4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T07:26:31.601075Z",
     "start_time": "2025-04-07T07:26:31.043097Z"
    }
   },
   "source": [
    "count = 1\n",
    "for i in images:\n",
    "    plt.subplot(4, 8, count)\n",
    "    plt.imshow(np.squeeze(i))\n",
    "    plt.title('num:%s' % labels[count - 1])\n",
    "    plt.xticks([])\n",
    "    count += 1\n",
    "    plt.axis(\"off\")\n",
    "plt.show()  # Print a total of 32 pictures in the group"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 32 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "id": "eeceb68c50aade08",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T07:26:31.619350Z",
     "start_time": "2025-04-07T07:26:31.616387Z"
    }
   },
   "source": [
    "import mindspore.nn as nn\n",
    "from mindspore.common.initializer import TruncatedNormal\n",
    "\n",
    "\n",
    "# Initialize 2D convolution function\n",
    "def conv(in_channels, out_channels, kernel_size, stride=1, padding=0):\n",
    "    \"\"\"Conv layer weight initial.\"\"\"\n",
    "    weight = weight_variable()\n",
    "    return nn.Conv2d(in_channels, out_channels,\n",
    "                     kernel_size=kernel_size, stride=stride, padding=padding,\n",
    "                     weight_init=weight, has_bias=False, pad_mode=\"valid\")\n",
    "\n",
    "\n",
    "# Initialize full connection layer\n",
    "def fc_with_initialize(input_channels, out_channels):\n",
    "    \"\"\"Fc layer weight initial.\"\"\"\n",
    "    weight = weight_variable()\n",
    "    bias = weight_variable()\n",
    "    return nn.Dense(input_channels, out_channels, weight, bias)\n",
    "\n",
    "\n",
    "# Set truncated normal distribution\n",
    "def weight_variable():\n",
    "    \"\"\"Weight initial.\"\"\"\n",
    "    return TruncatedNormal(0.02)"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "id": "ab061c42419732bb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T07:26:31.676329Z",
     "start_time": "2025-04-07T07:26:31.672893Z"
    }
   },
   "source": [
    "class LeNet5(nn.Cell):\n",
    "    \"\"\"Lenet network structure.\"\"\"\n",
    "\n",
    "    # define the operator required\n",
    "    def __init__(self):\n",
    "        super(LeNet5, self).__init__()\n",
    "        self.batch_size = 32  # 32 pictures in each group\n",
    "        self.conv1 = conv(1, 6,\n",
    "                          5)  # Convolution layer 1, 1 channel input (1 Figure), 6 channel output (6 figures), convolution core 5 * 5\n",
    "        self.conv2 = conv(6, 16, 5)  # Convolution layer 2,6-channel input, 16 channel output, convolution kernel 5 * 5\n",
    "        self.fc1 = fc_with_initialize(16 * 5 * 5, 120)\n",
    "        self.fc2 = fc_with_initialize(120, 84)\n",
    "        self.fc3 = fc_with_initialize(84, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.max_pool2d = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "    # use the preceding operators to construct networks\n",
    "    def construct(self, x):\n",
    "        x = self.conv1(x)  # 1*32*32-->6*28*28\n",
    "        x = self.relu(x)  # 6*28*28-->6*14*14\n",
    "        x = self.max_pool2d(x)  # Pool layer\n",
    "        x = self.conv2(x)  # Convolution layer\n",
    "        x = self.relu(x)  # Function excitation layer\n",
    "        x = self.max_pool2d(x)  # Pool layer\n",
    "        x = self.flatten(x)  # Dimensionality reduction\n",
    "        x = self.fc1(x)  # Full connection\n",
    "        x = self.relu(x)  # Function excitation layer\n",
    "        x = self.fc2(x)  # Full connection\n",
    "        x = self.relu(x)  # Function excitation layer\n",
    "        x = self.fc3(x)  # Full connection\n",
    "        return x"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "id": "6ebe0e11e661b22d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T07:26:31.733887Z",
     "start_time": "2025-04-07T07:26:31.723403Z"
    }
   },
   "source": [
    "network = LeNet5()\n",
    "print(network)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LeNet5<\n",
      "  (conv1): Conv2d<input_channels=1, output_channels=6, kernel_size=(5, 5), stride=(1, 1), pad_mode=valid, padding=0, dilation=(1, 1), group=1, has_bias=False, weight_init=<mindspore.common.initializer.TruncatedNormal object at 0x75e5d1b96fb0>, bias_init=None, format=NCHW>\n",
      "  (conv2): Conv2d<input_channels=6, output_channels=16, kernel_size=(5, 5), stride=(1, 1), pad_mode=valid, padding=0, dilation=(1, 1), group=1, has_bias=False, weight_init=<mindspore.common.initializer.TruncatedNormal object at 0x75e5ae003430>, bias_init=None, format=NCHW>\n",
      "  (fc1): Dense<input_channels=400, output_channels=120, has_bias=True>\n",
      "  (fc2): Dense<input_channels=120, output_channels=84, has_bias=True>\n",
      "  (fc3): Dense<input_channels=84, output_channels=10, has_bias=True>\n",
      "  (relu): ReLU<>\n",
      "  (max_pool2d): MaxPool2d<kernel_size=2, stride=2, pad_mode=VALID>\n",
      "  (flatten): Flatten<>\n",
      "  >\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "id": "9019e96f6bd58040",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T07:26:31.771492Z",
     "start_time": "2025-04-07T07:26:31.768900Z"
    }
   },
   "source": [
    "param = network.trainable_params()\n",
    "param"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter (name=conv1.weight, shape=(6, 1, 5, 5), dtype=Float32, requires_grad=True),\n",
       " Parameter (name=conv2.weight, shape=(16, 6, 5, 5), dtype=Float32, requires_grad=True),\n",
       " Parameter (name=fc1.weight, shape=(120, 400), dtype=Float32, requires_grad=True),\n",
       " Parameter (name=fc1.bias, shape=(120,), dtype=Float32, requires_grad=True),\n",
       " Parameter (name=fc2.weight, shape=(84, 120), dtype=Float32, requires_grad=True),\n",
       " Parameter (name=fc2.bias, shape=(84,), dtype=Float32, requires_grad=True),\n",
       " Parameter (name=fc3.weight, shape=(10, 84), dtype=Float32, requires_grad=True),\n",
       " Parameter (name=fc3.bias, shape=(10,), dtype=Float32, requires_grad=True)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "id": "de680f79303dcb0f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T07:26:31.821701Z",
     "start_time": "2025-04-07T07:26:31.818610Z"
    }
   },
   "source": [
    "# Training and testing related modules\n",
    "import argparse\n",
    "import os\n",
    "from mindspore import Tensor\n",
    "from mindspore.train.serialization import load_checkpoint, load_param_into_net\n",
    "from mindspore.train.callback import ModelCheckpoint, CheckpointConfig, LossMonitor, Callback\n",
    "from mindspore.train import Model\n",
    "from mindspore.nn.metrics import Accuracy\n",
    "from mindspore.nn.loss import SoftmaxCrossEntropyWithLogits\n",
    "\n",
    "\n",
    "def train_net(model, epoch_size, mnist_path, repeat_size, ckpoint_cb, step_loss_info):\n",
    "    \"\"\"Define the training method.\"\"\"\n",
    "    print(\"============== Starting Training ==============\")\n",
    "    # load training dataset\n",
    "    ds_train = create_dataset(os.path.join(mnist_path, \"train\"), 32, repeat_size)\n",
    "    model.train(epoch_size, ds_train, callbacks=[ckpoint_cb, LossMonitor(), step_loss_info], dataset_sink_mode=True)"
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "id": "70efac08893fd649",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T07:26:31.867261Z",
     "start_time": "2025-04-07T07:26:31.865015Z"
    }
   },
   "source": [
    "# Custom callback function\n",
    "class Step_loss_info(Callback):\n",
    "    def step_end(self, run_context):\n",
    "        cb_params = run_context.original_args()\n",
    "        # step_ Loss dictionary for saving loss value and step number information\n",
    "        step_loss[\"loss_value\"].append(str(cb_params.net_outputs))\n",
    "        step_loss[\"step\"].append(str(cb_params.cur_step_num))"
   ],
   "outputs": [],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "id": "78f652e02c2bd6d9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T07:27:52.809326Z",
     "start_time": "2025-04-07T07:26:31.913457Z"
    }
   },
   "source": [
    "import os\n",
    "\n",
    "if os.name == \"nt\":\n",
    "    os.system('del/f/s/q *.ckpt *.meta')  # Clean up old run files before in Windows\n",
    "else:\n",
    "    os.system('rm -f *.ckpt *.meta *.pb')  # Clean up old run files before in Linux\n",
    "\n",
    "lr = 0.01  # learning rate\n",
    "momentum = 0.9  #\n",
    "\n",
    "# create the network\n",
    "network = LeNet5()\n",
    "\n",
    "# define the optimizer\n",
    "net_opt = nn.Momentum(network.trainable_params(), lr, momentum)\n",
    "\n",
    "# define the loss function\n",
    "net_loss = SoftmaxCrossEntropyWithLogits(sparse=True, reduction='mean')\n",
    "# define the model\n",
    "model = Model(network, net_loss, net_opt, metrics={\"Accuracy\": Accuracy()})\n",
    "epoch_size = 10\n",
    "mnist_path = \"./data\"\n",
    "\n",
    "config_ck = CheckpointConfig(save_checkpoint_steps=125, keep_checkpoint_max=16)\n",
    "# save the network model and parameters for subsequence fine-tuning\n",
    "\n",
    "ckpoint_cb = ModelCheckpoint(prefix=\"checkpoint_lenet\", config=config_ck, directory=\"./checkpoints\")\n",
    "# group layers into an object with training and evaluation features\n",
    "step_loss = {\"step\": [], \"loss_value\": []}\n",
    "# step_ Loss dictionary for saving loss value and step number information\n",
    "step_loss_info = Step_loss_info()\n",
    "# save the steps and loss value\n",
    "repeat_size = 1\n",
    "train_net(model, epoch_size, mnist_path, repeat_size, ckpoint_cb, step_loss_info)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] ME(7418:129632434239040,MainProcess):2025-04-07-15:26:31.929.000 [mindspore/dataset/core/validator_helpers.py:744] 'Resize' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Resize' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(7418:129632434239040,MainProcess):2025-04-07-15:26:31.929.000 [mindspore/dataset/core/validator_helpers.py:744] 'Rescale' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Rescale' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(7418:129632434239040,MainProcess):2025-04-07-15:26:31.930.000 [mindspore/dataset/core/validator_helpers.py:744] 'Rescale' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Rescale' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(7418:129632434239040,MainProcess):2025-04-07-15:26:31.930.000 [mindspore/dataset/core/validator_helpers.py:744] 'HWC2CHW' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'HWC2CHW' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(7418:129632434239040,MainProcess):2025-04-07-15:26:31.930.000 [mindspore/dataset/core/validator_helpers.py:744] 'TypeCast' from mindspore.dataset.transforms.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'TypeCast' from mindspore.dataset.transforms instead.\n",
      "[WARNING] ME(7418:129632434239040,MainProcess):2025-04-07-15:26:31.932.000 [mindspore/train/model.py:1419] For Step_loss_info callback, {'step_end'} methods may not be supported in later version, Use methods prefixed with 'on_train' or 'on_eval' instead when using customized callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== Starting Training ==============\n",
      "epoch: 1 step: 1, loss is 2.299901008605957\n",
      "epoch: 1 step: 2, loss is 2.303295373916626\n",
      "epoch: 1 step: 3, loss is 2.2998125553131104\n",
      "epoch: 1 step: 4, loss is 2.3018369674682617\n",
      "epoch: 1 step: 5, loss is 2.3016414642333984\n",
      "epoch: 1 step: 6, loss is 2.2979884147644043\n",
      "epoch: 1 step: 7, loss is 2.3042800426483154\n",
      "epoch: 1 step: 8, loss is 2.3033785820007324\n",
      "epoch: 1 step: 9, loss is 2.307173490524292\n",
      "epoch: 1 step: 10, loss is 2.304697036743164\n",
      "epoch: 1 step: 11, loss is 2.306187152862549\n",
      "epoch: 1 step: 12, loss is 2.301429510116577\n",
      "epoch: 1 step: 13, loss is 2.299933671951294\n",
      "epoch: 1 step: 14, loss is 2.3036985397338867\n",
      "epoch: 1 step: 15, loss is 2.3019354343414307\n",
      "epoch: 1 step: 16, loss is 2.305600166320801\n",
      "epoch: 1 step: 17, loss is 2.2988944053649902\n",
      "epoch: 1 step: 18, loss is 2.2988169193267822\n",
      "epoch: 1 step: 19, loss is 2.303699493408203\n",
      "epoch: 1 step: 20, loss is 2.3090457916259766\n",
      "epoch: 1 step: 21, loss is 2.297564744949341\n",
      "epoch: 1 step: 22, loss is 2.296292543411255\n",
      "epoch: 1 step: 23, loss is 2.2984414100646973\n",
      "epoch: 1 step: 24, loss is 2.2984395027160645\n",
      "epoch: 1 step: 25, loss is 2.306952476501465\n",
      "epoch: 1 step: 26, loss is 2.3032546043395996\n",
      "epoch: 1 step: 27, loss is 2.3005027770996094\n",
      "epoch: 1 step: 28, loss is 2.308673620223999\n",
      "epoch: 1 step: 29, loss is 2.3032021522521973\n",
      "epoch: 1 step: 30, loss is 2.299957275390625\n",
      "epoch: 1 step: 31, loss is 2.310629367828369\n",
      "epoch: 1 step: 32, loss is 2.305337429046631\n",
      "epoch: 1 step: 33, loss is 2.2963361740112305\n",
      "epoch: 1 step: 34, loss is 2.3059043884277344\n",
      "epoch: 1 step: 35, loss is 2.2993109226226807\n",
      "epoch: 1 step: 36, loss is 2.3024275302886963\n",
      "epoch: 1 step: 37, loss is 2.2977371215820312\n",
      "epoch: 1 step: 38, loss is 2.298438549041748\n",
      "epoch: 1 step: 39, loss is 2.2962989807128906\n",
      "epoch: 1 step: 40, loss is 2.2972702980041504\n",
      "epoch: 1 step: 41, loss is 2.3065333366394043\n",
      "epoch: 1 step: 42, loss is 2.3106532096862793\n",
      "epoch: 1 step: 43, loss is 2.3042359352111816\n",
      "epoch: 1 step: 44, loss is 2.2977914810180664\n",
      "epoch: 1 step: 45, loss is 2.3055450916290283\n",
      "epoch: 1 step: 46, loss is 2.3106846809387207\n",
      "epoch: 1 step: 47, loss is 2.3108763694763184\n",
      "epoch: 1 step: 48, loss is 2.3001675605773926\n",
      "epoch: 1 step: 49, loss is 2.304694175720215\n",
      "epoch: 1 step: 50, loss is 2.2989373207092285\n",
      "epoch: 1 step: 51, loss is 2.3042545318603516\n",
      "epoch: 1 step: 52, loss is 2.3008334636688232\n",
      "epoch: 1 step: 53, loss is 2.297738790512085\n",
      "epoch: 1 step: 54, loss is 2.301445245742798\n",
      "epoch: 1 step: 55, loss is 2.305727243423462\n",
      "epoch: 1 step: 56, loss is 2.292354106903076\n",
      "epoch: 1 step: 57, loss is 2.2981860637664795\n",
      "epoch: 1 step: 58, loss is 2.2914469242095947\n",
      "epoch: 1 step: 59, loss is 2.30013370513916\n",
      "epoch: 1 step: 60, loss is 2.2967002391815186\n",
      "epoch: 1 step: 61, loss is 2.2966413497924805\n",
      "epoch: 1 step: 62, loss is 2.287372350692749\n",
      "epoch: 1 step: 63, loss is 2.3047101497650146\n",
      "epoch: 1 step: 64, loss is 2.3085598945617676\n",
      "epoch: 1 step: 65, loss is 2.290452480316162\n",
      "epoch: 1 step: 66, loss is 2.3042428493499756\n",
      "epoch: 1 step: 67, loss is 2.3093371391296387\n",
      "epoch: 1 step: 68, loss is 2.3018462657928467\n",
      "epoch: 1 step: 69, loss is 2.294417381286621\n",
      "epoch: 1 step: 70, loss is 2.30952787399292\n",
      "epoch: 1 step: 71, loss is 2.3194949626922607\n",
      "epoch: 1 step: 72, loss is 2.3025851249694824\n",
      "epoch: 1 step: 73, loss is 2.2980964183807373\n",
      "epoch: 1 step: 74, loss is 2.3020167350769043\n",
      "epoch: 1 step: 75, loss is 2.30141282081604\n",
      "epoch: 1 step: 76, loss is 2.3074374198913574\n",
      "epoch: 1 step: 77, loss is 2.2875208854675293\n",
      "epoch: 1 step: 78, loss is 2.294429302215576\n",
      "epoch: 1 step: 79, loss is 2.292417287826538\n",
      "epoch: 1 step: 80, loss is 2.303051710128784\n",
      "epoch: 1 step: 81, loss is 2.310610055923462\n",
      "epoch: 1 step: 82, loss is 2.3028805255889893\n",
      "epoch: 1 step: 83, loss is 2.314178228378296\n",
      "epoch: 1 step: 84, loss is 2.2902512550354004\n",
      "epoch: 1 step: 85, loss is 2.305220603942871\n",
      "epoch: 1 step: 86, loss is 2.308748245239258\n",
      "epoch: 1 step: 87, loss is 2.3061556816101074\n",
      "epoch: 1 step: 88, loss is 2.2986693382263184\n",
      "epoch: 1 step: 89, loss is 2.296610116958618\n",
      "epoch: 1 step: 90, loss is 2.3074841499328613\n",
      "epoch: 1 step: 91, loss is 2.302703857421875\n",
      "epoch: 1 step: 92, loss is 2.298722267150879\n",
      "epoch: 1 step: 93, loss is 2.3144845962524414\n",
      "epoch: 1 step: 94, loss is 2.2961244583129883\n",
      "epoch: 1 step: 95, loss is 2.287212371826172\n",
      "epoch: 1 step: 96, loss is 2.2911739349365234\n",
      "epoch: 1 step: 97, loss is 2.30430269241333\n",
      "epoch: 1 step: 98, loss is 2.298377752304077\n",
      "epoch: 1 step: 99, loss is 2.3167216777801514\n",
      "epoch: 1 step: 100, loss is 2.3066906929016113\n",
      "epoch: 1 step: 101, loss is 2.2921361923217773\n",
      "epoch: 1 step: 102, loss is 2.3078696727752686\n",
      "epoch: 1 step: 103, loss is 2.3068552017211914\n",
      "epoch: 1 step: 104, loss is 2.3171017169952393\n",
      "epoch: 1 step: 105, loss is 2.299149751663208\n",
      "epoch: 1 step: 106, loss is 2.3113768100738525\n",
      "epoch: 1 step: 107, loss is 2.2867510318756104\n",
      "epoch: 1 step: 108, loss is 2.3066563606262207\n",
      "epoch: 1 step: 109, loss is 2.2995855808258057\n",
      "epoch: 1 step: 110, loss is 2.306032180786133\n",
      "epoch: 1 step: 111, loss is 2.2997970581054688\n",
      "epoch: 1 step: 112, loss is 2.290724992752075\n",
      "epoch: 1 step: 113, loss is 2.3034212589263916\n",
      "epoch: 1 step: 114, loss is 2.312553882598877\n",
      "epoch: 1 step: 115, loss is 2.297881841659546\n",
      "epoch: 1 step: 116, loss is 2.3080925941467285\n",
      "epoch: 1 step: 117, loss is 2.297826051712036\n",
      "epoch: 1 step: 118, loss is 2.324108600616455\n",
      "epoch: 1 step: 119, loss is 2.3037219047546387\n",
      "epoch: 1 step: 120, loss is 2.3062753677368164\n",
      "epoch: 1 step: 121, loss is 2.303891181945801\n",
      "epoch: 1 step: 122, loss is 2.3073184490203857\n",
      "epoch: 1 step: 123, loss is 2.2959022521972656\n",
      "epoch: 1 step: 124, loss is 2.3037426471710205\n",
      "epoch: 1 step: 125, loss is 2.2917747497558594\n",
      "epoch: 1 step: 126, loss is 2.32045578956604\n",
      "epoch: 1 step: 127, loss is 2.30523943901062\n",
      "epoch: 1 step: 128, loss is 2.2963874340057373\n",
      "epoch: 1 step: 129, loss is 2.2939345836639404\n",
      "epoch: 1 step: 130, loss is 2.298001289367676\n",
      "epoch: 1 step: 131, loss is 2.3006467819213867\n",
      "epoch: 1 step: 132, loss is 2.3080427646636963\n",
      "epoch: 1 step: 133, loss is 2.306659460067749\n",
      "epoch: 1 step: 134, loss is 2.303844928741455\n",
      "epoch: 1 step: 135, loss is 2.310800313949585\n",
      "epoch: 1 step: 136, loss is 2.3150651454925537\n",
      "epoch: 1 step: 137, loss is 2.3043813705444336\n",
      "epoch: 1 step: 138, loss is 2.299651622772217\n",
      "epoch: 1 step: 139, loss is 2.2924656867980957\n",
      "epoch: 1 step: 140, loss is 2.301386594772339\n",
      "epoch: 1 step: 141, loss is 2.293793201446533\n",
      "epoch: 1 step: 142, loss is 2.2967753410339355\n",
      "epoch: 1 step: 143, loss is 2.311145305633545\n",
      "epoch: 1 step: 144, loss is 2.298631429672241\n",
      "epoch: 1 step: 145, loss is 2.301365375518799\n",
      "epoch: 1 step: 146, loss is 2.2937545776367188\n",
      "epoch: 1 step: 147, loss is 2.287576675415039\n",
      "epoch: 1 step: 148, loss is 2.3051862716674805\n",
      "epoch: 1 step: 149, loss is 2.3067634105682373\n",
      "epoch: 1 step: 150, loss is 2.301697254180908\n",
      "epoch: 1 step: 151, loss is 2.3090898990631104\n",
      "epoch: 1 step: 152, loss is 2.306183099746704\n",
      "epoch: 1 step: 153, loss is 2.3061363697052\n",
      "epoch: 1 step: 154, loss is 2.2859628200531006\n",
      "epoch: 1 step: 155, loss is 2.3002707958221436\n",
      "epoch: 1 step: 156, loss is 2.2976932525634766\n",
      "epoch: 1 step: 157, loss is 2.312490701675415\n",
      "epoch: 1 step: 158, loss is 2.308513879776001\n",
      "epoch: 1 step: 159, loss is 2.304356575012207\n",
      "epoch: 1 step: 160, loss is 2.299799919128418\n",
      "epoch: 1 step: 161, loss is 2.3160223960876465\n",
      "epoch: 1 step: 162, loss is 2.3102340698242188\n",
      "epoch: 1 step: 163, loss is 2.307011842727661\n",
      "epoch: 1 step: 164, loss is 2.2920210361480713\n",
      "epoch: 1 step: 165, loss is 2.2928245067596436\n",
      "epoch: 1 step: 166, loss is 2.3069331645965576\n",
      "epoch: 1 step: 167, loss is 2.301478147506714\n",
      "epoch: 1 step: 168, loss is 2.3029866218566895\n",
      "epoch: 1 step: 169, loss is 2.286328077316284\n",
      "epoch: 1 step: 170, loss is 2.311539888381958\n",
      "epoch: 1 step: 171, loss is 2.3030917644500732\n",
      "epoch: 1 step: 172, loss is 2.2995035648345947\n",
      "epoch: 1 step: 173, loss is 2.2788314819335938\n",
      "epoch: 1 step: 174, loss is 2.3064963817596436\n",
      "epoch: 1 step: 175, loss is 2.3097176551818848\n",
      "epoch: 1 step: 176, loss is 2.304185628890991\n",
      "epoch: 1 step: 177, loss is 2.3074417114257812\n",
      "epoch: 1 step: 178, loss is 2.298919677734375\n",
      "epoch: 1 step: 179, loss is 2.300874710083008\n",
      "epoch: 1 step: 180, loss is 2.3090319633483887\n",
      "epoch: 1 step: 181, loss is 2.3063361644744873\n",
      "epoch: 1 step: 182, loss is 2.3121144771575928\n",
      "epoch: 1 step: 183, loss is 2.291145086288452\n",
      "epoch: 1 step: 184, loss is 2.30159330368042\n",
      "epoch: 1 step: 185, loss is 2.28859543800354\n",
      "epoch: 1 step: 186, loss is 2.3032610416412354\n",
      "epoch: 1 step: 187, loss is 2.285289764404297\n",
      "epoch: 1 step: 188, loss is 2.310497283935547\n",
      "epoch: 1 step: 189, loss is 2.2983429431915283\n",
      "epoch: 1 step: 190, loss is 2.312427043914795\n",
      "epoch: 1 step: 191, loss is 2.314317226409912\n",
      "epoch: 1 step: 192, loss is 2.309527635574341\n",
      "epoch: 1 step: 193, loss is 2.3126487731933594\n",
      "epoch: 1 step: 194, loss is 2.3030598163604736\n",
      "epoch: 1 step: 195, loss is 2.299417734146118\n",
      "epoch: 1 step: 196, loss is 2.2932753562927246\n",
      "epoch: 1 step: 197, loss is 2.296666383743286\n",
      "epoch: 1 step: 198, loss is 2.3087124824523926\n",
      "epoch: 1 step: 199, loss is 2.293087959289551\n",
      "epoch: 1 step: 200, loss is 2.3084983825683594\n",
      "epoch: 1 step: 201, loss is 2.28096604347229\n",
      "epoch: 1 step: 202, loss is 2.298187255859375\n",
      "epoch: 1 step: 203, loss is 2.3056869506835938\n",
      "epoch: 1 step: 204, loss is 2.2999343872070312\n",
      "epoch: 1 step: 205, loss is 2.304091691970825\n",
      "epoch: 1 step: 206, loss is 2.314993381500244\n",
      "epoch: 1 step: 207, loss is 2.3057899475097656\n",
      "epoch: 1 step: 208, loss is 2.286227226257324\n",
      "epoch: 1 step: 209, loss is 2.293116807937622\n",
      "epoch: 1 step: 210, loss is 2.297428846359253\n",
      "epoch: 1 step: 211, loss is 2.29656982421875\n",
      "epoch: 1 step: 212, loss is 2.2952587604522705\n",
      "epoch: 1 step: 213, loss is 2.3061563968658447\n",
      "epoch: 1 step: 214, loss is 2.3063900470733643\n",
      "epoch: 1 step: 215, loss is 2.304849624633789\n",
      "epoch: 1 step: 216, loss is 2.3057944774627686\n",
      "epoch: 1 step: 217, loss is 2.3021671772003174\n",
      "epoch: 1 step: 218, loss is 2.2903387546539307\n",
      "epoch: 1 step: 219, loss is 2.3056490421295166\n",
      "epoch: 1 step: 220, loss is 2.301621198654175\n",
      "epoch: 1 step: 221, loss is 2.292616844177246\n",
      "epoch: 1 step: 222, loss is 2.292299509048462\n",
      "epoch: 1 step: 223, loss is 2.3117735385894775\n",
      "epoch: 1 step: 224, loss is 2.3076226711273193\n",
      "epoch: 1 step: 225, loss is 2.3200082778930664\n",
      "epoch: 1 step: 226, loss is 2.315899610519409\n",
      "epoch: 1 step: 227, loss is 2.280111312866211\n",
      "epoch: 1 step: 228, loss is 2.300360679626465\n",
      "epoch: 1 step: 229, loss is 2.2995450496673584\n",
      "epoch: 1 step: 230, loss is 2.3016245365142822\n",
      "epoch: 1 step: 231, loss is 2.3083181381225586\n",
      "epoch: 1 step: 232, loss is 2.2991554737091064\n",
      "epoch: 1 step: 233, loss is 2.3039956092834473\n",
      "epoch: 1 step: 234, loss is 2.300612211227417\n",
      "epoch: 1 step: 235, loss is 2.312307357788086\n",
      "epoch: 1 step: 236, loss is 2.3101470470428467\n",
      "epoch: 1 step: 237, loss is 2.296666383743286\n",
      "epoch: 1 step: 238, loss is 2.2878150939941406\n",
      "epoch: 1 step: 239, loss is 2.2672035694122314\n",
      "epoch: 1 step: 240, loss is 2.311650276184082\n",
      "epoch: 1 step: 241, loss is 2.3130290508270264\n",
      "epoch: 1 step: 242, loss is 2.320220708847046\n",
      "epoch: 1 step: 243, loss is 2.2980408668518066\n",
      "epoch: 1 step: 244, loss is 2.3114755153656006\n",
      "epoch: 1 step: 245, loss is 2.293827533721924\n",
      "epoch: 1 step: 246, loss is 2.3179590702056885\n",
      "epoch: 1 step: 247, loss is 2.302868604660034\n",
      "epoch: 1 step: 248, loss is 2.2991864681243896\n",
      "epoch: 1 step: 249, loss is 2.28920316696167\n",
      "epoch: 1 step: 250, loss is 2.291638135910034\n",
      "epoch: 1 step: 251, loss is 2.2898788452148438\n",
      "epoch: 1 step: 252, loss is 2.296497106552124\n",
      "epoch: 1 step: 253, loss is 2.316464900970459\n",
      "epoch: 1 step: 254, loss is 2.280856132507324\n",
      "epoch: 1 step: 255, loss is 2.305917263031006\n",
      "epoch: 1 step: 256, loss is 2.2783100605010986\n",
      "epoch: 1 step: 257, loss is 2.3079075813293457\n",
      "epoch: 1 step: 258, loss is 2.2875027656555176\n",
      "epoch: 1 step: 259, loss is 2.3096375465393066\n",
      "epoch: 1 step: 260, loss is 2.2955362796783447\n",
      "epoch: 1 step: 261, loss is 2.3039700984954834\n",
      "epoch: 1 step: 262, loss is 2.2925188541412354\n",
      "epoch: 1 step: 263, loss is 2.3067009449005127\n",
      "epoch: 1 step: 264, loss is 2.2920384407043457\n",
      "epoch: 1 step: 265, loss is 2.3019094467163086\n",
      "epoch: 1 step: 266, loss is 2.3001673221588135\n",
      "epoch: 1 step: 267, loss is 2.306516647338867\n",
      "epoch: 1 step: 268, loss is 2.3001365661621094\n",
      "epoch: 1 step: 269, loss is 2.2929580211639404\n",
      "epoch: 1 step: 270, loss is 2.297639846801758\n",
      "epoch: 1 step: 271, loss is 2.2952563762664795\n",
      "epoch: 1 step: 272, loss is 2.3133647441864014\n",
      "epoch: 1 step: 273, loss is 2.293748378753662\n",
      "epoch: 1 step: 274, loss is 2.316274404525757\n",
      "epoch: 1 step: 275, loss is 2.2811920642852783\n",
      "epoch: 1 step: 276, loss is 2.301084280014038\n",
      "epoch: 1 step: 277, loss is 2.2951714992523193\n",
      "epoch: 1 step: 278, loss is 2.3101348876953125\n",
      "epoch: 1 step: 279, loss is 2.298546552658081\n",
      "epoch: 1 step: 280, loss is 2.2883856296539307\n",
      "epoch: 1 step: 281, loss is 2.278494119644165\n",
      "epoch: 1 step: 282, loss is 2.298729658126831\n",
      "epoch: 1 step: 283, loss is 2.3157167434692383\n",
      "epoch: 1 step: 284, loss is 2.31259822845459\n",
      "epoch: 1 step: 285, loss is 2.2874765396118164\n",
      "epoch: 1 step: 286, loss is 2.2966785430908203\n",
      "epoch: 1 step: 287, loss is 2.2983338832855225\n",
      "epoch: 1 step: 288, loss is 2.327986240386963\n",
      "epoch: 1 step: 289, loss is 2.3220441341400146\n",
      "epoch: 1 step: 290, loss is 2.311471939086914\n",
      "epoch: 1 step: 291, loss is 2.2834229469299316\n",
      "epoch: 1 step: 292, loss is 2.3038415908813477\n",
      "epoch: 1 step: 293, loss is 2.2878623008728027\n",
      "epoch: 1 step: 294, loss is 2.284694194793701\n",
      "epoch: 1 step: 295, loss is 2.3169116973876953\n",
      "epoch: 1 step: 296, loss is 2.2994673252105713\n",
      "epoch: 1 step: 297, loss is 2.3057210445404053\n",
      "epoch: 1 step: 298, loss is 2.2959342002868652\n",
      "epoch: 1 step: 299, loss is 2.3065953254699707\n",
      "epoch: 1 step: 300, loss is 2.2974774837493896\n",
      "epoch: 1 step: 301, loss is 2.3035888671875\n",
      "epoch: 1 step: 302, loss is 2.3111062049865723\n",
      "epoch: 1 step: 303, loss is 2.2932400703430176\n",
      "epoch: 1 step: 304, loss is 2.30692720413208\n",
      "epoch: 1 step: 305, loss is 2.324922561645508\n",
      "epoch: 1 step: 306, loss is 2.306114673614502\n",
      "epoch: 1 step: 307, loss is 2.30867075920105\n",
      "epoch: 1 step: 308, loss is 2.3050613403320312\n",
      "epoch: 1 step: 309, loss is 2.281519651412964\n",
      "epoch: 1 step: 310, loss is 2.297685384750366\n",
      "epoch: 1 step: 311, loss is 2.295438289642334\n",
      "epoch: 1 step: 312, loss is 2.3138461112976074\n",
      "epoch: 1 step: 313, loss is 2.30718731880188\n",
      "epoch: 1 step: 314, loss is 2.2857277393341064\n",
      "epoch: 1 step: 315, loss is 2.30710768699646\n",
      "epoch: 1 step: 316, loss is 2.292160987854004\n",
      "epoch: 1 step: 317, loss is 2.2779898643493652\n",
      "epoch: 1 step: 318, loss is 2.3099989891052246\n",
      "epoch: 1 step: 319, loss is 2.306572914123535\n",
      "epoch: 1 step: 320, loss is 2.3171627521514893\n",
      "epoch: 1 step: 321, loss is 2.306232452392578\n",
      "epoch: 1 step: 322, loss is 2.3086376190185547\n",
      "epoch: 1 step: 323, loss is 2.3011672496795654\n",
      "epoch: 1 step: 324, loss is 2.292696714401245\n",
      "epoch: 1 step: 325, loss is 2.3021202087402344\n",
      "epoch: 1 step: 326, loss is 2.2871294021606445\n",
      "epoch: 1 step: 327, loss is 2.2827892303466797\n",
      "epoch: 1 step: 328, loss is 2.2928082942962646\n",
      "epoch: 1 step: 329, loss is 2.311251401901245\n",
      "epoch: 1 step: 330, loss is 2.2852060794830322\n",
      "epoch: 1 step: 331, loss is 2.3028109073638916\n",
      "epoch: 1 step: 332, loss is 2.304835319519043\n",
      "epoch: 1 step: 333, loss is 2.310905933380127\n",
      "epoch: 1 step: 334, loss is 2.304227352142334\n",
      "epoch: 1 step: 335, loss is 2.281890869140625\n",
      "epoch: 1 step: 336, loss is 2.3026158809661865\n",
      "epoch: 1 step: 337, loss is 2.294828176498413\n",
      "epoch: 1 step: 338, loss is 2.2984275817871094\n",
      "epoch: 1 step: 339, loss is 2.3035595417022705\n",
      "epoch: 1 step: 340, loss is 2.3056952953338623\n",
      "epoch: 1 step: 341, loss is 2.293926239013672\n",
      "epoch: 1 step: 342, loss is 2.2859880924224854\n",
      "epoch: 1 step: 343, loss is 2.299160957336426\n",
      "epoch: 1 step: 344, loss is 2.304851531982422\n",
      "epoch: 1 step: 345, loss is 2.3161580562591553\n",
      "epoch: 1 step: 346, loss is 2.3072986602783203\n",
      "epoch: 1 step: 347, loss is 2.2893118858337402\n",
      "epoch: 1 step: 348, loss is 2.3031811714172363\n",
      "epoch: 1 step: 349, loss is 2.320235252380371\n",
      "epoch: 1 step: 350, loss is 2.2895164489746094\n",
      "epoch: 1 step: 351, loss is 2.320390462875366\n",
      "epoch: 1 step: 352, loss is 2.2996585369110107\n",
      "epoch: 1 step: 353, loss is 2.325453519821167\n",
      "epoch: 1 step: 354, loss is 2.3003928661346436\n",
      "epoch: 1 step: 355, loss is 2.2922492027282715\n",
      "epoch: 1 step: 356, loss is 2.3027477264404297\n",
      "epoch: 1 step: 357, loss is 2.304076671600342\n",
      "epoch: 1 step: 358, loss is 2.2923038005828857\n",
      "epoch: 1 step: 359, loss is 2.300978183746338\n",
      "epoch: 1 step: 360, loss is 2.2867748737335205\n",
      "epoch: 1 step: 361, loss is 2.3048758506774902\n",
      "epoch: 1 step: 362, loss is 2.3053207397460938\n",
      "epoch: 1 step: 363, loss is 2.3258512020111084\n",
      "epoch: 1 step: 364, loss is 2.306199550628662\n",
      "epoch: 1 step: 365, loss is 2.2803726196289062\n",
      "epoch: 1 step: 366, loss is 2.312016725540161\n",
      "epoch: 1 step: 367, loss is 2.314915895462036\n",
      "epoch: 1 step: 368, loss is 2.2865090370178223\n",
      "epoch: 1 step: 369, loss is 2.315715789794922\n",
      "epoch: 1 step: 370, loss is 2.290874481201172\n",
      "epoch: 1 step: 371, loss is 2.283769130706787\n",
      "epoch: 1 step: 372, loss is 2.3077681064605713\n",
      "epoch: 1 step: 373, loss is 2.2879209518432617\n",
      "epoch: 1 step: 374, loss is 2.278907060623169\n",
      "epoch: 1 step: 375, loss is 2.3081681728363037\n",
      "epoch: 1 step: 376, loss is 2.3056905269622803\n",
      "epoch: 1 step: 377, loss is 2.313704013824463\n",
      "epoch: 1 step: 378, loss is 2.3019509315490723\n",
      "epoch: 1 step: 379, loss is 2.3079662322998047\n",
      "epoch: 1 step: 380, loss is 2.3005549907684326\n",
      "epoch: 1 step: 381, loss is 2.2922019958496094\n",
      "epoch: 1 step: 382, loss is 2.302197217941284\n",
      "epoch: 1 step: 383, loss is 2.301450729370117\n",
      "epoch: 1 step: 384, loss is 2.278498649597168\n",
      "epoch: 1 step: 385, loss is 2.290562629699707\n",
      "epoch: 1 step: 386, loss is 2.3075709342956543\n",
      "epoch: 1 step: 387, loss is 2.300459861755371\n",
      "epoch: 1 step: 388, loss is 2.3077943325042725\n",
      "epoch: 1 step: 389, loss is 2.3000407218933105\n",
      "epoch: 1 step: 390, loss is 2.3080880641937256\n",
      "epoch: 1 step: 391, loss is 2.3189525604248047\n",
      "epoch: 1 step: 392, loss is 2.3072402477264404\n",
      "epoch: 1 step: 393, loss is 2.2950801849365234\n",
      "epoch: 1 step: 394, loss is 2.317042827606201\n",
      "epoch: 1 step: 395, loss is 2.2956857681274414\n",
      "epoch: 1 step: 396, loss is 2.2855582237243652\n",
      "epoch: 1 step: 397, loss is 2.304882049560547\n",
      "epoch: 1 step: 398, loss is 2.2872121334075928\n",
      "epoch: 1 step: 399, loss is 2.3161823749542236\n",
      "epoch: 1 step: 400, loss is 2.3064472675323486\n",
      "epoch: 1 step: 401, loss is 2.3117196559906006\n",
      "epoch: 1 step: 402, loss is 2.3058953285217285\n",
      "epoch: 1 step: 403, loss is 2.281353712081909\n",
      "epoch: 1 step: 404, loss is 2.292593240737915\n",
      "epoch: 1 step: 405, loss is 2.3032443523406982\n",
      "epoch: 1 step: 406, loss is 2.2851972579956055\n",
      "epoch: 1 step: 407, loss is 2.3058154582977295\n",
      "epoch: 1 step: 408, loss is 2.2750637531280518\n",
      "epoch: 1 step: 409, loss is 2.293219566345215\n",
      "epoch: 1 step: 410, loss is 2.3255507946014404\n",
      "epoch: 1 step: 411, loss is 2.2945733070373535\n",
      "epoch: 1 step: 412, loss is 2.303703546524048\n",
      "epoch: 1 step: 413, loss is 2.298262119293213\n",
      "epoch: 1 step: 414, loss is 2.305159091949463\n",
      "epoch: 1 step: 415, loss is 2.2998154163360596\n",
      "epoch: 1 step: 416, loss is 2.2960498332977295\n",
      "epoch: 1 step: 417, loss is 2.297809362411499\n",
      "epoch: 1 step: 418, loss is 2.2906198501586914\n",
      "epoch: 1 step: 419, loss is 2.3014423847198486\n",
      "epoch: 1 step: 420, loss is 2.275300979614258\n",
      "epoch: 1 step: 421, loss is 2.3034653663635254\n",
      "epoch: 1 step: 422, loss is 2.2815871238708496\n",
      "epoch: 1 step: 423, loss is 2.299633502960205\n",
      "epoch: 1 step: 424, loss is 2.2999212741851807\n",
      "epoch: 1 step: 425, loss is 2.3244667053222656\n",
      "epoch: 1 step: 426, loss is 2.2934505939483643\n",
      "epoch: 1 step: 427, loss is 2.2919890880584717\n",
      "epoch: 1 step: 428, loss is 2.294562578201294\n",
      "epoch: 1 step: 429, loss is 2.2850685119628906\n",
      "epoch: 1 step: 430, loss is 2.2812459468841553\n",
      "epoch: 1 step: 431, loss is 2.2874655723571777\n",
      "epoch: 1 step: 432, loss is 2.3040735721588135\n",
      "epoch: 1 step: 433, loss is 2.296217441558838\n",
      "epoch: 1 step: 434, loss is 2.2971253395080566\n",
      "epoch: 1 step: 435, loss is 2.291970729827881\n",
      "epoch: 1 step: 436, loss is 2.2958827018737793\n",
      "epoch: 1 step: 437, loss is 2.306788444519043\n",
      "epoch: 1 step: 438, loss is 2.2959768772125244\n",
      "epoch: 1 step: 439, loss is 2.2924389839172363\n",
      "epoch: 1 step: 440, loss is 2.305222988128662\n",
      "epoch: 1 step: 441, loss is 2.3283300399780273\n",
      "epoch: 1 step: 442, loss is 2.2902822494506836\n",
      "epoch: 1 step: 443, loss is 2.2950363159179688\n",
      "epoch: 1 step: 444, loss is 2.3088042736053467\n",
      "epoch: 1 step: 445, loss is 2.289281129837036\n",
      "epoch: 1 step: 446, loss is 2.2627017498016357\n",
      "epoch: 1 step: 447, loss is 2.2931439876556396\n",
      "epoch: 1 step: 448, loss is 2.315175771713257\n",
      "epoch: 1 step: 449, loss is 2.303378105163574\n",
      "epoch: 1 step: 450, loss is 2.2869064807891846\n",
      "epoch: 1 step: 451, loss is 2.305656671524048\n",
      "epoch: 1 step: 452, loss is 2.2995057106018066\n",
      "epoch: 1 step: 453, loss is 2.2907657623291016\n",
      "epoch: 1 step: 454, loss is 2.314091682434082\n",
      "epoch: 1 step: 455, loss is 2.3133177757263184\n",
      "epoch: 1 step: 456, loss is 2.27024507522583\n",
      "epoch: 1 step: 457, loss is 2.2914159297943115\n",
      "epoch: 1 step: 458, loss is 2.2841126918792725\n",
      "epoch: 1 step: 459, loss is 2.3100404739379883\n",
      "epoch: 1 step: 460, loss is 2.2849762439727783\n",
      "epoch: 1 step: 461, loss is 2.3249783515930176\n",
      "epoch: 1 step: 462, loss is 2.317070245742798\n",
      "epoch: 1 step: 463, loss is 2.3002607822418213\n",
      "epoch: 1 step: 464, loss is 2.3018853664398193\n",
      "epoch: 1 step: 465, loss is 2.2974977493286133\n",
      "epoch: 1 step: 466, loss is 2.3046956062316895\n",
      "epoch: 1 step: 467, loss is 2.3132526874542236\n",
      "epoch: 1 step: 468, loss is 2.3083393573760986\n",
      "epoch: 1 step: 469, loss is 2.2950358390808105\n",
      "epoch: 1 step: 470, loss is 2.2698605060577393\n",
      "epoch: 1 step: 471, loss is 2.311005115509033\n",
      "epoch: 1 step: 472, loss is 2.285912275314331\n",
      "epoch: 1 step: 473, loss is 2.321211576461792\n",
      "epoch: 1 step: 474, loss is 2.298041582107544\n",
      "epoch: 1 step: 475, loss is 2.3080525398254395\n",
      "epoch: 1 step: 476, loss is 2.3175766468048096\n",
      "epoch: 1 step: 477, loss is 2.2868080139160156\n",
      "epoch: 1 step: 478, loss is 2.290865421295166\n",
      "epoch: 1 step: 479, loss is 2.295933485031128\n",
      "epoch: 1 step: 480, loss is 2.306082010269165\n",
      "epoch: 1 step: 481, loss is 2.2906765937805176\n",
      "epoch: 1 step: 482, loss is 2.3172667026519775\n",
      "epoch: 1 step: 483, loss is 2.283118486404419\n",
      "epoch: 1 step: 484, loss is 2.3011138439178467\n",
      "epoch: 1 step: 485, loss is 2.315584897994995\n",
      "epoch: 1 step: 486, loss is 2.308018922805786\n",
      "epoch: 1 step: 487, loss is 2.2832210063934326\n",
      "epoch: 1 step: 488, loss is 2.272991418838501\n",
      "epoch: 1 step: 489, loss is 2.2971112728118896\n",
      "epoch: 1 step: 490, loss is 2.3011441230773926\n",
      "epoch: 1 step: 491, loss is 2.3017797470092773\n",
      "epoch: 1 step: 492, loss is 2.289738893508911\n",
      "epoch: 1 step: 493, loss is 2.2949600219726562\n",
      "epoch: 1 step: 494, loss is 2.2920196056365967\n",
      "epoch: 1 step: 495, loss is 2.2965221405029297\n",
      "epoch: 1 step: 496, loss is 2.3216071128845215\n",
      "epoch: 1 step: 497, loss is 2.3398945331573486\n",
      "epoch: 1 step: 498, loss is 2.2967689037323\n",
      "epoch: 1 step: 499, loss is 2.3081862926483154\n",
      "epoch: 1 step: 500, loss is 2.3118698596954346\n",
      "epoch: 1 step: 501, loss is 2.3188438415527344\n",
      "epoch: 1 step: 502, loss is 2.2887089252471924\n",
      "epoch: 1 step: 503, loss is 2.3055169582366943\n",
      "epoch: 1 step: 504, loss is 2.2916100025177\n",
      "epoch: 1 step: 505, loss is 2.282005786895752\n",
      "epoch: 1 step: 506, loss is 2.286176919937134\n",
      "epoch: 1 step: 507, loss is 2.308830738067627\n",
      "epoch: 1 step: 508, loss is 2.325326442718506\n",
      "epoch: 1 step: 509, loss is 2.3150932788848877\n",
      "epoch: 1 step: 510, loss is 2.316174268722534\n",
      "epoch: 1 step: 511, loss is 2.283229351043701\n",
      "epoch: 1 step: 512, loss is 2.31756591796875\n",
      "epoch: 1 step: 513, loss is 2.2964444160461426\n",
      "epoch: 1 step: 514, loss is 2.2986700534820557\n",
      "epoch: 1 step: 515, loss is 2.318251609802246\n",
      "epoch: 1 step: 516, loss is 2.3076577186584473\n",
      "epoch: 1 step: 517, loss is 2.299901008605957\n",
      "epoch: 1 step: 518, loss is 2.2908849716186523\n",
      "epoch: 1 step: 519, loss is 2.2867648601531982\n",
      "epoch: 1 step: 520, loss is 2.282583475112915\n",
      "epoch: 1 step: 521, loss is 2.319488048553467\n",
      "epoch: 1 step: 522, loss is 2.2831482887268066\n",
      "epoch: 1 step: 523, loss is 2.3043324947357178\n",
      "epoch: 1 step: 524, loss is 2.2789642810821533\n",
      "epoch: 1 step: 525, loss is 2.295283555984497\n",
      "epoch: 1 step: 526, loss is 2.3070948123931885\n",
      "epoch: 1 step: 527, loss is 2.311249256134033\n",
      "epoch: 1 step: 528, loss is 2.2758209705352783\n",
      "epoch: 1 step: 529, loss is 2.268630266189575\n",
      "epoch: 1 step: 530, loss is 2.2959189414978027\n",
      "epoch: 1 step: 531, loss is 2.312771797180176\n",
      "epoch: 1 step: 532, loss is 2.296126365661621\n",
      "epoch: 1 step: 533, loss is 2.2977395057678223\n",
      "epoch: 1 step: 534, loss is 2.2841179370880127\n",
      "epoch: 1 step: 535, loss is 2.28739595413208\n",
      "epoch: 1 step: 536, loss is 2.2895050048828125\n",
      "epoch: 1 step: 537, loss is 2.317521095275879\n",
      "epoch: 1 step: 538, loss is 2.2886223793029785\n",
      "epoch: 1 step: 539, loss is 2.2843973636627197\n",
      "epoch: 1 step: 540, loss is 2.314467668533325\n",
      "epoch: 1 step: 541, loss is 2.2897160053253174\n",
      "epoch: 1 step: 542, loss is 2.290881633758545\n",
      "epoch: 1 step: 543, loss is 2.2765214443206787\n",
      "epoch: 1 step: 544, loss is 2.279888153076172\n",
      "epoch: 1 step: 545, loss is 2.296036958694458\n",
      "epoch: 1 step: 546, loss is 2.2742698192596436\n",
      "epoch: 1 step: 547, loss is 2.3028149604797363\n",
      "epoch: 1 step: 548, loss is 2.302004814147949\n",
      "epoch: 1 step: 549, loss is 2.3081681728363037\n",
      "epoch: 1 step: 550, loss is 2.3021695613861084\n",
      "epoch: 1 step: 551, loss is 2.2955527305603027\n",
      "epoch: 1 step: 552, loss is 2.2911322116851807\n",
      "epoch: 1 step: 553, loss is 2.2827937602996826\n",
      "epoch: 1 step: 554, loss is 2.2977633476257324\n",
      "epoch: 1 step: 555, loss is 2.298896074295044\n",
      "epoch: 1 step: 556, loss is 2.297302484512329\n",
      "epoch: 1 step: 557, loss is 2.2965831756591797\n",
      "epoch: 1 step: 558, loss is 2.2872371673583984\n",
      "epoch: 1 step: 559, loss is 2.292773962020874\n",
      "epoch: 1 step: 560, loss is 2.290323495864868\n",
      "epoch: 1 step: 561, loss is 2.2946555614471436\n",
      "epoch: 1 step: 562, loss is 2.294982433319092\n",
      "epoch: 1 step: 563, loss is 2.259622097015381\n",
      "epoch: 1 step: 564, loss is 2.282683849334717\n",
      "epoch: 1 step: 565, loss is 2.2835123538970947\n",
      "epoch: 1 step: 566, loss is 2.241192102432251\n",
      "epoch: 1 step: 567, loss is 2.2883687019348145\n",
      "epoch: 1 step: 568, loss is 2.253453016281128\n",
      "epoch: 1 step: 569, loss is 2.2697527408599854\n",
      "epoch: 1 step: 570, loss is 2.295631170272827\n",
      "epoch: 1 step: 571, loss is 2.286302328109741\n",
      "epoch: 1 step: 572, loss is 2.3005616664886475\n",
      "epoch: 1 step: 573, loss is 2.3086066246032715\n",
      "epoch: 1 step: 574, loss is 2.306504011154175\n",
      "epoch: 1 step: 575, loss is 2.282876491546631\n",
      "epoch: 1 step: 576, loss is 2.2809557914733887\n",
      "epoch: 1 step: 577, loss is 2.2610607147216797\n",
      "epoch: 1 step: 578, loss is 2.265270709991455\n",
      "epoch: 1 step: 579, loss is 2.2793848514556885\n",
      "epoch: 1 step: 580, loss is 2.282282829284668\n",
      "epoch: 1 step: 581, loss is 2.2679007053375244\n",
      "epoch: 1 step: 582, loss is 2.3069803714752197\n",
      "epoch: 1 step: 583, loss is 2.2700142860412598\n",
      "epoch: 1 step: 584, loss is 2.231703281402588\n",
      "epoch: 1 step: 585, loss is 2.2271523475646973\n",
      "epoch: 1 step: 586, loss is 2.2922074794769287\n",
      "epoch: 1 step: 587, loss is 2.2527289390563965\n",
      "epoch: 1 step: 588, loss is 2.2820417881011963\n",
      "epoch: 1 step: 589, loss is 2.251594305038452\n",
      "epoch: 1 step: 590, loss is 2.2238388061523438\n",
      "epoch: 1 step: 591, loss is 2.261446952819824\n",
      "epoch: 1 step: 592, loss is 2.2117605209350586\n",
      "epoch: 1 step: 593, loss is 2.233231782913208\n",
      "epoch: 1 step: 594, loss is 2.2444632053375244\n",
      "epoch: 1 step: 595, loss is 2.2406527996063232\n",
      "epoch: 1 step: 596, loss is 2.200502872467041\n",
      "epoch: 1 step: 597, loss is 2.266439914703369\n",
      "epoch: 1 step: 598, loss is 2.1791110038757324\n",
      "epoch: 1 step: 599, loss is 2.2044622898101807\n",
      "epoch: 1 step: 600, loss is 2.2301554679870605\n",
      "epoch: 1 step: 601, loss is 2.1550941467285156\n",
      "epoch: 1 step: 602, loss is 2.172914981842041\n",
      "epoch: 1 step: 603, loss is 2.2153713703155518\n",
      "epoch: 1 step: 604, loss is 2.1456000804901123\n",
      "epoch: 1 step: 605, loss is 2.102569580078125\n",
      "epoch: 1 step: 606, loss is 2.1249356269836426\n",
      "epoch: 1 step: 607, loss is 2.033571481704712\n",
      "epoch: 1 step: 608, loss is 2.151388168334961\n",
      "epoch: 1 step: 609, loss is 2.2505648136138916\n",
      "epoch: 1 step: 610, loss is 2.0780553817749023\n",
      "epoch: 1 step: 611, loss is 2.0816361904144287\n",
      "epoch: 1 step: 612, loss is 2.2003562450408936\n",
      "epoch: 1 step: 613, loss is 2.0334701538085938\n",
      "epoch: 1 step: 614, loss is 1.7699915170669556\n",
      "epoch: 1 step: 615, loss is 1.818961501121521\n",
      "epoch: 1 step: 616, loss is 1.8803967237472534\n",
      "epoch: 1 step: 617, loss is 2.01377534866333\n",
      "epoch: 1 step: 618, loss is 1.8101862668991089\n",
      "epoch: 1 step: 619, loss is 1.8828388452529907\n",
      "epoch: 1 step: 620, loss is 1.7813341617584229\n",
      "epoch: 1 step: 621, loss is 1.6790647506713867\n",
      "epoch: 1 step: 622, loss is 1.7631276845932007\n",
      "epoch: 1 step: 623, loss is 1.741011619567871\n",
      "epoch: 1 step: 624, loss is 1.5515010356903076\n",
      "epoch: 1 step: 625, loss is 1.450473427772522\n",
      "epoch: 1 step: 626, loss is 1.498253345489502\n",
      "epoch: 1 step: 627, loss is 1.6249183416366577\n",
      "epoch: 1 step: 628, loss is 1.4813777208328247\n",
      "epoch: 1 step: 629, loss is 1.8271478414535522\n",
      "epoch: 1 step: 630, loss is 1.5478140115737915\n",
      "epoch: 1 step: 631, loss is 1.5879157781600952\n",
      "epoch: 1 step: 632, loss is 1.4737330675125122\n",
      "epoch: 1 step: 633, loss is 1.4688925743103027\n",
      "epoch: 1 step: 634, loss is 1.5534881353378296\n",
      "epoch: 1 step: 635, loss is 1.2774206399917603\n",
      "epoch: 1 step: 636, loss is 1.2032434940338135\n",
      "epoch: 1 step: 637, loss is 1.1290932893753052\n",
      "epoch: 1 step: 638, loss is 1.5200005769729614\n",
      "epoch: 1 step: 639, loss is 1.0082743167877197\n",
      "epoch: 1 step: 640, loss is 1.3895505666732788\n",
      "epoch: 1 step: 641, loss is 1.3010458946228027\n",
      "epoch: 1 step: 642, loss is 1.2642148733139038\n",
      "epoch: 1 step: 643, loss is 1.108829140663147\n",
      "epoch: 1 step: 644, loss is 1.2455854415893555\n",
      "epoch: 1 step: 645, loss is 1.006038784980774\n",
      "epoch: 1 step: 646, loss is 1.0940830707550049\n",
      "epoch: 1 step: 647, loss is 1.0108120441436768\n",
      "epoch: 1 step: 648, loss is 1.0822608470916748\n",
      "epoch: 1 step: 649, loss is 0.9213991761207581\n",
      "epoch: 1 step: 650, loss is 0.8603700995445251\n",
      "epoch: 1 step: 651, loss is 1.0314316749572754\n",
      "epoch: 1 step: 652, loss is 1.240782618522644\n",
      "epoch: 1 step: 653, loss is 0.8659162521362305\n",
      "epoch: 1 step: 654, loss is 1.1536715030670166\n",
      "epoch: 1 step: 655, loss is 0.4688180088996887\n",
      "epoch: 1 step: 656, loss is 1.1884461641311646\n",
      "epoch: 1 step: 657, loss is 1.3357434272766113\n",
      "epoch: 1 step: 658, loss is 1.7515419721603394\n",
      "epoch: 1 step: 659, loss is 1.080020546913147\n",
      "epoch: 1 step: 660, loss is 1.178688883781433\n",
      "epoch: 1 step: 661, loss is 1.7437318563461304\n",
      "epoch: 1 step: 662, loss is 1.1065895557403564\n",
      "epoch: 1 step: 663, loss is 1.2072980403900146\n",
      "epoch: 1 step: 664, loss is 1.4757484197616577\n",
      "epoch: 1 step: 665, loss is 1.162658452987671\n",
      "epoch: 1 step: 666, loss is 1.1822924613952637\n",
      "epoch: 1 step: 667, loss is 1.230612874031067\n",
      "epoch: 1 step: 668, loss is 1.1940641403198242\n",
      "epoch: 1 step: 669, loss is 1.110398769378662\n",
      "epoch: 1 step: 670, loss is 0.9467707872390747\n",
      "epoch: 1 step: 671, loss is 1.0565577745437622\n",
      "epoch: 1 step: 672, loss is 0.9501892924308777\n",
      "epoch: 1 step: 673, loss is 1.3469338417053223\n",
      "epoch: 1 step: 674, loss is 1.1321184635162354\n",
      "epoch: 1 step: 675, loss is 1.0233653783798218\n",
      "epoch: 1 step: 676, loss is 0.8628154397010803\n",
      "epoch: 1 step: 677, loss is 0.8716903924942017\n",
      "epoch: 1 step: 678, loss is 1.0026623010635376\n",
      "epoch: 1 step: 679, loss is 0.6488825082778931\n",
      "epoch: 1 step: 680, loss is 0.756179928779602\n",
      "epoch: 1 step: 681, loss is 0.6300804018974304\n",
      "epoch: 1 step: 682, loss is 1.0270156860351562\n",
      "epoch: 1 step: 683, loss is 0.5826196074485779\n",
      "epoch: 1 step: 684, loss is 0.549408495426178\n",
      "epoch: 1 step: 685, loss is 0.5611830949783325\n",
      "epoch: 1 step: 686, loss is 0.5388298630714417\n",
      "epoch: 1 step: 687, loss is 0.8469432592391968\n",
      "epoch: 1 step: 688, loss is 0.5655174255371094\n",
      "epoch: 1 step: 689, loss is 0.9691737294197083\n",
      "epoch: 1 step: 690, loss is 0.7327335476875305\n",
      "epoch: 1 step: 691, loss is 0.6711397171020508\n",
      "epoch: 1 step: 692, loss is 0.5441294312477112\n",
      "epoch: 1 step: 693, loss is 0.6020243763923645\n",
      "epoch: 1 step: 694, loss is 0.6474671959877014\n",
      "epoch: 1 step: 695, loss is 0.3954165279865265\n",
      "epoch: 1 step: 696, loss is 0.614982008934021\n",
      "epoch: 1 step: 697, loss is 0.6525713205337524\n",
      "epoch: 1 step: 698, loss is 0.5514840483665466\n",
      "epoch: 1 step: 699, loss is 0.8102031946182251\n",
      "epoch: 1 step: 700, loss is 0.606645941734314\n",
      "epoch: 1 step: 701, loss is 1.1842964887619019\n",
      "epoch: 1 step: 702, loss is 0.6841486692428589\n",
      "epoch: 1 step: 703, loss is 0.5376964807510376\n",
      "epoch: 1 step: 704, loss is 0.6330865025520325\n",
      "epoch: 1 step: 705, loss is 0.35178619623184204\n",
      "epoch: 1 step: 706, loss is 0.4483860433101654\n",
      "epoch: 1 step: 707, loss is 0.786915123462677\n",
      "epoch: 1 step: 708, loss is 0.8225177526473999\n",
      "epoch: 1 step: 709, loss is 0.812474250793457\n",
      "epoch: 1 step: 710, loss is 0.4124990701675415\n",
      "epoch: 1 step: 711, loss is 0.913321316242218\n",
      "epoch: 1 step: 712, loss is 0.692183792591095\n",
      "epoch: 1 step: 713, loss is 0.5200364589691162\n",
      "epoch: 1 step: 714, loss is 0.4083797335624695\n",
      "epoch: 1 step: 715, loss is 0.5672475695610046\n",
      "epoch: 1 step: 716, loss is 0.9539140462875366\n",
      "epoch: 1 step: 717, loss is 0.8305827379226685\n",
      "epoch: 1 step: 718, loss is 0.6007468104362488\n",
      "epoch: 1 step: 719, loss is 1.0331315994262695\n",
      "epoch: 1 step: 720, loss is 0.9402445554733276\n",
      "epoch: 1 step: 721, loss is 0.4496658146381378\n",
      "epoch: 1 step: 722, loss is 0.6058269739151001\n",
      "epoch: 1 step: 723, loss is 0.45550018548965454\n",
      "epoch: 1 step: 724, loss is 0.6230108141899109\n",
      "epoch: 1 step: 725, loss is 0.4658665955066681\n",
      "epoch: 1 step: 726, loss is 0.9397246241569519\n",
      "epoch: 1 step: 727, loss is 0.37507298588752747\n",
      "epoch: 1 step: 728, loss is 0.40654414892196655\n",
      "epoch: 1 step: 729, loss is 0.3808497190475464\n",
      "epoch: 1 step: 730, loss is 0.46633732318878174\n",
      "epoch: 1 step: 731, loss is 0.43724149465560913\n",
      "epoch: 1 step: 732, loss is 0.5131831765174866\n",
      "epoch: 1 step: 733, loss is 0.7899133563041687\n",
      "epoch: 1 step: 734, loss is 0.41603976488113403\n",
      "epoch: 1 step: 735, loss is 0.701528787612915\n",
      "epoch: 1 step: 736, loss is 0.4966064691543579\n",
      "epoch: 1 step: 737, loss is 0.46537432074546814\n",
      "epoch: 1 step: 738, loss is 0.6561705470085144\n",
      "epoch: 1 step: 739, loss is 0.5866201519966125\n",
      "epoch: 1 step: 740, loss is 0.3329106867313385\n",
      "epoch: 1 step: 741, loss is 0.30057114362716675\n",
      "epoch: 1 step: 742, loss is 0.919608473777771\n",
      "epoch: 1 step: 743, loss is 0.2963135838508606\n",
      "epoch: 1 step: 744, loss is 0.44230905175209045\n",
      "epoch: 1 step: 745, loss is 0.4963054656982422\n",
      "epoch: 1 step: 746, loss is 0.4280471205711365\n",
      "epoch: 1 step: 747, loss is 0.6125416159629822\n",
      "epoch: 1 step: 748, loss is 0.44478633999824524\n",
      "epoch: 1 step: 749, loss is 0.49006345868110657\n",
      "epoch: 1 step: 750, loss is 0.9224686026573181\n",
      "epoch: 1 step: 751, loss is 0.635809063911438\n",
      "epoch: 1 step: 752, loss is 0.5483908653259277\n",
      "epoch: 1 step: 753, loss is 0.4594230651855469\n",
      "epoch: 1 step: 754, loss is 0.4152801036834717\n",
      "epoch: 1 step: 755, loss is 0.5709784030914307\n",
      "epoch: 1 step: 756, loss is 0.35712823271751404\n",
      "epoch: 1 step: 757, loss is 0.30864378809928894\n",
      "epoch: 1 step: 758, loss is 0.41118964552879333\n",
      "epoch: 1 step: 759, loss is 0.5629099607467651\n",
      "epoch: 1 step: 760, loss is 0.7506973743438721\n",
      "epoch: 1 step: 761, loss is 0.3626570701599121\n",
      "epoch: 1 step: 762, loss is 0.33972805738449097\n",
      "epoch: 1 step: 763, loss is 0.7028650045394897\n",
      "epoch: 1 step: 764, loss is 0.3399534523487091\n",
      "epoch: 1 step: 765, loss is 0.2644230127334595\n",
      "epoch: 1 step: 766, loss is 0.5745494365692139\n",
      "epoch: 1 step: 767, loss is 0.4624275863170624\n",
      "epoch: 1 step: 768, loss is 0.21732932329177856\n",
      "epoch: 1 step: 769, loss is 0.6574814915657043\n",
      "epoch: 1 step: 770, loss is 0.649267852306366\n",
      "epoch: 1 step: 771, loss is 0.5119165778160095\n",
      "epoch: 1 step: 772, loss is 0.4169551134109497\n",
      "epoch: 1 step: 773, loss is 0.66169273853302\n",
      "epoch: 1 step: 774, loss is 0.4559577703475952\n",
      "epoch: 1 step: 775, loss is 0.17245648801326752\n",
      "epoch: 1 step: 776, loss is 0.5259866714477539\n",
      "epoch: 1 step: 777, loss is 0.3287905156612396\n",
      "epoch: 1 step: 778, loss is 0.3553514778614044\n",
      "epoch: 1 step: 779, loss is 0.28234073519706726\n",
      "epoch: 1 step: 780, loss is 0.3477210998535156\n",
      "epoch: 1 step: 781, loss is 0.37387675046920776\n",
      "epoch: 1 step: 782, loss is 0.1846466064453125\n",
      "epoch: 1 step: 783, loss is 0.4922940135002136\n",
      "epoch: 1 step: 784, loss is 0.42920929193496704\n",
      "epoch: 1 step: 785, loss is 0.32634273171424866\n",
      "epoch: 1 step: 786, loss is 0.37324732542037964\n",
      "epoch: 1 step: 787, loss is 0.19991356134414673\n",
      "epoch: 1 step: 788, loss is 0.3680541515350342\n",
      "epoch: 1 step: 789, loss is 0.33063411712646484\n",
      "epoch: 1 step: 790, loss is 0.16018670797348022\n",
      "epoch: 1 step: 791, loss is 0.14068348705768585\n",
      "epoch: 1 step: 792, loss is 0.6436397433280945\n",
      "epoch: 1 step: 793, loss is 0.16392119228839874\n",
      "epoch: 1 step: 794, loss is 0.6813666820526123\n",
      "epoch: 1 step: 795, loss is 0.29748082160949707\n",
      "epoch: 1 step: 796, loss is 0.44026583433151245\n",
      "epoch: 1 step: 797, loss is 0.3831999897956848\n",
      "epoch: 1 step: 798, loss is 0.56611168384552\n",
      "epoch: 1 step: 799, loss is 0.49203571677207947\n",
      "epoch: 1 step: 800, loss is 0.5642403960227966\n",
      "epoch: 1 step: 801, loss is 0.126304030418396\n",
      "epoch: 1 step: 802, loss is 0.605417013168335\n",
      "epoch: 1 step: 803, loss is 0.18250960111618042\n",
      "epoch: 1 step: 804, loss is 0.30193769931793213\n",
      "epoch: 1 step: 805, loss is 0.8032362461090088\n",
      "epoch: 1 step: 806, loss is 0.2741311490535736\n",
      "epoch: 1 step: 807, loss is 0.5991671085357666\n",
      "epoch: 1 step: 808, loss is 0.16347913444042206\n",
      "epoch: 1 step: 809, loss is 0.3252701759338379\n",
      "epoch: 1 step: 810, loss is 0.21823982894420624\n",
      "epoch: 1 step: 811, loss is 0.3003567159175873\n",
      "epoch: 1 step: 812, loss is 0.3243654668331146\n",
      "epoch: 1 step: 813, loss is 0.3904152512550354\n",
      "epoch: 1 step: 814, loss is 0.40118855237960815\n",
      "epoch: 1 step: 815, loss is 0.36511051654815674\n",
      "epoch: 1 step: 816, loss is 0.14013679325580597\n",
      "epoch: 1 step: 817, loss is 0.3085671365261078\n",
      "epoch: 1 step: 818, loss is 0.0680442601442337\n",
      "epoch: 1 step: 819, loss is 0.44062694907188416\n",
      "epoch: 1 step: 820, loss is 0.4868994951248169\n",
      "epoch: 1 step: 821, loss is 0.3634240925312042\n",
      "epoch: 1 step: 822, loss is 0.31473106145858765\n",
      "epoch: 1 step: 823, loss is 0.15008383989334106\n",
      "epoch: 1 step: 824, loss is 0.19262468814849854\n",
      "epoch: 1 step: 825, loss is 0.6180490851402283\n",
      "epoch: 1 step: 826, loss is 0.354167640209198\n",
      "epoch: 1 step: 827, loss is 0.45664894580841064\n",
      "epoch: 1 step: 828, loss is 0.2037208080291748\n",
      "epoch: 1 step: 829, loss is 0.12809967994689941\n",
      "epoch: 1 step: 830, loss is 0.12906506657600403\n",
      "epoch: 1 step: 831, loss is 0.3710101246833801\n",
      "epoch: 1 step: 832, loss is 0.2848473787307739\n",
      "epoch: 1 step: 833, loss is 0.373496413230896\n",
      "epoch: 1 step: 834, loss is 0.42672228813171387\n",
      "epoch: 1 step: 835, loss is 0.3344399929046631\n",
      "epoch: 1 step: 836, loss is 0.21229395270347595\n",
      "epoch: 1 step: 837, loss is 0.12233825773000717\n",
      "epoch: 1 step: 838, loss is 0.16786301136016846\n",
      "epoch: 1 step: 839, loss is 0.4287775754928589\n",
      "epoch: 1 step: 840, loss is 0.039923883974552155\n",
      "epoch: 1 step: 841, loss is 0.12325052917003632\n",
      "epoch: 1 step: 842, loss is 0.32256606221199036\n",
      "epoch: 1 step: 843, loss is 0.5236263275146484\n",
      "epoch: 1 step: 844, loss is 0.07625686377286911\n",
      "epoch: 1 step: 845, loss is 0.18573129177093506\n",
      "epoch: 1 step: 846, loss is 0.1805957704782486\n",
      "epoch: 1 step: 847, loss is 0.2214823067188263\n",
      "epoch: 1 step: 848, loss is 0.09525252878665924\n",
      "epoch: 1 step: 849, loss is 0.7248433232307434\n",
      "epoch: 1 step: 850, loss is 0.09755927324295044\n",
      "epoch: 1 step: 851, loss is 0.3668339252471924\n",
      "epoch: 1 step: 852, loss is 0.17023366689682007\n",
      "epoch: 1 step: 853, loss is 0.23815174400806427\n",
      "epoch: 1 step: 854, loss is 0.1350586861371994\n",
      "epoch: 1 step: 855, loss is 0.6021308302879333\n",
      "epoch: 1 step: 856, loss is 0.23181450366973877\n",
      "epoch: 1 step: 857, loss is 0.11834432184696198\n",
      "epoch: 1 step: 858, loss is 0.06506981700658798\n",
      "epoch: 1 step: 859, loss is 0.4361339211463928\n",
      "epoch: 1 step: 860, loss is 0.1467226892709732\n",
      "epoch: 1 step: 861, loss is 0.4533841013908386\n",
      "epoch: 1 step: 862, loss is 0.14409352838993073\n",
      "epoch: 1 step: 863, loss is 0.5367118120193481\n",
      "epoch: 1 step: 864, loss is 0.300815224647522\n",
      "epoch: 1 step: 865, loss is 0.2453031986951828\n",
      "epoch: 1 step: 866, loss is 0.4039982557296753\n",
      "epoch: 1 step: 867, loss is 0.36246800422668457\n",
      "epoch: 1 step: 868, loss is 0.38931959867477417\n",
      "epoch: 1 step: 869, loss is 0.22420647740364075\n",
      "epoch: 1 step: 870, loss is 0.24674548208713531\n",
      "epoch: 1 step: 871, loss is 0.12879841029644012\n",
      "epoch: 1 step: 872, loss is 0.04989870637655258\n",
      "epoch: 1 step: 873, loss is 0.07743986696004868\n",
      "epoch: 1 step: 874, loss is 0.12933461368083954\n",
      "epoch: 1 step: 875, loss is 0.2905251085758209\n",
      "epoch: 1 step: 876, loss is 0.2038198858499527\n",
      "epoch: 1 step: 877, loss is 0.25981321930885315\n",
      "epoch: 1 step: 878, loss is 0.11425895243883133\n",
      "epoch: 1 step: 879, loss is 0.26718053221702576\n",
      "epoch: 1 step: 880, loss is 0.519728422164917\n",
      "epoch: 1 step: 881, loss is 0.4253571033477783\n",
      "epoch: 1 step: 882, loss is 0.2276127189397812\n",
      "epoch: 1 step: 883, loss is 0.3430960476398468\n",
      "epoch: 1 step: 884, loss is 0.28037863969802856\n",
      "epoch: 1 step: 885, loss is 0.3374083340167999\n",
      "epoch: 1 step: 886, loss is 0.20131313800811768\n",
      "epoch: 1 step: 887, loss is 0.21579422056674957\n",
      "epoch: 1 step: 888, loss is 0.1719907969236374\n",
      "epoch: 1 step: 889, loss is 0.5554032921791077\n",
      "epoch: 1 step: 890, loss is 0.21400916576385498\n",
      "epoch: 1 step: 891, loss is 0.25537559390068054\n",
      "epoch: 1 step: 892, loss is 0.2936704456806183\n",
      "epoch: 1 step: 893, loss is 0.40958502888679504\n",
      "epoch: 1 step: 894, loss is 0.14027944207191467\n",
      "epoch: 1 step: 895, loss is 0.2959842383861542\n",
      "epoch: 1 step: 896, loss is 0.12940755486488342\n",
      "epoch: 1 step: 897, loss is 0.3834775686264038\n",
      "epoch: 1 step: 898, loss is 0.24279069900512695\n",
      "epoch: 1 step: 899, loss is 0.2043304443359375\n",
      "epoch: 1 step: 900, loss is 0.2945188283920288\n",
      "epoch: 1 step: 901, loss is 0.1896771788597107\n",
      "epoch: 1 step: 902, loss is 0.15552771091461182\n",
      "epoch: 1 step: 903, loss is 0.4111233055591583\n",
      "epoch: 1 step: 904, loss is 0.1689833402633667\n",
      "epoch: 1 step: 905, loss is 0.26770710945129395\n",
      "epoch: 1 step: 906, loss is 0.18503105640411377\n",
      "epoch: 1 step: 907, loss is 0.17134642601013184\n",
      "epoch: 1 step: 908, loss is 0.23182176053524017\n",
      "epoch: 1 step: 909, loss is 0.10583500564098358\n",
      "epoch: 1 step: 910, loss is 0.04224320873618126\n",
      "epoch: 1 step: 911, loss is 0.23212048411369324\n",
      "epoch: 1 step: 912, loss is 0.3003753423690796\n",
      "epoch: 1 step: 913, loss is 0.2998136878013611\n",
      "epoch: 1 step: 914, loss is 0.21796347200870514\n",
      "epoch: 1 step: 915, loss is 0.31478792428970337\n",
      "epoch: 1 step: 916, loss is 0.1892566829919815\n",
      "epoch: 1 step: 917, loss is 0.2825324535369873\n",
      "epoch: 1 step: 918, loss is 0.3073255717754364\n",
      "epoch: 1 step: 919, loss is 0.047054681926965714\n",
      "epoch: 1 step: 920, loss is 0.19151072204113007\n",
      "epoch: 1 step: 921, loss is 0.287055104970932\n",
      "epoch: 1 step: 922, loss is 0.2635413408279419\n",
      "epoch: 1 step: 923, loss is 0.0953621044754982\n",
      "epoch: 1 step: 924, loss is 0.2276819497346878\n",
      "epoch: 1 step: 925, loss is 0.3719094395637512\n",
      "epoch: 1 step: 926, loss is 0.11763029545545578\n",
      "epoch: 1 step: 927, loss is 0.327789843082428\n",
      "epoch: 1 step: 928, loss is 0.5151757001876831\n",
      "epoch: 1 step: 929, loss is 0.3353217542171478\n",
      "epoch: 1 step: 930, loss is 0.5230239033699036\n",
      "epoch: 1 step: 931, loss is 0.07940690964460373\n",
      "epoch: 1 step: 932, loss is 0.17473435401916504\n",
      "epoch: 1 step: 933, loss is 0.06373360753059387\n",
      "epoch: 1 step: 934, loss is 0.19991867244243622\n",
      "epoch: 1 step: 935, loss is 0.04018143564462662\n",
      "epoch: 1 step: 936, loss is 0.6581705212593079\n",
      "epoch: 1 step: 937, loss is 0.18314555287361145\n",
      "epoch: 1 step: 938, loss is 0.5267578959465027\n",
      "epoch: 1 step: 939, loss is 0.34771594405174255\n",
      "epoch: 1 step: 940, loss is 0.19611690938472748\n",
      "epoch: 1 step: 941, loss is 0.5345682501792908\n",
      "epoch: 1 step: 942, loss is 0.18814601004123688\n",
      "epoch: 1 step: 943, loss is 0.15496163070201874\n",
      "epoch: 1 step: 944, loss is 0.4020428955554962\n",
      "epoch: 1 step: 945, loss is 0.497213214635849\n",
      "epoch: 1 step: 946, loss is 0.15881721675395966\n",
      "epoch: 1 step: 947, loss is 0.07443972676992416\n",
      "epoch: 1 step: 948, loss is 0.267078697681427\n",
      "epoch: 1 step: 949, loss is 0.12077657878398895\n",
      "epoch: 1 step: 950, loss is 0.1588285267353058\n",
      "epoch: 1 step: 951, loss is 0.44771087169647217\n",
      "epoch: 1 step: 952, loss is 0.2525653839111328\n",
      "epoch: 1 step: 953, loss is 0.1263447254896164\n",
      "epoch: 1 step: 954, loss is 0.13110168278217316\n",
      "epoch: 1 step: 955, loss is 0.2149685174226761\n",
      "epoch: 1 step: 956, loss is 0.20397260785102844\n",
      "epoch: 1 step: 957, loss is 0.07123598456382751\n",
      "epoch: 1 step: 958, loss is 0.6937008500099182\n",
      "epoch: 1 step: 959, loss is 0.3006173074245453\n",
      "epoch: 1 step: 960, loss is 0.1740638017654419\n",
      "epoch: 1 step: 961, loss is 0.055630192160606384\n",
      "epoch: 1 step: 962, loss is 0.07748015224933624\n",
      "epoch: 1 step: 963, loss is 0.04835048317909241\n",
      "epoch: 1 step: 964, loss is 0.20124933123588562\n",
      "epoch: 1 step: 965, loss is 0.09549111872911453\n",
      "epoch: 1 step: 966, loss is 0.16844946146011353\n",
      "epoch: 1 step: 967, loss is 0.15863578021526337\n",
      "epoch: 1 step: 968, loss is 0.37722694873809814\n",
      "epoch: 1 step: 969, loss is 0.06025697663426399\n",
      "epoch: 1 step: 970, loss is 0.16564419865608215\n",
      "epoch: 1 step: 971, loss is 0.03951549530029297\n",
      "epoch: 1 step: 972, loss is 0.3269058167934418\n",
      "epoch: 1 step: 973, loss is 0.37559592723846436\n",
      "epoch: 1 step: 974, loss is 0.5577993392944336\n",
      "epoch: 1 step: 975, loss is 0.17029860615730286\n",
      "epoch: 1 step: 976, loss is 0.29869145154953003\n",
      "epoch: 1 step: 977, loss is 0.08853798359632492\n",
      "epoch: 1 step: 978, loss is 0.0674751028418541\n",
      "epoch: 1 step: 979, loss is 0.3613888621330261\n",
      "epoch: 1 step: 980, loss is 0.2394455373287201\n",
      "epoch: 1 step: 981, loss is 0.21975593268871307\n",
      "epoch: 1 step: 982, loss is 0.2724948823451996\n",
      "epoch: 1 step: 983, loss is 0.3815111517906189\n",
      "epoch: 1 step: 984, loss is 0.27211788296699524\n",
      "epoch: 1 step: 985, loss is 0.12457714974880219\n",
      "epoch: 1 step: 986, loss is 0.09083029627799988\n",
      "epoch: 1 step: 987, loss is 0.15787872672080994\n",
      "epoch: 1 step: 988, loss is 0.21408706903457642\n",
      "epoch: 1 step: 989, loss is 0.16258056461811066\n",
      "epoch: 1 step: 990, loss is 0.32553815841674805\n",
      "epoch: 1 step: 991, loss is 0.07252706587314606\n",
      "epoch: 1 step: 992, loss is 0.07626274228096008\n",
      "epoch: 1 step: 993, loss is 0.6202600598335266\n",
      "epoch: 1 step: 994, loss is 0.48381394147872925\n",
      "epoch: 1 step: 995, loss is 0.10874371975660324\n",
      "epoch: 1 step: 996, loss is 0.06700818985700607\n",
      "epoch: 1 step: 997, loss is 0.3308832347393036\n",
      "epoch: 1 step: 998, loss is 0.22651658952236176\n",
      "epoch: 1 step: 999, loss is 0.1359226554632187\n",
      "epoch: 1 step: 1000, loss is 0.029352590441703796\n",
      "epoch: 1 step: 1001, loss is 0.25490885972976685\n",
      "epoch: 1 step: 1002, loss is 0.05254735052585602\n",
      "epoch: 1 step: 1003, loss is 0.1969611793756485\n",
      "epoch: 1 step: 1004, loss is 0.09952478855848312\n",
      "epoch: 1 step: 1005, loss is 0.18110120296478271\n",
      "epoch: 1 step: 1006, loss is 0.13465826213359833\n",
      "epoch: 1 step: 1007, loss is 0.19909104704856873\n",
      "epoch: 1 step: 1008, loss is 0.21256358921527863\n",
      "epoch: 1 step: 1009, loss is 0.025617210194468498\n",
      "epoch: 1 step: 1010, loss is 0.15229682624340057\n",
      "epoch: 1 step: 1011, loss is 0.07343672215938568\n",
      "epoch: 1 step: 1012, loss is 0.21749278903007507\n",
      "epoch: 1 step: 1013, loss is 0.25693461298942566\n",
      "epoch: 1 step: 1014, loss is 0.06190943717956543\n",
      "epoch: 1 step: 1015, loss is 0.03418683260679245\n",
      "epoch: 1 step: 1016, loss is 0.09727019816637039\n",
      "epoch: 1 step: 1017, loss is 0.10978420823812485\n",
      "epoch: 1 step: 1018, loss is 0.22332894802093506\n",
      "epoch: 1 step: 1019, loss is 0.05463562160730362\n",
      "epoch: 1 step: 1020, loss is 0.33488211035728455\n",
      "epoch: 1 step: 1021, loss is 0.5989436507225037\n",
      "epoch: 1 step: 1022, loss is 0.1807468831539154\n",
      "epoch: 1 step: 1023, loss is 0.03321385383605957\n",
      "epoch: 1 step: 1024, loss is 0.06305243074893951\n",
      "epoch: 1 step: 1025, loss is 0.28619587421417236\n",
      "epoch: 1 step: 1026, loss is 0.1475960612297058\n",
      "epoch: 1 step: 1027, loss is 0.09228000789880753\n",
      "epoch: 1 step: 1028, loss is 0.11874064803123474\n",
      "epoch: 1 step: 1029, loss is 0.17764388024806976\n",
      "epoch: 1 step: 1030, loss is 0.288224458694458\n",
      "epoch: 1 step: 1031, loss is 0.14702586829662323\n",
      "epoch: 1 step: 1032, loss is 0.08671420067548752\n",
      "epoch: 1 step: 1033, loss is 0.13082891702651978\n",
      "epoch: 1 step: 1034, loss is 0.1546519547700882\n",
      "epoch: 1 step: 1035, loss is 0.28233346343040466\n",
      "epoch: 1 step: 1036, loss is 0.17622992396354675\n",
      "epoch: 1 step: 1037, loss is 0.06677275151014328\n",
      "epoch: 1 step: 1038, loss is 0.13627764582633972\n",
      "epoch: 1 step: 1039, loss is 0.039803214371204376\n",
      "epoch: 1 step: 1040, loss is 0.12437578290700912\n",
      "epoch: 1 step: 1041, loss is 0.035869356244802475\n",
      "epoch: 1 step: 1042, loss is 0.11489882320165634\n",
      "epoch: 1 step: 1043, loss is 0.10062678158283234\n",
      "epoch: 1 step: 1044, loss is 0.13089656829833984\n",
      "epoch: 1 step: 1045, loss is 0.18468670547008514\n",
      "epoch: 1 step: 1046, loss is 0.03165852278470993\n",
      "epoch: 1 step: 1047, loss is 0.025986552238464355\n",
      "epoch: 1 step: 1048, loss is 0.31311652064323425\n",
      "epoch: 1 step: 1049, loss is 0.18251973390579224\n",
      "epoch: 1 step: 1050, loss is 0.13950563967227936\n",
      "epoch: 1 step: 1051, loss is 0.03480790555477142\n",
      "epoch: 1 step: 1052, loss is 0.04774891585111618\n",
      "epoch: 1 step: 1053, loss is 0.1867825984954834\n",
      "epoch: 1 step: 1054, loss is 0.09642378985881805\n",
      "epoch: 1 step: 1055, loss is 0.20392823219299316\n",
      "epoch: 1 step: 1056, loss is 0.071040078997612\n",
      "epoch: 1 step: 1057, loss is 0.18337686359882355\n",
      "epoch: 1 step: 1058, loss is 0.40752533078193665\n",
      "epoch: 1 step: 1059, loss is 0.079008549451828\n",
      "epoch: 1 step: 1060, loss is 0.4350211024284363\n",
      "epoch: 1 step: 1061, loss is 0.2637232542037964\n",
      "epoch: 1 step: 1062, loss is 0.3142964541912079\n",
      "epoch: 1 step: 1063, loss is 0.485393762588501\n",
      "epoch: 1 step: 1064, loss is 0.505909264087677\n",
      "epoch: 1 step: 1065, loss is 0.3166349232196808\n",
      "epoch: 1 step: 1066, loss is 0.2209514081478119\n",
      "epoch: 1 step: 1067, loss is 0.04978223517537117\n",
      "epoch: 1 step: 1068, loss is 0.191792830824852\n",
      "epoch: 1 step: 1069, loss is 0.09233152866363525\n",
      "epoch: 1 step: 1070, loss is 0.2371458262205124\n",
      "epoch: 1 step: 1071, loss is 0.093744195997715\n",
      "epoch: 1 step: 1072, loss is 0.015590748749673367\n",
      "epoch: 1 step: 1073, loss is 0.0648542270064354\n",
      "epoch: 1 step: 1074, loss is 0.2545700967311859\n",
      "epoch: 1 step: 1075, loss is 0.24548323452472687\n",
      "epoch: 1 step: 1076, loss is 0.24881868064403534\n",
      "epoch: 1 step: 1077, loss is 0.2142716348171234\n",
      "epoch: 1 step: 1078, loss is 0.21201452612876892\n",
      "epoch: 1 step: 1079, loss is 0.36789053678512573\n",
      "epoch: 1 step: 1080, loss is 0.0472211129963398\n",
      "epoch: 1 step: 1081, loss is 0.18971198797225952\n",
      "epoch: 1 step: 1082, loss is 0.04003368690609932\n",
      "epoch: 1 step: 1083, loss is 0.22749187052249908\n",
      "epoch: 1 step: 1084, loss is 0.08335412293672562\n",
      "epoch: 1 step: 1085, loss is 0.11353404074907303\n",
      "epoch: 1 step: 1086, loss is 0.11638832837343216\n",
      "epoch: 1 step: 1087, loss is 0.05549890547990799\n",
      "epoch: 1 step: 1088, loss is 0.11013422161340714\n",
      "epoch: 1 step: 1089, loss is 0.26217225193977356\n",
      "epoch: 1 step: 1090, loss is 0.06102153658866882\n",
      "epoch: 1 step: 1091, loss is 0.07724766433238983\n",
      "epoch: 1 step: 1092, loss is 0.4338189959526062\n",
      "epoch: 1 step: 1093, loss is 0.016235174611210823\n",
      "epoch: 1 step: 1094, loss is 0.25253668427467346\n",
      "epoch: 1 step: 1095, loss is 0.10266944766044617\n",
      "epoch: 1 step: 1096, loss is 0.21545986831188202\n",
      "epoch: 1 step: 1097, loss is 0.07747075706720352\n",
      "epoch: 1 step: 1098, loss is 0.16294492781162262\n",
      "epoch: 1 step: 1099, loss is 0.2459212839603424\n",
      "epoch: 1 step: 1100, loss is 0.5773516893386841\n",
      "epoch: 1 step: 1101, loss is 0.2860201299190521\n",
      "epoch: 1 step: 1102, loss is 0.053031228482723236\n",
      "epoch: 1 step: 1103, loss is 0.07541732490062714\n",
      "epoch: 1 step: 1104, loss is 0.035209015011787415\n",
      "epoch: 1 step: 1105, loss is 0.17306828498840332\n",
      "epoch: 1 step: 1106, loss is 0.16238141059875488\n",
      "epoch: 1 step: 1107, loss is 0.2134612500667572\n",
      "epoch: 1 step: 1108, loss is 0.16620153188705444\n",
      "epoch: 1 step: 1109, loss is 0.0685114935040474\n",
      "epoch: 1 step: 1110, loss is 0.06424450129270554\n",
      "epoch: 1 step: 1111, loss is 0.07546766847372055\n",
      "epoch: 1 step: 1112, loss is 0.1372644454240799\n",
      "epoch: 1 step: 1113, loss is 0.20955507457256317\n",
      "epoch: 1 step: 1114, loss is 0.03694457560777664\n",
      "epoch: 1 step: 1115, loss is 0.08296909183263779\n",
      "epoch: 1 step: 1116, loss is 0.4752146899700165\n",
      "epoch: 1 step: 1117, loss is 0.29646041989326477\n",
      "epoch: 1 step: 1118, loss is 0.5772996544837952\n",
      "epoch: 1 step: 1119, loss is 0.1786433458328247\n",
      "epoch: 1 step: 1120, loss is 0.018563786521553993\n",
      "epoch: 1 step: 1121, loss is 0.2518042027950287\n",
      "epoch: 1 step: 1122, loss is 0.05262773111462593\n",
      "epoch: 1 step: 1123, loss is 0.14164656400680542\n",
      "epoch: 1 step: 1124, loss is 0.31252050399780273\n",
      "epoch: 1 step: 1125, loss is 0.3061434030532837\n",
      "epoch: 1 step: 1126, loss is 0.19189317524433136\n",
      "epoch: 1 step: 1127, loss is 0.05422311648726463\n",
      "epoch: 1 step: 1128, loss is 0.30791744589805603\n",
      "epoch: 1 step: 1129, loss is 0.04599492996931076\n",
      "epoch: 1 step: 1130, loss is 0.07567012310028076\n",
      "epoch: 1 step: 1131, loss is 0.056945279240608215\n",
      "epoch: 1 step: 1132, loss is 0.02787819691002369\n",
      "epoch: 1 step: 1133, loss is 0.09449958056211472\n",
      "epoch: 1 step: 1134, loss is 0.032757315784692764\n",
      "epoch: 1 step: 1135, loss is 0.1460132896900177\n",
      "epoch: 1 step: 1136, loss is 0.24023321270942688\n",
      "epoch: 1 step: 1137, loss is 0.05664435774087906\n",
      "epoch: 1 step: 1138, loss is 0.14219367504119873\n",
      "epoch: 1 step: 1139, loss is 0.05856943130493164\n",
      "epoch: 1 step: 1140, loss is 0.2600650489330292\n",
      "epoch: 1 step: 1141, loss is 0.10839870572090149\n",
      "epoch: 1 step: 1142, loss is 0.08973299711942673\n",
      "epoch: 1 step: 1143, loss is 0.09876502305269241\n",
      "epoch: 1 step: 1144, loss is 0.10379825532436371\n",
      "epoch: 1 step: 1145, loss is 0.3445797860622406\n",
      "epoch: 1 step: 1146, loss is 0.08248439431190491\n",
      "epoch: 1 step: 1147, loss is 0.21198393404483795\n",
      "epoch: 1 step: 1148, loss is 0.08479535579681396\n",
      "epoch: 1 step: 1149, loss is 0.36252689361572266\n",
      "epoch: 1 step: 1150, loss is 0.2866881787776947\n",
      "epoch: 1 step: 1151, loss is 0.0992651954293251\n",
      "epoch: 1 step: 1152, loss is 0.018533214926719666\n",
      "epoch: 1 step: 1153, loss is 0.16856852173805237\n",
      "epoch: 1 step: 1154, loss is 0.1618085503578186\n",
      "epoch: 1 step: 1155, loss is 0.2794024646282196\n",
      "epoch: 1 step: 1156, loss is 0.12013699114322662\n",
      "epoch: 1 step: 1157, loss is 0.151004359126091\n",
      "epoch: 1 step: 1158, loss is 0.029251396656036377\n",
      "epoch: 1 step: 1159, loss is 0.14447809755802155\n",
      "epoch: 1 step: 1160, loss is 0.08321790397167206\n",
      "epoch: 1 step: 1161, loss is 0.23353531956672668\n",
      "epoch: 1 step: 1162, loss is 0.40059298276901245\n",
      "epoch: 1 step: 1163, loss is 0.024521181359887123\n",
      "epoch: 1 step: 1164, loss is 0.2845625579357147\n",
      "epoch: 1 step: 1165, loss is 0.22452737390995026\n",
      "epoch: 1 step: 1166, loss is 0.1817813515663147\n",
      "epoch: 1 step: 1167, loss is 0.06025988608598709\n",
      "epoch: 1 step: 1168, loss is 0.16952554881572723\n",
      "epoch: 1 step: 1169, loss is 0.34408634901046753\n",
      "epoch: 1 step: 1170, loss is 0.07227444648742676\n",
      "epoch: 1 step: 1171, loss is 0.04429646208882332\n",
      "epoch: 1 step: 1172, loss is 0.1645914614200592\n",
      "epoch: 1 step: 1173, loss is 0.033642109483480453\n",
      "epoch: 1 step: 1174, loss is 0.09240614622831345\n",
      "epoch: 1 step: 1175, loss is 0.1994888186454773\n",
      "epoch: 1 step: 1176, loss is 0.03688078001141548\n",
      "epoch: 1 step: 1177, loss is 0.24111872911453247\n",
      "epoch: 1 step: 1178, loss is 0.3265102803707123\n",
      "epoch: 1 step: 1179, loss is 0.0690392255783081\n",
      "epoch: 1 step: 1180, loss is 0.19358235597610474\n",
      "epoch: 1 step: 1181, loss is 0.04256509244441986\n",
      "epoch: 1 step: 1182, loss is 0.044796135276556015\n",
      "epoch: 1 step: 1183, loss is 0.31787535548210144\n",
      "epoch: 1 step: 1184, loss is 0.09351208806037903\n",
      "epoch: 1 step: 1185, loss is 0.06977629661560059\n",
      "epoch: 1 step: 1186, loss is 0.03260279819369316\n",
      "epoch: 1 step: 1187, loss is 0.3482803702354431\n",
      "epoch: 1 step: 1188, loss is 0.02948136068880558\n",
      "epoch: 1 step: 1189, loss is 0.05204438418149948\n",
      "epoch: 1 step: 1190, loss is 0.20856468379497528\n",
      "epoch: 1 step: 1191, loss is 0.20685310661792755\n",
      "epoch: 1 step: 1192, loss is 0.11775144189596176\n",
      "epoch: 1 step: 1193, loss is 0.23421400785446167\n",
      "epoch: 1 step: 1194, loss is 0.029564112424850464\n",
      "epoch: 1 step: 1195, loss is 0.22491604089736938\n",
      "epoch: 1 step: 1196, loss is 0.0373542457818985\n",
      "epoch: 1 step: 1197, loss is 0.08291106671094894\n",
      "epoch: 1 step: 1198, loss is 0.15946480631828308\n",
      "epoch: 1 step: 1199, loss is 0.021897336468100548\n",
      "epoch: 1 step: 1200, loss is 0.18222089111804962\n",
      "epoch: 1 step: 1201, loss is 0.3976830542087555\n",
      "epoch: 1 step: 1202, loss is 0.21266275644302368\n",
      "epoch: 1 step: 1203, loss is 0.4599342346191406\n",
      "epoch: 1 step: 1204, loss is 0.043527282774448395\n",
      "epoch: 1 step: 1205, loss is 0.024520374834537506\n",
      "epoch: 1 step: 1206, loss is 0.028439927846193314\n",
      "epoch: 1 step: 1207, loss is 0.08386854827404022\n",
      "epoch: 1 step: 1208, loss is 0.17111335694789886\n",
      "epoch: 1 step: 1209, loss is 0.09943105280399323\n",
      "epoch: 1 step: 1210, loss is 0.10284555703401566\n",
      "epoch: 1 step: 1211, loss is 0.12180619686841965\n",
      "epoch: 1 step: 1212, loss is 0.11087910085916519\n",
      "epoch: 1 step: 1213, loss is 0.12776662409305573\n",
      "epoch: 1 step: 1214, loss is 0.04704543575644493\n",
      "epoch: 1 step: 1215, loss is 0.056709837168455124\n",
      "epoch: 1 step: 1216, loss is 0.09688796103000641\n",
      "epoch: 1 step: 1217, loss is 0.4189065098762512\n",
      "epoch: 1 step: 1218, loss is 0.11515791714191437\n",
      "epoch: 1 step: 1219, loss is 0.013706447556614876\n",
      "epoch: 1 step: 1220, loss is 0.011538826860487461\n",
      "epoch: 1 step: 1221, loss is 0.009312361478805542\n",
      "epoch: 1 step: 1222, loss is 0.10225671529769897\n",
      "epoch: 1 step: 1223, loss is 0.03404897451400757\n",
      "epoch: 1 step: 1224, loss is 0.07189183682203293\n",
      "epoch: 1 step: 1225, loss is 0.05554157868027687\n",
      "epoch: 1 step: 1226, loss is 0.20029442012310028\n",
      "epoch: 1 step: 1227, loss is 0.28354594111442566\n",
      "epoch: 1 step: 1228, loss is 0.13772767782211304\n",
      "epoch: 1 step: 1229, loss is 0.13248585164546967\n",
      "epoch: 1 step: 1230, loss is 0.014198080636560917\n",
      "epoch: 1 step: 1231, loss is 0.015545089729130268\n",
      "epoch: 1 step: 1232, loss is 0.08642327785491943\n",
      "epoch: 1 step: 1233, loss is 0.011578770354390144\n",
      "epoch: 1 step: 1234, loss is 0.008171748369932175\n",
      "epoch: 1 step: 1235, loss is 0.3569265902042389\n",
      "epoch: 1 step: 1236, loss is 0.14911116659641266\n",
      "epoch: 1 step: 1237, loss is 0.20464809238910675\n",
      "epoch: 1 step: 1238, loss is 0.2209753692150116\n",
      "epoch: 1 step: 1239, loss is 0.06477717310190201\n",
      "epoch: 1 step: 1240, loss is 0.020192349329590797\n",
      "epoch: 1 step: 1241, loss is 0.09047233313322067\n",
      "epoch: 1 step: 1242, loss is 0.21172286570072174\n",
      "epoch: 1 step: 1243, loss is 0.09428747743368149\n",
      "epoch: 1 step: 1244, loss is 0.06039261445403099\n",
      "epoch: 1 step: 1245, loss is 0.2524341344833374\n",
      "epoch: 1 step: 1246, loss is 0.05187450349330902\n",
      "epoch: 1 step: 1247, loss is 0.45193684101104736\n",
      "epoch: 1 step: 1248, loss is 0.10764399915933609\n",
      "epoch: 1 step: 1249, loss is 0.01937246136367321\n",
      "epoch: 1 step: 1250, loss is 0.3600807189941406\n",
      "epoch: 1 step: 1251, loss is 0.12090356647968292\n",
      "epoch: 1 step: 1252, loss is 0.1920117735862732\n",
      "epoch: 1 step: 1253, loss is 0.10873709619045258\n",
      "epoch: 1 step: 1254, loss is 0.2447601705789566\n",
      "epoch: 1 step: 1255, loss is 0.17948760092258453\n",
      "epoch: 1 step: 1256, loss is 0.3149113655090332\n",
      "epoch: 1 step: 1257, loss is 0.31624624133110046\n",
      "epoch: 1 step: 1258, loss is 0.11469320952892303\n",
      "epoch: 1 step: 1259, loss is 0.23895083367824554\n",
      "epoch: 1 step: 1260, loss is 0.048003457486629486\n",
      "epoch: 1 step: 1261, loss is 0.07972259074449539\n",
      "epoch: 1 step: 1262, loss is 0.15667062997817993\n",
      "epoch: 1 step: 1263, loss is 0.060718901455402374\n",
      "epoch: 1 step: 1264, loss is 0.15091285109519958\n",
      "epoch: 1 step: 1265, loss is 0.3056880235671997\n",
      "epoch: 1 step: 1266, loss is 0.5228420495986938\n",
      "epoch: 1 step: 1267, loss is 0.20591402053833008\n",
      "epoch: 1 step: 1268, loss is 0.08029186725616455\n",
      "epoch: 1 step: 1269, loss is 0.15090632438659668\n",
      "epoch: 1 step: 1270, loss is 0.2453283816576004\n",
      "epoch: 1 step: 1271, loss is 0.11622128635644913\n",
      "epoch: 1 step: 1272, loss is 0.03027253970503807\n",
      "epoch: 1 step: 1273, loss is 0.20856021344661713\n",
      "epoch: 1 step: 1274, loss is 0.06414433568716049\n",
      "epoch: 1 step: 1275, loss is 0.14137576520442963\n",
      "epoch: 1 step: 1276, loss is 0.18238510191440582\n",
      "epoch: 1 step: 1277, loss is 0.3782474994659424\n",
      "epoch: 1 step: 1278, loss is 0.025150364264845848\n",
      "epoch: 1 step: 1279, loss is 0.08599492907524109\n",
      "epoch: 1 step: 1280, loss is 0.15778778493404388\n",
      "epoch: 1 step: 1281, loss is 0.09709999710321426\n",
      "epoch: 1 step: 1282, loss is 0.04875342920422554\n",
      "epoch: 1 step: 1283, loss is 0.12908898293972015\n",
      "epoch: 1 step: 1284, loss is 0.17976690828800201\n",
      "epoch: 1 step: 1285, loss is 0.024781402200460434\n",
      "epoch: 1 step: 1286, loss is 0.18570905923843384\n",
      "epoch: 1 step: 1287, loss is 0.024215709418058395\n",
      "epoch: 1 step: 1288, loss is 0.2596109211444855\n",
      "epoch: 1 step: 1289, loss is 0.12530329823493958\n",
      "epoch: 1 step: 1290, loss is 0.08938857167959213\n",
      "epoch: 1 step: 1291, loss is 0.190917506814003\n",
      "epoch: 1 step: 1292, loss is 0.08975387364625931\n",
      "epoch: 1 step: 1293, loss is 0.029184285551309586\n",
      "epoch: 1 step: 1294, loss is 0.028054730966687202\n",
      "epoch: 1 step: 1295, loss is 0.24238422513008118\n",
      "epoch: 1 step: 1296, loss is 0.12171164155006409\n",
      "epoch: 1 step: 1297, loss is 0.19221477210521698\n",
      "epoch: 1 step: 1298, loss is 0.08559443056583405\n",
      "epoch: 1 step: 1299, loss is 0.22437144815921783\n",
      "epoch: 1 step: 1300, loss is 0.174685999751091\n",
      "epoch: 1 step: 1301, loss is 0.22986233234405518\n",
      "epoch: 1 step: 1302, loss is 0.027855856344103813\n",
      "epoch: 1 step: 1303, loss is 0.03539029881358147\n",
      "epoch: 1 step: 1304, loss is 0.03386642038822174\n",
      "epoch: 1 step: 1305, loss is 0.06565042585134506\n",
      "epoch: 1 step: 1306, loss is 0.14620620012283325\n",
      "epoch: 1 step: 1307, loss is 0.12088527530431747\n",
      "epoch: 1 step: 1308, loss is 0.03785675764083862\n",
      "epoch: 1 step: 1309, loss is 0.14977513253688812\n",
      "epoch: 1 step: 1310, loss is 0.07005391269922256\n",
      "epoch: 1 step: 1311, loss is 0.1534963697195053\n",
      "epoch: 1 step: 1312, loss is 0.11898115277290344\n",
      "epoch: 1 step: 1313, loss is 0.005729881580919027\n",
      "epoch: 1 step: 1314, loss is 0.052897002547979355\n",
      "epoch: 1 step: 1315, loss is 0.01154886744916439\n",
      "epoch: 1 step: 1316, loss is 0.11676426231861115\n",
      "epoch: 1 step: 1317, loss is 0.15207919478416443\n",
      "epoch: 1 step: 1318, loss is 0.029829416424036026\n",
      "epoch: 1 step: 1319, loss is 0.36629247665405273\n",
      "epoch: 1 step: 1320, loss is 0.08171568065881729\n",
      "epoch: 1 step: 1321, loss is 0.1574057787656784\n",
      "epoch: 1 step: 1322, loss is 0.07982449978590012\n",
      "epoch: 1 step: 1323, loss is 0.4422800540924072\n",
      "epoch: 1 step: 1324, loss is 0.10401177406311035\n",
      "epoch: 1 step: 1325, loss is 0.21270492672920227\n",
      "epoch: 1 step: 1326, loss is 0.046067014336586\n",
      "epoch: 1 step: 1327, loss is 0.2601168155670166\n",
      "epoch: 1 step: 1328, loss is 0.30834299325942993\n",
      "epoch: 1 step: 1329, loss is 0.1109185665845871\n",
      "epoch: 1 step: 1330, loss is 0.39652320742607117\n",
      "epoch: 1 step: 1331, loss is 0.2148875594139099\n",
      "epoch: 1 step: 1332, loss is 0.14092616736888885\n",
      "epoch: 1 step: 1333, loss is 0.15320870280265808\n",
      "epoch: 1 step: 1334, loss is 0.2093442678451538\n",
      "epoch: 1 step: 1335, loss is 0.18532809615135193\n",
      "epoch: 1 step: 1336, loss is 0.3298535943031311\n",
      "epoch: 1 step: 1337, loss is 0.13688179850578308\n",
      "epoch: 1 step: 1338, loss is 0.024860799312591553\n",
      "epoch: 1 step: 1339, loss is 0.04028842970728874\n",
      "epoch: 1 step: 1340, loss is 0.12938129901885986\n",
      "epoch: 1 step: 1341, loss is 0.20368753373622894\n",
      "epoch: 1 step: 1342, loss is 0.30942830443382263\n",
      "epoch: 1 step: 1343, loss is 0.1472557634115219\n",
      "epoch: 1 step: 1344, loss is 0.23700420558452606\n",
      "epoch: 1 step: 1345, loss is 0.05526871234178543\n",
      "epoch: 1 step: 1346, loss is 0.03554154932498932\n",
      "epoch: 1 step: 1347, loss is 0.2729197144508362\n",
      "epoch: 1 step: 1348, loss is 0.32187405228614807\n",
      "epoch: 1 step: 1349, loss is 0.0716034322977066\n",
      "epoch: 1 step: 1350, loss is 0.23471598327159882\n",
      "epoch: 1 step: 1351, loss is 0.006440676283091307\n",
      "epoch: 1 step: 1352, loss is 0.027522698044776917\n",
      "epoch: 1 step: 1353, loss is 0.19530482590198517\n",
      "epoch: 1 step: 1354, loss is 0.3120998442173004\n",
      "epoch: 1 step: 1355, loss is 0.05129417032003403\n",
      "epoch: 1 step: 1356, loss is 0.09800218045711517\n",
      "epoch: 1 step: 1357, loss is 0.10970553755760193\n",
      "epoch: 1 step: 1358, loss is 0.19119583070278168\n",
      "epoch: 1 step: 1359, loss is 0.30458033084869385\n",
      "epoch: 1 step: 1360, loss is 0.23082828521728516\n",
      "epoch: 1 step: 1361, loss is 0.1086975559592247\n",
      "epoch: 1 step: 1362, loss is 0.098973348736763\n",
      "epoch: 1 step: 1363, loss is 0.0799359604716301\n",
      "epoch: 1 step: 1364, loss is 0.04478689283132553\n",
      "epoch: 1 step: 1365, loss is 0.11808227002620697\n",
      "epoch: 1 step: 1366, loss is 0.13906000554561615\n",
      "epoch: 1 step: 1367, loss is 0.09768220782279968\n",
      "epoch: 1 step: 1368, loss is 0.38112547993659973\n",
      "epoch: 1 step: 1369, loss is 0.012922757305204868\n",
      "epoch: 1 step: 1370, loss is 0.1500689834356308\n",
      "epoch: 1 step: 1371, loss is 0.015020448714494705\n",
      "epoch: 1 step: 1372, loss is 0.30681443214416504\n",
      "epoch: 1 step: 1373, loss is 0.0285408403724432\n",
      "epoch: 1 step: 1374, loss is 0.08047623932361603\n",
      "epoch: 1 step: 1375, loss is 0.12153911590576172\n",
      "epoch: 1 step: 1376, loss is 0.0709133893251419\n",
      "epoch: 1 step: 1377, loss is 0.06685961782932281\n",
      "epoch: 1 step: 1378, loss is 0.027777185663580894\n",
      "epoch: 1 step: 1379, loss is 0.16449148952960968\n",
      "epoch: 1 step: 1380, loss is 0.20081381499767303\n",
      "epoch: 1 step: 1381, loss is 0.035462506115436554\n",
      "epoch: 1 step: 1382, loss is 0.07952185720205307\n",
      "epoch: 1 step: 1383, loss is 0.05375736206769943\n",
      "epoch: 1 step: 1384, loss is 0.06670299917459488\n",
      "epoch: 1 step: 1385, loss is 0.06199536472558975\n",
      "epoch: 1 step: 1386, loss is 0.027397697791457176\n",
      "epoch: 1 step: 1387, loss is 0.1411971151828766\n",
      "epoch: 1 step: 1388, loss is 0.005661225412040949\n",
      "epoch: 1 step: 1389, loss is 0.08090056478977203\n",
      "epoch: 1 step: 1390, loss is 0.04089865833520889\n",
      "epoch: 1 step: 1391, loss is 0.22889991104602814\n",
      "epoch: 1 step: 1392, loss is 0.027999509125947952\n",
      "epoch: 1 step: 1393, loss is 0.08022148162126541\n",
      "epoch: 1 step: 1394, loss is 0.1895671784877777\n",
      "epoch: 1 step: 1395, loss is 0.1294453740119934\n",
      "epoch: 1 step: 1396, loss is 0.021738803014159203\n",
      "epoch: 1 step: 1397, loss is 0.011994794011116028\n",
      "epoch: 1 step: 1398, loss is 0.1653408408164978\n",
      "epoch: 1 step: 1399, loss is 0.10275188833475113\n",
      "epoch: 1 step: 1400, loss is 0.053488437086343765\n",
      "epoch: 1 step: 1401, loss is 0.135907843708992\n",
      "epoch: 1 step: 1402, loss is 0.053866367787122726\n",
      "epoch: 1 step: 1403, loss is 0.10387717187404633\n",
      "epoch: 1 step: 1404, loss is 0.1612188220024109\n",
      "epoch: 1 step: 1405, loss is 0.0068575660698115826\n",
      "epoch: 1 step: 1406, loss is 0.05579262599349022\n",
      "epoch: 1 step: 1407, loss is 0.09817436337471008\n",
      "epoch: 1 step: 1408, loss is 0.25879865884780884\n",
      "epoch: 1 step: 1409, loss is 0.055847734212875366\n",
      "epoch: 1 step: 1410, loss is 0.1150563657283783\n",
      "epoch: 1 step: 1411, loss is 0.009020470082759857\n",
      "epoch: 1 step: 1412, loss is 0.25108078122138977\n",
      "epoch: 1 step: 1413, loss is 0.053519107401371\n",
      "epoch: 1 step: 1414, loss is 0.34974491596221924\n",
      "epoch: 1 step: 1415, loss is 0.06324674934148788\n",
      "epoch: 1 step: 1416, loss is 0.0957171693444252\n",
      "epoch: 1 step: 1417, loss is 0.04249425232410431\n",
      "epoch: 1 step: 1418, loss is 0.09789039939641953\n",
      "epoch: 1 step: 1419, loss is 0.10348298400640488\n",
      "epoch: 1 step: 1420, loss is 0.09956954419612885\n",
      "epoch: 1 step: 1421, loss is 0.05555611476302147\n",
      "epoch: 1 step: 1422, loss is 0.03347483649849892\n",
      "epoch: 1 step: 1423, loss is 0.6031039953231812\n",
      "epoch: 1 step: 1424, loss is 0.05678292363882065\n",
      "epoch: 1 step: 1425, loss is 0.026962267234921455\n",
      "epoch: 1 step: 1426, loss is 0.0734826996922493\n",
      "epoch: 1 step: 1427, loss is 0.27547791600227356\n",
      "epoch: 1 step: 1428, loss is 0.21832707524299622\n",
      "epoch: 1 step: 1429, loss is 0.20148229598999023\n",
      "epoch: 1 step: 1430, loss is 0.10055618733167648\n",
      "epoch: 1 step: 1431, loss is 0.2660723626613617\n",
      "epoch: 1 step: 1432, loss is 0.10405413061380386\n",
      "epoch: 1 step: 1433, loss is 0.2860543727874756\n",
      "epoch: 1 step: 1434, loss is 0.02079949900507927\n",
      "epoch: 1 step: 1435, loss is 0.04690207913517952\n",
      "epoch: 1 step: 1436, loss is 0.2793877124786377\n",
      "epoch: 1 step: 1437, loss is 0.3748490810394287\n",
      "epoch: 1 step: 1438, loss is 0.2863117754459381\n",
      "epoch: 1 step: 1439, loss is 0.29956483840942383\n",
      "epoch: 1 step: 1440, loss is 0.04051234945654869\n",
      "epoch: 1 step: 1441, loss is 0.3010319471359253\n",
      "epoch: 1 step: 1442, loss is 0.14279979467391968\n",
      "epoch: 1 step: 1443, loss is 0.0817587673664093\n",
      "epoch: 1 step: 1444, loss is 0.0582989901304245\n",
      "epoch: 1 step: 1445, loss is 0.19682802259922028\n",
      "epoch: 1 step: 1446, loss is 0.3779035210609436\n",
      "epoch: 1 step: 1447, loss is 0.344417005777359\n",
      "epoch: 1 step: 1448, loss is 0.05664979666471481\n",
      "epoch: 1 step: 1449, loss is 0.1628849059343338\n",
      "epoch: 1 step: 1450, loss is 0.37666434049606323\n",
      "epoch: 1 step: 1451, loss is 0.022486500442028046\n",
      "epoch: 1 step: 1452, loss is 0.307684063911438\n",
      "epoch: 1 step: 1453, loss is 0.2021787017583847\n",
      "epoch: 1 step: 1454, loss is 0.1974482238292694\n",
      "epoch: 1 step: 1455, loss is 0.0728025883436203\n",
      "epoch: 1 step: 1456, loss is 0.19962264597415924\n",
      "epoch: 1 step: 1457, loss is 0.3901359438896179\n",
      "epoch: 1 step: 1458, loss is 0.07543016225099564\n",
      "epoch: 1 step: 1459, loss is 0.17308667302131653\n",
      "epoch: 1 step: 1460, loss is 0.10090558230876923\n",
      "epoch: 1 step: 1461, loss is 0.0769229605793953\n",
      "epoch: 1 step: 1462, loss is 0.26794856786727905\n",
      "epoch: 1 step: 1463, loss is 0.18821142613887787\n",
      "epoch: 1 step: 1464, loss is 0.4008007049560547\n",
      "epoch: 1 step: 1465, loss is 0.08962699770927429\n",
      "epoch: 1 step: 1466, loss is 0.03281809762120247\n",
      "epoch: 1 step: 1467, loss is 0.15379753708839417\n",
      "epoch: 1 step: 1468, loss is 0.08881993591785431\n",
      "epoch: 1 step: 1469, loss is 0.09609804302453995\n",
      "epoch: 1 step: 1470, loss is 0.1343027800321579\n",
      "epoch: 1 step: 1471, loss is 0.26703956723213196\n",
      "epoch: 1 step: 1472, loss is 0.07308705151081085\n",
      "epoch: 1 step: 1473, loss is 0.032270368188619614\n",
      "epoch: 1 step: 1474, loss is 0.03759417310357094\n",
      "epoch: 1 step: 1475, loss is 0.1119782030582428\n",
      "epoch: 1 step: 1476, loss is 0.15327024459838867\n",
      "epoch: 1 step: 1477, loss is 0.05290578678250313\n",
      "epoch: 1 step: 1478, loss is 0.10709594935178757\n",
      "epoch: 1 step: 1479, loss is 0.13784442842006683\n",
      "epoch: 1 step: 1480, loss is 0.20922371745109558\n",
      "epoch: 1 step: 1481, loss is 0.05658085271716118\n",
      "epoch: 1 step: 1482, loss is 0.11243017762899399\n",
      "epoch: 1 step: 1483, loss is 0.18428939580917358\n",
      "epoch: 1 step: 1484, loss is 0.16637325286865234\n",
      "epoch: 1 step: 1485, loss is 0.17874003946781158\n",
      "epoch: 1 step: 1486, loss is 0.17168718576431274\n",
      "epoch: 1 step: 1487, loss is 0.03845887631177902\n",
      "epoch: 1 step: 1488, loss is 0.09608262777328491\n",
      "epoch: 1 step: 1489, loss is 0.029393497854471207\n",
      "epoch: 1 step: 1490, loss is 0.023964397609233856\n",
      "epoch: 1 step: 1491, loss is 0.09518645703792572\n",
      "epoch: 1 step: 1492, loss is 0.10547275841236115\n",
      "epoch: 1 step: 1493, loss is 0.04081239923834801\n",
      "epoch: 1 step: 1494, loss is 0.07414716482162476\n",
      "epoch: 1 step: 1495, loss is 0.008316921070218086\n",
      "epoch: 1 step: 1496, loss is 0.26789960265159607\n",
      "epoch: 1 step: 1497, loss is 0.1182420551776886\n",
      "epoch: 1 step: 1498, loss is 0.013855119235813618\n",
      "epoch: 1 step: 1499, loss is 0.013097933493554592\n",
      "epoch: 1 step: 1500, loss is 0.16340215504169464\n",
      "epoch: 1 step: 1501, loss is 0.26332592964172363\n",
      "epoch: 1 step: 1502, loss is 0.10864328593015671\n",
      "epoch: 1 step: 1503, loss is 0.08551853150129318\n",
      "epoch: 1 step: 1504, loss is 0.003847123822197318\n",
      "epoch: 1 step: 1505, loss is 0.03070387989282608\n",
      "epoch: 1 step: 1506, loss is 0.02998799830675125\n",
      "epoch: 1 step: 1507, loss is 0.25593632459640503\n",
      "epoch: 1 step: 1508, loss is 0.039879944175481796\n",
      "epoch: 1 step: 1509, loss is 0.3131505846977234\n",
      "epoch: 1 step: 1510, loss is 0.006905571557581425\n",
      "epoch: 1 step: 1511, loss is 0.13559246063232422\n",
      "epoch: 1 step: 1512, loss is 0.050665345042943954\n",
      "epoch: 1 step: 1513, loss is 0.01302292849868536\n",
      "epoch: 1 step: 1514, loss is 0.11237673461437225\n",
      "epoch: 1 step: 1515, loss is 0.18636257946491241\n",
      "epoch: 1 step: 1516, loss is 0.044521838426589966\n",
      "epoch: 1 step: 1517, loss is 0.08974021673202515\n",
      "epoch: 1 step: 1518, loss is 0.12224949896335602\n",
      "epoch: 1 step: 1519, loss is 0.03366709128022194\n",
      "epoch: 1 step: 1520, loss is 0.42179322242736816\n",
      "epoch: 1 step: 1521, loss is 0.03774445503950119\n",
      "epoch: 1 step: 1522, loss is 0.16122761368751526\n",
      "epoch: 1 step: 1523, loss is 0.05220538005232811\n",
      "epoch: 1 step: 1524, loss is 0.049293383955955505\n",
      "epoch: 1 step: 1525, loss is 0.18259559571743011\n",
      "epoch: 1 step: 1526, loss is 0.3730897903442383\n",
      "epoch: 1 step: 1527, loss is 0.07990847527980804\n",
      "epoch: 1 step: 1528, loss is 0.29095298051834106\n",
      "epoch: 1 step: 1529, loss is 0.054132673889398575\n",
      "epoch: 1 step: 1530, loss is 0.04375641793012619\n",
      "epoch: 1 step: 1531, loss is 0.030149737372994423\n",
      "epoch: 1 step: 1532, loss is 0.20256127417087555\n",
      "epoch: 1 step: 1533, loss is 0.05839419737458229\n",
      "epoch: 1 step: 1534, loss is 0.15868838131427765\n",
      "epoch: 1 step: 1535, loss is 0.03755239397287369\n",
      "epoch: 1 step: 1536, loss is 0.10184130072593689\n",
      "epoch: 1 step: 1537, loss is 0.13906626403331757\n",
      "epoch: 1 step: 1538, loss is 0.06651844829320908\n",
      "epoch: 1 step: 1539, loss is 0.09616439044475555\n",
      "epoch: 1 step: 1540, loss is 0.026342667639255524\n",
      "epoch: 1 step: 1541, loss is 0.011773246340453625\n",
      "epoch: 1 step: 1542, loss is 0.07899311929941177\n",
      "epoch: 1 step: 1543, loss is 0.1446499079465866\n",
      "epoch: 1 step: 1544, loss is 0.011615373194217682\n",
      "epoch: 1 step: 1545, loss is 0.23029005527496338\n",
      "epoch: 1 step: 1546, loss is 0.09776344150304794\n",
      "epoch: 1 step: 1547, loss is 0.021526969969272614\n",
      "epoch: 1 step: 1548, loss is 0.020975546911358833\n",
      "epoch: 1 step: 1549, loss is 0.09159442037343979\n",
      "epoch: 1 step: 1550, loss is 0.0649518221616745\n",
      "epoch: 1 step: 1551, loss is 0.1489681899547577\n",
      "epoch: 1 step: 1552, loss is 0.07188878953456879\n",
      "epoch: 1 step: 1553, loss is 0.036792583763599396\n",
      "epoch: 1 step: 1554, loss is 0.18445050716400146\n",
      "epoch: 1 step: 1555, loss is 0.09892690926790237\n",
      "epoch: 1 step: 1556, loss is 0.0034522847272455692\n",
      "epoch: 1 step: 1557, loss is 0.11364307254552841\n",
      "epoch: 1 step: 1558, loss is 0.0068557350896298885\n",
      "epoch: 1 step: 1559, loss is 0.23368851840496063\n",
      "epoch: 1 step: 1560, loss is 0.006083944346755743\n",
      "epoch: 1 step: 1561, loss is 0.03146999329328537\n",
      "epoch: 1 step: 1562, loss is 0.061727721244096756\n",
      "epoch: 1 step: 1563, loss is 0.2903803586959839\n",
      "epoch: 1 step: 1564, loss is 0.20262105762958527\n",
      "epoch: 1 step: 1565, loss is 0.21840500831604004\n",
      "epoch: 1 step: 1566, loss is 0.028160257264971733\n",
      "epoch: 1 step: 1567, loss is 0.08523822575807571\n",
      "epoch: 1 step: 1568, loss is 0.09415426850318909\n",
      "epoch: 1 step: 1569, loss is 0.07661475241184235\n",
      "epoch: 1 step: 1570, loss is 0.10455290973186493\n",
      "epoch: 1 step: 1571, loss is 0.06738357245922089\n",
      "epoch: 1 step: 1572, loss is 0.17141014337539673\n",
      "epoch: 1 step: 1573, loss is 0.24779745936393738\n",
      "epoch: 1 step: 1574, loss is 0.083896204829216\n",
      "epoch: 1 step: 1575, loss is 0.03515419363975525\n",
      "epoch: 1 step: 1576, loss is 0.6049109101295471\n",
      "epoch: 1 step: 1577, loss is 0.08423182368278503\n",
      "epoch: 1 step: 1578, loss is 0.03719338774681091\n",
      "epoch: 1 step: 1579, loss is 0.04682781919836998\n",
      "epoch: 1 step: 1580, loss is 0.035167209804058075\n",
      "epoch: 1 step: 1581, loss is 0.15820226073265076\n",
      "epoch: 1 step: 1582, loss is 0.11961913853883743\n",
      "epoch: 1 step: 1583, loss is 0.21710342168807983\n",
      "epoch: 1 step: 1584, loss is 0.24583609402179718\n",
      "epoch: 1 step: 1585, loss is 0.0720391720533371\n",
      "epoch: 1 step: 1586, loss is 0.020780805498361588\n",
      "epoch: 1 step: 1587, loss is 0.059444062411785126\n",
      "epoch: 1 step: 1588, loss is 0.28177782893180847\n",
      "epoch: 1 step: 1589, loss is 0.04739978164434433\n",
      "epoch: 1 step: 1590, loss is 0.3412196934223175\n",
      "epoch: 1 step: 1591, loss is 0.1045820564031601\n",
      "epoch: 1 step: 1592, loss is 0.15576891601085663\n",
      "epoch: 1 step: 1593, loss is 0.033288534730672836\n",
      "epoch: 1 step: 1594, loss is 0.04311055690050125\n",
      "epoch: 1 step: 1595, loss is 0.03329964727163315\n",
      "epoch: 1 step: 1596, loss is 0.06511373072862625\n",
      "epoch: 1 step: 1597, loss is 0.09768756479024887\n",
      "epoch: 1 step: 1598, loss is 0.005738777574151754\n",
      "epoch: 1 step: 1599, loss is 0.04485641419887543\n",
      "epoch: 1 step: 1600, loss is 0.04093177616596222\n",
      "epoch: 1 step: 1601, loss is 0.09744898229837418\n",
      "epoch: 1 step: 1602, loss is 0.06028302013874054\n",
      "epoch: 1 step: 1603, loss is 0.02847360260784626\n",
      "epoch: 1 step: 1604, loss is 0.008095178753137589\n",
      "epoch: 1 step: 1605, loss is 0.01464507170021534\n",
      "epoch: 1 step: 1606, loss is 0.05113520473241806\n",
      "epoch: 1 step: 1607, loss is 0.021231811493635178\n",
      "epoch: 1 step: 1608, loss is 0.04292502626776695\n",
      "epoch: 1 step: 1609, loss is 0.03552858904004097\n",
      "epoch: 1 step: 1610, loss is 0.10425285995006561\n",
      "epoch: 1 step: 1611, loss is 0.007915987633168697\n",
      "epoch: 1 step: 1612, loss is 0.14406900107860565\n",
      "epoch: 1 step: 1613, loss is 0.2329166978597641\n",
      "epoch: 1 step: 1614, loss is 0.027485227212309837\n",
      "epoch: 1 step: 1615, loss is 0.27018213272094727\n",
      "epoch: 1 step: 1616, loss is 0.02174505963921547\n",
      "epoch: 1 step: 1617, loss is 0.017081568017601967\n",
      "epoch: 1 step: 1618, loss is 0.01972043514251709\n",
      "epoch: 1 step: 1619, loss is 0.03496153652667999\n",
      "epoch: 1 step: 1620, loss is 0.06544623523950577\n",
      "epoch: 1 step: 1621, loss is 0.19841156899929047\n",
      "epoch: 1 step: 1622, loss is 0.04287413880228996\n",
      "epoch: 1 step: 1623, loss is 0.02238895371556282\n",
      "epoch: 1 step: 1624, loss is 0.2537139654159546\n",
      "epoch: 1 step: 1625, loss is 0.03309538587927818\n",
      "epoch: 1 step: 1626, loss is 0.45232662558555603\n",
      "epoch: 1 step: 1627, loss is 0.007652680389583111\n",
      "epoch: 1 step: 1628, loss is 0.03524931147694588\n",
      "epoch: 1 step: 1629, loss is 0.0526822991669178\n",
      "epoch: 1 step: 1630, loss is 0.09874480962753296\n",
      "epoch: 1 step: 1631, loss is 0.017312251031398773\n",
      "epoch: 1 step: 1632, loss is 0.09945596754550934\n",
      "epoch: 1 step: 1633, loss is 0.19349509477615356\n",
      "epoch: 1 step: 1634, loss is 0.029421839863061905\n",
      "epoch: 1 step: 1635, loss is 0.012937875464558601\n",
      "epoch: 1 step: 1636, loss is 0.05833500251173973\n",
      "epoch: 1 step: 1637, loss is 0.0028450682293623686\n",
      "epoch: 1 step: 1638, loss is 0.07269708067178726\n",
      "epoch: 1 step: 1639, loss is 0.11998267471790314\n",
      "epoch: 1 step: 1640, loss is 0.17435310781002045\n",
      "epoch: 1 step: 1641, loss is 0.010895505547523499\n",
      "epoch: 1 step: 1642, loss is 0.0979970172047615\n",
      "epoch: 1 step: 1643, loss is 0.18255288898944855\n",
      "epoch: 1 step: 1644, loss is 0.04838838800787926\n",
      "epoch: 1 step: 1645, loss is 0.14890946447849274\n",
      "epoch: 1 step: 1646, loss is 0.1641012281179428\n",
      "epoch: 1 step: 1647, loss is 0.2849627435207367\n",
      "epoch: 1 step: 1648, loss is 0.10672063380479813\n",
      "epoch: 1 step: 1649, loss is 0.038485702127218246\n",
      "epoch: 1 step: 1650, loss is 0.03400515392422676\n",
      "epoch: 1 step: 1651, loss is 0.2756657898426056\n",
      "epoch: 1 step: 1652, loss is 0.11831346899271011\n",
      "epoch: 1 step: 1653, loss is 0.3282584846019745\n",
      "epoch: 1 step: 1654, loss is 0.012314185500144958\n",
      "epoch: 1 step: 1655, loss is 0.04446379095315933\n",
      "epoch: 1 step: 1656, loss is 0.022617070004343987\n",
      "epoch: 1 step: 1657, loss is 0.07593389600515366\n",
      "epoch: 1 step: 1658, loss is 0.2388923615217209\n",
      "epoch: 1 step: 1659, loss is 0.015517283231019974\n",
      "epoch: 1 step: 1660, loss is 0.012463729828596115\n",
      "epoch: 1 step: 1661, loss is 0.13137440383434296\n",
      "epoch: 1 step: 1662, loss is 0.010351440869271755\n",
      "epoch: 1 step: 1663, loss is 0.11898762732744217\n",
      "epoch: 1 step: 1664, loss is 0.29702889919281006\n",
      "epoch: 1 step: 1665, loss is 0.21436330676078796\n",
      "epoch: 1 step: 1666, loss is 0.13642114400863647\n",
      "epoch: 1 step: 1667, loss is 0.017699681222438812\n",
      "epoch: 1 step: 1668, loss is 0.08684513717889786\n",
      "epoch: 1 step: 1669, loss is 0.02068362571299076\n",
      "epoch: 1 step: 1670, loss is 0.02057272382080555\n",
      "epoch: 1 step: 1671, loss is 0.21234561502933502\n",
      "epoch: 1 step: 1672, loss is 0.030148537829518318\n",
      "epoch: 1 step: 1673, loss is 0.12199865281581879\n",
      "epoch: 1 step: 1674, loss is 0.16958443820476532\n",
      "epoch: 1 step: 1675, loss is 0.06777982413768768\n",
      "epoch: 1 step: 1676, loss is 0.021684935316443443\n",
      "epoch: 1 step: 1677, loss is 0.02197449654340744\n",
      "epoch: 1 step: 1678, loss is 0.00935361534357071\n",
      "epoch: 1 step: 1679, loss is 0.0391007661819458\n",
      "epoch: 1 step: 1680, loss is 0.020848030224442482\n",
      "epoch: 1 step: 1681, loss is 0.1616562008857727\n",
      "epoch: 1 step: 1682, loss is 0.06208544969558716\n",
      "epoch: 1 step: 1683, loss is 0.025042811408638954\n",
      "epoch: 1 step: 1684, loss is 0.17114956676959991\n",
      "epoch: 1 step: 1685, loss is 0.16937367618083954\n",
      "epoch: 1 step: 1686, loss is 0.07042678445577621\n",
      "epoch: 1 step: 1687, loss is 0.09597759693861008\n",
      "epoch: 1 step: 1688, loss is 0.05049725994467735\n",
      "epoch: 1 step: 1689, loss is 0.026598336175084114\n",
      "epoch: 1 step: 1690, loss is 0.0918998047709465\n",
      "epoch: 1 step: 1691, loss is 0.0315089114010334\n",
      "epoch: 1 step: 1692, loss is 0.06955380737781525\n",
      "epoch: 1 step: 1693, loss is 0.12400907278060913\n",
      "epoch: 1 step: 1694, loss is 0.14278465509414673\n",
      "epoch: 1 step: 1695, loss is 0.04271073639392853\n",
      "epoch: 1 step: 1696, loss is 0.013382336124777794\n",
      "epoch: 1 step: 1697, loss is 0.010023525916039944\n",
      "epoch: 1 step: 1698, loss is 0.02195109985768795\n",
      "epoch: 1 step: 1699, loss is 0.09721145033836365\n",
      "epoch: 1 step: 1700, loss is 0.004395469091832638\n",
      "epoch: 1 step: 1701, loss is 0.06815869361162186\n",
      "epoch: 1 step: 1702, loss is 0.13413479924201965\n",
      "epoch: 1 step: 1703, loss is 0.3161408603191376\n",
      "epoch: 1 step: 1704, loss is 0.04476394131779671\n",
      "epoch: 1 step: 1705, loss is 0.04878181591629982\n",
      "epoch: 1 step: 1706, loss is 0.05892188474535942\n",
      "epoch: 1 step: 1707, loss is 0.09449737519025803\n",
      "epoch: 1 step: 1708, loss is 0.0034589425195008516\n",
      "epoch: 1 step: 1709, loss is 0.004779830574989319\n",
      "epoch: 1 step: 1710, loss is 0.032901015132665634\n",
      "epoch: 1 step: 1711, loss is 0.004785516299307346\n",
      "epoch: 1 step: 1712, loss is 0.12035058438777924\n",
      "epoch: 1 step: 1713, loss is 0.05851894989609718\n",
      "epoch: 1 step: 1714, loss is 0.009937514550983906\n",
      "epoch: 1 step: 1715, loss is 0.19630134105682373\n",
      "epoch: 1 step: 1716, loss is 0.09469138085842133\n",
      "epoch: 1 step: 1717, loss is 0.021382207050919533\n",
      "epoch: 1 step: 1718, loss is 0.10011807829141617\n",
      "epoch: 1 step: 1719, loss is 0.030231980606913567\n",
      "epoch: 1 step: 1720, loss is 0.011927461251616478\n",
      "epoch: 1 step: 1721, loss is 0.10290279984474182\n",
      "epoch: 1 step: 1722, loss is 0.04540923610329628\n",
      "epoch: 1 step: 1723, loss is 0.05002059414982796\n",
      "epoch: 1 step: 1724, loss is 0.09327422082424164\n",
      "epoch: 1 step: 1725, loss is 0.18284949660301208\n",
      "epoch: 1 step: 1726, loss is 0.03452329710125923\n",
      "epoch: 1 step: 1727, loss is 0.13324670493602753\n",
      "epoch: 1 step: 1728, loss is 0.1627616584300995\n",
      "epoch: 1 step: 1729, loss is 0.04752125218510628\n",
      "epoch: 1 step: 1730, loss is 0.029126564040780067\n",
      "epoch: 1 step: 1731, loss is 0.053497955203056335\n",
      "epoch: 1 step: 1732, loss is 0.02186085842549801\n",
      "epoch: 1 step: 1733, loss is 0.01303949672728777\n",
      "epoch: 1 step: 1734, loss is 0.09369860589504242\n",
      "epoch: 1 step: 1735, loss is 0.22386398911476135\n",
      "epoch: 1 step: 1736, loss is 0.20233191549777985\n",
      "epoch: 1 step: 1737, loss is 0.10048973560333252\n",
      "epoch: 1 step: 1738, loss is 0.08021277189254761\n",
      "epoch: 1 step: 1739, loss is 0.049488674849271774\n",
      "epoch: 1 step: 1740, loss is 0.021347980946302414\n",
      "epoch: 1 step: 1741, loss is 0.09842070937156677\n",
      "epoch: 1 step: 1742, loss is 0.22191643714904785\n",
      "epoch: 1 step: 1743, loss is 0.19504375755786896\n",
      "epoch: 1 step: 1744, loss is 0.010939350351691246\n",
      "epoch: 1 step: 1745, loss is 0.15413492918014526\n",
      "epoch: 1 step: 1746, loss is 0.09709382802248001\n",
      "epoch: 1 step: 1747, loss is 0.0450592003762722\n",
      "epoch: 1 step: 1748, loss is 0.05154275521636009\n",
      "epoch: 1 step: 1749, loss is 0.5365538001060486\n",
      "epoch: 1 step: 1750, loss is 0.02195579558610916\n",
      "epoch: 1 step: 1751, loss is 0.14162114262580872\n",
      "epoch: 1 step: 1752, loss is 0.034579258412122726\n",
      "epoch: 1 step: 1753, loss is 0.004634006414562464\n",
      "epoch: 1 step: 1754, loss is 0.01703299582004547\n",
      "epoch: 1 step: 1755, loss is 0.26789310574531555\n",
      "epoch: 1 step: 1756, loss is 0.23540142178535461\n",
      "epoch: 1 step: 1757, loss is 0.05686842277646065\n",
      "epoch: 1 step: 1758, loss is 0.07881428301334381\n",
      "epoch: 1 step: 1759, loss is 0.10924561321735382\n",
      "epoch: 1 step: 1760, loss is 0.3596731126308441\n",
      "epoch: 1 step: 1761, loss is 0.22924095392227173\n",
      "epoch: 1 step: 1762, loss is 0.020963823422789574\n",
      "epoch: 1 step: 1763, loss is 0.21167615056037903\n",
      "epoch: 1 step: 1764, loss is 0.3277391791343689\n",
      "epoch: 1 step: 1765, loss is 0.031815122812986374\n",
      "epoch: 1 step: 1766, loss is 0.07734743505716324\n",
      "epoch: 1 step: 1767, loss is 0.34004315733909607\n",
      "epoch: 1 step: 1768, loss is 0.037050262093544006\n",
      "epoch: 1 step: 1769, loss is 0.13879230618476868\n",
      "epoch: 1 step: 1770, loss is 0.11682053655385971\n",
      "epoch: 1 step: 1771, loss is 0.04716022312641144\n",
      "epoch: 1 step: 1772, loss is 0.08777351677417755\n",
      "epoch: 1 step: 1773, loss is 0.1272154450416565\n",
      "epoch: 1 step: 1774, loss is 0.027850273996591568\n",
      "epoch: 1 step: 1775, loss is 0.010607020929455757\n",
      "epoch: 1 step: 1776, loss is 0.0750245526432991\n",
      "epoch: 1 step: 1777, loss is 0.02392231673002243\n",
      "epoch: 1 step: 1778, loss is 0.16481657326221466\n",
      "epoch: 1 step: 1779, loss is 0.16178640723228455\n",
      "epoch: 1 step: 1780, loss is 0.18771684169769287\n",
      "epoch: 1 step: 1781, loss is 0.023585863411426544\n",
      "epoch: 1 step: 1782, loss is 0.026122063398361206\n",
      "epoch: 1 step: 1783, loss is 0.22454378008842468\n",
      "epoch: 1 step: 1784, loss is 0.01626090332865715\n",
      "epoch: 1 step: 1785, loss is 0.14565761387348175\n",
      "epoch: 1 step: 1786, loss is 0.2914259433746338\n",
      "epoch: 1 step: 1787, loss is 0.13349640369415283\n",
      "epoch: 1 step: 1788, loss is 0.022331666201353073\n",
      "epoch: 1 step: 1789, loss is 0.006964555941522121\n",
      "epoch: 1 step: 1790, loss is 0.16182155907154083\n",
      "epoch: 1 step: 1791, loss is 0.10126899927854538\n",
      "epoch: 1 step: 1792, loss is 0.0956595316529274\n",
      "epoch: 1 step: 1793, loss is 0.15553194284439087\n",
      "epoch: 1 step: 1794, loss is 0.20232737064361572\n",
      "epoch: 1 step: 1795, loss is 0.09766633808612823\n",
      "epoch: 1 step: 1796, loss is 0.22976580262184143\n",
      "epoch: 1 step: 1797, loss is 0.15570573508739471\n",
      "epoch: 1 step: 1798, loss is 0.005054714623838663\n",
      "epoch: 1 step: 1799, loss is 0.03623095154762268\n",
      "epoch: 1 step: 1800, loss is 0.11066711694002151\n",
      "epoch: 1 step: 1801, loss is 0.09022074192762375\n",
      "epoch: 1 step: 1802, loss is 0.11175627261400223\n",
      "epoch: 1 step: 1803, loss is 0.06697042286396027\n",
      "epoch: 1 step: 1804, loss is 0.03732706978917122\n",
      "epoch: 1 step: 1805, loss is 0.08938787877559662\n",
      "epoch: 1 step: 1806, loss is 0.08445930480957031\n",
      "epoch: 1 step: 1807, loss is 0.03403906151652336\n",
      "epoch: 1 step: 1808, loss is 0.011133166961371899\n",
      "epoch: 1 step: 1809, loss is 0.189039409160614\n",
      "epoch: 1 step: 1810, loss is 0.20667831599712372\n",
      "epoch: 1 step: 1811, loss is 0.33874133229255676\n",
      "epoch: 1 step: 1812, loss is 0.04779324680566788\n",
      "epoch: 1 step: 1813, loss is 0.1685963124036789\n",
      "epoch: 1 step: 1814, loss is 0.04443831369280815\n",
      "epoch: 1 step: 1815, loss is 0.006087133660912514\n",
      "epoch: 1 step: 1816, loss is 0.088048554956913\n",
      "epoch: 1 step: 1817, loss is 0.1948135495185852\n",
      "epoch: 1 step: 1818, loss is 0.050636596977710724\n",
      "epoch: 1 step: 1819, loss is 0.13304518163204193\n",
      "epoch: 1 step: 1820, loss is 0.0939645767211914\n",
      "epoch: 1 step: 1821, loss is 0.06395566463470459\n",
      "epoch: 1 step: 1822, loss is 0.05169416964054108\n",
      "epoch: 1 step: 1823, loss is 0.021039435639977455\n",
      "epoch: 1 step: 1824, loss is 0.29050499200820923\n",
      "epoch: 1 step: 1825, loss is 0.06447316706180573\n",
      "epoch: 1 step: 1826, loss is 0.06313849985599518\n",
      "epoch: 1 step: 1827, loss is 0.026379257440567017\n",
      "epoch: 1 step: 1828, loss is 0.10272664576768875\n",
      "epoch: 1 step: 1829, loss is 0.20327307283878326\n",
      "epoch: 1 step: 1830, loss is 0.10033643245697021\n",
      "epoch: 1 step: 1831, loss is 0.15879783034324646\n",
      "epoch: 1 step: 1832, loss is 0.12490212172269821\n",
      "epoch: 1 step: 1833, loss is 0.08944594860076904\n",
      "epoch: 1 step: 1834, loss is 0.0048981038853526115\n",
      "epoch: 1 step: 1835, loss is 0.01380553375929594\n",
      "epoch: 1 step: 1836, loss is 0.05042312294244766\n",
      "epoch: 1 step: 1837, loss is 0.030598919838666916\n",
      "epoch: 1 step: 1838, loss is 0.0065278285183012486\n",
      "epoch: 1 step: 1839, loss is 0.02762255258858204\n",
      "epoch: 1 step: 1840, loss is 0.016987735405564308\n",
      "epoch: 1 step: 1841, loss is 0.21695955097675323\n",
      "epoch: 1 step: 1842, loss is 0.09108810126781464\n",
      "epoch: 1 step: 1843, loss is 0.02798323705792427\n",
      "epoch: 1 step: 1844, loss is 0.035951223224401474\n",
      "epoch: 1 step: 1845, loss is 0.28198009729385376\n",
      "epoch: 1 step: 1846, loss is 0.0925353616476059\n",
      "epoch: 1 step: 1847, loss is 0.03739432245492935\n",
      "epoch: 1 step: 1848, loss is 0.31870537996292114\n",
      "epoch: 1 step: 1849, loss is 0.024640724062919617\n",
      "epoch: 1 step: 1850, loss is 0.028046300634741783\n",
      "epoch: 1 step: 1851, loss is 0.04221244528889656\n",
      "epoch: 1 step: 1852, loss is 0.3232841193675995\n",
      "epoch: 1 step: 1853, loss is 0.004752232227474451\n",
      "epoch: 1 step: 1854, loss is 0.017638443037867546\n",
      "epoch: 1 step: 1855, loss is 0.07672501355409622\n",
      "epoch: 1 step: 1856, loss is 0.04819716140627861\n",
      "epoch: 1 step: 1857, loss is 0.07797607779502869\n",
      "epoch: 1 step: 1858, loss is 0.011084254831075668\n",
      "epoch: 1 step: 1859, loss is 0.2228754609823227\n",
      "epoch: 1 step: 1860, loss is 0.011199447326362133\n",
      "epoch: 1 step: 1861, loss is 0.05751628056168556\n",
      "epoch: 1 step: 1862, loss is 0.24230550229549408\n",
      "epoch: 1 step: 1863, loss is 0.044158924371004105\n",
      "epoch: 1 step: 1864, loss is 0.01596258021891117\n",
      "epoch: 1 step: 1865, loss is 0.03735588118433952\n",
      "epoch: 1 step: 1866, loss is 0.032925862818956375\n",
      "epoch: 1 step: 1867, loss is 0.007508325856178999\n",
      "epoch: 1 step: 1868, loss is 0.012294248677790165\n",
      "epoch: 1 step: 1869, loss is 0.006329285446554422\n",
      "epoch: 1 step: 1870, loss is 0.007961569353938103\n",
      "epoch: 1 step: 1871, loss is 0.10346627235412598\n",
      "epoch: 1 step: 1872, loss is 0.20497334003448486\n",
      "epoch: 1 step: 1873, loss is 0.056049853563308716\n",
      "epoch: 1 step: 1874, loss is 0.016559641808271408\n",
      "epoch: 1 step: 1875, loss is 0.12981373071670532\n",
      "epoch: 2 step: 1, loss is 0.12641172111034393\n",
      "epoch: 2 step: 2, loss is 0.05885744467377663\n",
      "epoch: 2 step: 3, loss is 0.10214117169380188\n",
      "epoch: 2 step: 4, loss is 0.10830137878656387\n",
      "epoch: 2 step: 5, loss is 0.17182153463363647\n",
      "epoch: 2 step: 6, loss is 0.232630655169487\n",
      "epoch: 2 step: 7, loss is 0.006953391712158918\n",
      "epoch: 2 step: 8, loss is 0.07477008551359177\n",
      "epoch: 2 step: 9, loss is 0.13873055577278137\n",
      "epoch: 2 step: 10, loss is 0.04435836151242256\n",
      "epoch: 2 step: 11, loss is 0.12713856995105743\n",
      "epoch: 2 step: 12, loss is 0.15738292038440704\n",
      "epoch: 2 step: 13, loss is 0.10654526948928833\n",
      "epoch: 2 step: 14, loss is 0.014301320537924767\n",
      "epoch: 2 step: 15, loss is 0.08849889039993286\n",
      "epoch: 2 step: 16, loss is 0.02525160275399685\n",
      "epoch: 2 step: 17, loss is 0.053846150636672974\n",
      "epoch: 2 step: 18, loss is 0.03336179256439209\n",
      "epoch: 2 step: 19, loss is 0.04821108654141426\n",
      "epoch: 2 step: 20, loss is 0.07069554924964905\n",
      "epoch: 2 step: 21, loss is 0.1239342987537384\n",
      "epoch: 2 step: 22, loss is 0.08418278396129608\n",
      "epoch: 2 step: 23, loss is 0.021799730136990547\n",
      "epoch: 2 step: 24, loss is 0.01025925762951374\n",
      "epoch: 2 step: 25, loss is 0.07357966899871826\n",
      "epoch: 2 step: 26, loss is 0.0477822870016098\n",
      "epoch: 2 step: 27, loss is 0.23315837979316711\n",
      "epoch: 2 step: 28, loss is 0.16212157905101776\n",
      "epoch: 2 step: 29, loss is 0.1613280028104782\n",
      "epoch: 2 step: 30, loss is 0.013668847270309925\n",
      "epoch: 2 step: 31, loss is 0.00405914057046175\n",
      "epoch: 2 step: 32, loss is 0.20456938445568085\n",
      "epoch: 2 step: 33, loss is 0.04156043007969856\n",
      "epoch: 2 step: 34, loss is 0.27052611112594604\n",
      "epoch: 2 step: 35, loss is 0.17043820023536682\n",
      "epoch: 2 step: 36, loss is 0.007849249988794327\n",
      "epoch: 2 step: 37, loss is 0.004974348936229944\n",
      "epoch: 2 step: 38, loss is 0.2900046706199646\n",
      "epoch: 2 step: 39, loss is 0.04197420924901962\n",
      "epoch: 2 step: 40, loss is 0.0732455924153328\n",
      "epoch: 2 step: 41, loss is 0.11119592934846878\n",
      "epoch: 2 step: 42, loss is 0.022267278283834457\n",
      "epoch: 2 step: 43, loss is 0.43373778462409973\n",
      "epoch: 2 step: 44, loss is 0.05353473871946335\n",
      "epoch: 2 step: 45, loss is 0.03406798467040062\n",
      "epoch: 2 step: 46, loss is 0.028540493920445442\n",
      "epoch: 2 step: 47, loss is 0.1430736780166626\n",
      "epoch: 2 step: 48, loss is 0.12644731998443604\n",
      "epoch: 2 step: 49, loss is 0.09784379601478577\n",
      "epoch: 2 step: 50, loss is 0.019311541691422462\n",
      "epoch: 2 step: 51, loss is 0.0438825860619545\n",
      "epoch: 2 step: 52, loss is 0.02162252552807331\n",
      "epoch: 2 step: 53, loss is 0.05776619911193848\n",
      "epoch: 2 step: 54, loss is 0.020692165940999985\n",
      "epoch: 2 step: 55, loss is 0.22703507542610168\n",
      "epoch: 2 step: 56, loss is 0.16823819279670715\n",
      "epoch: 2 step: 57, loss is 0.15318730473518372\n",
      "epoch: 2 step: 58, loss is 0.02320205606520176\n",
      "epoch: 2 step: 59, loss is 0.27646929025650024\n",
      "epoch: 2 step: 60, loss is 0.08937431871891022\n",
      "epoch: 2 step: 61, loss is 0.032627373933792114\n",
      "epoch: 2 step: 62, loss is 0.10552395135164261\n",
      "epoch: 2 step: 63, loss is 0.01861148700118065\n",
      "epoch: 2 step: 64, loss is 0.047088298946619034\n",
      "epoch: 2 step: 65, loss is 0.027871742844581604\n",
      "epoch: 2 step: 66, loss is 0.026234913617372513\n",
      "epoch: 2 step: 67, loss is 0.12763869762420654\n",
      "epoch: 2 step: 68, loss is 0.0251708272844553\n",
      "epoch: 2 step: 69, loss is 0.3571561872959137\n",
      "epoch: 2 step: 70, loss is 0.10171952843666077\n",
      "epoch: 2 step: 71, loss is 0.010684824548661709\n",
      "epoch: 2 step: 72, loss is 0.424791544675827\n",
      "epoch: 2 step: 73, loss is 0.013128526508808136\n",
      "epoch: 2 step: 74, loss is 0.13853341341018677\n",
      "epoch: 2 step: 75, loss is 0.11883638799190521\n",
      "epoch: 2 step: 76, loss is 0.14560310542583466\n",
      "epoch: 2 step: 77, loss is 0.03397553786635399\n",
      "epoch: 2 step: 78, loss is 0.050956908613443375\n",
      "epoch: 2 step: 79, loss is 0.06858278065919876\n",
      "epoch: 2 step: 80, loss is 0.10116579383611679\n",
      "epoch: 2 step: 81, loss is 0.030366891995072365\n",
      "epoch: 2 step: 82, loss is 0.1130661591887474\n",
      "epoch: 2 step: 83, loss is 0.014189748093485832\n",
      "epoch: 2 step: 84, loss is 0.03287631645798683\n",
      "epoch: 2 step: 85, loss is 0.01917158253490925\n",
      "epoch: 2 step: 86, loss is 0.01635497435927391\n",
      "epoch: 2 step: 87, loss is 0.04550884664058685\n",
      "epoch: 2 step: 88, loss is 0.06428372859954834\n",
      "epoch: 2 step: 89, loss is 0.0906769186258316\n",
      "epoch: 2 step: 90, loss is 0.05818367749452591\n",
      "epoch: 2 step: 91, loss is 0.023664698004722595\n",
      "epoch: 2 step: 92, loss is 0.008403755724430084\n",
      "epoch: 2 step: 93, loss is 0.02341625466942787\n",
      "epoch: 2 step: 94, loss is 0.04617704823613167\n",
      "epoch: 2 step: 95, loss is 0.007188466377556324\n",
      "epoch: 2 step: 96, loss is 0.13242964446544647\n",
      "epoch: 2 step: 97, loss is 0.06999649852514267\n",
      "epoch: 2 step: 98, loss is 0.009461789391934872\n",
      "epoch: 2 step: 99, loss is 0.03638024255633354\n",
      "epoch: 2 step: 100, loss is 0.1281442642211914\n",
      "epoch: 2 step: 101, loss is 0.08940296620130539\n",
      "epoch: 2 step: 102, loss is 0.05330934748053551\n",
      "epoch: 2 step: 103, loss is 0.06720020622015\n",
      "epoch: 2 step: 104, loss is 0.09707605093717575\n",
      "epoch: 2 step: 105, loss is 0.027810541912913322\n",
      "epoch: 2 step: 106, loss is 0.06117086112499237\n",
      "epoch: 2 step: 107, loss is 0.04095650464296341\n",
      "epoch: 2 step: 108, loss is 0.20753230154514313\n",
      "epoch: 2 step: 109, loss is 0.013635694980621338\n",
      "epoch: 2 step: 110, loss is 0.04931372031569481\n",
      "epoch: 2 step: 111, loss is 0.1280752271413803\n",
      "epoch: 2 step: 112, loss is 0.009913038462400436\n",
      "epoch: 2 step: 113, loss is 0.010185645893216133\n",
      "epoch: 2 step: 114, loss is 0.20673392713069916\n",
      "epoch: 2 step: 115, loss is 0.05405861511826515\n",
      "epoch: 2 step: 116, loss is 0.044243138283491135\n",
      "epoch: 2 step: 117, loss is 0.0025533544830977917\n",
      "epoch: 2 step: 118, loss is 0.25921207666397095\n",
      "epoch: 2 step: 119, loss is 0.1505831927061081\n",
      "epoch: 2 step: 120, loss is 0.016282442957162857\n",
      "epoch: 2 step: 121, loss is 0.09417784214019775\n",
      "epoch: 2 step: 122, loss is 0.26511579751968384\n",
      "epoch: 2 step: 123, loss is 0.08753030747175217\n",
      "epoch: 2 step: 124, loss is 0.03872844949364662\n",
      "epoch: 2 step: 125, loss is 0.03131921961903572\n",
      "epoch: 2 step: 126, loss is 0.00934397242963314\n",
      "epoch: 2 step: 127, loss is 0.08263338357210159\n",
      "epoch: 2 step: 128, loss is 0.017014771699905396\n",
      "epoch: 2 step: 129, loss is 0.07723479717969894\n",
      "epoch: 2 step: 130, loss is 0.05724388733506203\n",
      "epoch: 2 step: 131, loss is 0.37556037306785583\n",
      "epoch: 2 step: 132, loss is 0.0454525351524353\n",
      "epoch: 2 step: 133, loss is 0.008053136058151722\n",
      "epoch: 2 step: 134, loss is 0.027652274817228317\n",
      "epoch: 2 step: 135, loss is 0.1939435452222824\n",
      "epoch: 2 step: 136, loss is 0.020019255578517914\n",
      "epoch: 2 step: 137, loss is 0.055686019361019135\n",
      "epoch: 2 step: 138, loss is 0.06648639589548111\n",
      "epoch: 2 step: 139, loss is 0.008165126666426659\n",
      "epoch: 2 step: 140, loss is 0.03668910264968872\n",
      "epoch: 2 step: 141, loss is 0.25206294655799866\n",
      "epoch: 2 step: 142, loss is 0.04693593084812164\n",
      "epoch: 2 step: 143, loss is 0.06430986523628235\n",
      "epoch: 2 step: 144, loss is 0.03876008838415146\n",
      "epoch: 2 step: 145, loss is 0.13746611773967743\n",
      "epoch: 2 step: 146, loss is 0.011126253753900528\n",
      "epoch: 2 step: 147, loss is 0.002832540310919285\n",
      "epoch: 2 step: 148, loss is 0.024687154218554497\n",
      "epoch: 2 step: 149, loss is 0.07717666774988174\n",
      "epoch: 2 step: 150, loss is 0.033463701605796814\n",
      "epoch: 2 step: 151, loss is 0.05487264692783356\n",
      "epoch: 2 step: 152, loss is 0.042498715221881866\n",
      "epoch: 2 step: 153, loss is 0.009507602080702782\n",
      "epoch: 2 step: 154, loss is 0.009874426759779453\n",
      "epoch: 2 step: 155, loss is 0.07790637016296387\n",
      "epoch: 2 step: 156, loss is 0.07430445402860641\n",
      "epoch: 2 step: 157, loss is 0.03528133034706116\n",
      "epoch: 2 step: 158, loss is 0.03572956472635269\n",
      "epoch: 2 step: 159, loss is 0.08146224170923233\n",
      "epoch: 2 step: 160, loss is 0.05515173450112343\n",
      "epoch: 2 step: 161, loss is 0.00933010783046484\n",
      "epoch: 2 step: 162, loss is 0.21356849372386932\n",
      "epoch: 2 step: 163, loss is 0.0032355135772377253\n",
      "epoch: 2 step: 164, loss is 0.038760095834732056\n",
      "epoch: 2 step: 165, loss is 0.13035733997821808\n",
      "epoch: 2 step: 166, loss is 0.00904904492199421\n",
      "epoch: 2 step: 167, loss is 0.010804200544953346\n",
      "epoch: 2 step: 168, loss is 0.09096404910087585\n",
      "epoch: 2 step: 169, loss is 0.002189530758187175\n",
      "epoch: 2 step: 170, loss is 0.024945354089140892\n",
      "epoch: 2 step: 171, loss is 0.09279416501522064\n",
      "epoch: 2 step: 172, loss is 0.009020566008985043\n",
      "epoch: 2 step: 173, loss is 0.021691787987947464\n",
      "epoch: 2 step: 174, loss is 0.1319989264011383\n",
      "epoch: 2 step: 175, loss is 0.011200569570064545\n",
      "epoch: 2 step: 176, loss is 0.026029856875538826\n",
      "epoch: 2 step: 177, loss is 0.06732382625341415\n",
      "epoch: 2 step: 178, loss is 0.02310842275619507\n",
      "epoch: 2 step: 179, loss is 0.08245861530303955\n",
      "epoch: 2 step: 180, loss is 0.09264626353979111\n",
      "epoch: 2 step: 181, loss is 0.1272798627614975\n",
      "epoch: 2 step: 182, loss is 0.016163349151611328\n",
      "epoch: 2 step: 183, loss is 0.011866077780723572\n",
      "epoch: 2 step: 184, loss is 0.006595953833311796\n",
      "epoch: 2 step: 185, loss is 0.036480192095041275\n",
      "epoch: 2 step: 186, loss is 0.13771773874759674\n",
      "epoch: 2 step: 187, loss is 0.24038301408290863\n",
      "epoch: 2 step: 188, loss is 0.007547542452812195\n",
      "epoch: 2 step: 189, loss is 0.07861018925905228\n",
      "epoch: 2 step: 190, loss is 0.13028155267238617\n",
      "epoch: 2 step: 191, loss is 0.16862264275550842\n",
      "epoch: 2 step: 192, loss is 0.08228379487991333\n",
      "epoch: 2 step: 193, loss is 0.0010020709596574306\n",
      "epoch: 2 step: 194, loss is 0.04459851607680321\n",
      "epoch: 2 step: 195, loss is 0.01647227816283703\n",
      "epoch: 2 step: 196, loss is 0.006161099299788475\n",
      "epoch: 2 step: 197, loss is 0.07804674655199051\n",
      "epoch: 2 step: 198, loss is 0.41441911458969116\n",
      "epoch: 2 step: 199, loss is 0.036450259387493134\n",
      "epoch: 2 step: 200, loss is 0.053645603358745575\n",
      "epoch: 2 step: 201, loss is 0.11022982001304626\n",
      "epoch: 2 step: 202, loss is 0.01221608929336071\n",
      "epoch: 2 step: 203, loss is 0.12273111194372177\n",
      "epoch: 2 step: 204, loss is 0.005737190134823322\n",
      "epoch: 2 step: 205, loss is 0.013982502743601799\n",
      "epoch: 2 step: 206, loss is 0.05349982529878616\n",
      "epoch: 2 step: 207, loss is 0.055077917873859406\n",
      "epoch: 2 step: 208, loss is 0.052474815398454666\n",
      "epoch: 2 step: 209, loss is 0.006494342815130949\n",
      "epoch: 2 step: 210, loss is 0.01622825115919113\n",
      "epoch: 2 step: 211, loss is 0.10100384056568146\n",
      "epoch: 2 step: 212, loss is 0.03247825428843498\n",
      "epoch: 2 step: 213, loss is 0.029144320636987686\n",
      "epoch: 2 step: 214, loss is 0.07003019750118256\n",
      "epoch: 2 step: 215, loss is 0.024021722376346588\n",
      "epoch: 2 step: 216, loss is 0.05189652368426323\n",
      "epoch: 2 step: 217, loss is 0.030005760490894318\n",
      "epoch: 2 step: 218, loss is 0.025647714734077454\n",
      "epoch: 2 step: 219, loss is 0.07417336106300354\n",
      "epoch: 2 step: 220, loss is 0.015280217863619328\n",
      "epoch: 2 step: 221, loss is 0.06597775965929031\n",
      "epoch: 2 step: 222, loss is 0.1877438724040985\n",
      "epoch: 2 step: 223, loss is 0.09606734663248062\n",
      "epoch: 2 step: 224, loss is 0.005095085594803095\n",
      "epoch: 2 step: 225, loss is 0.0027581106405705214\n",
      "epoch: 2 step: 226, loss is 0.2158380001783371\n",
      "epoch: 2 step: 227, loss is 0.014382007531821728\n",
      "epoch: 2 step: 228, loss is 0.04350854828953743\n",
      "epoch: 2 step: 229, loss is 0.1381129026412964\n",
      "epoch: 2 step: 230, loss is 0.01640118658542633\n",
      "epoch: 2 step: 231, loss is 0.11790329217910767\n",
      "epoch: 2 step: 232, loss is 0.01033678650856018\n",
      "epoch: 2 step: 233, loss is 0.032919008284807205\n",
      "epoch: 2 step: 234, loss is 0.04252232238650322\n",
      "epoch: 2 step: 235, loss is 0.010106245055794716\n",
      "epoch: 2 step: 236, loss is 0.2456514686346054\n",
      "epoch: 2 step: 237, loss is 0.0009855080861598253\n",
      "epoch: 2 step: 238, loss is 0.08005433529615402\n",
      "epoch: 2 step: 239, loss is 0.10299771279096603\n",
      "epoch: 2 step: 240, loss is 0.06307187676429749\n",
      "epoch: 2 step: 241, loss is 0.11268524825572968\n",
      "epoch: 2 step: 242, loss is 0.11478615552186966\n",
      "epoch: 2 step: 243, loss is 0.006599247921258211\n",
      "epoch: 2 step: 244, loss is 0.0050647505559027195\n",
      "epoch: 2 step: 245, loss is 0.005633596330881119\n",
      "epoch: 2 step: 246, loss is 0.14381206035614014\n",
      "epoch: 2 step: 247, loss is 0.05250765010714531\n",
      "epoch: 2 step: 248, loss is 0.0023051826283335686\n",
      "epoch: 2 step: 249, loss is 0.022052176296710968\n",
      "epoch: 2 step: 250, loss is 0.1323915272951126\n",
      "epoch: 2 step: 251, loss is 0.15014150738716125\n",
      "epoch: 2 step: 252, loss is 0.1646099090576172\n",
      "epoch: 2 step: 253, loss is 0.15788258612155914\n",
      "epoch: 2 step: 254, loss is 0.2501634955406189\n",
      "epoch: 2 step: 255, loss is 0.25166240334510803\n",
      "epoch: 2 step: 256, loss is 0.20113326609134674\n",
      "epoch: 2 step: 257, loss is 0.015225807204842567\n",
      "epoch: 2 step: 258, loss is 0.02405793033540249\n",
      "epoch: 2 step: 259, loss is 0.029933122918009758\n",
      "epoch: 2 step: 260, loss is 0.21853426098823547\n",
      "epoch: 2 step: 261, loss is 0.2754465937614441\n",
      "epoch: 2 step: 262, loss is 0.039498865604400635\n",
      "epoch: 2 step: 263, loss is 0.07814962416887283\n",
      "epoch: 2 step: 264, loss is 0.12367750704288483\n",
      "epoch: 2 step: 265, loss is 0.11774123460054398\n",
      "epoch: 2 step: 266, loss is 0.021969664841890335\n",
      "epoch: 2 step: 267, loss is 0.0055405143648386\n",
      "epoch: 2 step: 268, loss is 0.01550402119755745\n",
      "epoch: 2 step: 269, loss is 0.011237084865570068\n",
      "epoch: 2 step: 270, loss is 0.059001341462135315\n",
      "epoch: 2 step: 271, loss is 0.008902995847165585\n",
      "epoch: 2 step: 272, loss is 0.05870497599244118\n",
      "epoch: 2 step: 273, loss is 0.024987690150737762\n",
      "epoch: 2 step: 274, loss is 0.007824058644473553\n",
      "epoch: 2 step: 275, loss is 0.07532034069299698\n",
      "epoch: 2 step: 276, loss is 0.1008160263299942\n",
      "epoch: 2 step: 277, loss is 0.021144449710845947\n",
      "epoch: 2 step: 278, loss is 0.006595288403332233\n",
      "epoch: 2 step: 279, loss is 0.028108377009630203\n",
      "epoch: 2 step: 280, loss is 0.18628722429275513\n",
      "epoch: 2 step: 281, loss is 0.21037714183330536\n",
      "epoch: 2 step: 282, loss is 0.11719338595867157\n",
      "epoch: 2 step: 283, loss is 0.06947435438632965\n",
      "epoch: 2 step: 284, loss is 0.03399476408958435\n",
      "epoch: 2 step: 285, loss is 0.0745067223906517\n",
      "epoch: 2 step: 286, loss is 0.13834144175052643\n",
      "epoch: 2 step: 287, loss is 0.036639440804719925\n",
      "epoch: 2 step: 288, loss is 0.05846145749092102\n",
      "epoch: 2 step: 289, loss is 0.021285779774188995\n",
      "epoch: 2 step: 290, loss is 0.02230270951986313\n",
      "epoch: 2 step: 291, loss is 0.12620949745178223\n",
      "epoch: 2 step: 292, loss is 0.1444646418094635\n",
      "epoch: 2 step: 293, loss is 0.026161203160881996\n",
      "epoch: 2 step: 294, loss is 0.013257166370749474\n",
      "epoch: 2 step: 295, loss is 0.039828017354011536\n",
      "epoch: 2 step: 296, loss is 0.06093771010637283\n",
      "epoch: 2 step: 297, loss is 0.03575550392270088\n",
      "epoch: 2 step: 298, loss is 0.19508284330368042\n",
      "epoch: 2 step: 299, loss is 0.01512698270380497\n",
      "epoch: 2 step: 300, loss is 0.010234004817903042\n",
      "epoch: 2 step: 301, loss is 0.023140374571084976\n",
      "epoch: 2 step: 302, loss is 0.09593258053064346\n",
      "epoch: 2 step: 303, loss is 0.2139829397201538\n",
      "epoch: 2 step: 304, loss is 0.013541476801037788\n",
      "epoch: 2 step: 305, loss is 0.021094325929880142\n",
      "epoch: 2 step: 306, loss is 0.058316998183727264\n",
      "epoch: 2 step: 307, loss is 0.16077086329460144\n",
      "epoch: 2 step: 308, loss is 0.08790574222803116\n",
      "epoch: 2 step: 309, loss is 0.043115414679050446\n",
      "epoch: 2 step: 310, loss is 0.023836975917220116\n",
      "epoch: 2 step: 311, loss is 0.09052640944719315\n",
      "epoch: 2 step: 312, loss is 0.04162335395812988\n",
      "epoch: 2 step: 313, loss is 0.034361664205789566\n",
      "epoch: 2 step: 314, loss is 0.0998300164937973\n",
      "epoch: 2 step: 315, loss is 0.08664682507514954\n",
      "epoch: 2 step: 316, loss is 0.005662077106535435\n",
      "epoch: 2 step: 317, loss is 0.003235520562157035\n",
      "epoch: 2 step: 318, loss is 0.1220305934548378\n",
      "epoch: 2 step: 319, loss is 0.04021811485290527\n",
      "epoch: 2 step: 320, loss is 0.042024724185466766\n",
      "epoch: 2 step: 321, loss is 0.15702399611473083\n",
      "epoch: 2 step: 322, loss is 0.13853882253170013\n",
      "epoch: 2 step: 323, loss is 0.048659902065992355\n",
      "epoch: 2 step: 324, loss is 0.0710521936416626\n",
      "epoch: 2 step: 325, loss is 0.0868191197514534\n",
      "epoch: 2 step: 326, loss is 0.2774184048175812\n",
      "epoch: 2 step: 327, loss is 0.29626479744911194\n",
      "epoch: 2 step: 328, loss is 0.22495312988758087\n",
      "epoch: 2 step: 329, loss is 0.009306508116424084\n",
      "epoch: 2 step: 330, loss is 0.02352914772927761\n",
      "epoch: 2 step: 331, loss is 0.19207946956157684\n",
      "epoch: 2 step: 332, loss is 0.12347725033760071\n",
      "epoch: 2 step: 333, loss is 0.008826292119920254\n",
      "epoch: 2 step: 334, loss is 0.018280891701579094\n",
      "epoch: 2 step: 335, loss is 0.23539432883262634\n",
      "epoch: 2 step: 336, loss is 0.19439032673835754\n",
      "epoch: 2 step: 337, loss is 0.11014460027217865\n",
      "epoch: 2 step: 338, loss is 0.22880138456821442\n",
      "epoch: 2 step: 339, loss is 0.33680400252342224\n",
      "epoch: 2 step: 340, loss is 0.25165387988090515\n",
      "epoch: 2 step: 341, loss is 0.04926986247301102\n",
      "epoch: 2 step: 342, loss is 0.0224602110683918\n",
      "epoch: 2 step: 343, loss is 0.09266705811023712\n",
      "epoch: 2 step: 344, loss is 0.186128631234169\n",
      "epoch: 2 step: 345, loss is 0.09525305777788162\n",
      "epoch: 2 step: 346, loss is 0.021345142275094986\n",
      "epoch: 2 step: 347, loss is 0.014479556120932102\n",
      "epoch: 2 step: 348, loss is 0.14071756601333618\n",
      "epoch: 2 step: 349, loss is 0.0240363460034132\n",
      "epoch: 2 step: 350, loss is 0.0798555538058281\n",
      "epoch: 2 step: 351, loss is 0.014717712998390198\n",
      "epoch: 2 step: 352, loss is 0.09811405092477798\n",
      "epoch: 2 step: 353, loss is 0.08359126001596451\n",
      "epoch: 2 step: 354, loss is 0.020007945597171783\n",
      "epoch: 2 step: 355, loss is 0.1492789387702942\n",
      "epoch: 2 step: 356, loss is 0.14669851958751678\n",
      "epoch: 2 step: 357, loss is 0.059521015733480453\n",
      "epoch: 2 step: 358, loss is 0.10864601284265518\n",
      "epoch: 2 step: 359, loss is 0.026434658095240593\n",
      "epoch: 2 step: 360, loss is 0.43677353858947754\n",
      "epoch: 2 step: 361, loss is 0.22202709317207336\n",
      "epoch: 2 step: 362, loss is 0.10324058681726456\n",
      "epoch: 2 step: 363, loss is 0.06536390632390976\n",
      "epoch: 2 step: 364, loss is 0.010897278785705566\n",
      "epoch: 2 step: 365, loss is 0.10983248800039291\n",
      "epoch: 2 step: 366, loss is 0.1493256688117981\n",
      "epoch: 2 step: 367, loss is 0.10690134763717651\n",
      "epoch: 2 step: 368, loss is 0.07401643693447113\n",
      "epoch: 2 step: 369, loss is 0.045577675104141235\n",
      "epoch: 2 step: 370, loss is 0.05867694318294525\n",
      "epoch: 2 step: 371, loss is 0.010327375493943691\n",
      "epoch: 2 step: 372, loss is 0.11649353802204132\n",
      "epoch: 2 step: 373, loss is 0.013279330916702747\n",
      "epoch: 2 step: 374, loss is 0.1559668630361557\n",
      "epoch: 2 step: 375, loss is 0.012753716669976711\n",
      "epoch: 2 step: 376, loss is 0.012642547488212585\n",
      "epoch: 2 step: 377, loss is 0.008598425425589085\n",
      "epoch: 2 step: 378, loss is 0.04594483599066734\n",
      "epoch: 2 step: 379, loss is 0.07868953794240952\n",
      "epoch: 2 step: 380, loss is 0.036580659449100494\n",
      "epoch: 2 step: 381, loss is 0.10721978545188904\n",
      "epoch: 2 step: 382, loss is 0.058023758232593536\n",
      "epoch: 2 step: 383, loss is 0.00911609549075365\n",
      "epoch: 2 step: 384, loss is 0.09289926290512085\n",
      "epoch: 2 step: 385, loss is 0.09269079566001892\n",
      "epoch: 2 step: 386, loss is 0.008644276298582554\n",
      "epoch: 2 step: 387, loss is 0.04684723541140556\n",
      "epoch: 2 step: 388, loss is 0.2268209606409073\n",
      "epoch: 2 step: 389, loss is 0.009041573852300644\n",
      "epoch: 2 step: 390, loss is 0.04271871596574783\n",
      "epoch: 2 step: 391, loss is 0.0011398125207051635\n",
      "epoch: 2 step: 392, loss is 0.028009532019495964\n",
      "epoch: 2 step: 393, loss is 0.07261822372674942\n",
      "epoch: 2 step: 394, loss is 0.050856027752161026\n",
      "epoch: 2 step: 395, loss is 0.15688399970531464\n",
      "epoch: 2 step: 396, loss is 0.021851548925042152\n",
      "epoch: 2 step: 397, loss is 0.00421688100323081\n",
      "epoch: 2 step: 398, loss is 0.14743325114250183\n",
      "epoch: 2 step: 399, loss is 0.06048835441470146\n",
      "epoch: 2 step: 400, loss is 0.09253633767366409\n",
      "epoch: 2 step: 401, loss is 0.09224940091371536\n",
      "epoch: 2 step: 402, loss is 0.03209992125630379\n",
      "epoch: 2 step: 403, loss is 0.25568917393684387\n",
      "epoch: 2 step: 404, loss is 0.13699708878993988\n",
      "epoch: 2 step: 405, loss is 0.008709397166967392\n",
      "epoch: 2 step: 406, loss is 0.10911031812429428\n",
      "epoch: 2 step: 407, loss is 0.017147768288850784\n",
      "epoch: 2 step: 408, loss is 0.013399528339505196\n",
      "epoch: 2 step: 409, loss is 0.0011703139171004295\n",
      "epoch: 2 step: 410, loss is 0.10818792879581451\n",
      "epoch: 2 step: 411, loss is 0.28363874554634094\n",
      "epoch: 2 step: 412, loss is 0.01494208537042141\n",
      "epoch: 2 step: 413, loss is 0.02488020434975624\n",
      "epoch: 2 step: 414, loss is 0.12708187103271484\n",
      "epoch: 2 step: 415, loss is 0.05685011297464371\n",
      "epoch: 2 step: 416, loss is 0.10885238647460938\n",
      "epoch: 2 step: 417, loss is 0.017434444278478622\n",
      "epoch: 2 step: 418, loss is 0.028908787295222282\n",
      "epoch: 2 step: 419, loss is 0.2155774086713791\n",
      "epoch: 2 step: 420, loss is 0.11693000793457031\n",
      "epoch: 2 step: 421, loss is 0.17998678982257843\n",
      "epoch: 2 step: 422, loss is 0.11199694871902466\n",
      "epoch: 2 step: 423, loss is 0.024623943492770195\n",
      "epoch: 2 step: 424, loss is 0.14510753750801086\n",
      "epoch: 2 step: 425, loss is 0.05617875978350639\n",
      "epoch: 2 step: 426, loss is 0.005680429749190807\n",
      "epoch: 2 step: 427, loss is 0.05587990581989288\n",
      "epoch: 2 step: 428, loss is 0.1579466015100479\n",
      "epoch: 2 step: 429, loss is 0.01594667322933674\n",
      "epoch: 2 step: 430, loss is 0.11162415146827698\n",
      "epoch: 2 step: 431, loss is 0.22349296510219574\n",
      "epoch: 2 step: 432, loss is 0.02536880597472191\n",
      "epoch: 2 step: 433, loss is 0.17546939849853516\n",
      "epoch: 2 step: 434, loss is 0.04105690121650696\n",
      "epoch: 2 step: 435, loss is 0.1399584710597992\n",
      "epoch: 2 step: 436, loss is 0.12407947331666946\n",
      "epoch: 2 step: 437, loss is 0.03228944167494774\n",
      "epoch: 2 step: 438, loss is 0.021465029567480087\n",
      "epoch: 2 step: 439, loss is 0.0023885052651166916\n",
      "epoch: 2 step: 440, loss is 0.026504211127758026\n",
      "epoch: 2 step: 441, loss is 0.015255911275744438\n",
      "epoch: 2 step: 442, loss is 0.05727290362119675\n",
      "epoch: 2 step: 443, loss is 0.1533811390399933\n",
      "epoch: 2 step: 444, loss is 0.0182520542293787\n",
      "epoch: 2 step: 445, loss is 0.1398129016160965\n",
      "epoch: 2 step: 446, loss is 0.09512031823396683\n",
      "epoch: 2 step: 447, loss is 0.003542150603607297\n",
      "epoch: 2 step: 448, loss is 0.002034784760326147\n",
      "epoch: 2 step: 449, loss is 0.00799611397087574\n",
      "epoch: 2 step: 450, loss is 0.06682275980710983\n",
      "epoch: 2 step: 451, loss is 0.12442850321531296\n",
      "epoch: 2 step: 452, loss is 0.24797585606575012\n",
      "epoch: 2 step: 453, loss is 0.0382368266582489\n",
      "epoch: 2 step: 454, loss is 0.03458355739712715\n",
      "epoch: 2 step: 455, loss is 0.027654089033603668\n",
      "epoch: 2 step: 456, loss is 0.17536279559135437\n",
      "epoch: 2 step: 457, loss is 0.01549477782100439\n",
      "epoch: 2 step: 458, loss is 0.008919669315218925\n",
      "epoch: 2 step: 459, loss is 0.005354087334126234\n",
      "epoch: 2 step: 460, loss is 0.07616303116083145\n",
      "epoch: 2 step: 461, loss is 0.05317133665084839\n",
      "epoch: 2 step: 462, loss is 0.036983273923397064\n",
      "epoch: 2 step: 463, loss is 0.037445489317178726\n",
      "epoch: 2 step: 464, loss is 0.029269438236951828\n",
      "epoch: 2 step: 465, loss is 0.18858908116817474\n",
      "epoch: 2 step: 466, loss is 0.07813401520252228\n",
      "epoch: 2 step: 467, loss is 0.04120920971035957\n",
      "epoch: 2 step: 468, loss is 0.11358346045017242\n",
      "epoch: 2 step: 469, loss is 0.10700030624866486\n",
      "epoch: 2 step: 470, loss is 0.12423102557659149\n",
      "epoch: 2 step: 471, loss is 0.20888487994670868\n",
      "epoch: 2 step: 472, loss is 0.008651054464280605\n",
      "epoch: 2 step: 473, loss is 0.004026758950203657\n",
      "epoch: 2 step: 474, loss is 0.06281210482120514\n",
      "epoch: 2 step: 475, loss is 0.007715966552495956\n",
      "epoch: 2 step: 476, loss is 0.040388911962509155\n",
      "epoch: 2 step: 477, loss is 0.0020939980167895555\n",
      "epoch: 2 step: 478, loss is 0.04087213799357414\n",
      "epoch: 2 step: 479, loss is 0.0914897546172142\n",
      "epoch: 2 step: 480, loss is 0.25484341382980347\n",
      "epoch: 2 step: 481, loss is 0.13981428742408752\n",
      "epoch: 2 step: 482, loss is 0.2319730669260025\n",
      "epoch: 2 step: 483, loss is 0.07840531319379807\n",
      "epoch: 2 step: 484, loss is 0.09360436350107193\n",
      "epoch: 2 step: 485, loss is 0.09649117290973663\n",
      "epoch: 2 step: 486, loss is 0.17436851561069489\n",
      "epoch: 2 step: 487, loss is 0.20481210947036743\n",
      "epoch: 2 step: 488, loss is 0.0590587854385376\n",
      "epoch: 2 step: 489, loss is 0.002839767374098301\n",
      "epoch: 2 step: 490, loss is 0.004164200741797686\n",
      "epoch: 2 step: 491, loss is 0.06629720330238342\n",
      "epoch: 2 step: 492, loss is 0.024909773841500282\n",
      "epoch: 2 step: 493, loss is 0.13044831156730652\n",
      "epoch: 2 step: 494, loss is 0.04234268143773079\n",
      "epoch: 2 step: 495, loss is 0.040820855647325516\n",
      "epoch: 2 step: 496, loss is 0.022746041417121887\n",
      "epoch: 2 step: 497, loss is 0.048075392842292786\n",
      "epoch: 2 step: 498, loss is 0.05330982059240341\n",
      "epoch: 2 step: 499, loss is 0.022900434210896492\n",
      "epoch: 2 step: 500, loss is 0.05042627081274986\n",
      "epoch: 2 step: 501, loss is 0.26329782605171204\n",
      "epoch: 2 step: 502, loss is 0.03735331818461418\n",
      "epoch: 2 step: 503, loss is 0.17295314371585846\n",
      "epoch: 2 step: 504, loss is 0.007135314866900444\n",
      "epoch: 2 step: 505, loss is 0.0594322569668293\n",
      "epoch: 2 step: 506, loss is 0.1845564842224121\n",
      "epoch: 2 step: 507, loss is 0.00789943989366293\n",
      "epoch: 2 step: 508, loss is 0.044872771948575974\n",
      "epoch: 2 step: 509, loss is 0.011331628076732159\n",
      "epoch: 2 step: 510, loss is 0.39467835426330566\n",
      "epoch: 2 step: 511, loss is 0.059910617768764496\n",
      "epoch: 2 step: 512, loss is 0.2639937698841095\n",
      "epoch: 2 step: 513, loss is 0.16836047172546387\n",
      "epoch: 2 step: 514, loss is 0.02439170889556408\n",
      "epoch: 2 step: 515, loss is 0.08463691920042038\n",
      "epoch: 2 step: 516, loss is 0.017264800146222115\n",
      "epoch: 2 step: 517, loss is 0.1860284060239792\n",
      "epoch: 2 step: 518, loss is 0.015544154681265354\n",
      "epoch: 2 step: 519, loss is 0.05430212244391441\n",
      "epoch: 2 step: 520, loss is 0.07634469866752625\n",
      "epoch: 2 step: 521, loss is 0.07627842575311661\n",
      "epoch: 2 step: 522, loss is 0.22115010023117065\n",
      "epoch: 2 step: 523, loss is 0.04110913351178169\n",
      "epoch: 2 step: 524, loss is 0.16245904564857483\n",
      "epoch: 2 step: 525, loss is 0.028841184452176094\n",
      "epoch: 2 step: 526, loss is 0.006083344109356403\n",
      "epoch: 2 step: 527, loss is 0.14856398105621338\n",
      "epoch: 2 step: 528, loss is 0.10018400102853775\n",
      "epoch: 2 step: 529, loss is 0.05692106857895851\n",
      "epoch: 2 step: 530, loss is 0.06156701222062111\n",
      "epoch: 2 step: 531, loss is 0.036660004407167435\n",
      "epoch: 2 step: 532, loss is 0.19745174050331116\n",
      "epoch: 2 step: 533, loss is 0.0621974915266037\n",
      "epoch: 2 step: 534, loss is 0.08551852405071259\n",
      "epoch: 2 step: 535, loss is 0.07493719458580017\n",
      "epoch: 2 step: 536, loss is 0.035740941762924194\n",
      "epoch: 2 step: 537, loss is 0.03929980471730232\n",
      "epoch: 2 step: 538, loss is 0.007116840220987797\n",
      "epoch: 2 step: 539, loss is 0.008648465387523174\n",
      "epoch: 2 step: 540, loss is 0.02492149919271469\n",
      "epoch: 2 step: 541, loss is 0.0036792801693081856\n",
      "epoch: 2 step: 542, loss is 0.16450421512126923\n",
      "epoch: 2 step: 543, loss is 0.04175801947712898\n",
      "epoch: 2 step: 544, loss is 0.1868932992219925\n",
      "epoch: 2 step: 545, loss is 0.0035139124374836683\n",
      "epoch: 2 step: 546, loss is 0.24686528742313385\n",
      "epoch: 2 step: 547, loss is 0.03995196893811226\n",
      "epoch: 2 step: 548, loss is 0.06322170048952103\n",
      "epoch: 2 step: 549, loss is 0.049675919115543365\n",
      "epoch: 2 step: 550, loss is 0.08379187434911728\n",
      "epoch: 2 step: 551, loss is 0.07832875102758408\n",
      "epoch: 2 step: 552, loss is 0.012033157050609589\n",
      "epoch: 2 step: 553, loss is 0.05658708140254021\n",
      "epoch: 2 step: 554, loss is 0.028308285400271416\n",
      "epoch: 2 step: 555, loss is 0.02868242748081684\n",
      "epoch: 2 step: 556, loss is 0.09690363705158234\n",
      "epoch: 2 step: 557, loss is 0.2301948219537735\n",
      "epoch: 2 step: 558, loss is 0.007119219284504652\n",
      "epoch: 2 step: 559, loss is 0.19839803874492645\n",
      "epoch: 2 step: 560, loss is 0.030643710866570473\n",
      "epoch: 2 step: 561, loss is 0.2256462574005127\n",
      "epoch: 2 step: 562, loss is 0.24319393932819366\n",
      "epoch: 2 step: 563, loss is 0.008957442827522755\n",
      "epoch: 2 step: 564, loss is 0.02656921185553074\n",
      "epoch: 2 step: 565, loss is 0.12414969503879547\n",
      "epoch: 2 step: 566, loss is 0.03252556174993515\n",
      "epoch: 2 step: 567, loss is 0.006987725850194693\n",
      "epoch: 2 step: 568, loss is 0.014757407829165459\n",
      "epoch: 2 step: 569, loss is 0.009877773933112621\n",
      "epoch: 2 step: 570, loss is 0.07350591570138931\n",
      "epoch: 2 step: 571, loss is 0.0400957353413105\n",
      "epoch: 2 step: 572, loss is 0.15134552121162415\n",
      "epoch: 2 step: 573, loss is 0.16909559071063995\n",
      "epoch: 2 step: 574, loss is 0.09783363342285156\n",
      "epoch: 2 step: 575, loss is 0.08452855795621872\n",
      "epoch: 2 step: 576, loss is 0.19880828261375427\n",
      "epoch: 2 step: 577, loss is 0.012327304109930992\n",
      "epoch: 2 step: 578, loss is 0.009581813588738441\n",
      "epoch: 2 step: 579, loss is 0.31098616123199463\n",
      "epoch: 2 step: 580, loss is 0.006689587142318487\n",
      "epoch: 2 step: 581, loss is 0.033353131264448166\n",
      "epoch: 2 step: 582, loss is 0.027520088478922844\n",
      "epoch: 2 step: 583, loss is 0.06444380432367325\n",
      "epoch: 2 step: 584, loss is 0.1353926956653595\n",
      "epoch: 2 step: 585, loss is 0.020683519542217255\n",
      "epoch: 2 step: 586, loss is 0.010484018363058567\n",
      "epoch: 2 step: 587, loss is 0.004959399811923504\n",
      "epoch: 2 step: 588, loss is 0.015323098748922348\n",
      "epoch: 2 step: 589, loss is 0.039500605314970016\n",
      "epoch: 2 step: 590, loss is 0.23086786270141602\n",
      "epoch: 2 step: 591, loss is 0.0891178771853447\n",
      "epoch: 2 step: 592, loss is 0.014857767149806023\n",
      "epoch: 2 step: 593, loss is 0.025327147915959358\n",
      "epoch: 2 step: 594, loss is 0.05025997385382652\n",
      "epoch: 2 step: 595, loss is 0.01702413335442543\n",
      "epoch: 2 step: 596, loss is 0.20900259912014008\n",
      "epoch: 2 step: 597, loss is 0.09591803699731827\n",
      "epoch: 2 step: 598, loss is 0.08850545436143875\n",
      "epoch: 2 step: 599, loss is 0.008034689351916313\n",
      "epoch: 2 step: 600, loss is 0.01882266066968441\n",
      "epoch: 2 step: 601, loss is 0.01950063928961754\n",
      "epoch: 2 step: 602, loss is 0.02991650439798832\n",
      "epoch: 2 step: 603, loss is 0.35264280438423157\n",
      "epoch: 2 step: 604, loss is 0.059887275099754333\n",
      "epoch: 2 step: 605, loss is 0.10624822229146957\n",
      "epoch: 2 step: 606, loss is 0.03131238371133804\n",
      "epoch: 2 step: 607, loss is 0.0536172054708004\n",
      "epoch: 2 step: 608, loss is 0.17155000567436218\n",
      "epoch: 2 step: 609, loss is 0.3247526288032532\n",
      "epoch: 2 step: 610, loss is 0.006320342421531677\n",
      "epoch: 2 step: 611, loss is 0.014804670587182045\n",
      "epoch: 2 step: 612, loss is 0.13505463302135468\n",
      "epoch: 2 step: 613, loss is 0.08992854505777359\n",
      "epoch: 2 step: 614, loss is 0.17512190341949463\n",
      "epoch: 2 step: 615, loss is 0.21110130846500397\n",
      "epoch: 2 step: 616, loss is 0.12188621610403061\n",
      "epoch: 2 step: 617, loss is 0.15474346280097961\n",
      "epoch: 2 step: 618, loss is 0.05634424835443497\n",
      "epoch: 2 step: 619, loss is 0.12619701027870178\n",
      "epoch: 2 step: 620, loss is 0.11855415999889374\n",
      "epoch: 2 step: 621, loss is 0.07514271140098572\n",
      "epoch: 2 step: 622, loss is 0.02320644073188305\n",
      "epoch: 2 step: 623, loss is 0.02725367434322834\n",
      "epoch: 2 step: 624, loss is 0.07885705679655075\n",
      "epoch: 2 step: 625, loss is 0.09208912402391434\n",
      "epoch: 2 step: 626, loss is 0.015680819749832153\n",
      "epoch: 2 step: 627, loss is 0.030104976147413254\n",
      "epoch: 2 step: 628, loss is 0.014706720598042011\n",
      "epoch: 2 step: 629, loss is 0.07749990373849869\n",
      "epoch: 2 step: 630, loss is 0.006100608501583338\n",
      "epoch: 2 step: 631, loss is 0.03335047885775566\n",
      "epoch: 2 step: 632, loss is 0.021952448412775993\n",
      "epoch: 2 step: 633, loss is 0.021637240424752235\n",
      "epoch: 2 step: 634, loss is 0.16668085753917694\n",
      "epoch: 2 step: 635, loss is 0.019730983301997185\n",
      "epoch: 2 step: 636, loss is 0.13061834871768951\n",
      "epoch: 2 step: 637, loss is 0.03326798230409622\n",
      "epoch: 2 step: 638, loss is 0.022701730951666832\n",
      "epoch: 2 step: 639, loss is 0.009083176963031292\n",
      "epoch: 2 step: 640, loss is 0.004096765071153641\n",
      "epoch: 2 step: 641, loss is 0.018495691940188408\n",
      "epoch: 2 step: 642, loss is 0.01010668184608221\n",
      "epoch: 2 step: 643, loss is 0.027146397158503532\n",
      "epoch: 2 step: 644, loss is 0.014875638298690319\n",
      "epoch: 2 step: 645, loss is 0.008777793496847153\n",
      "epoch: 2 step: 646, loss is 0.0083256084471941\n",
      "epoch: 2 step: 647, loss is 0.00852085743099451\n",
      "epoch: 2 step: 648, loss is 0.03482428938150406\n",
      "epoch: 2 step: 649, loss is 0.05218559503555298\n",
      "epoch: 2 step: 650, loss is 0.1339142769575119\n",
      "epoch: 2 step: 651, loss is 0.009423112496733665\n",
      "epoch: 2 step: 652, loss is 0.04021114856004715\n",
      "epoch: 2 step: 653, loss is 0.005890898872166872\n",
      "epoch: 2 step: 654, loss is 0.20788346230983734\n",
      "epoch: 2 step: 655, loss is 0.06846821308135986\n",
      "epoch: 2 step: 656, loss is 0.034045107662677765\n",
      "epoch: 2 step: 657, loss is 0.0018707751296460629\n",
      "epoch: 2 step: 658, loss is 0.013665441423654556\n",
      "epoch: 2 step: 659, loss is 0.0109946234151721\n",
      "epoch: 2 step: 660, loss is 0.0053047607652843\n",
      "epoch: 2 step: 661, loss is 0.027288859710097313\n",
      "epoch: 2 step: 662, loss is 0.015546106733381748\n",
      "epoch: 2 step: 663, loss is 0.37749817967414856\n",
      "epoch: 2 step: 664, loss is 0.08729752898216248\n",
      "epoch: 2 step: 665, loss is 0.0007426303927786648\n",
      "epoch: 2 step: 666, loss is 0.011238574981689453\n",
      "epoch: 2 step: 667, loss is 0.22520428895950317\n",
      "epoch: 2 step: 668, loss is 0.10514125972986221\n",
      "epoch: 2 step: 669, loss is 0.015222734771668911\n",
      "epoch: 2 step: 670, loss is 0.03269044682383537\n",
      "epoch: 2 step: 671, loss is 0.058874428272247314\n",
      "epoch: 2 step: 672, loss is 0.002810184843838215\n",
      "epoch: 2 step: 673, loss is 0.16667011380195618\n",
      "epoch: 2 step: 674, loss is 0.4139750301837921\n",
      "epoch: 2 step: 675, loss is 0.16103608906269073\n",
      "epoch: 2 step: 676, loss is 0.010662544518709183\n",
      "epoch: 2 step: 677, loss is 0.025058262050151825\n",
      "epoch: 2 step: 678, loss is 0.2890847623348236\n",
      "epoch: 2 step: 679, loss is 0.020667079836130142\n",
      "epoch: 2 step: 680, loss is 0.007648071274161339\n",
      "epoch: 2 step: 681, loss is 0.11913570761680603\n",
      "epoch: 2 step: 682, loss is 0.24510346353054047\n",
      "epoch: 2 step: 683, loss is 0.030508391559123993\n",
      "epoch: 2 step: 684, loss is 0.013570171780884266\n",
      "epoch: 2 step: 685, loss is 0.04364357143640518\n",
      "epoch: 2 step: 686, loss is 0.19376330077648163\n",
      "epoch: 2 step: 687, loss is 0.07035138458013535\n",
      "epoch: 2 step: 688, loss is 0.007296789437532425\n",
      "epoch: 2 step: 689, loss is 0.004750621505081654\n",
      "epoch: 2 step: 690, loss is 0.016534654423594475\n",
      "epoch: 2 step: 691, loss is 0.02987978234887123\n",
      "epoch: 2 step: 692, loss is 0.2703242003917694\n",
      "epoch: 2 step: 693, loss is 0.0924304723739624\n",
      "epoch: 2 step: 694, loss is 0.025782594457268715\n",
      "epoch: 2 step: 695, loss is 0.049852155148983\n",
      "epoch: 2 step: 696, loss is 0.07174987345933914\n",
      "epoch: 2 step: 697, loss is 0.06738807260990143\n",
      "epoch: 2 step: 698, loss is 0.007852312177419662\n",
      "epoch: 2 step: 699, loss is 0.008626977913081646\n",
      "epoch: 2 step: 700, loss is 0.008011844009160995\n",
      "epoch: 2 step: 701, loss is 0.006496507208794355\n",
      "epoch: 2 step: 702, loss is 0.03242473304271698\n",
      "epoch: 2 step: 703, loss is 0.2592165470123291\n",
      "epoch: 2 step: 704, loss is 0.015935270115733147\n",
      "epoch: 2 step: 705, loss is 0.004930491559207439\n",
      "epoch: 2 step: 706, loss is 0.005279012024402618\n",
      "epoch: 2 step: 707, loss is 0.13335444033145905\n",
      "epoch: 2 step: 708, loss is 0.011257832869887352\n",
      "epoch: 2 step: 709, loss is 0.1365405172109604\n",
      "epoch: 2 step: 710, loss is 0.24689407646656036\n",
      "epoch: 2 step: 711, loss is 0.12759722769260406\n",
      "epoch: 2 step: 712, loss is 0.013528619892895222\n",
      "epoch: 2 step: 713, loss is 0.06692304462194443\n",
      "epoch: 2 step: 714, loss is 0.02459181845188141\n",
      "epoch: 2 step: 715, loss is 0.3647838532924652\n",
      "epoch: 2 step: 716, loss is 0.10529417544603348\n",
      "epoch: 2 step: 717, loss is 0.011945872567594051\n",
      "epoch: 2 step: 718, loss is 0.09860357642173767\n",
      "epoch: 2 step: 719, loss is 0.29375407099723816\n",
      "epoch: 2 step: 720, loss is 0.023669248446822166\n",
      "epoch: 2 step: 721, loss is 0.009632599540054798\n",
      "epoch: 2 step: 722, loss is 0.14905911684036255\n",
      "epoch: 2 step: 723, loss is 0.00495857885107398\n",
      "epoch: 2 step: 724, loss is 0.03644264116883278\n",
      "epoch: 2 step: 725, loss is 0.2587878406047821\n",
      "epoch: 2 step: 726, loss is 0.01743316277861595\n",
      "epoch: 2 step: 727, loss is 0.014129000715911388\n",
      "epoch: 2 step: 728, loss is 0.08567968755960464\n",
      "epoch: 2 step: 729, loss is 0.0052320403046905994\n",
      "epoch: 2 step: 730, loss is 0.011121486313641071\n",
      "epoch: 2 step: 731, loss is 0.09960322827100754\n",
      "epoch: 2 step: 732, loss is 0.19814252853393555\n",
      "epoch: 2 step: 733, loss is 0.06018022075295448\n",
      "epoch: 2 step: 734, loss is 0.09548924118280411\n",
      "epoch: 2 step: 735, loss is 0.031949203461408615\n",
      "epoch: 2 step: 736, loss is 0.026608461514115334\n",
      "epoch: 2 step: 737, loss is 0.040029507130384445\n",
      "epoch: 2 step: 738, loss is 0.13181310892105103\n",
      "epoch: 2 step: 739, loss is 0.04741841182112694\n",
      "epoch: 2 step: 740, loss is 0.04502013325691223\n",
      "epoch: 2 step: 741, loss is 0.06437529623508453\n",
      "epoch: 2 step: 742, loss is 0.03012957237660885\n",
      "epoch: 2 step: 743, loss is 0.11524733901023865\n",
      "epoch: 2 step: 744, loss is 0.037791408598423004\n",
      "epoch: 2 step: 745, loss is 0.01871165819466114\n",
      "epoch: 2 step: 746, loss is 0.023787613958120346\n",
      "epoch: 2 step: 747, loss is 0.01691361516714096\n",
      "epoch: 2 step: 748, loss is 0.10461737960577011\n",
      "epoch: 2 step: 749, loss is 0.007794031873345375\n",
      "epoch: 2 step: 750, loss is 0.0030162082985043526\n",
      "epoch: 2 step: 751, loss is 0.07178554683923721\n",
      "epoch: 2 step: 752, loss is 0.01217024214565754\n",
      "epoch: 2 step: 753, loss is 0.04800240322947502\n",
      "epoch: 2 step: 754, loss is 0.020542113110423088\n",
      "epoch: 2 step: 755, loss is 0.011620920151472092\n",
      "epoch: 2 step: 756, loss is 0.011922847479581833\n",
      "epoch: 2 step: 757, loss is 0.0063965884037315845\n",
      "epoch: 2 step: 758, loss is 0.002056770259514451\n",
      "epoch: 2 step: 759, loss is 0.23637175559997559\n",
      "epoch: 2 step: 760, loss is 0.0037671697791665792\n",
      "epoch: 2 step: 761, loss is 0.035801615566015244\n",
      "epoch: 2 step: 762, loss is 0.13475926220417023\n",
      "epoch: 2 step: 763, loss is 0.014548227190971375\n",
      "epoch: 2 step: 764, loss is 0.024769289419054985\n",
      "epoch: 2 step: 765, loss is 0.07144501060247421\n",
      "epoch: 2 step: 766, loss is 0.03313524276018143\n",
      "epoch: 2 step: 767, loss is 0.16957831382751465\n",
      "epoch: 2 step: 768, loss is 0.0006138914613984525\n",
      "epoch: 2 step: 769, loss is 0.004509201738983393\n",
      "epoch: 2 step: 770, loss is 0.020339153707027435\n",
      "epoch: 2 step: 771, loss is 0.004832512233406305\n",
      "epoch: 2 step: 772, loss is 0.04931106045842171\n",
      "epoch: 2 step: 773, loss is 0.016146091744303703\n",
      "epoch: 2 step: 774, loss is 0.024857353419065475\n",
      "epoch: 2 step: 775, loss is 0.2614721357822418\n",
      "epoch: 2 step: 776, loss is 0.02112789824604988\n",
      "epoch: 2 step: 777, loss is 0.07236285507678986\n",
      "epoch: 2 step: 778, loss is 0.00588143989443779\n",
      "epoch: 2 step: 779, loss is 0.030072549358010292\n",
      "epoch: 2 step: 780, loss is 0.08338165283203125\n",
      "epoch: 2 step: 781, loss is 0.047197822481393814\n",
      "epoch: 2 step: 782, loss is 0.01281278021633625\n",
      "epoch: 2 step: 783, loss is 0.0038289122749119997\n",
      "epoch: 2 step: 784, loss is 0.10497143119573593\n",
      "epoch: 2 step: 785, loss is 0.007290289271622896\n",
      "epoch: 2 step: 786, loss is 0.013306048698723316\n",
      "epoch: 2 step: 787, loss is 0.14301811158657074\n",
      "epoch: 2 step: 788, loss is 0.10907912999391556\n",
      "epoch: 2 step: 789, loss is 0.01576242968440056\n",
      "epoch: 2 step: 790, loss is 0.032459698617458344\n",
      "epoch: 2 step: 791, loss is 0.05052275210618973\n",
      "epoch: 2 step: 792, loss is 0.0780462920665741\n",
      "epoch: 2 step: 793, loss is 0.21032431721687317\n",
      "epoch: 2 step: 794, loss is 0.08430449664592743\n",
      "epoch: 2 step: 795, loss is 0.10747712850570679\n",
      "epoch: 2 step: 796, loss is 0.24089479446411133\n",
      "epoch: 2 step: 797, loss is 0.021607594564557076\n",
      "epoch: 2 step: 798, loss is 0.011334232054650784\n",
      "epoch: 2 step: 799, loss is 0.16444139182567596\n",
      "epoch: 2 step: 800, loss is 0.008768923580646515\n",
      "epoch: 2 step: 801, loss is 0.04170041158795357\n",
      "epoch: 2 step: 802, loss is 0.0040575494058430195\n",
      "epoch: 2 step: 803, loss is 0.03057142160832882\n",
      "epoch: 2 step: 804, loss is 0.06925052404403687\n",
      "epoch: 2 step: 805, loss is 0.10650595277547836\n",
      "epoch: 2 step: 806, loss is 0.025850925594568253\n",
      "epoch: 2 step: 807, loss is 0.27403494715690613\n",
      "epoch: 2 step: 808, loss is 0.1495470255613327\n",
      "epoch: 2 step: 809, loss is 0.008607779629528522\n",
      "epoch: 2 step: 810, loss is 0.021311772987246513\n",
      "epoch: 2 step: 811, loss is 0.019509848207235336\n",
      "epoch: 2 step: 812, loss is 0.10921667516231537\n",
      "epoch: 2 step: 813, loss is 0.011362330988049507\n",
      "epoch: 2 step: 814, loss is 0.1036301851272583\n",
      "epoch: 2 step: 815, loss is 0.017941642552614212\n",
      "epoch: 2 step: 816, loss is 0.1165972352027893\n",
      "epoch: 2 step: 817, loss is 0.21595285832881927\n",
      "epoch: 2 step: 818, loss is 0.049562495201826096\n",
      "epoch: 2 step: 819, loss is 0.11728129535913467\n",
      "epoch: 2 step: 820, loss is 0.16383135318756104\n",
      "epoch: 2 step: 821, loss is 0.04966314509510994\n",
      "epoch: 2 step: 822, loss is 0.017941929399967194\n",
      "epoch: 2 step: 823, loss is 0.06633176654577255\n",
      "epoch: 2 step: 824, loss is 0.18185776472091675\n",
      "epoch: 2 step: 825, loss is 0.12986652553081512\n",
      "epoch: 2 step: 826, loss is 0.026568537577986717\n",
      "epoch: 2 step: 827, loss is 0.023128550499677658\n",
      "epoch: 2 step: 828, loss is 0.007994061335921288\n",
      "epoch: 2 step: 829, loss is 0.028295695781707764\n",
      "epoch: 2 step: 830, loss is 0.011766038835048676\n",
      "epoch: 2 step: 831, loss is 0.08698289096355438\n",
      "epoch: 2 step: 832, loss is 0.005266726482659578\n",
      "epoch: 2 step: 833, loss is 0.01787838339805603\n",
      "epoch: 2 step: 834, loss is 0.3342215120792389\n",
      "epoch: 2 step: 835, loss is 0.003814948722720146\n",
      "epoch: 2 step: 836, loss is 0.030703477561473846\n",
      "epoch: 2 step: 837, loss is 0.17766501009464264\n",
      "epoch: 2 step: 838, loss is 0.00378976296633482\n",
      "epoch: 2 step: 839, loss is 0.12347952276468277\n",
      "epoch: 2 step: 840, loss is 0.18417973816394806\n",
      "epoch: 2 step: 841, loss is 0.004374837968498468\n",
      "epoch: 2 step: 842, loss is 0.27800488471984863\n",
      "epoch: 2 step: 843, loss is 0.0333680622279644\n",
      "epoch: 2 step: 844, loss is 0.012701887637376785\n",
      "epoch: 2 step: 845, loss is 0.018076112493872643\n",
      "epoch: 2 step: 846, loss is 0.036512576043605804\n",
      "epoch: 2 step: 847, loss is 0.11653532087802887\n",
      "epoch: 2 step: 848, loss is 0.008690346032381058\n",
      "epoch: 2 step: 849, loss is 0.05187263712286949\n",
      "epoch: 2 step: 850, loss is 0.0024996392894536257\n",
      "epoch: 2 step: 851, loss is 0.05437725409865379\n",
      "epoch: 2 step: 852, loss is 0.02586008422076702\n",
      "epoch: 2 step: 853, loss is 0.009466678835451603\n",
      "epoch: 2 step: 854, loss is 0.035188864916563034\n",
      "epoch: 2 step: 855, loss is 0.013762528076767921\n",
      "epoch: 2 step: 856, loss is 0.227890282869339\n",
      "epoch: 2 step: 857, loss is 0.1118776947259903\n",
      "epoch: 2 step: 858, loss is 0.05304518714547157\n",
      "epoch: 2 step: 859, loss is 0.06080343574285507\n",
      "epoch: 2 step: 860, loss is 0.08803070336580276\n",
      "epoch: 2 step: 861, loss is 0.019173365086317062\n",
      "epoch: 2 step: 862, loss is 0.02103886567056179\n",
      "epoch: 2 step: 863, loss is 0.016766244545578957\n",
      "epoch: 2 step: 864, loss is 0.058609697967767715\n",
      "epoch: 2 step: 865, loss is 0.23417145013809204\n",
      "epoch: 2 step: 866, loss is 0.038101691752672195\n",
      "epoch: 2 step: 867, loss is 0.10121355950832367\n",
      "epoch: 2 step: 868, loss is 0.10754664242267609\n",
      "epoch: 2 step: 869, loss is 0.014623235911130905\n",
      "epoch: 2 step: 870, loss is 0.025736944749951363\n",
      "epoch: 2 step: 871, loss is 0.00842023640871048\n",
      "epoch: 2 step: 872, loss is 0.003488795133307576\n",
      "epoch: 2 step: 873, loss is 0.043365757912397385\n",
      "epoch: 2 step: 874, loss is 0.1550387740135193\n",
      "epoch: 2 step: 875, loss is 0.1814488172531128\n",
      "epoch: 2 step: 876, loss is 0.06300152838230133\n",
      "epoch: 2 step: 877, loss is 0.060864031314849854\n",
      "epoch: 2 step: 878, loss is 0.08574072271585464\n",
      "epoch: 2 step: 879, loss is 0.04476882517337799\n",
      "epoch: 2 step: 880, loss is 0.10720540583133698\n",
      "epoch: 2 step: 881, loss is 0.0072599612176418304\n",
      "epoch: 2 step: 882, loss is 0.06920922547578812\n",
      "epoch: 2 step: 883, loss is 0.04144209250807762\n",
      "epoch: 2 step: 884, loss is 0.006369726732373238\n",
      "epoch: 2 step: 885, loss is 0.09202239662408829\n",
      "epoch: 2 step: 886, loss is 0.005694651510566473\n",
      "epoch: 2 step: 887, loss is 0.007019820157438517\n",
      "epoch: 2 step: 888, loss is 0.09903553128242493\n",
      "epoch: 2 step: 889, loss is 0.0846443697810173\n",
      "epoch: 2 step: 890, loss is 0.23646777868270874\n",
      "epoch: 2 step: 891, loss is 0.019386745989322662\n",
      "epoch: 2 step: 892, loss is 0.04423213377594948\n",
      "epoch: 2 step: 893, loss is 0.07014644145965576\n",
      "epoch: 2 step: 894, loss is 0.02118997648358345\n",
      "epoch: 2 step: 895, loss is 0.05686501786112785\n",
      "epoch: 2 step: 896, loss is 0.002850265707820654\n",
      "epoch: 2 step: 897, loss is 0.08080652356147766\n",
      "epoch: 2 step: 898, loss is 0.010481479577720165\n",
      "epoch: 2 step: 899, loss is 0.13319504261016846\n",
      "epoch: 2 step: 900, loss is 0.005697593092918396\n",
      "epoch: 2 step: 901, loss is 0.09220641851425171\n",
      "epoch: 2 step: 902, loss is 0.0148002365604043\n",
      "epoch: 2 step: 903, loss is 0.012866981327533722\n",
      "epoch: 2 step: 904, loss is 0.05103302374482155\n",
      "epoch: 2 step: 905, loss is 0.06163950264453888\n",
      "epoch: 2 step: 906, loss is 0.005949830170720816\n",
      "epoch: 2 step: 907, loss is 0.11983136087656021\n",
      "epoch: 2 step: 908, loss is 0.28318169713020325\n",
      "epoch: 2 step: 909, loss is 0.08140420913696289\n",
      "epoch: 2 step: 910, loss is 0.1275521218776703\n",
      "epoch: 2 step: 911, loss is 0.04858445003628731\n",
      "epoch: 2 step: 912, loss is 0.01814902573823929\n",
      "epoch: 2 step: 913, loss is 0.08212587982416153\n",
      "epoch: 2 step: 914, loss is 0.0918416976928711\n",
      "epoch: 2 step: 915, loss is 0.025991514325141907\n",
      "epoch: 2 step: 916, loss is 0.00969353411346674\n",
      "epoch: 2 step: 917, loss is 0.018688276410102844\n",
      "epoch: 2 step: 918, loss is 0.22681209444999695\n",
      "epoch: 2 step: 919, loss is 0.0825301855802536\n",
      "epoch: 2 step: 920, loss is 0.17521558701992035\n",
      "epoch: 2 step: 921, loss is 0.11822415888309479\n",
      "epoch: 2 step: 922, loss is 0.09710469841957092\n",
      "epoch: 2 step: 923, loss is 0.08697591722011566\n",
      "epoch: 2 step: 924, loss is 0.19534772634506226\n",
      "epoch: 2 step: 925, loss is 0.15285827219486237\n",
      "epoch: 2 step: 926, loss is 0.004640479106456041\n",
      "epoch: 2 step: 927, loss is 0.10268198698759079\n",
      "epoch: 2 step: 928, loss is 0.013498718850314617\n",
      "epoch: 2 step: 929, loss is 0.2554295063018799\n",
      "epoch: 2 step: 930, loss is 0.0069496105425059795\n",
      "epoch: 2 step: 931, loss is 0.06604006141424179\n",
      "epoch: 2 step: 932, loss is 0.07038075476884842\n",
      "epoch: 2 step: 933, loss is 0.040395788848400116\n",
      "epoch: 2 step: 934, loss is 0.13095295429229736\n",
      "epoch: 2 step: 935, loss is 0.058662015944719315\n",
      "epoch: 2 step: 936, loss is 0.04351721331477165\n",
      "epoch: 2 step: 937, loss is 0.16807644069194794\n",
      "epoch: 2 step: 938, loss is 0.0064172944985330105\n",
      "epoch: 2 step: 939, loss is 0.204591765999794\n",
      "epoch: 2 step: 940, loss is 0.01170765608549118\n",
      "epoch: 2 step: 941, loss is 0.03643039986491203\n",
      "epoch: 2 step: 942, loss is 0.24251514673233032\n",
      "epoch: 2 step: 943, loss is 0.12095816433429718\n",
      "epoch: 2 step: 944, loss is 0.11505623161792755\n",
      "epoch: 2 step: 945, loss is 0.1937110275030136\n",
      "epoch: 2 step: 946, loss is 0.009922515600919724\n",
      "epoch: 2 step: 947, loss is 0.011773968115448952\n",
      "epoch: 2 step: 948, loss is 0.07332611829042435\n",
      "epoch: 2 step: 949, loss is 0.028664827346801758\n",
      "epoch: 2 step: 950, loss is 0.08097060024738312\n",
      "epoch: 2 step: 951, loss is 0.203075110912323\n",
      "epoch: 2 step: 952, loss is 0.01299085933715105\n",
      "epoch: 2 step: 953, loss is 0.10669030249118805\n",
      "epoch: 2 step: 954, loss is 0.014684304594993591\n",
      "epoch: 2 step: 955, loss is 0.021656407043337822\n",
      "epoch: 2 step: 956, loss is 0.09815802425146103\n",
      "epoch: 2 step: 957, loss is 0.02649618685245514\n",
      "epoch: 2 step: 958, loss is 0.1629890501499176\n",
      "epoch: 2 step: 959, loss is 0.01599694974720478\n",
      "epoch: 2 step: 960, loss is 0.05435667932033539\n",
      "epoch: 2 step: 961, loss is 0.1992635577917099\n",
      "epoch: 2 step: 962, loss is 0.05430321395397186\n",
      "epoch: 2 step: 963, loss is 0.046158019453287125\n",
      "epoch: 2 step: 964, loss is 0.28964754939079285\n",
      "epoch: 2 step: 965, loss is 0.10419866442680359\n",
      "epoch: 2 step: 966, loss is 0.006964452099055052\n",
      "epoch: 2 step: 967, loss is 0.005737319588661194\n",
      "epoch: 2 step: 968, loss is 0.0664951503276825\n",
      "epoch: 2 step: 969, loss is 0.03152192011475563\n",
      "epoch: 2 step: 970, loss is 0.48171287775039673\n",
      "epoch: 2 step: 971, loss is 0.013950233347713947\n",
      "epoch: 2 step: 972, loss is 0.01479574665427208\n",
      "epoch: 2 step: 973, loss is 0.18706591427326202\n",
      "epoch: 2 step: 974, loss is 0.015280948020517826\n",
      "epoch: 2 step: 975, loss is 0.1986778974533081\n",
      "epoch: 2 step: 976, loss is 0.09734329581260681\n",
      "epoch: 2 step: 977, loss is 0.03761972114443779\n",
      "epoch: 2 step: 978, loss is 0.02045145444571972\n",
      "epoch: 2 step: 979, loss is 0.017202215269207954\n",
      "epoch: 2 step: 980, loss is 0.004561576992273331\n",
      "epoch: 2 step: 981, loss is 0.008821707218885422\n",
      "epoch: 2 step: 982, loss is 0.05660758912563324\n",
      "epoch: 2 step: 983, loss is 0.010606433264911175\n",
      "epoch: 2 step: 984, loss is 0.10596074163913727\n",
      "epoch: 2 step: 985, loss is 0.005790154915302992\n",
      "epoch: 2 step: 986, loss is 0.03263846039772034\n",
      "epoch: 2 step: 987, loss is 0.03592175990343094\n",
      "epoch: 2 step: 988, loss is 0.21515709161758423\n",
      "epoch: 2 step: 989, loss is 0.05612443387508392\n",
      "epoch: 2 step: 990, loss is 0.044184669852256775\n",
      "epoch: 2 step: 991, loss is 0.057558201253414154\n",
      "epoch: 2 step: 992, loss is 0.06216549500823021\n",
      "epoch: 2 step: 993, loss is 0.04882039129734039\n",
      "epoch: 2 step: 994, loss is 0.06638304144144058\n",
      "epoch: 2 step: 995, loss is 0.05038977786898613\n",
      "epoch: 2 step: 996, loss is 0.01219811663031578\n",
      "epoch: 2 step: 997, loss is 0.007561433129012585\n",
      "epoch: 2 step: 998, loss is 0.07180412858724594\n",
      "epoch: 2 step: 999, loss is 0.018973903730511665\n",
      "epoch: 2 step: 1000, loss is 0.003911621402949095\n",
      "epoch: 2 step: 1001, loss is 0.013386606238782406\n",
      "epoch: 2 step: 1002, loss is 0.03706026449799538\n",
      "epoch: 2 step: 1003, loss is 0.0561220645904541\n",
      "epoch: 2 step: 1004, loss is 0.04947522655129433\n",
      "epoch: 2 step: 1005, loss is 0.02445717714726925\n",
      "epoch: 2 step: 1006, loss is 0.003056537127122283\n",
      "epoch: 2 step: 1007, loss is 0.0047042276710271835\n",
      "epoch: 2 step: 1008, loss is 0.03320753574371338\n",
      "epoch: 2 step: 1009, loss is 0.004534868989139795\n",
      "epoch: 2 step: 1010, loss is 0.023739466443657875\n",
      "epoch: 2 step: 1011, loss is 0.052920617163181305\n",
      "epoch: 2 step: 1012, loss is 0.015764139592647552\n",
      "epoch: 2 step: 1013, loss is 0.046404507011175156\n",
      "epoch: 2 step: 1014, loss is 0.0008713089628145099\n",
      "epoch: 2 step: 1015, loss is 0.009507974609732628\n",
      "epoch: 2 step: 1016, loss is 0.10454025119543076\n",
      "epoch: 2 step: 1017, loss is 0.13113053143024445\n",
      "epoch: 2 step: 1018, loss is 0.09123338758945465\n",
      "epoch: 2 step: 1019, loss is 0.18519578874111176\n",
      "epoch: 2 step: 1020, loss is 0.1046685203909874\n",
      "epoch: 2 step: 1021, loss is 0.008818907663226128\n",
      "epoch: 2 step: 1022, loss is 0.018612636253237724\n",
      "epoch: 2 step: 1023, loss is 0.026264555752277374\n",
      "epoch: 2 step: 1024, loss is 0.022131286561489105\n",
      "epoch: 2 step: 1025, loss is 0.030575718730688095\n",
      "epoch: 2 step: 1026, loss is 0.076729416847229\n",
      "epoch: 2 step: 1027, loss is 0.046833209693431854\n",
      "epoch: 2 step: 1028, loss is 0.011288427747786045\n",
      "epoch: 2 step: 1029, loss is 0.2197754681110382\n",
      "epoch: 2 step: 1030, loss is 0.016056444495916367\n",
      "epoch: 2 step: 1031, loss is 0.02422081120312214\n",
      "epoch: 2 step: 1032, loss is 0.013068036176264286\n",
      "epoch: 2 step: 1033, loss is 0.29534298181533813\n",
      "epoch: 2 step: 1034, loss is 0.0361800454556942\n",
      "epoch: 2 step: 1035, loss is 0.043141767382621765\n",
      "epoch: 2 step: 1036, loss is 0.21685877442359924\n",
      "epoch: 2 step: 1037, loss is 0.03634282574057579\n",
      "epoch: 2 step: 1038, loss is 0.10058822482824326\n",
      "epoch: 2 step: 1039, loss is 0.11191365867853165\n",
      "epoch: 2 step: 1040, loss is 0.04944242909550667\n",
      "epoch: 2 step: 1041, loss is 0.0884094163775444\n",
      "epoch: 2 step: 1042, loss is 0.04680623114109039\n",
      "epoch: 2 step: 1043, loss is 0.0432601161301136\n",
      "epoch: 2 step: 1044, loss is 0.13643798232078552\n",
      "epoch: 2 step: 1045, loss is 0.035661112517118454\n",
      "epoch: 2 step: 1046, loss is 0.13549934327602386\n",
      "epoch: 2 step: 1047, loss is 0.09061101824045181\n",
      "epoch: 2 step: 1048, loss is 0.20265187323093414\n",
      "epoch: 2 step: 1049, loss is 0.009226219728589058\n",
      "epoch: 2 step: 1050, loss is 0.024963071569800377\n",
      "epoch: 2 step: 1051, loss is 0.018708201125264168\n",
      "epoch: 2 step: 1052, loss is 0.02706531062722206\n",
      "epoch: 2 step: 1053, loss is 0.3761410415172577\n",
      "epoch: 2 step: 1054, loss is 0.005900181829929352\n",
      "epoch: 2 step: 1055, loss is 0.054961249232292175\n",
      "epoch: 2 step: 1056, loss is 0.026900190860033035\n",
      "epoch: 2 step: 1057, loss is 0.21457968652248383\n",
      "epoch: 2 step: 1058, loss is 0.21884703636169434\n",
      "epoch: 2 step: 1059, loss is 0.0016760831931605935\n",
      "epoch: 2 step: 1060, loss is 0.08743473142385483\n",
      "epoch: 2 step: 1061, loss is 0.07381811738014221\n",
      "epoch: 2 step: 1062, loss is 0.0791296735405922\n",
      "epoch: 2 step: 1063, loss is 0.058002784848213196\n",
      "epoch: 2 step: 1064, loss is 0.01299638208001852\n",
      "epoch: 2 step: 1065, loss is 0.0048581804148852825\n",
      "epoch: 2 step: 1066, loss is 0.0014474038034677505\n",
      "epoch: 2 step: 1067, loss is 0.015326952561736107\n",
      "epoch: 2 step: 1068, loss is 0.03317045047879219\n",
      "epoch: 2 step: 1069, loss is 0.014641374349594116\n",
      "epoch: 2 step: 1070, loss is 0.009815030731260777\n",
      "epoch: 2 step: 1071, loss is 0.03516542911529541\n",
      "epoch: 2 step: 1072, loss is 0.0024973908439278603\n",
      "epoch: 2 step: 1073, loss is 0.012227502651512623\n",
      "epoch: 2 step: 1074, loss is 0.08369199186563492\n",
      "epoch: 2 step: 1075, loss is 0.14526186883449554\n",
      "epoch: 2 step: 1076, loss is 0.058203816413879395\n",
      "epoch: 2 step: 1077, loss is 0.22750693559646606\n",
      "epoch: 2 step: 1078, loss is 0.02293221838772297\n",
      "epoch: 2 step: 1079, loss is 0.11409389227628708\n",
      "epoch: 2 step: 1080, loss is 0.07602547854185104\n",
      "epoch: 2 step: 1081, loss is 0.04464472457766533\n",
      "epoch: 2 step: 1082, loss is 0.04080136865377426\n",
      "epoch: 2 step: 1083, loss is 0.1679680198431015\n",
      "epoch: 2 step: 1084, loss is 0.056885186582803726\n",
      "epoch: 2 step: 1085, loss is 0.02845367044210434\n",
      "epoch: 2 step: 1086, loss is 0.04434880614280701\n",
      "epoch: 2 step: 1087, loss is 0.045913852751255035\n",
      "epoch: 2 step: 1088, loss is 0.05090663209557533\n",
      "epoch: 2 step: 1089, loss is 0.05485713481903076\n",
      "epoch: 2 step: 1090, loss is 0.05977797880768776\n",
      "epoch: 2 step: 1091, loss is 0.10293228179216385\n",
      "epoch: 2 step: 1092, loss is 0.04348599538207054\n",
      "epoch: 2 step: 1093, loss is 0.100374236702919\n",
      "epoch: 2 step: 1094, loss is 0.018319103866815567\n",
      "epoch: 2 step: 1095, loss is 0.18811047077178955\n",
      "epoch: 2 step: 1096, loss is 0.019207438454031944\n",
      "epoch: 2 step: 1097, loss is 0.020320668816566467\n",
      "epoch: 2 step: 1098, loss is 0.04657531529664993\n",
      "epoch: 2 step: 1099, loss is 0.004998080898076296\n",
      "epoch: 2 step: 1100, loss is 0.0780840665102005\n",
      "epoch: 2 step: 1101, loss is 0.009640616364777088\n",
      "epoch: 2 step: 1102, loss is 0.12033160775899887\n",
      "epoch: 2 step: 1103, loss is 0.004714681766927242\n",
      "epoch: 2 step: 1104, loss is 0.010704325512051582\n",
      "epoch: 2 step: 1105, loss is 0.3387664258480072\n",
      "epoch: 2 step: 1106, loss is 0.3525754511356354\n",
      "epoch: 2 step: 1107, loss is 0.01631173864006996\n",
      "epoch: 2 step: 1108, loss is 0.026939798146486282\n",
      "epoch: 2 step: 1109, loss is 0.17824749648571014\n",
      "epoch: 2 step: 1110, loss is 0.1161101907491684\n",
      "epoch: 2 step: 1111, loss is 0.003223258536309004\n",
      "epoch: 2 step: 1112, loss is 0.03822221979498863\n",
      "epoch: 2 step: 1113, loss is 0.09185365587472916\n",
      "epoch: 2 step: 1114, loss is 0.08210983127355576\n",
      "epoch: 2 step: 1115, loss is 0.016967864707112312\n",
      "epoch: 2 step: 1116, loss is 0.0811338797211647\n",
      "epoch: 2 step: 1117, loss is 0.08133618533611298\n",
      "epoch: 2 step: 1118, loss is 0.03950699791312218\n",
      "epoch: 2 step: 1119, loss is 0.007724531460553408\n",
      "epoch: 2 step: 1120, loss is 0.21992363035678864\n",
      "epoch: 2 step: 1121, loss is 0.0026547927409410477\n",
      "epoch: 2 step: 1122, loss is 0.14127714931964874\n",
      "epoch: 2 step: 1123, loss is 0.060297075659036636\n",
      "epoch: 2 step: 1124, loss is 0.039112821221351624\n",
      "epoch: 2 step: 1125, loss is 0.015473385341465473\n",
      "epoch: 2 step: 1126, loss is 0.03795386478304863\n",
      "epoch: 2 step: 1127, loss is 0.17771559953689575\n",
      "epoch: 2 step: 1128, loss is 0.022590285167098045\n",
      "epoch: 2 step: 1129, loss is 0.015564929693937302\n",
      "epoch: 2 step: 1130, loss is 0.12902191281318665\n",
      "epoch: 2 step: 1131, loss is 0.09069591760635376\n",
      "epoch: 2 step: 1132, loss is 0.279579222202301\n",
      "epoch: 2 step: 1133, loss is 0.08245421200990677\n",
      "epoch: 2 step: 1134, loss is 0.04678413271903992\n",
      "epoch: 2 step: 1135, loss is 0.019833030179142952\n",
      "epoch: 2 step: 1136, loss is 0.04615966975688934\n",
      "epoch: 2 step: 1137, loss is 0.09617172181606293\n",
      "epoch: 2 step: 1138, loss is 0.18344300985336304\n",
      "epoch: 2 step: 1139, loss is 0.21370217204093933\n",
      "epoch: 2 step: 1140, loss is 0.012810259126126766\n",
      "epoch: 2 step: 1141, loss is 0.11348821967840195\n",
      "epoch: 2 step: 1142, loss is 0.07405608147382736\n",
      "epoch: 2 step: 1143, loss is 0.09988877177238464\n",
      "epoch: 2 step: 1144, loss is 0.02087121456861496\n",
      "epoch: 2 step: 1145, loss is 0.06048094481229782\n",
      "epoch: 2 step: 1146, loss is 0.09491356462240219\n",
      "epoch: 2 step: 1147, loss is 0.05597466975450516\n",
      "epoch: 2 step: 1148, loss is 0.01180892251431942\n",
      "epoch: 2 step: 1149, loss is 0.01375986635684967\n",
      "epoch: 2 step: 1150, loss is 0.046591274440288544\n",
      "epoch: 2 step: 1151, loss is 0.00431819399818778\n",
      "epoch: 2 step: 1152, loss is 0.0037074917927384377\n",
      "epoch: 2 step: 1153, loss is 0.00425849761813879\n",
      "epoch: 2 step: 1154, loss is 0.02818320505321026\n",
      "epoch: 2 step: 1155, loss is 0.04985858127474785\n",
      "epoch: 2 step: 1156, loss is 0.005312273744493723\n",
      "epoch: 2 step: 1157, loss is 0.14312410354614258\n",
      "epoch: 2 step: 1158, loss is 0.059611789882183075\n",
      "epoch: 2 step: 1159, loss is 0.03390387445688248\n",
      "epoch: 2 step: 1160, loss is 0.04269355162978172\n",
      "epoch: 2 step: 1161, loss is 0.020043691620230675\n",
      "epoch: 2 step: 1162, loss is 0.008637561462819576\n",
      "epoch: 2 step: 1163, loss is 0.011411101557314396\n",
      "epoch: 2 step: 1164, loss is 0.3658633232116699\n",
      "epoch: 2 step: 1165, loss is 0.028224047273397446\n",
      "epoch: 2 step: 1166, loss is 0.09394914656877518\n",
      "epoch: 2 step: 1167, loss is 0.007524432148784399\n",
      "epoch: 2 step: 1168, loss is 0.011633026413619518\n",
      "epoch: 2 step: 1169, loss is 0.028541840612888336\n",
      "epoch: 2 step: 1170, loss is 0.005952015984803438\n",
      "epoch: 2 step: 1171, loss is 0.029088810086250305\n",
      "epoch: 2 step: 1172, loss is 0.08130452781915665\n",
      "epoch: 2 step: 1173, loss is 0.030641306191682816\n",
      "epoch: 2 step: 1174, loss is 0.04328049719333649\n",
      "epoch: 2 step: 1175, loss is 0.07233412563800812\n",
      "epoch: 2 step: 1176, loss is 0.10720027983188629\n",
      "epoch: 2 step: 1177, loss is 0.013646074570715427\n",
      "epoch: 2 step: 1178, loss is 0.009029857814311981\n",
      "epoch: 2 step: 1179, loss is 0.1330219954252243\n",
      "epoch: 2 step: 1180, loss is 0.0014970958000048995\n",
      "epoch: 2 step: 1181, loss is 0.01210144255310297\n",
      "epoch: 2 step: 1182, loss is 0.03977756202220917\n",
      "epoch: 2 step: 1183, loss is 0.015082493424415588\n",
      "epoch: 2 step: 1184, loss is 0.18547344207763672\n",
      "epoch: 2 step: 1185, loss is 0.04139000549912453\n",
      "epoch: 2 step: 1186, loss is 0.1789814829826355\n",
      "epoch: 2 step: 1187, loss is 0.22099430859088898\n",
      "epoch: 2 step: 1188, loss is 0.019014909863471985\n",
      "epoch: 2 step: 1189, loss is 0.03387451171875\n",
      "epoch: 2 step: 1190, loss is 0.0922178328037262\n",
      "epoch: 2 step: 1191, loss is 0.13977473974227905\n",
      "epoch: 2 step: 1192, loss is 0.0025373254902660847\n",
      "epoch: 2 step: 1193, loss is 0.0020602885633707047\n",
      "epoch: 2 step: 1194, loss is 0.008975067175924778\n",
      "epoch: 2 step: 1195, loss is 0.014178491197526455\n",
      "epoch: 2 step: 1196, loss is 0.07677676528692245\n",
      "epoch: 2 step: 1197, loss is 0.22294840216636658\n",
      "epoch: 2 step: 1198, loss is 0.015957210212945938\n",
      "epoch: 2 step: 1199, loss is 0.022092018276453018\n",
      "epoch: 2 step: 1200, loss is 0.08695942163467407\n",
      "epoch: 2 step: 1201, loss is 0.02890937030315399\n",
      "epoch: 2 step: 1202, loss is 0.0317426435649395\n",
      "epoch: 2 step: 1203, loss is 0.07957638800144196\n",
      "epoch: 2 step: 1204, loss is 0.010580248199403286\n",
      "epoch: 2 step: 1205, loss is 0.1333877593278885\n",
      "epoch: 2 step: 1206, loss is 0.009140274487435818\n",
      "epoch: 2 step: 1207, loss is 0.08619042485952377\n",
      "epoch: 2 step: 1208, loss is 0.004791718907654285\n",
      "epoch: 2 step: 1209, loss is 0.012619265355169773\n",
      "epoch: 2 step: 1210, loss is 0.07454181462526321\n",
      "epoch: 2 step: 1211, loss is 0.012997332029044628\n",
      "epoch: 2 step: 1212, loss is 0.1589890867471695\n",
      "epoch: 2 step: 1213, loss is 0.0016007379163056612\n",
      "epoch: 2 step: 1214, loss is 0.16775217652320862\n",
      "epoch: 2 step: 1215, loss is 0.018445711582899094\n",
      "epoch: 2 step: 1216, loss is 0.00535248126834631\n",
      "epoch: 2 step: 1217, loss is 0.022583920508623123\n",
      "epoch: 2 step: 1218, loss is 0.018626509234309196\n",
      "epoch: 2 step: 1219, loss is 0.008422352373600006\n",
      "epoch: 2 step: 1220, loss is 0.03447185456752777\n",
      "epoch: 2 step: 1221, loss is 0.07625482231378555\n",
      "epoch: 2 step: 1222, loss is 0.038604531437158585\n",
      "epoch: 2 step: 1223, loss is 0.006120616570115089\n",
      "epoch: 2 step: 1224, loss is 0.02531581185758114\n",
      "epoch: 2 step: 1225, loss is 0.005234495736658573\n",
      "epoch: 2 step: 1226, loss is 0.16956251859664917\n",
      "epoch: 2 step: 1227, loss is 0.02283046767115593\n",
      "epoch: 2 step: 1228, loss is 0.001565100857988\n",
      "epoch: 2 step: 1229, loss is 0.004668555688112974\n",
      "epoch: 2 step: 1230, loss is 0.01791418343782425\n",
      "epoch: 2 step: 1231, loss is 0.0037346379831433296\n",
      "epoch: 2 step: 1232, loss is 0.16578851640224457\n",
      "epoch: 2 step: 1233, loss is 0.015679871663451195\n",
      "epoch: 2 step: 1234, loss is 0.012884563766419888\n",
      "epoch: 2 step: 1235, loss is 0.022302741184830666\n",
      "epoch: 2 step: 1236, loss is 0.013033759780228138\n",
      "epoch: 2 step: 1237, loss is 0.00151046272367239\n",
      "epoch: 2 step: 1238, loss is 0.014956509694457054\n",
      "epoch: 2 step: 1239, loss is 0.07995264232158661\n",
      "epoch: 2 step: 1240, loss is 0.10931488126516342\n",
      "epoch: 2 step: 1241, loss is 0.18484912812709808\n",
      "epoch: 2 step: 1242, loss is 0.03117014467716217\n",
      "epoch: 2 step: 1243, loss is 0.1860230565071106\n",
      "epoch: 2 step: 1244, loss is 0.1302771270275116\n",
      "epoch: 2 step: 1245, loss is 0.004675912670791149\n",
      "epoch: 2 step: 1246, loss is 0.0042880503460764885\n",
      "epoch: 2 step: 1247, loss is 0.0014010935556143522\n",
      "epoch: 2 step: 1248, loss is 0.002811811864376068\n",
      "epoch: 2 step: 1249, loss is 0.10693897306919098\n",
      "epoch: 2 step: 1250, loss is 0.004342538304626942\n",
      "epoch: 2 step: 1251, loss is 0.03895588591694832\n",
      "epoch: 2 step: 1252, loss is 0.36997148394584656\n",
      "epoch: 2 step: 1253, loss is 0.004876019898802042\n",
      "epoch: 2 step: 1254, loss is 0.1814074069261551\n",
      "epoch: 2 step: 1255, loss is 0.08275795727968216\n",
      "epoch: 2 step: 1256, loss is 0.08582956343889236\n",
      "epoch: 2 step: 1257, loss is 0.04305054619908333\n",
      "epoch: 2 step: 1258, loss is 0.12635113298892975\n",
      "epoch: 2 step: 1259, loss is 0.017794528976082802\n",
      "epoch: 2 step: 1260, loss is 0.07040655612945557\n",
      "epoch: 2 step: 1261, loss is 0.2633771002292633\n",
      "epoch: 2 step: 1262, loss is 0.049027711153030396\n",
      "epoch: 2 step: 1263, loss is 0.025110945105552673\n",
      "epoch: 2 step: 1264, loss is 0.02582080475986004\n",
      "epoch: 2 step: 1265, loss is 0.08023916929960251\n",
      "epoch: 2 step: 1266, loss is 0.010499916970729828\n",
      "epoch: 2 step: 1267, loss is 0.3260434865951538\n",
      "epoch: 2 step: 1268, loss is 0.017893407493829727\n",
      "epoch: 2 step: 1269, loss is 0.15167705714702606\n",
      "epoch: 2 step: 1270, loss is 0.24901477992534637\n",
      "epoch: 2 step: 1271, loss is 0.05885405093431473\n",
      "epoch: 2 step: 1272, loss is 0.06737732887268066\n",
      "epoch: 2 step: 1273, loss is 0.15120293200016022\n",
      "epoch: 2 step: 1274, loss is 0.01609470695257187\n",
      "epoch: 2 step: 1275, loss is 0.10424935072660446\n",
      "epoch: 2 step: 1276, loss is 0.017925526946783066\n",
      "epoch: 2 step: 1277, loss is 0.007928255014121532\n",
      "epoch: 2 step: 1278, loss is 0.02069241739809513\n",
      "epoch: 2 step: 1279, loss is 0.034730780869722366\n",
      "epoch: 2 step: 1280, loss is 0.12127607315778732\n",
      "epoch: 2 step: 1281, loss is 0.03706681728363037\n",
      "epoch: 2 step: 1282, loss is 0.021950408816337585\n",
      "epoch: 2 step: 1283, loss is 0.10834527760744095\n",
      "epoch: 2 step: 1284, loss is 0.0100967762991786\n",
      "epoch: 2 step: 1285, loss is 0.10581613332033157\n",
      "epoch: 2 step: 1286, loss is 0.16312016546726227\n",
      "epoch: 2 step: 1287, loss is 0.07203639298677444\n",
      "epoch: 2 step: 1288, loss is 0.0015628146938979626\n",
      "epoch: 2 step: 1289, loss is 0.003168609691783786\n",
      "epoch: 2 step: 1290, loss is 0.08585720509290695\n",
      "epoch: 2 step: 1291, loss is 0.010944482870399952\n",
      "epoch: 2 step: 1292, loss is 0.008236568421125412\n",
      "epoch: 2 step: 1293, loss is 0.02660658396780491\n",
      "epoch: 2 step: 1294, loss is 0.034091703593730927\n",
      "epoch: 2 step: 1295, loss is 0.14664646983146667\n",
      "epoch: 2 step: 1296, loss is 0.16181933879852295\n",
      "epoch: 2 step: 1297, loss is 0.21556256711483002\n",
      "epoch: 2 step: 1298, loss is 0.010638083331286907\n",
      "epoch: 2 step: 1299, loss is 0.1557195633649826\n",
      "epoch: 2 step: 1300, loss is 0.1323193460702896\n",
      "epoch: 2 step: 1301, loss is 0.11107771098613739\n",
      "epoch: 2 step: 1302, loss is 0.037621594965457916\n",
      "epoch: 2 step: 1303, loss is 0.04391813650727272\n",
      "epoch: 2 step: 1304, loss is 0.16046425700187683\n",
      "epoch: 2 step: 1305, loss is 0.0070843324065208435\n",
      "epoch: 2 step: 1306, loss is 0.1323404312133789\n",
      "epoch: 2 step: 1307, loss is 0.018260225653648376\n",
      "epoch: 2 step: 1308, loss is 0.0014001090312376618\n",
      "epoch: 2 step: 1309, loss is 0.01681332290172577\n",
      "epoch: 2 step: 1310, loss is 0.1169707402586937\n",
      "epoch: 2 step: 1311, loss is 0.052869148552417755\n",
      "epoch: 2 step: 1312, loss is 0.00449413014575839\n",
      "epoch: 2 step: 1313, loss is 0.09628265351057053\n",
      "epoch: 2 step: 1314, loss is 0.004700694233179092\n",
      "epoch: 2 step: 1315, loss is 0.0293955747038126\n",
      "epoch: 2 step: 1316, loss is 0.0921061784029007\n",
      "epoch: 2 step: 1317, loss is 0.022320320829749107\n",
      "epoch: 2 step: 1318, loss is 0.018952365964651108\n",
      "epoch: 2 step: 1319, loss is 0.08810117095708847\n",
      "epoch: 2 step: 1320, loss is 0.4558483958244324\n",
      "epoch: 2 step: 1321, loss is 0.014893739484250546\n",
      "epoch: 2 step: 1322, loss is 0.20671290159225464\n",
      "epoch: 2 step: 1323, loss is 0.02298864908516407\n",
      "epoch: 2 step: 1324, loss is 0.17676739394664764\n",
      "epoch: 2 step: 1325, loss is 0.09714797884225845\n",
      "epoch: 2 step: 1326, loss is 0.17876990139484406\n",
      "epoch: 2 step: 1327, loss is 0.008770168758928776\n",
      "epoch: 2 step: 1328, loss is 0.013801034539937973\n",
      "epoch: 2 step: 1329, loss is 0.13453355431556702\n",
      "epoch: 2 step: 1330, loss is 0.012801620177924633\n",
      "epoch: 2 step: 1331, loss is 0.011950857006013393\n",
      "epoch: 2 step: 1332, loss is 0.026983119547367096\n",
      "epoch: 2 step: 1333, loss is 0.0854300931096077\n",
      "epoch: 2 step: 1334, loss is 0.2063404768705368\n",
      "epoch: 2 step: 1335, loss is 0.3681645691394806\n",
      "epoch: 2 step: 1336, loss is 0.04577936977148056\n",
      "epoch: 2 step: 1337, loss is 0.025508934631943703\n",
      "epoch: 2 step: 1338, loss is 0.038301289081573486\n",
      "epoch: 2 step: 1339, loss is 0.028343023732304573\n",
      "epoch: 2 step: 1340, loss is 0.022968517616391182\n",
      "epoch: 2 step: 1341, loss is 0.07208743691444397\n",
      "epoch: 2 step: 1342, loss is 0.05401557311415672\n",
      "epoch: 2 step: 1343, loss is 0.05697179585695267\n",
      "epoch: 2 step: 1344, loss is 0.01785496063530445\n",
      "epoch: 2 step: 1345, loss is 0.04001142829656601\n",
      "epoch: 2 step: 1346, loss is 0.039895642548799515\n",
      "epoch: 2 step: 1347, loss is 0.0215662382543087\n",
      "epoch: 2 step: 1348, loss is 0.09096299111843109\n",
      "epoch: 2 step: 1349, loss is 0.015771836042404175\n",
      "epoch: 2 step: 1350, loss is 0.07716485112905502\n",
      "epoch: 2 step: 1351, loss is 0.005770720075815916\n",
      "epoch: 2 step: 1352, loss is 0.032618265599012375\n",
      "epoch: 2 step: 1353, loss is 0.029319491237401962\n",
      "epoch: 2 step: 1354, loss is 0.08194629102945328\n",
      "epoch: 2 step: 1355, loss is 0.009437604807317257\n",
      "epoch: 2 step: 1356, loss is 0.007915807887911797\n",
      "epoch: 2 step: 1357, loss is 0.10939300060272217\n",
      "epoch: 2 step: 1358, loss is 0.2057417631149292\n",
      "epoch: 2 step: 1359, loss is 0.019483298063278198\n",
      "epoch: 2 step: 1360, loss is 0.14048080146312714\n",
      "epoch: 2 step: 1361, loss is 0.021518666297197342\n",
      "epoch: 2 step: 1362, loss is 0.1941855251789093\n",
      "epoch: 2 step: 1363, loss is 0.05829823389649391\n",
      "epoch: 2 step: 1364, loss is 0.10774583369493484\n",
      "epoch: 2 step: 1365, loss is 0.0013745136093348265\n",
      "epoch: 2 step: 1366, loss is 0.035446908324956894\n",
      "epoch: 2 step: 1367, loss is 0.3661389648914337\n",
      "epoch: 2 step: 1368, loss is 0.003028352279216051\n",
      "epoch: 2 step: 1369, loss is 0.00810406357049942\n",
      "epoch: 2 step: 1370, loss is 0.15462344884872437\n",
      "epoch: 2 step: 1371, loss is 0.0016994591569527984\n",
      "epoch: 2 step: 1372, loss is 0.03995238244533539\n",
      "epoch: 2 step: 1373, loss is 0.02276410534977913\n",
      "epoch: 2 step: 1374, loss is 0.0631491094827652\n",
      "epoch: 2 step: 1375, loss is 0.06089506298303604\n",
      "epoch: 2 step: 1376, loss is 0.06949906796216965\n",
      "epoch: 2 step: 1377, loss is 0.07892052829265594\n",
      "epoch: 2 step: 1378, loss is 0.003131084842607379\n",
      "epoch: 2 step: 1379, loss is 0.15162800252437592\n",
      "epoch: 2 step: 1380, loss is 0.05020022764801979\n",
      "epoch: 2 step: 1381, loss is 0.06268525868654251\n",
      "epoch: 2 step: 1382, loss is 0.009944621473550797\n",
      "epoch: 2 step: 1383, loss is 0.14196142554283142\n",
      "epoch: 2 step: 1384, loss is 0.28467389941215515\n",
      "epoch: 2 step: 1385, loss is 0.011425712145864964\n",
      "epoch: 2 step: 1386, loss is 0.002489946549758315\n",
      "epoch: 2 step: 1387, loss is 0.13580091297626495\n",
      "epoch: 2 step: 1388, loss is 0.10401762276887894\n",
      "epoch: 2 step: 1389, loss is 0.009989696554839611\n",
      "epoch: 2 step: 1390, loss is 0.017259547486901283\n",
      "epoch: 2 step: 1391, loss is 0.13747695088386536\n",
      "epoch: 2 step: 1392, loss is 0.009187964722514153\n",
      "epoch: 2 step: 1393, loss is 0.041133761405944824\n",
      "epoch: 2 step: 1394, loss is 0.07345954328775406\n",
      "epoch: 2 step: 1395, loss is 0.11891001462936401\n",
      "epoch: 2 step: 1396, loss is 0.05991572514176369\n",
      "epoch: 2 step: 1397, loss is 0.037761859595775604\n",
      "epoch: 2 step: 1398, loss is 0.04259021207690239\n",
      "epoch: 2 step: 1399, loss is 0.009872102178633213\n",
      "epoch: 2 step: 1400, loss is 0.03727203235030174\n",
      "epoch: 2 step: 1401, loss is 0.04265127331018448\n",
      "epoch: 2 step: 1402, loss is 0.14471133053302765\n",
      "epoch: 2 step: 1403, loss is 0.019702784717082977\n",
      "epoch: 2 step: 1404, loss is 0.10166215151548386\n",
      "epoch: 2 step: 1405, loss is 0.056923169642686844\n",
      "epoch: 2 step: 1406, loss is 0.2371443659067154\n",
      "epoch: 2 step: 1407, loss is 0.01005186140537262\n",
      "epoch: 2 step: 1408, loss is 0.008434712886810303\n",
      "epoch: 2 step: 1409, loss is 0.004040223080664873\n",
      "epoch: 2 step: 1410, loss is 0.04350920766592026\n",
      "epoch: 2 step: 1411, loss is 0.0035473413299769163\n",
      "epoch: 2 step: 1412, loss is 0.07221665978431702\n",
      "epoch: 2 step: 1413, loss is 0.10277418792247772\n",
      "epoch: 2 step: 1414, loss is 0.1673317700624466\n",
      "epoch: 2 step: 1415, loss is 0.002064365893602371\n",
      "epoch: 2 step: 1416, loss is 0.01740247569978237\n",
      "epoch: 2 step: 1417, loss is 0.008643420413136482\n",
      "epoch: 2 step: 1418, loss is 0.06723954528570175\n",
      "epoch: 2 step: 1419, loss is 0.019149314612150192\n",
      "epoch: 2 step: 1420, loss is 0.011649930849671364\n",
      "epoch: 2 step: 1421, loss is 0.005582565441727638\n",
      "epoch: 2 step: 1422, loss is 0.0040318020619452\n",
      "epoch: 2 step: 1423, loss is 0.008561775088310242\n",
      "epoch: 2 step: 1424, loss is 0.28512200713157654\n",
      "epoch: 2 step: 1425, loss is 0.01310350839048624\n",
      "epoch: 2 step: 1426, loss is 0.04569603130221367\n",
      "epoch: 2 step: 1427, loss is 0.05065125972032547\n",
      "epoch: 2 step: 1428, loss is 0.18998681008815765\n",
      "epoch: 2 step: 1429, loss is 0.03536737337708473\n",
      "epoch: 2 step: 1430, loss is 0.1792198121547699\n",
      "epoch: 2 step: 1431, loss is 0.009247861802577972\n",
      "epoch: 2 step: 1432, loss is 0.0028060937765985727\n",
      "epoch: 2 step: 1433, loss is 0.16991263628005981\n",
      "epoch: 2 step: 1434, loss is 0.06582216173410416\n",
      "epoch: 2 step: 1435, loss is 0.007067172788083553\n",
      "epoch: 2 step: 1436, loss is 0.010378256440162659\n",
      "epoch: 2 step: 1437, loss is 0.007650448475033045\n",
      "epoch: 2 step: 1438, loss is 0.19754593074321747\n",
      "epoch: 2 step: 1439, loss is 0.013713055290281773\n",
      "epoch: 2 step: 1440, loss is 0.001903160591609776\n",
      "epoch: 2 step: 1441, loss is 0.04502427577972412\n",
      "epoch: 2 step: 1442, loss is 0.05017698556184769\n",
      "epoch: 2 step: 1443, loss is 0.15979161858558655\n",
      "epoch: 2 step: 1444, loss is 0.005792549345642328\n",
      "epoch: 2 step: 1445, loss is 0.14512598514556885\n",
      "epoch: 2 step: 1446, loss is 0.09517015516757965\n",
      "epoch: 2 step: 1447, loss is 0.02110808528959751\n",
      "epoch: 2 step: 1448, loss is 0.005204140208661556\n",
      "epoch: 2 step: 1449, loss is 0.02171141281723976\n",
      "epoch: 2 step: 1450, loss is 0.00800764374434948\n",
      "epoch: 2 step: 1451, loss is 0.0033220448531210423\n",
      "epoch: 2 step: 1452, loss is 0.026385068893432617\n",
      "epoch: 2 step: 1453, loss is 0.060516271740198135\n",
      "epoch: 2 step: 1454, loss is 0.0766964852809906\n",
      "epoch: 2 step: 1455, loss is 0.1988602727651596\n",
      "epoch: 2 step: 1456, loss is 0.28362590074539185\n",
      "epoch: 2 step: 1457, loss is 0.05222475528717041\n",
      "epoch: 2 step: 1458, loss is 0.012144182808697224\n",
      "epoch: 2 step: 1459, loss is 0.13812695443630219\n",
      "epoch: 2 step: 1460, loss is 0.012063502334058285\n",
      "epoch: 2 step: 1461, loss is 0.05815403535962105\n",
      "epoch: 2 step: 1462, loss is 0.037212204188108444\n",
      "epoch: 2 step: 1463, loss is 0.007190728560090065\n",
      "epoch: 2 step: 1464, loss is 0.08609219640493393\n",
      "epoch: 2 step: 1465, loss is 0.019853074103593826\n",
      "epoch: 2 step: 1466, loss is 0.01838073506951332\n",
      "epoch: 2 step: 1467, loss is 0.00420877942815423\n",
      "epoch: 2 step: 1468, loss is 0.00047778463340364397\n",
      "epoch: 2 step: 1469, loss is 0.002287725917994976\n",
      "epoch: 2 step: 1470, loss is 0.21712626516819\n",
      "epoch: 2 step: 1471, loss is 0.005390622653067112\n",
      "epoch: 2 step: 1472, loss is 0.1455126702785492\n",
      "epoch: 2 step: 1473, loss is 0.04449271410703659\n",
      "epoch: 2 step: 1474, loss is 0.04207747057080269\n",
      "epoch: 2 step: 1475, loss is 0.023531045764684677\n",
      "epoch: 2 step: 1476, loss is 0.04946829006075859\n",
      "epoch: 2 step: 1477, loss is 0.0009182968642562628\n",
      "epoch: 2 step: 1478, loss is 0.05074258893728256\n",
      "epoch: 2 step: 1479, loss is 0.023582492023706436\n",
      "epoch: 2 step: 1480, loss is 0.06143834441900253\n",
      "epoch: 2 step: 1481, loss is 0.01593909040093422\n",
      "epoch: 2 step: 1482, loss is 0.16361601650714874\n",
      "epoch: 2 step: 1483, loss is 0.003417402971535921\n",
      "epoch: 2 step: 1484, loss is 0.18889594078063965\n",
      "epoch: 2 step: 1485, loss is 0.04688151180744171\n",
      "epoch: 2 step: 1486, loss is 0.038754068315029144\n",
      "epoch: 2 step: 1487, loss is 0.1656927466392517\n",
      "epoch: 2 step: 1488, loss is 0.03534570336341858\n",
      "epoch: 2 step: 1489, loss is 0.10992178320884705\n",
      "epoch: 2 step: 1490, loss is 0.034130293875932693\n",
      "epoch: 2 step: 1491, loss is 0.08334726840257645\n",
      "epoch: 2 step: 1492, loss is 0.004336426500231028\n",
      "epoch: 2 step: 1493, loss is 0.08730144053697586\n",
      "epoch: 2 step: 1494, loss is 0.12903515994548798\n",
      "epoch: 2 step: 1495, loss is 0.19575496017932892\n",
      "epoch: 2 step: 1496, loss is 0.03372279927134514\n",
      "epoch: 2 step: 1497, loss is 0.06577105820178986\n",
      "epoch: 2 step: 1498, loss is 0.002491771476343274\n",
      "epoch: 2 step: 1499, loss is 0.042930833995342255\n",
      "epoch: 2 step: 1500, loss is 0.13260997831821442\n",
      "epoch: 2 step: 1501, loss is 0.012295999564230442\n",
      "epoch: 2 step: 1502, loss is 0.0563562735915184\n",
      "epoch: 2 step: 1503, loss is 0.012975727207958698\n",
      "epoch: 2 step: 1504, loss is 0.030310437083244324\n",
      "epoch: 2 step: 1505, loss is 0.016383351758122444\n",
      "epoch: 2 step: 1506, loss is 0.08803042769432068\n",
      "epoch: 2 step: 1507, loss is 0.09353207796812057\n",
      "epoch: 2 step: 1508, loss is 0.0909467563033104\n",
      "epoch: 2 step: 1509, loss is 0.08733963966369629\n",
      "epoch: 2 step: 1510, loss is 0.03544212877750397\n",
      "epoch: 2 step: 1511, loss is 0.008698219433426857\n",
      "epoch: 2 step: 1512, loss is 0.011020995676517487\n",
      "epoch: 2 step: 1513, loss is 0.013671932741999626\n",
      "epoch: 2 step: 1514, loss is 0.006829849444329739\n",
      "epoch: 2 step: 1515, loss is 0.08236373960971832\n",
      "epoch: 2 step: 1516, loss is 0.007125047966837883\n",
      "epoch: 2 step: 1517, loss is 0.03623185679316521\n",
      "epoch: 2 step: 1518, loss is 0.005001457873731852\n",
      "epoch: 2 step: 1519, loss is 0.01525556854903698\n",
      "epoch: 2 step: 1520, loss is 0.024607475847005844\n",
      "epoch: 2 step: 1521, loss is 0.0033265158999711275\n",
      "epoch: 2 step: 1522, loss is 0.0012205003295093775\n",
      "epoch: 2 step: 1523, loss is 0.05932724475860596\n",
      "epoch: 2 step: 1524, loss is 0.03568540886044502\n",
      "epoch: 2 step: 1525, loss is 0.0016126336995512247\n",
      "epoch: 2 step: 1526, loss is 0.004013806581497192\n",
      "epoch: 2 step: 1527, loss is 0.013706539757549763\n",
      "epoch: 2 step: 1528, loss is 0.13274171948432922\n",
      "epoch: 2 step: 1529, loss is 0.04329949617385864\n",
      "epoch: 2 step: 1530, loss is 0.14477582275867462\n",
      "epoch: 2 step: 1531, loss is 0.010400851257145405\n",
      "epoch: 2 step: 1532, loss is 0.12211451679468155\n",
      "epoch: 2 step: 1533, loss is 0.0022339331917464733\n",
      "epoch: 2 step: 1534, loss is 0.06330346316099167\n",
      "epoch: 2 step: 1535, loss is 0.0719028189778328\n",
      "epoch: 2 step: 1536, loss is 0.008330874145030975\n",
      "epoch: 2 step: 1537, loss is 0.0034439738374203444\n",
      "epoch: 2 step: 1538, loss is 0.055884797126054764\n",
      "epoch: 2 step: 1539, loss is 0.02129056304693222\n",
      "epoch: 2 step: 1540, loss is 0.0015579690225422382\n",
      "epoch: 2 step: 1541, loss is 0.1836370974779129\n",
      "epoch: 2 step: 1542, loss is 0.01018229965120554\n",
      "epoch: 2 step: 1543, loss is 0.006928950548171997\n",
      "epoch: 2 step: 1544, loss is 0.009252483956515789\n",
      "epoch: 2 step: 1545, loss is 0.004329681396484375\n",
      "epoch: 2 step: 1546, loss is 0.07603233307600021\n",
      "epoch: 2 step: 1547, loss is 0.04724904149770737\n",
      "epoch: 2 step: 1548, loss is 0.022680237889289856\n",
      "epoch: 2 step: 1549, loss is 0.021481016650795937\n",
      "epoch: 2 step: 1550, loss is 0.002023878274485469\n",
      "epoch: 2 step: 1551, loss is 0.044630732387304306\n",
      "epoch: 2 step: 1552, loss is 0.04728145897388458\n",
      "epoch: 2 step: 1553, loss is 0.1448948234319687\n",
      "epoch: 2 step: 1554, loss is 0.049585483968257904\n",
      "epoch: 2 step: 1555, loss is 0.006824026349931955\n",
      "epoch: 2 step: 1556, loss is 0.05899393558502197\n",
      "epoch: 2 step: 1557, loss is 0.04758218675851822\n",
      "epoch: 2 step: 1558, loss is 0.00798839796334505\n",
      "epoch: 2 step: 1559, loss is 0.005262061487883329\n",
      "epoch: 2 step: 1560, loss is 0.0064763519912958145\n",
      "epoch: 2 step: 1561, loss is 0.1685420274734497\n",
      "epoch: 2 step: 1562, loss is 0.010381795465946198\n",
      "epoch: 2 step: 1563, loss is 0.0050840964540839195\n",
      "epoch: 2 step: 1564, loss is 0.0018376503139734268\n",
      "epoch: 2 step: 1565, loss is 0.02727956511080265\n",
      "epoch: 2 step: 1566, loss is 0.0029718014411628246\n",
      "epoch: 2 step: 1567, loss is 0.07883254438638687\n",
      "epoch: 2 step: 1568, loss is 0.01732877641916275\n",
      "epoch: 2 step: 1569, loss is 0.028691168874502182\n",
      "epoch: 2 step: 1570, loss is 0.012236841022968292\n",
      "epoch: 2 step: 1571, loss is 0.0133292768150568\n",
      "epoch: 2 step: 1572, loss is 0.006322587374597788\n",
      "epoch: 2 step: 1573, loss is 0.039691247045993805\n",
      "epoch: 2 step: 1574, loss is 0.0012618800392374396\n",
      "epoch: 2 step: 1575, loss is 0.08036710321903229\n",
      "epoch: 2 step: 1576, loss is 0.00998436938971281\n",
      "epoch: 2 step: 1577, loss is 0.1174265444278717\n",
      "epoch: 2 step: 1578, loss is 0.0007424720097333193\n",
      "epoch: 2 step: 1579, loss is 0.008021054789423943\n",
      "epoch: 2 step: 1580, loss is 0.01749955676496029\n",
      "epoch: 2 step: 1581, loss is 0.0033082838635891676\n",
      "epoch: 2 step: 1582, loss is 0.00183424586430192\n",
      "epoch: 2 step: 1583, loss is 0.1585925668478012\n",
      "epoch: 2 step: 1584, loss is 0.15999674797058105\n",
      "epoch: 2 step: 1585, loss is 0.004123501013964415\n",
      "epoch: 2 step: 1586, loss is 0.07930715382099152\n",
      "epoch: 2 step: 1587, loss is 0.12219202518463135\n",
      "epoch: 2 step: 1588, loss is 0.06795385479927063\n",
      "epoch: 2 step: 1589, loss is 0.08434411138296127\n",
      "epoch: 2 step: 1590, loss is 0.0033073483500629663\n",
      "epoch: 2 step: 1591, loss is 0.03260178491473198\n",
      "epoch: 2 step: 1592, loss is 0.007131217047572136\n",
      "epoch: 2 step: 1593, loss is 0.023490741848945618\n",
      "epoch: 2 step: 1594, loss is 0.0836649090051651\n",
      "epoch: 2 step: 1595, loss is 0.05845330283045769\n",
      "epoch: 2 step: 1596, loss is 0.2356107383966446\n",
      "epoch: 2 step: 1597, loss is 0.02501443214714527\n",
      "epoch: 2 step: 1598, loss is 0.13958740234375\n",
      "epoch: 2 step: 1599, loss is 0.005421874113380909\n",
      "epoch: 2 step: 1600, loss is 0.28058913350105286\n",
      "epoch: 2 step: 1601, loss is 0.0009580532205291092\n",
      "epoch: 2 step: 1602, loss is 0.002459676703438163\n",
      "epoch: 2 step: 1603, loss is 0.0013920204946771264\n",
      "epoch: 2 step: 1604, loss is 0.09927171468734741\n",
      "epoch: 2 step: 1605, loss is 0.3461907207965851\n",
      "epoch: 2 step: 1606, loss is 0.003296926850453019\n",
      "epoch: 2 step: 1607, loss is 0.026641856878995895\n",
      "epoch: 2 step: 1608, loss is 0.08386243134737015\n",
      "epoch: 2 step: 1609, loss is 0.03022943064570427\n",
      "epoch: 2 step: 1610, loss is 0.026166493073105812\n",
      "epoch: 2 step: 1611, loss is 0.028352800756692886\n",
      "epoch: 2 step: 1612, loss is 0.09840283542871475\n",
      "epoch: 2 step: 1613, loss is 0.041622597724199295\n",
      "epoch: 2 step: 1614, loss is 0.030143864452838898\n",
      "epoch: 2 step: 1615, loss is 0.015657400712370872\n",
      "epoch: 2 step: 1616, loss is 0.03760823979973793\n",
      "epoch: 2 step: 1617, loss is 0.013692563399672508\n",
      "epoch: 2 step: 1618, loss is 0.1550392508506775\n",
      "epoch: 2 step: 1619, loss is 0.0033381073735654354\n",
      "epoch: 2 step: 1620, loss is 0.16910770535469055\n",
      "epoch: 2 step: 1621, loss is 0.010906566865742207\n",
      "epoch: 2 step: 1622, loss is 0.04605424404144287\n",
      "epoch: 2 step: 1623, loss is 0.028954053297638893\n",
      "epoch: 2 step: 1624, loss is 0.2271450012922287\n",
      "epoch: 2 step: 1625, loss is 0.03358534723520279\n",
      "epoch: 2 step: 1626, loss is 0.051561884582042694\n",
      "epoch: 2 step: 1627, loss is 0.05489909648895264\n",
      "epoch: 2 step: 1628, loss is 0.021035190671682358\n",
      "epoch: 2 step: 1629, loss is 0.004606429487466812\n",
      "epoch: 2 step: 1630, loss is 0.026317259296774864\n",
      "epoch: 2 step: 1631, loss is 0.06649372726678848\n",
      "epoch: 2 step: 1632, loss is 0.20066793262958527\n",
      "epoch: 2 step: 1633, loss is 0.007668394595384598\n",
      "epoch: 2 step: 1634, loss is 0.14123579859733582\n",
      "epoch: 2 step: 1635, loss is 0.04724578186869621\n",
      "epoch: 2 step: 1636, loss is 0.12356863915920258\n",
      "epoch: 2 step: 1637, loss is 0.017937060445547104\n",
      "epoch: 2 step: 1638, loss is 0.12375447899103165\n",
      "epoch: 2 step: 1639, loss is 0.057625915855169296\n",
      "epoch: 2 step: 1640, loss is 0.04124099388718605\n",
      "epoch: 2 step: 1641, loss is 0.006125955376774073\n",
      "epoch: 2 step: 1642, loss is 0.2235998660326004\n",
      "epoch: 2 step: 1643, loss is 0.07627354562282562\n",
      "epoch: 2 step: 1644, loss is 0.010707365348935127\n",
      "epoch: 2 step: 1645, loss is 0.1195937916636467\n",
      "epoch: 2 step: 1646, loss is 0.006677377503365278\n",
      "epoch: 2 step: 1647, loss is 0.009698227979242802\n",
      "epoch: 2 step: 1648, loss is 0.01599174365401268\n",
      "epoch: 2 step: 1649, loss is 0.03201041370630264\n",
      "epoch: 2 step: 1650, loss is 0.10582345724105835\n",
      "epoch: 2 step: 1651, loss is 0.004206799436360598\n",
      "epoch: 2 step: 1652, loss is 0.052182793617248535\n",
      "epoch: 2 step: 1653, loss is 0.0322454608976841\n",
      "epoch: 2 step: 1654, loss is 0.01751553639769554\n",
      "epoch: 2 step: 1655, loss is 0.005925494711846113\n",
      "epoch: 2 step: 1656, loss is 0.034158702939748764\n",
      "epoch: 2 step: 1657, loss is 0.009415654465556145\n",
      "epoch: 2 step: 1658, loss is 0.014112778939306736\n",
      "epoch: 2 step: 1659, loss is 0.15111742913722992\n",
      "epoch: 2 step: 1660, loss is 0.0332210399210453\n",
      "epoch: 2 step: 1661, loss is 0.010167405009269714\n",
      "epoch: 2 step: 1662, loss is 0.023697765544056892\n",
      "epoch: 2 step: 1663, loss is 0.14462120831012726\n",
      "epoch: 2 step: 1664, loss is 0.05126495286822319\n",
      "epoch: 2 step: 1665, loss is 0.001056258799508214\n",
      "epoch: 2 step: 1666, loss is 0.08745832741260529\n",
      "epoch: 2 step: 1667, loss is 0.04068782925605774\n",
      "epoch: 2 step: 1668, loss is 0.01609250158071518\n",
      "epoch: 2 step: 1669, loss is 0.048972539603710175\n",
      "epoch: 2 step: 1670, loss is 0.15538077056407928\n",
      "epoch: 2 step: 1671, loss is 0.04199397191405296\n",
      "epoch: 2 step: 1672, loss is 0.2481856346130371\n",
      "epoch: 2 step: 1673, loss is 0.005545956548303366\n",
      "epoch: 2 step: 1674, loss is 0.115402452647686\n",
      "epoch: 2 step: 1675, loss is 0.015593217685818672\n",
      "epoch: 2 step: 1676, loss is 0.08322280645370483\n",
      "epoch: 2 step: 1677, loss is 0.049875304102897644\n",
      "epoch: 2 step: 1678, loss is 0.005599272903054953\n",
      "epoch: 2 step: 1679, loss is 0.059658896178007126\n",
      "epoch: 2 step: 1680, loss is 0.0031463620252907276\n",
      "epoch: 2 step: 1681, loss is 0.040174588561058044\n",
      "epoch: 2 step: 1682, loss is 0.15022780001163483\n",
      "epoch: 2 step: 1683, loss is 0.09892252087593079\n",
      "epoch: 2 step: 1684, loss is 0.003934585023671389\n",
      "epoch: 2 step: 1685, loss is 0.09228453040122986\n",
      "epoch: 2 step: 1686, loss is 0.005934908986091614\n",
      "epoch: 2 step: 1687, loss is 0.0981215164065361\n",
      "epoch: 2 step: 1688, loss is 0.010289636440575123\n",
      "epoch: 2 step: 1689, loss is 0.16139265894889832\n",
      "epoch: 2 step: 1690, loss is 0.047943115234375\n",
      "epoch: 2 step: 1691, loss is 0.03723595663905144\n",
      "epoch: 2 step: 1692, loss is 0.03196372836828232\n",
      "epoch: 2 step: 1693, loss is 0.13740701973438263\n",
      "epoch: 2 step: 1694, loss is 0.0008359085768461227\n",
      "epoch: 2 step: 1695, loss is 0.0017558502731844783\n",
      "epoch: 2 step: 1696, loss is 0.013171919621527195\n",
      "epoch: 2 step: 1697, loss is 0.0025822161696851254\n",
      "epoch: 2 step: 1698, loss is 0.011369531974196434\n",
      "epoch: 2 step: 1699, loss is 0.04471756890416145\n",
      "epoch: 2 step: 1700, loss is 0.0007982701645232737\n",
      "epoch: 2 step: 1701, loss is 0.16330194473266602\n",
      "epoch: 2 step: 1702, loss is 0.3195289373397827\n",
      "epoch: 2 step: 1703, loss is 0.0007448959513567388\n",
      "epoch: 2 step: 1704, loss is 0.22780679166316986\n",
      "epoch: 2 step: 1705, loss is 0.05166354775428772\n",
      "epoch: 2 step: 1706, loss is 0.0021143739577382803\n",
      "epoch: 2 step: 1707, loss is 0.012692554853856564\n",
      "epoch: 2 step: 1708, loss is 0.013529759831726551\n",
      "epoch: 2 step: 1709, loss is 0.04950593784451485\n",
      "epoch: 2 step: 1710, loss is 0.006605517119169235\n",
      "epoch: 2 step: 1711, loss is 0.05056723207235336\n",
      "epoch: 2 step: 1712, loss is 0.010190673172473907\n",
      "epoch: 2 step: 1713, loss is 0.1595480740070343\n",
      "epoch: 2 step: 1714, loss is 0.11556963622570038\n",
      "epoch: 2 step: 1715, loss is 0.03175613284111023\n",
      "epoch: 2 step: 1716, loss is 0.11088717728853226\n",
      "epoch: 2 step: 1717, loss is 0.0026364363729953766\n",
      "epoch: 2 step: 1718, loss is 0.028558766469359398\n",
      "epoch: 2 step: 1719, loss is 0.0373295396566391\n",
      "epoch: 2 step: 1720, loss is 0.28756678104400635\n",
      "epoch: 2 step: 1721, loss is 0.013205178081989288\n",
      "epoch: 2 step: 1722, loss is 0.03202248364686966\n",
      "epoch: 2 step: 1723, loss is 0.17361517250537872\n",
      "epoch: 2 step: 1724, loss is 0.0018417807295918465\n",
      "epoch: 2 step: 1725, loss is 0.007608449552208185\n",
      "epoch: 2 step: 1726, loss is 0.015865573659539223\n",
      "epoch: 2 step: 1727, loss is 0.02577992156147957\n",
      "epoch: 2 step: 1728, loss is 0.032685503363609314\n",
      "epoch: 2 step: 1729, loss is 0.07016286253929138\n",
      "epoch: 2 step: 1730, loss is 0.08082433044910431\n",
      "epoch: 2 step: 1731, loss is 0.07465850561857224\n",
      "epoch: 2 step: 1732, loss is 0.015941722318530083\n",
      "epoch: 2 step: 1733, loss is 0.0875944122672081\n",
      "epoch: 2 step: 1734, loss is 0.010341661982238293\n",
      "epoch: 2 step: 1735, loss is 0.00240931729786098\n",
      "epoch: 2 step: 1736, loss is 0.039637938141822815\n",
      "epoch: 2 step: 1737, loss is 0.04653509333729744\n",
      "epoch: 2 step: 1738, loss is 0.004231914412230253\n",
      "epoch: 2 step: 1739, loss is 0.0520615354180336\n",
      "epoch: 2 step: 1740, loss is 0.030171867460012436\n",
      "epoch: 2 step: 1741, loss is 0.006380175240337849\n",
      "epoch: 2 step: 1742, loss is 0.07919301837682724\n",
      "epoch: 2 step: 1743, loss is 0.25778740644454956\n",
      "epoch: 2 step: 1744, loss is 0.003936714492738247\n",
      "epoch: 2 step: 1745, loss is 0.014654640108346939\n",
      "epoch: 2 step: 1746, loss is 0.021412450820207596\n",
      "epoch: 2 step: 1747, loss is 0.1659431904554367\n",
      "epoch: 2 step: 1748, loss is 0.045324649661779404\n",
      "epoch: 2 step: 1749, loss is 0.02308237925171852\n",
      "epoch: 2 step: 1750, loss is 0.0018099489388987422\n",
      "epoch: 2 step: 1751, loss is 0.04501675069332123\n",
      "epoch: 2 step: 1752, loss is 0.004737833049148321\n",
      "epoch: 2 step: 1753, loss is 0.07333023846149445\n",
      "epoch: 2 step: 1754, loss is 0.004785419441759586\n",
      "epoch: 2 step: 1755, loss is 0.025440536439418793\n",
      "epoch: 2 step: 1756, loss is 0.17407730221748352\n",
      "epoch: 2 step: 1757, loss is 0.013831727206707\n",
      "epoch: 2 step: 1758, loss is 0.01223452016711235\n",
      "epoch: 2 step: 1759, loss is 0.0009677665657363832\n",
      "epoch: 2 step: 1760, loss is 0.018685825169086456\n",
      "epoch: 2 step: 1761, loss is 0.08692746609449387\n",
      "epoch: 2 step: 1762, loss is 0.27477389574050903\n",
      "epoch: 2 step: 1763, loss is 0.17668505012989044\n",
      "epoch: 2 step: 1764, loss is 0.0009362204000353813\n",
      "epoch: 2 step: 1765, loss is 0.13557225465774536\n",
      "epoch: 2 step: 1766, loss is 0.0033397951629012823\n",
      "epoch: 2 step: 1767, loss is 0.18583881855010986\n",
      "epoch: 2 step: 1768, loss is 0.02055036462843418\n",
      "epoch: 2 step: 1769, loss is 0.10631677508354187\n",
      "epoch: 2 step: 1770, loss is 0.007964758202433586\n",
      "epoch: 2 step: 1771, loss is 0.1350025236606598\n",
      "epoch: 2 step: 1772, loss is 0.07087016850709915\n",
      "epoch: 2 step: 1773, loss is 0.010705140419304371\n",
      "epoch: 2 step: 1774, loss is 0.020811904221773148\n",
      "epoch: 2 step: 1775, loss is 0.12036007642745972\n",
      "epoch: 2 step: 1776, loss is 0.13186146318912506\n",
      "epoch: 2 step: 1777, loss is 0.10742909461259842\n",
      "epoch: 2 step: 1778, loss is 0.11897216737270355\n",
      "epoch: 2 step: 1779, loss is 0.011409549973905087\n",
      "epoch: 2 step: 1780, loss is 0.20965220034122467\n",
      "epoch: 2 step: 1781, loss is 0.013339290395379066\n",
      "epoch: 2 step: 1782, loss is 0.023743074387311935\n",
      "epoch: 2 step: 1783, loss is 0.06836925446987152\n",
      "epoch: 2 step: 1784, loss is 0.010660027153789997\n",
      "epoch: 2 step: 1785, loss is 0.12316789478063583\n",
      "epoch: 2 step: 1786, loss is 0.007192631717771292\n",
      "epoch: 2 step: 1787, loss is 0.014553720131516457\n",
      "epoch: 2 step: 1788, loss is 0.062150295823812485\n",
      "epoch: 2 step: 1789, loss is 0.006341119762510061\n",
      "epoch: 2 step: 1790, loss is 0.060069117695093155\n",
      "epoch: 2 step: 1791, loss is 0.0054834382608532906\n",
      "epoch: 2 step: 1792, loss is 0.14276596903800964\n",
      "epoch: 2 step: 1793, loss is 0.0343133881688118\n",
      "epoch: 2 step: 1794, loss is 0.011462947353720665\n",
      "epoch: 2 step: 1795, loss is 0.011701870709657669\n",
      "epoch: 2 step: 1796, loss is 0.04422722011804581\n",
      "epoch: 2 step: 1797, loss is 0.056043773889541626\n",
      "epoch: 2 step: 1798, loss is 0.01756661757826805\n",
      "epoch: 2 step: 1799, loss is 0.07555095851421356\n",
      "epoch: 2 step: 1800, loss is 0.004327579401433468\n",
      "epoch: 2 step: 1801, loss is 0.019310051575303078\n",
      "epoch: 2 step: 1802, loss is 0.005987019278109074\n",
      "epoch: 2 step: 1803, loss is 0.03861420974135399\n",
      "epoch: 2 step: 1804, loss is 0.2449817955493927\n",
      "epoch: 2 step: 1805, loss is 0.005881885066628456\n",
      "epoch: 2 step: 1806, loss is 0.27856117486953735\n",
      "epoch: 2 step: 1807, loss is 0.03643697500228882\n",
      "epoch: 2 step: 1808, loss is 0.004258894827216864\n",
      "epoch: 2 step: 1809, loss is 0.14835773408412933\n",
      "epoch: 2 step: 1810, loss is 0.17075523734092712\n",
      "epoch: 2 step: 1811, loss is 0.03755123168230057\n",
      "epoch: 2 step: 1812, loss is 0.05609362572431564\n",
      "epoch: 2 step: 1813, loss is 0.0032382290810346603\n",
      "epoch: 2 step: 1814, loss is 0.010373868979513645\n",
      "epoch: 2 step: 1815, loss is 0.005253293085843325\n",
      "epoch: 2 step: 1816, loss is 0.03085477091372013\n",
      "epoch: 2 step: 1817, loss is 0.2895771563053131\n",
      "epoch: 2 step: 1818, loss is 0.04745945706963539\n",
      "epoch: 2 step: 1819, loss is 0.2909107804298401\n",
      "epoch: 2 step: 1820, loss is 0.010198854841291904\n",
      "epoch: 2 step: 1821, loss is 0.0061163753271102905\n",
      "epoch: 2 step: 1822, loss is 0.014875472523272038\n",
      "epoch: 2 step: 1823, loss is 0.04635586217045784\n",
      "epoch: 2 step: 1824, loss is 0.014930743724107742\n",
      "epoch: 2 step: 1825, loss is 0.28081339597702026\n",
      "epoch: 2 step: 1826, loss is 0.10795436054468155\n",
      "epoch: 2 step: 1827, loss is 0.10957273840904236\n",
      "epoch: 2 step: 1828, loss is 0.011247438378632069\n",
      "epoch: 2 step: 1829, loss is 0.08015395700931549\n",
      "epoch: 2 step: 1830, loss is 0.006415244657546282\n",
      "epoch: 2 step: 1831, loss is 0.0008602619636803865\n",
      "epoch: 2 step: 1832, loss is 0.0033268521074205637\n",
      "epoch: 2 step: 1833, loss is 0.005698809400200844\n",
      "epoch: 2 step: 1834, loss is 0.0037972324062138796\n",
      "epoch: 2 step: 1835, loss is 0.006586212199181318\n",
      "epoch: 2 step: 1836, loss is 0.03747057542204857\n",
      "epoch: 2 step: 1837, loss is 0.014598120003938675\n",
      "epoch: 2 step: 1838, loss is 0.005851789843291044\n",
      "epoch: 2 step: 1839, loss is 0.02798796072602272\n",
      "epoch: 2 step: 1840, loss is 0.04266759008169174\n",
      "epoch: 2 step: 1841, loss is 0.060999397188425064\n",
      "epoch: 2 step: 1842, loss is 0.09163855016231537\n",
      "epoch: 2 step: 1843, loss is 0.04501555114984512\n",
      "epoch: 2 step: 1844, loss is 0.04193394258618355\n",
      "epoch: 2 step: 1845, loss is 0.001702240202575922\n",
      "epoch: 2 step: 1846, loss is 0.0006301788380369544\n",
      "epoch: 2 step: 1847, loss is 0.0076673598960042\n",
      "epoch: 2 step: 1848, loss is 0.010637851431965828\n",
      "epoch: 2 step: 1849, loss is 0.013136562891304493\n",
      "epoch: 2 step: 1850, loss is 0.006948589812964201\n",
      "epoch: 2 step: 1851, loss is 0.10164637863636017\n",
      "epoch: 2 step: 1852, loss is 0.1948852837085724\n",
      "epoch: 2 step: 1853, loss is 0.05879870057106018\n",
      "epoch: 2 step: 1854, loss is 0.06227908656001091\n",
      "epoch: 2 step: 1855, loss is 0.013454076834022999\n",
      "epoch: 2 step: 1856, loss is 0.015003823675215244\n",
      "epoch: 2 step: 1857, loss is 0.019172469154000282\n",
      "epoch: 2 step: 1858, loss is 0.007350229658186436\n",
      "epoch: 2 step: 1859, loss is 0.024463890120387077\n",
      "epoch: 2 step: 1860, loss is 0.009505065158009529\n",
      "epoch: 2 step: 1861, loss is 0.028410928323864937\n",
      "epoch: 2 step: 1862, loss is 0.026535725221037865\n",
      "epoch: 2 step: 1863, loss is 0.033475618809461594\n",
      "epoch: 2 step: 1864, loss is 0.026119867339730263\n",
      "epoch: 2 step: 1865, loss is 0.024280015379190445\n",
      "epoch: 2 step: 1866, loss is 0.014200126752257347\n",
      "epoch: 2 step: 1867, loss is 0.004066594876348972\n",
      "epoch: 2 step: 1868, loss is 0.027170008048415184\n",
      "epoch: 2 step: 1869, loss is 0.05633851885795593\n",
      "epoch: 2 step: 1870, loss is 0.045246709138154984\n",
      "epoch: 2 step: 1871, loss is 0.05329384282231331\n",
      "epoch: 2 step: 1872, loss is 0.03823507949709892\n",
      "epoch: 2 step: 1873, loss is 0.09814324229955673\n",
      "epoch: 2 step: 1874, loss is 0.007498188875615597\n",
      "epoch: 2 step: 1875, loss is 0.0005025026039220393\n",
      "epoch: 3 step: 1, loss is 0.011337526142597198\n",
      "epoch: 3 step: 2, loss is 0.04606396332383156\n",
      "epoch: 3 step: 3, loss is 0.12106738239526749\n",
      "epoch: 3 step: 4, loss is 0.0764196440577507\n",
      "epoch: 3 step: 5, loss is 0.07577398419380188\n",
      "epoch: 3 step: 6, loss is 0.0022372829262167215\n",
      "epoch: 3 step: 7, loss is 0.03236466646194458\n",
      "epoch: 3 step: 8, loss is 0.02042556181550026\n",
      "epoch: 3 step: 9, loss is 0.002308130031451583\n",
      "epoch: 3 step: 10, loss is 0.5858010053634644\n",
      "epoch: 3 step: 11, loss is 0.004470203537493944\n",
      "epoch: 3 step: 12, loss is 0.07452259212732315\n",
      "epoch: 3 step: 13, loss is 0.008812738582491875\n",
      "epoch: 3 step: 14, loss is 0.032904013991355896\n",
      "epoch: 3 step: 15, loss is 0.02525738812983036\n",
      "epoch: 3 step: 16, loss is 0.007350629195570946\n",
      "epoch: 3 step: 17, loss is 0.015004200860857964\n",
      "epoch: 3 step: 18, loss is 0.013780458830296993\n",
      "epoch: 3 step: 19, loss is 0.002054511336609721\n",
      "epoch: 3 step: 20, loss is 0.027833381667733192\n",
      "epoch: 3 step: 21, loss is 0.15655356645584106\n",
      "epoch: 3 step: 22, loss is 0.010454399511218071\n",
      "epoch: 3 step: 23, loss is 0.08911755681037903\n",
      "epoch: 3 step: 24, loss is 0.0035132826305925846\n",
      "epoch: 3 step: 25, loss is 0.004806248005479574\n",
      "epoch: 3 step: 26, loss is 0.0027450942434370518\n",
      "epoch: 3 step: 27, loss is 0.016346698626875877\n",
      "epoch: 3 step: 28, loss is 0.11506824195384979\n",
      "epoch: 3 step: 29, loss is 0.0738031193614006\n",
      "epoch: 3 step: 30, loss is 0.005224223248660564\n",
      "epoch: 3 step: 31, loss is 0.048395246267318726\n",
      "epoch: 3 step: 32, loss is 0.03973780944943428\n",
      "epoch: 3 step: 33, loss is 0.011757081374526024\n",
      "epoch: 3 step: 34, loss is 0.07604504376649857\n",
      "epoch: 3 step: 35, loss is 0.21083787083625793\n",
      "epoch: 3 step: 36, loss is 0.0006128196255303919\n",
      "epoch: 3 step: 37, loss is 0.0516488142311573\n",
      "epoch: 3 step: 38, loss is 0.0038442350924015045\n",
      "epoch: 3 step: 39, loss is 0.08866704255342484\n",
      "epoch: 3 step: 40, loss is 0.11575757712125778\n",
      "epoch: 3 step: 41, loss is 0.08397471904754639\n",
      "epoch: 3 step: 42, loss is 0.06627273559570312\n",
      "epoch: 3 step: 43, loss is 0.0570099875330925\n",
      "epoch: 3 step: 44, loss is 0.04404807090759277\n",
      "epoch: 3 step: 45, loss is 0.008166876621544361\n",
      "epoch: 3 step: 46, loss is 0.02217910997569561\n",
      "epoch: 3 step: 47, loss is 0.0018581101903691888\n",
      "epoch: 3 step: 48, loss is 0.007161599118262529\n",
      "epoch: 3 step: 49, loss is 0.005090004298835993\n",
      "epoch: 3 step: 50, loss is 0.08693519234657288\n",
      "epoch: 3 step: 51, loss is 0.008703751489520073\n",
      "epoch: 3 step: 52, loss is 0.07488102465867996\n",
      "epoch: 3 step: 53, loss is 0.005286891013383865\n",
      "epoch: 3 step: 54, loss is 0.041121188551187515\n",
      "epoch: 3 step: 55, loss is 0.00192354922182858\n",
      "epoch: 3 step: 56, loss is 0.009398932568728924\n",
      "epoch: 3 step: 57, loss is 0.014175228774547577\n",
      "epoch: 3 step: 58, loss is 0.06184466555714607\n",
      "epoch: 3 step: 59, loss is 0.08157691359519958\n",
      "epoch: 3 step: 60, loss is 0.003130757249891758\n",
      "epoch: 3 step: 61, loss is 0.2471226453781128\n",
      "epoch: 3 step: 62, loss is 0.009082886390388012\n",
      "epoch: 3 step: 63, loss is 0.014587631449103355\n",
      "epoch: 3 step: 64, loss is 0.0842619389295578\n",
      "epoch: 3 step: 65, loss is 0.02265436202287674\n",
      "epoch: 3 step: 66, loss is 0.005896216724067926\n",
      "epoch: 3 step: 67, loss is 0.03526286035776138\n",
      "epoch: 3 step: 68, loss is 0.016402944922447205\n",
      "epoch: 3 step: 69, loss is 0.03843960165977478\n",
      "epoch: 3 step: 70, loss is 0.0227198526263237\n",
      "epoch: 3 step: 71, loss is 0.003993837162852287\n",
      "epoch: 3 step: 72, loss is 0.028897106647491455\n",
      "epoch: 3 step: 73, loss is 0.03261863812804222\n",
      "epoch: 3 step: 74, loss is 0.07607967406511307\n",
      "epoch: 3 step: 75, loss is 0.02186225913465023\n",
      "epoch: 3 step: 76, loss is 0.1461533159017563\n",
      "epoch: 3 step: 77, loss is 0.010164614766836166\n",
      "epoch: 3 step: 78, loss is 0.00439866166561842\n",
      "epoch: 3 step: 79, loss is 0.04868661239743233\n",
      "epoch: 3 step: 80, loss is 0.0008256678702309728\n",
      "epoch: 3 step: 81, loss is 0.00021391241170931607\n",
      "epoch: 3 step: 82, loss is 0.001592496526427567\n",
      "epoch: 3 step: 83, loss is 0.033770833164453506\n",
      "epoch: 3 step: 84, loss is 0.061342786997556686\n",
      "epoch: 3 step: 85, loss is 0.009057881310582161\n",
      "epoch: 3 step: 86, loss is 0.061866164207458496\n",
      "epoch: 3 step: 87, loss is 0.0002495752996765077\n",
      "epoch: 3 step: 88, loss is 0.0017270379466935992\n",
      "epoch: 3 step: 89, loss is 0.0010804012417793274\n",
      "epoch: 3 step: 90, loss is 0.021462714299559593\n",
      "epoch: 3 step: 91, loss is 0.01675206609070301\n",
      "epoch: 3 step: 92, loss is 0.124130018055439\n",
      "epoch: 3 step: 93, loss is 0.004248305223882198\n",
      "epoch: 3 step: 94, loss is 0.07515028864145279\n",
      "epoch: 3 step: 95, loss is 0.3012140691280365\n",
      "epoch: 3 step: 96, loss is 0.01590639166533947\n",
      "epoch: 3 step: 97, loss is 0.11465805023908615\n",
      "epoch: 3 step: 98, loss is 0.001286640646867454\n",
      "epoch: 3 step: 99, loss is 0.006316109094768763\n",
      "epoch: 3 step: 100, loss is 0.02225251868367195\n",
      "epoch: 3 step: 101, loss is 0.03788316249847412\n",
      "epoch: 3 step: 102, loss is 0.030238721519708633\n",
      "epoch: 3 step: 103, loss is 0.0035107883159071207\n",
      "epoch: 3 step: 104, loss is 0.2258489727973938\n",
      "epoch: 3 step: 105, loss is 0.03895619139075279\n",
      "epoch: 3 step: 106, loss is 0.013111919164657593\n",
      "epoch: 3 step: 107, loss is 0.09936313331127167\n",
      "epoch: 3 step: 108, loss is 0.010072833858430386\n",
      "epoch: 3 step: 109, loss is 0.0018908771453425288\n",
      "epoch: 3 step: 110, loss is 0.4005615711212158\n",
      "epoch: 3 step: 111, loss is 0.011498687788844109\n",
      "epoch: 3 step: 112, loss is 0.037594784051179886\n",
      "epoch: 3 step: 113, loss is 0.050636183470487595\n",
      "epoch: 3 step: 114, loss is 0.006486766040325165\n",
      "epoch: 3 step: 115, loss is 0.04623398557305336\n",
      "epoch: 3 step: 116, loss is 0.08883797377347946\n",
      "epoch: 3 step: 117, loss is 0.07237070798873901\n",
      "epoch: 3 step: 118, loss is 0.07953950017690659\n",
      "epoch: 3 step: 119, loss is 0.0065263379365205765\n",
      "epoch: 3 step: 120, loss is 0.003365530166774988\n",
      "epoch: 3 step: 121, loss is 0.033310145139694214\n",
      "epoch: 3 step: 122, loss is 0.029555583372712135\n",
      "epoch: 3 step: 123, loss is 0.003522861050441861\n",
      "epoch: 3 step: 124, loss is 0.0337529256939888\n",
      "epoch: 3 step: 125, loss is 0.005956933367997408\n",
      "epoch: 3 step: 126, loss is 0.12844416499137878\n",
      "epoch: 3 step: 127, loss is 0.002292381599545479\n",
      "epoch: 3 step: 128, loss is 0.0003079358139075339\n",
      "epoch: 3 step: 129, loss is 0.02497684769332409\n",
      "epoch: 3 step: 130, loss is 0.13967537879943848\n",
      "epoch: 3 step: 131, loss is 0.03771563246846199\n",
      "epoch: 3 step: 132, loss is 0.0025223398115485907\n",
      "epoch: 3 step: 133, loss is 0.005622816737741232\n",
      "epoch: 3 step: 134, loss is 0.0508120097219944\n",
      "epoch: 3 step: 135, loss is 0.03909924253821373\n",
      "epoch: 3 step: 136, loss is 0.013381428085267544\n",
      "epoch: 3 step: 137, loss is 0.02461518719792366\n",
      "epoch: 3 step: 138, loss is 0.39939430356025696\n",
      "epoch: 3 step: 139, loss is 0.010812855325639248\n",
      "epoch: 3 step: 140, loss is 0.05187061056494713\n",
      "epoch: 3 step: 141, loss is 0.04220189154148102\n",
      "epoch: 3 step: 142, loss is 0.08842776715755463\n",
      "epoch: 3 step: 143, loss is 0.0574922114610672\n",
      "epoch: 3 step: 144, loss is 0.003913847263902426\n",
      "epoch: 3 step: 145, loss is 0.0005156115512363613\n",
      "epoch: 3 step: 146, loss is 0.009754439815878868\n",
      "epoch: 3 step: 147, loss is 0.0017874095356091857\n",
      "epoch: 3 step: 148, loss is 0.27197855710983276\n",
      "epoch: 3 step: 149, loss is 0.02398790791630745\n",
      "epoch: 3 step: 150, loss is 0.004491024184972048\n",
      "epoch: 3 step: 151, loss is 0.007023234851658344\n",
      "epoch: 3 step: 152, loss is 0.06911266595125198\n",
      "epoch: 3 step: 153, loss is 0.052255891263484955\n",
      "epoch: 3 step: 154, loss is 0.01481523085385561\n",
      "epoch: 3 step: 155, loss is 0.02655864506959915\n",
      "epoch: 3 step: 156, loss is 0.016158731654286385\n",
      "epoch: 3 step: 157, loss is 0.21546487510204315\n",
      "epoch: 3 step: 158, loss is 0.06287059187889099\n",
      "epoch: 3 step: 159, loss is 0.007380823139101267\n",
      "epoch: 3 step: 160, loss is 0.10889210551977158\n",
      "epoch: 3 step: 161, loss is 0.04589417576789856\n",
      "epoch: 3 step: 162, loss is 0.005323642399162054\n",
      "epoch: 3 step: 163, loss is 0.014953197911381721\n",
      "epoch: 3 step: 164, loss is 0.05257114768028259\n",
      "epoch: 3 step: 165, loss is 0.022969035431742668\n",
      "epoch: 3 step: 166, loss is 0.03739546239376068\n",
      "epoch: 3 step: 167, loss is 0.32565534114837646\n",
      "epoch: 3 step: 168, loss is 0.030005095526576042\n",
      "epoch: 3 step: 169, loss is 0.16565831005573273\n",
      "epoch: 3 step: 170, loss is 0.0057570612989366055\n",
      "epoch: 3 step: 171, loss is 0.07394736260175705\n",
      "epoch: 3 step: 172, loss is 0.07089387625455856\n",
      "epoch: 3 step: 173, loss is 0.0912293866276741\n",
      "epoch: 3 step: 174, loss is 0.00795464962720871\n",
      "epoch: 3 step: 175, loss is 0.19433298707008362\n",
      "epoch: 3 step: 176, loss is 0.0016358388820663095\n",
      "epoch: 3 step: 177, loss is 0.03192421793937683\n",
      "epoch: 3 step: 178, loss is 0.02373950369656086\n",
      "epoch: 3 step: 179, loss is 0.014420310035347939\n",
      "epoch: 3 step: 180, loss is 0.013549662195146084\n",
      "epoch: 3 step: 181, loss is 0.05981897935271263\n",
      "epoch: 3 step: 182, loss is 0.06093079224228859\n",
      "epoch: 3 step: 183, loss is 0.1064835712313652\n",
      "epoch: 3 step: 184, loss is 0.0017685452476143837\n",
      "epoch: 3 step: 185, loss is 0.05408228933811188\n",
      "epoch: 3 step: 186, loss is 0.013548440299928188\n",
      "epoch: 3 step: 187, loss is 0.0028080588672310114\n",
      "epoch: 3 step: 188, loss is 0.023701902478933334\n",
      "epoch: 3 step: 189, loss is 0.001043228548951447\n",
      "epoch: 3 step: 190, loss is 0.008025023154914379\n",
      "epoch: 3 step: 191, loss is 0.056221913546323776\n",
      "epoch: 3 step: 192, loss is 0.0020597288385033607\n",
      "epoch: 3 step: 193, loss is 0.0013853004202246666\n",
      "epoch: 3 step: 194, loss is 0.005325787700712681\n",
      "epoch: 3 step: 195, loss is 0.009464004077017307\n",
      "epoch: 3 step: 196, loss is 0.009415974840521812\n",
      "epoch: 3 step: 197, loss is 0.03987271338701248\n",
      "epoch: 3 step: 198, loss is 0.03408180549740791\n",
      "epoch: 3 step: 199, loss is 0.07679345458745956\n",
      "epoch: 3 step: 200, loss is 0.009443259797990322\n",
      "epoch: 3 step: 201, loss is 0.016627876088023186\n",
      "epoch: 3 step: 202, loss is 0.05154993385076523\n",
      "epoch: 3 step: 203, loss is 0.022590456530451775\n",
      "epoch: 3 step: 204, loss is 0.13779225945472717\n",
      "epoch: 3 step: 205, loss is 0.013150853104889393\n",
      "epoch: 3 step: 206, loss is 0.0017966271843761206\n",
      "epoch: 3 step: 207, loss is 0.001355414162389934\n",
      "epoch: 3 step: 208, loss is 0.00800353568047285\n",
      "epoch: 3 step: 209, loss is 0.006737523712217808\n",
      "epoch: 3 step: 210, loss is 0.017281007021665573\n",
      "epoch: 3 step: 211, loss is 0.013021108694374561\n",
      "epoch: 3 step: 212, loss is 0.004863101523369551\n",
      "epoch: 3 step: 213, loss is 0.0007543652318418026\n",
      "epoch: 3 step: 214, loss is 0.0081379609182477\n",
      "epoch: 3 step: 215, loss is 0.005681714043021202\n",
      "epoch: 3 step: 216, loss is 0.0011470647295936942\n",
      "epoch: 3 step: 217, loss is 0.03136207163333893\n",
      "epoch: 3 step: 218, loss is 0.0018426906317472458\n",
      "epoch: 3 step: 219, loss is 0.027877481654286385\n",
      "epoch: 3 step: 220, loss is 0.01438916102051735\n",
      "epoch: 3 step: 221, loss is 0.01773553341627121\n",
      "epoch: 3 step: 222, loss is 0.0035852943547070026\n",
      "epoch: 3 step: 223, loss is 0.04973636195063591\n",
      "epoch: 3 step: 224, loss is 0.02001357451081276\n",
      "epoch: 3 step: 225, loss is 0.009037387557327747\n",
      "epoch: 3 step: 226, loss is 0.21676650643348694\n",
      "epoch: 3 step: 227, loss is 0.0014635182451456785\n",
      "epoch: 3 step: 228, loss is 0.002727469662204385\n",
      "epoch: 3 step: 229, loss is 0.003321232972666621\n",
      "epoch: 3 step: 230, loss is 0.0012425212189555168\n",
      "epoch: 3 step: 231, loss is 0.006010361015796661\n",
      "epoch: 3 step: 232, loss is 0.0005187925999052823\n",
      "epoch: 3 step: 233, loss is 0.023449599742889404\n",
      "epoch: 3 step: 234, loss is 0.009086055681109428\n",
      "epoch: 3 step: 235, loss is 0.1069960817694664\n",
      "epoch: 3 step: 236, loss is 0.003204201813787222\n",
      "epoch: 3 step: 237, loss is 0.0014368090778589249\n",
      "epoch: 3 step: 238, loss is 0.0017348382389172912\n",
      "epoch: 3 step: 239, loss is 0.0010056492174044251\n",
      "epoch: 3 step: 240, loss is 0.0002944712759926915\n",
      "epoch: 3 step: 241, loss is 0.0004905015230178833\n",
      "epoch: 3 step: 242, loss is 0.06753778457641602\n",
      "epoch: 3 step: 243, loss is 0.010964963585138321\n",
      "epoch: 3 step: 244, loss is 0.15090397000312805\n",
      "epoch: 3 step: 245, loss is 0.1051889955997467\n",
      "epoch: 3 step: 246, loss is 0.0070304530672729015\n",
      "epoch: 3 step: 247, loss is 0.004758266266435385\n",
      "epoch: 3 step: 248, loss is 0.01178685948252678\n",
      "epoch: 3 step: 249, loss is 0.10755163431167603\n",
      "epoch: 3 step: 250, loss is 0.009566252119839191\n",
      "epoch: 3 step: 251, loss is 0.0018735431367531419\n",
      "epoch: 3 step: 252, loss is 0.2691180109977722\n",
      "epoch: 3 step: 253, loss is 0.008340446278452873\n",
      "epoch: 3 step: 254, loss is 0.017217200249433517\n",
      "epoch: 3 step: 255, loss is 0.006787510123103857\n",
      "epoch: 3 step: 256, loss is 0.0035859812051057816\n",
      "epoch: 3 step: 257, loss is 0.019329417496919632\n",
      "epoch: 3 step: 258, loss is 0.021487155929207802\n",
      "epoch: 3 step: 259, loss is 0.0942971259355545\n",
      "epoch: 3 step: 260, loss is 0.003625405952334404\n",
      "epoch: 3 step: 261, loss is 0.04763110727071762\n",
      "epoch: 3 step: 262, loss is 0.030164998024702072\n",
      "epoch: 3 step: 263, loss is 0.06338883191347122\n",
      "epoch: 3 step: 264, loss is 0.12485453486442566\n",
      "epoch: 3 step: 265, loss is 0.006447837688028812\n",
      "epoch: 3 step: 266, loss is 0.008584218099713326\n",
      "epoch: 3 step: 267, loss is 0.07452717423439026\n",
      "epoch: 3 step: 268, loss is 0.10306007415056229\n",
      "epoch: 3 step: 269, loss is 0.15830400586128235\n",
      "epoch: 3 step: 270, loss is 0.007030327804386616\n",
      "epoch: 3 step: 271, loss is 0.09021910279989243\n",
      "epoch: 3 step: 272, loss is 0.0029734880663454533\n",
      "epoch: 3 step: 273, loss is 0.057542264461517334\n",
      "epoch: 3 step: 274, loss is 0.030438998714089394\n",
      "epoch: 3 step: 275, loss is 0.047679152339696884\n",
      "epoch: 3 step: 276, loss is 0.03924557566642761\n",
      "epoch: 3 step: 277, loss is 0.005269036162644625\n",
      "epoch: 3 step: 278, loss is 0.06934778392314911\n",
      "epoch: 3 step: 279, loss is 0.15681177377700806\n",
      "epoch: 3 step: 280, loss is 0.005052793771028519\n",
      "epoch: 3 step: 281, loss is 0.00470868032425642\n",
      "epoch: 3 step: 282, loss is 0.18166247010231018\n",
      "epoch: 3 step: 283, loss is 0.24570946395397186\n",
      "epoch: 3 step: 284, loss is 0.008992895483970642\n",
      "epoch: 3 step: 285, loss is 0.007807224057614803\n",
      "epoch: 3 step: 286, loss is 0.095549076795578\n",
      "epoch: 3 step: 287, loss is 0.21319839358329773\n",
      "epoch: 3 step: 288, loss is 0.23054289817810059\n",
      "epoch: 3 step: 289, loss is 0.022115740925073624\n",
      "epoch: 3 step: 290, loss is 0.09375010430812836\n",
      "epoch: 3 step: 291, loss is 0.010085980407893658\n",
      "epoch: 3 step: 292, loss is 0.11739467829465866\n",
      "epoch: 3 step: 293, loss is 0.13578636944293976\n",
      "epoch: 3 step: 294, loss is 0.14068955183029175\n",
      "epoch: 3 step: 295, loss is 0.013786080293357372\n",
      "epoch: 3 step: 296, loss is 0.028590304777026176\n",
      "epoch: 3 step: 297, loss is 0.00393449142575264\n",
      "epoch: 3 step: 298, loss is 0.016269555315375328\n",
      "epoch: 3 step: 299, loss is 0.0028069843538105488\n",
      "epoch: 3 step: 300, loss is 0.01522315014153719\n",
      "epoch: 3 step: 301, loss is 0.0465213768184185\n",
      "epoch: 3 step: 302, loss is 0.033604808151721954\n",
      "epoch: 3 step: 303, loss is 0.02095830999314785\n",
      "epoch: 3 step: 304, loss is 0.0044518145732581615\n",
      "epoch: 3 step: 305, loss is 0.017116978764533997\n",
      "epoch: 3 step: 306, loss is 0.12004698812961578\n",
      "epoch: 3 step: 307, loss is 0.0709356740117073\n",
      "epoch: 3 step: 308, loss is 0.06764533370733261\n",
      "epoch: 3 step: 309, loss is 0.0058770873583853245\n",
      "epoch: 3 step: 310, loss is 0.14486220479011536\n",
      "epoch: 3 step: 311, loss is 0.009791591204702854\n",
      "epoch: 3 step: 312, loss is 0.011767150834202766\n",
      "epoch: 3 step: 313, loss is 0.007114105857908726\n",
      "epoch: 3 step: 314, loss is 0.02500825561583042\n",
      "epoch: 3 step: 315, loss is 0.00038293577381409705\n",
      "epoch: 3 step: 316, loss is 0.016950929537415504\n",
      "epoch: 3 step: 317, loss is 0.01684265211224556\n",
      "epoch: 3 step: 318, loss is 0.04699117690324783\n",
      "epoch: 3 step: 319, loss is 0.006220878567546606\n",
      "epoch: 3 step: 320, loss is 0.03486527502536774\n",
      "epoch: 3 step: 321, loss is 0.27011239528656006\n",
      "epoch: 3 step: 322, loss is 0.012809713371098042\n",
      "epoch: 3 step: 323, loss is 0.006505132652819157\n",
      "epoch: 3 step: 324, loss is 0.019992507994174957\n",
      "epoch: 3 step: 325, loss is 0.002840587170794606\n",
      "epoch: 3 step: 326, loss is 0.030624065548181534\n",
      "epoch: 3 step: 327, loss is 0.022283731028437614\n",
      "epoch: 3 step: 328, loss is 0.0015230654971674085\n",
      "epoch: 3 step: 329, loss is 0.003904735902324319\n",
      "epoch: 3 step: 330, loss is 0.007463884074240923\n",
      "epoch: 3 step: 331, loss is 0.053234443068504333\n",
      "epoch: 3 step: 332, loss is 0.015527037903666496\n",
      "epoch: 3 step: 333, loss is 0.10767590999603271\n",
      "epoch: 3 step: 334, loss is 0.23207946121692657\n",
      "epoch: 3 step: 335, loss is 0.0055007003247737885\n",
      "epoch: 3 step: 336, loss is 0.04708860442042351\n",
      "epoch: 3 step: 337, loss is 0.01088973693549633\n",
      "epoch: 3 step: 338, loss is 0.051031868904829025\n",
      "epoch: 3 step: 339, loss is 0.007596397306770086\n",
      "epoch: 3 step: 340, loss is 0.0246542077511549\n",
      "epoch: 3 step: 341, loss is 0.006886322982609272\n",
      "epoch: 3 step: 342, loss is 0.057552777230739594\n",
      "epoch: 3 step: 343, loss is 0.048271410167217255\n",
      "epoch: 3 step: 344, loss is 0.0378565639257431\n",
      "epoch: 3 step: 345, loss is 0.028068941086530685\n",
      "epoch: 3 step: 346, loss is 0.234483003616333\n",
      "epoch: 3 step: 347, loss is 0.28687795996665955\n",
      "epoch: 3 step: 348, loss is 0.0015255114994943142\n",
      "epoch: 3 step: 349, loss is 0.030277017503976822\n",
      "epoch: 3 step: 350, loss is 0.04752911627292633\n",
      "epoch: 3 step: 351, loss is 0.3015110194683075\n",
      "epoch: 3 step: 352, loss is 0.02303665317595005\n",
      "epoch: 3 step: 353, loss is 0.00247631361708045\n",
      "epoch: 3 step: 354, loss is 0.07348823547363281\n",
      "epoch: 3 step: 355, loss is 0.014038321562111378\n",
      "epoch: 3 step: 356, loss is 0.007406684570014477\n",
      "epoch: 3 step: 357, loss is 0.001998899271711707\n",
      "epoch: 3 step: 358, loss is 0.0005939201219007373\n",
      "epoch: 3 step: 359, loss is 0.0048120697028934956\n",
      "epoch: 3 step: 360, loss is 0.015059289522469044\n",
      "epoch: 3 step: 361, loss is 0.07112275063991547\n",
      "epoch: 3 step: 362, loss is 0.06735877692699432\n",
      "epoch: 3 step: 363, loss is 0.05131655931472778\n",
      "epoch: 3 step: 364, loss is 0.09253348410129547\n",
      "epoch: 3 step: 365, loss is 0.009624424390494823\n",
      "epoch: 3 step: 366, loss is 0.03803445026278496\n",
      "epoch: 3 step: 367, loss is 0.11953968554735184\n",
      "epoch: 3 step: 368, loss is 0.006548403762280941\n",
      "epoch: 3 step: 369, loss is 0.012206769548356533\n",
      "epoch: 3 step: 370, loss is 0.0008468424784950912\n",
      "epoch: 3 step: 371, loss is 0.029415395110845566\n",
      "epoch: 3 step: 372, loss is 0.013674360699951649\n",
      "epoch: 3 step: 373, loss is 0.219279482960701\n",
      "epoch: 3 step: 374, loss is 0.088602714240551\n",
      "epoch: 3 step: 375, loss is 0.007989225909113884\n",
      "epoch: 3 step: 376, loss is 0.009315230883657932\n",
      "epoch: 3 step: 377, loss is 0.07351241260766983\n",
      "epoch: 3 step: 378, loss is 0.04259095340967178\n",
      "epoch: 3 step: 379, loss is 0.009455228224396706\n",
      "epoch: 3 step: 380, loss is 0.006487882696092129\n",
      "epoch: 3 step: 381, loss is 0.012020326219499111\n",
      "epoch: 3 step: 382, loss is 0.005921975243836641\n",
      "epoch: 3 step: 383, loss is 0.0013330635847523808\n",
      "epoch: 3 step: 384, loss is 0.008623223751783371\n",
      "epoch: 3 step: 385, loss is 0.0012468657223507762\n",
      "epoch: 3 step: 386, loss is 0.002087715547531843\n",
      "epoch: 3 step: 387, loss is 0.10459624975919724\n",
      "epoch: 3 step: 388, loss is 0.025250624865293503\n",
      "epoch: 3 step: 389, loss is 0.008975984528660774\n",
      "epoch: 3 step: 390, loss is 0.0473468042910099\n",
      "epoch: 3 step: 391, loss is 0.03083646483719349\n",
      "epoch: 3 step: 392, loss is 0.20301641523838043\n",
      "epoch: 3 step: 393, loss is 0.02085656300187111\n",
      "epoch: 3 step: 394, loss is 0.06334155797958374\n",
      "epoch: 3 step: 395, loss is 0.0620419941842556\n",
      "epoch: 3 step: 396, loss is 0.04253735765814781\n",
      "epoch: 3 step: 397, loss is 0.006970264948904514\n",
      "epoch: 3 step: 398, loss is 0.18430356681346893\n",
      "epoch: 3 step: 399, loss is 0.028326742351055145\n",
      "epoch: 3 step: 400, loss is 0.021278195083141327\n",
      "epoch: 3 step: 401, loss is 0.0021594525314867496\n",
      "epoch: 3 step: 402, loss is 0.1432424932718277\n",
      "epoch: 3 step: 403, loss is 0.284010112285614\n",
      "epoch: 3 step: 404, loss is 0.006304943934082985\n",
      "epoch: 3 step: 405, loss is 0.004502197727560997\n",
      "epoch: 3 step: 406, loss is 0.014327622950077057\n",
      "epoch: 3 step: 407, loss is 0.2129002958536148\n",
      "epoch: 3 step: 408, loss is 0.016022009775042534\n",
      "epoch: 3 step: 409, loss is 0.008798118680715561\n",
      "epoch: 3 step: 410, loss is 0.0011516213417053223\n",
      "epoch: 3 step: 411, loss is 0.09343604743480682\n",
      "epoch: 3 step: 412, loss is 0.016246184706687927\n",
      "epoch: 3 step: 413, loss is 0.03975049406290054\n",
      "epoch: 3 step: 414, loss is 0.017027704045176506\n",
      "epoch: 3 step: 415, loss is 0.22361642122268677\n",
      "epoch: 3 step: 416, loss is 0.04652007669210434\n",
      "epoch: 3 step: 417, loss is 0.02107083611190319\n",
      "epoch: 3 step: 418, loss is 0.07774774730205536\n",
      "epoch: 3 step: 419, loss is 0.00612598517909646\n",
      "epoch: 3 step: 420, loss is 0.0014416500926017761\n",
      "epoch: 3 step: 421, loss is 0.035438865423202515\n",
      "epoch: 3 step: 422, loss is 0.0190291665494442\n",
      "epoch: 3 step: 423, loss is 0.04369071125984192\n",
      "epoch: 3 step: 424, loss is 0.0761914923787117\n",
      "epoch: 3 step: 425, loss is 0.020809685811400414\n",
      "epoch: 3 step: 426, loss is 0.04563615843653679\n",
      "epoch: 3 step: 427, loss is 0.0012144164647907019\n",
      "epoch: 3 step: 428, loss is 0.0942431315779686\n",
      "epoch: 3 step: 429, loss is 0.031228195875883102\n",
      "epoch: 3 step: 430, loss is 0.09724117070436478\n",
      "epoch: 3 step: 431, loss is 0.08615855872631073\n",
      "epoch: 3 step: 432, loss is 0.0340721532702446\n",
      "epoch: 3 step: 433, loss is 0.006678804289549589\n",
      "epoch: 3 step: 434, loss is 0.0026504951529204845\n",
      "epoch: 3 step: 435, loss is 0.0044316696003079414\n",
      "epoch: 3 step: 436, loss is 0.0047777877189219\n",
      "epoch: 3 step: 437, loss is 0.1697036325931549\n",
      "epoch: 3 step: 438, loss is 0.07311166077852249\n",
      "epoch: 3 step: 439, loss is 0.2829725742340088\n",
      "epoch: 3 step: 440, loss is 0.24427886307239532\n",
      "epoch: 3 step: 441, loss is 0.023061824962496758\n",
      "epoch: 3 step: 442, loss is 0.1223335862159729\n",
      "epoch: 3 step: 443, loss is 0.07535581290721893\n",
      "epoch: 3 step: 444, loss is 0.16414232552051544\n",
      "epoch: 3 step: 445, loss is 0.18577373027801514\n",
      "epoch: 3 step: 446, loss is 0.017672592774033546\n",
      "epoch: 3 step: 447, loss is 0.08572376519441605\n",
      "epoch: 3 step: 448, loss is 0.059647366404533386\n",
      "epoch: 3 step: 449, loss is 0.10388542711734772\n",
      "epoch: 3 step: 450, loss is 0.16621969640254974\n",
      "epoch: 3 step: 451, loss is 0.01238829642534256\n",
      "epoch: 3 step: 452, loss is 0.16215597093105316\n",
      "epoch: 3 step: 453, loss is 0.02733338065445423\n",
      "epoch: 3 step: 454, loss is 0.00740515673533082\n",
      "epoch: 3 step: 455, loss is 0.13224296271800995\n",
      "epoch: 3 step: 456, loss is 0.0027981128077954054\n",
      "epoch: 3 step: 457, loss is 0.012924164533615112\n",
      "epoch: 3 step: 458, loss is 0.0297890342772007\n",
      "epoch: 3 step: 459, loss is 0.1329059898853302\n",
      "epoch: 3 step: 460, loss is 0.004867992829531431\n",
      "epoch: 3 step: 461, loss is 0.00704615656286478\n",
      "epoch: 3 step: 462, loss is 0.1821206659078598\n",
      "epoch: 3 step: 463, loss is 0.004195080138742924\n",
      "epoch: 3 step: 464, loss is 0.007502314168959856\n",
      "epoch: 3 step: 465, loss is 0.005692801438271999\n",
      "epoch: 3 step: 466, loss is 0.006504408549517393\n",
      "epoch: 3 step: 467, loss is 0.010724581778049469\n",
      "epoch: 3 step: 468, loss is 0.03406824171543121\n",
      "epoch: 3 step: 469, loss is 0.013461939990520477\n",
      "epoch: 3 step: 470, loss is 0.05576443299651146\n",
      "epoch: 3 step: 471, loss is 0.018134893849492073\n",
      "epoch: 3 step: 472, loss is 0.10113900899887085\n",
      "epoch: 3 step: 473, loss is 0.029038557782769203\n",
      "epoch: 3 step: 474, loss is 0.2210434526205063\n",
      "epoch: 3 step: 475, loss is 0.11699891090393066\n",
      "epoch: 3 step: 476, loss is 0.009518992155790329\n",
      "epoch: 3 step: 477, loss is 0.06856939196586609\n",
      "epoch: 3 step: 478, loss is 0.019825562834739685\n",
      "epoch: 3 step: 479, loss is 0.09856917709112167\n",
      "epoch: 3 step: 480, loss is 0.03918605297803879\n",
      "epoch: 3 step: 481, loss is 0.051946818828582764\n",
      "epoch: 3 step: 482, loss is 0.006994853261858225\n",
      "epoch: 3 step: 483, loss is 0.027548201382160187\n",
      "epoch: 3 step: 484, loss is 0.14002639055252075\n",
      "epoch: 3 step: 485, loss is 0.030841223895549774\n",
      "epoch: 3 step: 486, loss is 0.006202413234859705\n",
      "epoch: 3 step: 487, loss is 0.015329905785620213\n",
      "epoch: 3 step: 488, loss is 0.016198426485061646\n",
      "epoch: 3 step: 489, loss is 0.00505613349378109\n",
      "epoch: 3 step: 490, loss is 0.00895328726619482\n",
      "epoch: 3 step: 491, loss is 0.021573897451162338\n",
      "epoch: 3 step: 492, loss is 0.12414226680994034\n",
      "epoch: 3 step: 493, loss is 0.01608699932694435\n",
      "epoch: 3 step: 494, loss is 0.024585872888565063\n",
      "epoch: 3 step: 495, loss is 0.05390157178044319\n",
      "epoch: 3 step: 496, loss is 0.3168010115623474\n",
      "epoch: 3 step: 497, loss is 0.04478827118873596\n",
      "epoch: 3 step: 498, loss is 0.13985444605350494\n",
      "epoch: 3 step: 499, loss is 0.1140204668045044\n",
      "epoch: 3 step: 500, loss is 0.0005561714060604572\n",
      "epoch: 3 step: 501, loss is 0.007365365978330374\n",
      "epoch: 3 step: 502, loss is 0.029036497697234154\n",
      "epoch: 3 step: 503, loss is 0.11934559792280197\n",
      "epoch: 3 step: 504, loss is 0.1460900902748108\n",
      "epoch: 3 step: 505, loss is 0.004175698384642601\n",
      "epoch: 3 step: 506, loss is 0.015278451144695282\n",
      "epoch: 3 step: 507, loss is 0.039547063410282135\n",
      "epoch: 3 step: 508, loss is 0.01165564451366663\n",
      "epoch: 3 step: 509, loss is 0.002398774726316333\n",
      "epoch: 3 step: 510, loss is 0.12612716853618622\n",
      "epoch: 3 step: 511, loss is 0.0541868582367897\n",
      "epoch: 3 step: 512, loss is 0.007752661127597094\n",
      "epoch: 3 step: 513, loss is 0.015766048803925514\n",
      "epoch: 3 step: 514, loss is 0.0066721695475280285\n",
      "epoch: 3 step: 515, loss is 0.011134038679301739\n",
      "epoch: 3 step: 516, loss is 0.04056067392230034\n",
      "epoch: 3 step: 517, loss is 0.013200635090470314\n",
      "epoch: 3 step: 518, loss is 0.1102176085114479\n",
      "epoch: 3 step: 519, loss is 0.008146898820996284\n",
      "epoch: 3 step: 520, loss is 0.006602787878364325\n",
      "epoch: 3 step: 521, loss is 0.01440072525292635\n",
      "epoch: 3 step: 522, loss is 0.016380872577428818\n",
      "epoch: 3 step: 523, loss is 0.12290428578853607\n",
      "epoch: 3 step: 524, loss is 0.04532875865697861\n",
      "epoch: 3 step: 525, loss is 0.0036838087253272533\n",
      "epoch: 3 step: 526, loss is 0.011830500327050686\n",
      "epoch: 3 step: 527, loss is 0.011382520198822021\n",
      "epoch: 3 step: 528, loss is 0.012798667885363102\n",
      "epoch: 3 step: 529, loss is 0.026934558525681496\n",
      "epoch: 3 step: 530, loss is 0.008598743006587029\n",
      "epoch: 3 step: 531, loss is 0.027371913194656372\n",
      "epoch: 3 step: 532, loss is 0.0031551257707178593\n",
      "epoch: 3 step: 533, loss is 0.07798155397176743\n",
      "epoch: 3 step: 534, loss is 0.14725930988788605\n",
      "epoch: 3 step: 535, loss is 0.008454683236777782\n",
      "epoch: 3 step: 536, loss is 0.129553884267807\n",
      "epoch: 3 step: 537, loss is 0.0004246821627020836\n",
      "epoch: 3 step: 538, loss is 0.08669743686914444\n",
      "epoch: 3 step: 539, loss is 0.016581077128648758\n",
      "epoch: 3 step: 540, loss is 0.010875677689909935\n",
      "epoch: 3 step: 541, loss is 0.002777888672426343\n",
      "epoch: 3 step: 542, loss is 0.029955293983221054\n",
      "epoch: 3 step: 543, loss is 0.0006027469644322991\n",
      "epoch: 3 step: 544, loss is 0.0012438471894711256\n",
      "epoch: 3 step: 545, loss is 0.01106894202530384\n",
      "epoch: 3 step: 546, loss is 0.03305581957101822\n",
      "epoch: 3 step: 547, loss is 0.03314489126205444\n",
      "epoch: 3 step: 548, loss is 0.03604399785399437\n",
      "epoch: 3 step: 549, loss is 0.04753326624631882\n",
      "epoch: 3 step: 550, loss is 0.003520474536344409\n",
      "epoch: 3 step: 551, loss is 0.09483900666236877\n",
      "epoch: 3 step: 552, loss is 0.000663208425976336\n",
      "epoch: 3 step: 553, loss is 0.011768296360969543\n",
      "epoch: 3 step: 554, loss is 0.003929856698960066\n",
      "epoch: 3 step: 555, loss is 0.0570785291492939\n",
      "epoch: 3 step: 556, loss is 0.02213330566883087\n",
      "epoch: 3 step: 557, loss is 0.002439083531498909\n",
      "epoch: 3 step: 558, loss is 0.08524779230356216\n",
      "epoch: 3 step: 559, loss is 0.0715608075261116\n",
      "epoch: 3 step: 560, loss is 0.0015675720060244203\n",
      "epoch: 3 step: 561, loss is 0.016619009897112846\n",
      "epoch: 3 step: 562, loss is 0.12134739756584167\n",
      "epoch: 3 step: 563, loss is 0.01291463989764452\n",
      "epoch: 3 step: 564, loss is 0.0017113297944888473\n",
      "epoch: 3 step: 565, loss is 0.003183667780831456\n",
      "epoch: 3 step: 566, loss is 0.011249840259552002\n",
      "epoch: 3 step: 567, loss is 0.003377690212801099\n",
      "epoch: 3 step: 568, loss is 0.10008563846349716\n",
      "epoch: 3 step: 569, loss is 0.048965126276016235\n",
      "epoch: 3 step: 570, loss is 0.022882530465722084\n",
      "epoch: 3 step: 571, loss is 0.0013754572719335556\n",
      "epoch: 3 step: 572, loss is 0.14357386529445648\n",
      "epoch: 3 step: 573, loss is 0.05142708122730255\n",
      "epoch: 3 step: 574, loss is 0.03523216024041176\n",
      "epoch: 3 step: 575, loss is 0.004554377868771553\n",
      "epoch: 3 step: 576, loss is 0.002804684219881892\n",
      "epoch: 3 step: 577, loss is 0.03383965417742729\n",
      "epoch: 3 step: 578, loss is 0.08922859281301498\n",
      "epoch: 3 step: 579, loss is 0.04458014294505119\n",
      "epoch: 3 step: 580, loss is 0.0016804015031084418\n",
      "epoch: 3 step: 581, loss is 0.014915906824171543\n",
      "epoch: 3 step: 582, loss is 0.0014607126358896494\n",
      "epoch: 3 step: 583, loss is 0.1251322627067566\n",
      "epoch: 3 step: 584, loss is 0.00502887973561883\n",
      "epoch: 3 step: 585, loss is 0.037580810487270355\n",
      "epoch: 3 step: 586, loss is 0.010010257363319397\n",
      "epoch: 3 step: 587, loss is 0.0740087628364563\n",
      "epoch: 3 step: 588, loss is 0.022649861872196198\n",
      "epoch: 3 step: 589, loss is 0.22052942216396332\n",
      "epoch: 3 step: 590, loss is 0.01377616822719574\n",
      "epoch: 3 step: 591, loss is 0.05511200800538063\n",
      "epoch: 3 step: 592, loss is 0.14916886389255524\n",
      "epoch: 3 step: 593, loss is 0.04781994968652725\n",
      "epoch: 3 step: 594, loss is 0.1352451890707016\n",
      "epoch: 3 step: 595, loss is 0.00989576056599617\n",
      "epoch: 3 step: 596, loss is 0.023538675159215927\n",
      "epoch: 3 step: 597, loss is 0.01111856009811163\n",
      "epoch: 3 step: 598, loss is 0.0052332570776343346\n",
      "epoch: 3 step: 599, loss is 0.0035045556724071503\n",
      "epoch: 3 step: 600, loss is 0.025152618065476418\n",
      "epoch: 3 step: 601, loss is 0.0204848013818264\n",
      "epoch: 3 step: 602, loss is 0.08989567309617996\n",
      "epoch: 3 step: 603, loss is 0.00039754080353304744\n",
      "epoch: 3 step: 604, loss is 0.004231100901961327\n",
      "epoch: 3 step: 605, loss is 0.21013207733631134\n",
      "epoch: 3 step: 606, loss is 0.0013805830385535955\n",
      "epoch: 3 step: 607, loss is 0.07566671073436737\n",
      "epoch: 3 step: 608, loss is 0.0128422100096941\n",
      "epoch: 3 step: 609, loss is 0.0010181994875892997\n",
      "epoch: 3 step: 610, loss is 0.032217707484960556\n",
      "epoch: 3 step: 611, loss is 0.002636752789840102\n",
      "epoch: 3 step: 612, loss is 0.014724371023476124\n",
      "epoch: 3 step: 613, loss is 0.013569235801696777\n",
      "epoch: 3 step: 614, loss is 0.023323383182287216\n",
      "epoch: 3 step: 615, loss is 0.04323483631014824\n",
      "epoch: 3 step: 616, loss is 0.01584366150200367\n",
      "epoch: 3 step: 617, loss is 0.0016747922636568546\n",
      "epoch: 3 step: 618, loss is 0.03632737696170807\n",
      "epoch: 3 step: 619, loss is 0.00671354029327631\n",
      "epoch: 3 step: 620, loss is 0.005846710409969091\n",
      "epoch: 3 step: 621, loss is 0.06328792124986649\n",
      "epoch: 3 step: 622, loss is 0.005212495103478432\n",
      "epoch: 3 step: 623, loss is 0.0010811578249558806\n",
      "epoch: 3 step: 624, loss is 0.12752953171730042\n",
      "epoch: 3 step: 625, loss is 0.17552156746387482\n",
      "epoch: 3 step: 626, loss is 0.06030440703034401\n",
      "epoch: 3 step: 627, loss is 0.007797833066433668\n",
      "epoch: 3 step: 628, loss is 0.003018195740878582\n",
      "epoch: 3 step: 629, loss is 0.12719623744487762\n",
      "epoch: 3 step: 630, loss is 0.0026632186491042376\n",
      "epoch: 3 step: 631, loss is 0.04329729825258255\n",
      "epoch: 3 step: 632, loss is 0.048520419746637344\n",
      "epoch: 3 step: 633, loss is 0.03351260721683502\n",
      "epoch: 3 step: 634, loss is 0.011993835680186749\n",
      "epoch: 3 step: 635, loss is 0.005971609149128199\n",
      "epoch: 3 step: 636, loss is 0.010259000584483147\n",
      "epoch: 3 step: 637, loss is 0.09399627894163132\n",
      "epoch: 3 step: 638, loss is 0.022357866168022156\n",
      "epoch: 3 step: 639, loss is 0.0009815028170123696\n",
      "epoch: 3 step: 640, loss is 0.043221235275268555\n",
      "epoch: 3 step: 641, loss is 0.055285628885030746\n",
      "epoch: 3 step: 642, loss is 0.025397993624210358\n",
      "epoch: 3 step: 643, loss is 0.21420219540596008\n",
      "epoch: 3 step: 644, loss is 0.06313139945268631\n",
      "epoch: 3 step: 645, loss is 0.19037950038909912\n",
      "epoch: 3 step: 646, loss is 0.00986495427787304\n",
      "epoch: 3 step: 647, loss is 0.08944634348154068\n",
      "epoch: 3 step: 648, loss is 0.1282632052898407\n",
      "epoch: 3 step: 649, loss is 0.07619675248861313\n",
      "epoch: 3 step: 650, loss is 0.1111447736620903\n",
      "epoch: 3 step: 651, loss is 0.0027001933194696903\n",
      "epoch: 3 step: 652, loss is 0.002455313690006733\n",
      "epoch: 3 step: 653, loss is 0.005060514435172081\n",
      "epoch: 3 step: 654, loss is 0.08496840298175812\n",
      "epoch: 3 step: 655, loss is 0.05340263992547989\n",
      "epoch: 3 step: 656, loss is 0.005858695600181818\n",
      "epoch: 3 step: 657, loss is 0.07332447916269302\n",
      "epoch: 3 step: 658, loss is 0.004536924883723259\n",
      "epoch: 3 step: 659, loss is 0.155497208237648\n",
      "epoch: 3 step: 660, loss is 0.18849340081214905\n",
      "epoch: 3 step: 661, loss is 0.14626643061637878\n",
      "epoch: 3 step: 662, loss is 0.07317918539047241\n",
      "epoch: 3 step: 663, loss is 0.03056289628148079\n",
      "epoch: 3 step: 664, loss is 0.015872489660978317\n",
      "epoch: 3 step: 665, loss is 0.06564222276210785\n",
      "epoch: 3 step: 666, loss is 0.007463605608791113\n",
      "epoch: 3 step: 667, loss is 0.015028723515570164\n",
      "epoch: 3 step: 668, loss is 0.0051591466180980206\n",
      "epoch: 3 step: 669, loss is 0.12381289154291153\n",
      "epoch: 3 step: 670, loss is 0.03667603060603142\n",
      "epoch: 3 step: 671, loss is 0.028674647212028503\n",
      "epoch: 3 step: 672, loss is 0.11993084102869034\n",
      "epoch: 3 step: 673, loss is 0.058464210480451584\n",
      "epoch: 3 step: 674, loss is 0.006099522113800049\n",
      "epoch: 3 step: 675, loss is 0.005743769928812981\n",
      "epoch: 3 step: 676, loss is 0.0019481800263747573\n",
      "epoch: 3 step: 677, loss is 0.0006928695365786552\n",
      "epoch: 3 step: 678, loss is 0.006446863058954477\n",
      "epoch: 3 step: 679, loss is 0.1270391196012497\n",
      "epoch: 3 step: 680, loss is 0.009322409518063068\n",
      "epoch: 3 step: 681, loss is 0.0024632508866488934\n",
      "epoch: 3 step: 682, loss is 0.0024186447262763977\n",
      "epoch: 3 step: 683, loss is 0.025822211056947708\n",
      "epoch: 3 step: 684, loss is 0.0687045156955719\n",
      "epoch: 3 step: 685, loss is 0.13245363533496857\n",
      "epoch: 3 step: 686, loss is 0.003869788721203804\n",
      "epoch: 3 step: 687, loss is 0.02653200738132\n",
      "epoch: 3 step: 688, loss is 0.11368798464536667\n",
      "epoch: 3 step: 689, loss is 0.08572632074356079\n",
      "epoch: 3 step: 690, loss is 0.00822879932820797\n",
      "epoch: 3 step: 691, loss is 0.029753880575299263\n",
      "epoch: 3 step: 692, loss is 0.09706337004899979\n",
      "epoch: 3 step: 693, loss is 0.023771537467837334\n",
      "epoch: 3 step: 694, loss is 0.003506389679387212\n",
      "epoch: 3 step: 695, loss is 0.015220262110233307\n",
      "epoch: 3 step: 696, loss is 0.017003197222948074\n",
      "epoch: 3 step: 697, loss is 0.012906891293823719\n",
      "epoch: 3 step: 698, loss is 0.023886384442448616\n",
      "epoch: 3 step: 699, loss is 0.011715118773281574\n",
      "epoch: 3 step: 700, loss is 0.0010751103982329369\n",
      "epoch: 3 step: 701, loss is 0.0033939776476472616\n",
      "epoch: 3 step: 702, loss is 0.014863449148833752\n",
      "epoch: 3 step: 703, loss is 0.0098241763189435\n",
      "epoch: 3 step: 704, loss is 0.0648602694272995\n",
      "epoch: 3 step: 705, loss is 0.014631705358624458\n",
      "epoch: 3 step: 706, loss is 0.006653169635683298\n",
      "epoch: 3 step: 707, loss is 0.15250425040721893\n",
      "epoch: 3 step: 708, loss is 0.04739627242088318\n",
      "epoch: 3 step: 709, loss is 0.018330294638872147\n",
      "epoch: 3 step: 710, loss is 0.017226530238986015\n",
      "epoch: 3 step: 711, loss is 0.009471838362514973\n",
      "epoch: 3 step: 712, loss is 0.004210041835904121\n",
      "epoch: 3 step: 713, loss is 0.236949622631073\n",
      "epoch: 3 step: 714, loss is 0.002190562430769205\n",
      "epoch: 3 step: 715, loss is 0.0024278033524751663\n",
      "epoch: 3 step: 716, loss is 0.05566995218396187\n",
      "epoch: 3 step: 717, loss is 0.01792987994849682\n",
      "epoch: 3 step: 718, loss is 0.08551741391420364\n",
      "epoch: 3 step: 719, loss is 0.13086983561515808\n",
      "epoch: 3 step: 720, loss is 0.016588272526860237\n",
      "epoch: 3 step: 721, loss is 0.05227573961019516\n",
      "epoch: 3 step: 722, loss is 0.16169288754463196\n",
      "epoch: 3 step: 723, loss is 0.02614285983145237\n",
      "epoch: 3 step: 724, loss is 0.05959951877593994\n",
      "epoch: 3 step: 725, loss is 0.05965268984436989\n",
      "epoch: 3 step: 726, loss is 0.06066399812698364\n",
      "epoch: 3 step: 727, loss is 0.016074135899543762\n",
      "epoch: 3 step: 728, loss is 0.02548840455710888\n",
      "epoch: 3 step: 729, loss is 0.029411207884550095\n",
      "epoch: 3 step: 730, loss is 0.019636262208223343\n",
      "epoch: 3 step: 731, loss is 0.1346697211265564\n",
      "epoch: 3 step: 732, loss is 0.0017478959634900093\n",
      "epoch: 3 step: 733, loss is 0.002372570103034377\n",
      "epoch: 3 step: 734, loss is 0.18653978407382965\n",
      "epoch: 3 step: 735, loss is 0.026546264067292213\n",
      "epoch: 3 step: 736, loss is 0.037951577454805374\n",
      "epoch: 3 step: 737, loss is 0.0024279530625790358\n",
      "epoch: 3 step: 738, loss is 0.014785276725888252\n",
      "epoch: 3 step: 739, loss is 0.054515887051820755\n",
      "epoch: 3 step: 740, loss is 0.010198470205068588\n",
      "epoch: 3 step: 741, loss is 0.0006647765403613448\n",
      "epoch: 3 step: 742, loss is 0.0076987105421721935\n",
      "epoch: 3 step: 743, loss is 0.009027589112520218\n",
      "epoch: 3 step: 744, loss is 0.014189443551003933\n",
      "epoch: 3 step: 745, loss is 0.1812259703874588\n",
      "epoch: 3 step: 746, loss is 0.0032931410241872072\n",
      "epoch: 3 step: 747, loss is 0.007583498489111662\n",
      "epoch: 3 step: 748, loss is 0.022661538794636726\n",
      "epoch: 3 step: 749, loss is 0.002514007966965437\n",
      "epoch: 3 step: 750, loss is 0.003002546727657318\n",
      "epoch: 3 step: 751, loss is 0.010709119029343128\n",
      "epoch: 3 step: 752, loss is 0.1480318009853363\n",
      "epoch: 3 step: 753, loss is 0.013429866172373295\n",
      "epoch: 3 step: 754, loss is 0.011017527431249619\n",
      "epoch: 3 step: 755, loss is 0.024806629866361618\n",
      "epoch: 3 step: 756, loss is 0.23624025285243988\n",
      "epoch: 3 step: 757, loss is 0.009491929784417152\n",
      "epoch: 3 step: 758, loss is 0.008546574041247368\n",
      "epoch: 3 step: 759, loss is 0.01141429878771305\n",
      "epoch: 3 step: 760, loss is 0.003350894432514906\n",
      "epoch: 3 step: 761, loss is 0.20341338217258453\n",
      "epoch: 3 step: 762, loss is 0.008344888687133789\n",
      "epoch: 3 step: 763, loss is 0.0026815696619451046\n",
      "epoch: 3 step: 764, loss is 0.024134768173098564\n",
      "epoch: 3 step: 765, loss is 0.0026303939521312714\n",
      "epoch: 3 step: 766, loss is 0.011042043566703796\n",
      "epoch: 3 step: 767, loss is 0.008406835608184338\n",
      "epoch: 3 step: 768, loss is 0.00153327954467386\n",
      "epoch: 3 step: 769, loss is 0.005111795384436846\n",
      "epoch: 3 step: 770, loss is 0.19140741229057312\n",
      "epoch: 3 step: 771, loss is 0.0066106016747653484\n",
      "epoch: 3 step: 772, loss is 0.047432150691747665\n",
      "epoch: 3 step: 773, loss is 0.020911937579512596\n",
      "epoch: 3 step: 774, loss is 0.01126453373581171\n",
      "epoch: 3 step: 775, loss is 0.025721417739987373\n",
      "epoch: 3 step: 776, loss is 0.0016411392716690898\n",
      "epoch: 3 step: 777, loss is 0.023116301745176315\n",
      "epoch: 3 step: 778, loss is 0.23253710567951202\n",
      "epoch: 3 step: 779, loss is 0.004175517708063126\n",
      "epoch: 3 step: 780, loss is 0.0009378266404382885\n",
      "epoch: 3 step: 781, loss is 0.11157327890396118\n",
      "epoch: 3 step: 782, loss is 0.003075558692216873\n",
      "epoch: 3 step: 783, loss is 0.023313427343964577\n",
      "epoch: 3 step: 784, loss is 0.030999401584267616\n",
      "epoch: 3 step: 785, loss is 0.036784783005714417\n",
      "epoch: 3 step: 786, loss is 0.03901490196585655\n",
      "epoch: 3 step: 787, loss is 0.15769954025745392\n",
      "epoch: 3 step: 788, loss is 0.04437359794974327\n",
      "epoch: 3 step: 789, loss is 0.06106209009885788\n",
      "epoch: 3 step: 790, loss is 0.038964126259088516\n",
      "epoch: 3 step: 791, loss is 0.0036825297866016626\n",
      "epoch: 3 step: 792, loss is 0.02011028118431568\n",
      "epoch: 3 step: 793, loss is 0.13545671105384827\n",
      "epoch: 3 step: 794, loss is 0.04292352870106697\n",
      "epoch: 3 step: 795, loss is 0.01879381202161312\n",
      "epoch: 3 step: 796, loss is 0.01384196151047945\n",
      "epoch: 3 step: 797, loss is 0.09721259027719498\n",
      "epoch: 3 step: 798, loss is 0.015309743583202362\n",
      "epoch: 3 step: 799, loss is 0.0066363937221467495\n",
      "epoch: 3 step: 800, loss is 0.0033820888493210077\n",
      "epoch: 3 step: 801, loss is 0.05433158949017525\n",
      "epoch: 3 step: 802, loss is 0.003203235799446702\n",
      "epoch: 3 step: 803, loss is 0.010585315525531769\n",
      "epoch: 3 step: 804, loss is 0.0035950259771198034\n",
      "epoch: 3 step: 805, loss is 0.0025820876471698284\n",
      "epoch: 3 step: 806, loss is 0.22307509183883667\n",
      "epoch: 3 step: 807, loss is 0.01786106452345848\n",
      "epoch: 3 step: 808, loss is 0.0013628409942612052\n",
      "epoch: 3 step: 809, loss is 0.03132177144289017\n",
      "epoch: 3 step: 810, loss is 0.00687303626909852\n",
      "epoch: 3 step: 811, loss is 0.08684532344341278\n",
      "epoch: 3 step: 812, loss is 0.014593245461583138\n",
      "epoch: 3 step: 813, loss is 0.14968065917491913\n",
      "epoch: 3 step: 814, loss is 0.12430666387081146\n",
      "epoch: 3 step: 815, loss is 0.11346539855003357\n",
      "epoch: 3 step: 816, loss is 0.13917946815490723\n",
      "epoch: 3 step: 817, loss is 0.035887252539396286\n",
      "epoch: 3 step: 818, loss is 0.08352158963680267\n",
      "epoch: 3 step: 819, loss is 0.15182580053806305\n",
      "epoch: 3 step: 820, loss is 0.0016626925207674503\n",
      "epoch: 3 step: 821, loss is 0.005633719265460968\n",
      "epoch: 3 step: 822, loss is 0.0013576155761256814\n",
      "epoch: 3 step: 823, loss is 0.018241414800286293\n",
      "epoch: 3 step: 824, loss is 0.014033681713044643\n",
      "epoch: 3 step: 825, loss is 0.009561077691614628\n",
      "epoch: 3 step: 826, loss is 0.07767004519701004\n",
      "epoch: 3 step: 827, loss is 0.009301060810685158\n",
      "epoch: 3 step: 828, loss is 0.024603141471743584\n",
      "epoch: 3 step: 829, loss is 0.10009920597076416\n",
      "epoch: 3 step: 830, loss is 0.00880513060837984\n",
      "epoch: 3 step: 831, loss is 0.23112419247627258\n",
      "epoch: 3 step: 832, loss is 0.06734637916088104\n",
      "epoch: 3 step: 833, loss is 0.009931889362633228\n",
      "epoch: 3 step: 834, loss is 0.041824307292699814\n",
      "epoch: 3 step: 835, loss is 0.002325706882402301\n",
      "epoch: 3 step: 836, loss is 0.026466129347682\n",
      "epoch: 3 step: 837, loss is 0.03905069828033447\n",
      "epoch: 3 step: 838, loss is 0.15692608058452606\n",
      "epoch: 3 step: 839, loss is 0.01794934831559658\n",
      "epoch: 3 step: 840, loss is 0.08909792453050613\n",
      "epoch: 3 step: 841, loss is 0.005380799528211355\n",
      "epoch: 3 step: 842, loss is 0.12220782786607742\n",
      "epoch: 3 step: 843, loss is 0.00043942054617218673\n",
      "epoch: 3 step: 844, loss is 0.0021023263689130545\n",
      "epoch: 3 step: 845, loss is 0.035572025924921036\n",
      "epoch: 3 step: 846, loss is 0.01549403928220272\n",
      "epoch: 3 step: 847, loss is 0.050710611045360565\n",
      "epoch: 3 step: 848, loss is 0.4343734383583069\n",
      "epoch: 3 step: 849, loss is 0.008109981194138527\n",
      "epoch: 3 step: 850, loss is 0.001985349226742983\n",
      "epoch: 3 step: 851, loss is 0.013087067753076553\n",
      "epoch: 3 step: 852, loss is 0.0052200378850102425\n",
      "epoch: 3 step: 853, loss is 0.018071496859192848\n",
      "epoch: 3 step: 854, loss is 0.0009389279293827713\n",
      "epoch: 3 step: 855, loss is 0.0012674507452175021\n",
      "epoch: 3 step: 856, loss is 0.004353594500571489\n",
      "epoch: 3 step: 857, loss is 0.022605443373322487\n",
      "epoch: 3 step: 858, loss is 0.15056003630161285\n",
      "epoch: 3 step: 859, loss is 0.020760780200362206\n",
      "epoch: 3 step: 860, loss is 0.005320367403328419\n",
      "epoch: 3 step: 861, loss is 0.01490810140967369\n",
      "epoch: 3 step: 862, loss is 0.04394611716270447\n",
      "epoch: 3 step: 863, loss is 0.020938748493790627\n",
      "epoch: 3 step: 864, loss is 0.005069400649517775\n",
      "epoch: 3 step: 865, loss is 0.0070440457202494144\n",
      "epoch: 3 step: 866, loss is 0.017613835632801056\n",
      "epoch: 3 step: 867, loss is 0.000471733626909554\n",
      "epoch: 3 step: 868, loss is 0.00223326962441206\n",
      "epoch: 3 step: 869, loss is 0.14347250759601593\n",
      "epoch: 3 step: 870, loss is 0.0061951992101967335\n",
      "epoch: 3 step: 871, loss is 0.025881070643663406\n",
      "epoch: 3 step: 872, loss is 0.0006701311795040965\n",
      "epoch: 3 step: 873, loss is 0.002968510612845421\n",
      "epoch: 3 step: 874, loss is 0.13198064267635345\n",
      "epoch: 3 step: 875, loss is 0.03574736416339874\n",
      "epoch: 3 step: 876, loss is 0.007986537180840969\n",
      "epoch: 3 step: 877, loss is 0.002074135234579444\n",
      "epoch: 3 step: 878, loss is 0.001165766385383904\n",
      "epoch: 3 step: 879, loss is 0.0030839238315820694\n",
      "epoch: 3 step: 880, loss is 0.024719586595892906\n",
      "epoch: 3 step: 881, loss is 0.01668957807123661\n",
      "epoch: 3 step: 882, loss is 0.002956897486001253\n",
      "epoch: 3 step: 883, loss is 0.01837577484548092\n",
      "epoch: 3 step: 884, loss is 0.001198528683744371\n",
      "epoch: 3 step: 885, loss is 0.063454769551754\n",
      "epoch: 3 step: 886, loss is 0.004643074702471495\n",
      "epoch: 3 step: 887, loss is 0.0010616442887112498\n",
      "epoch: 3 step: 888, loss is 0.004670278634876013\n",
      "epoch: 3 step: 889, loss is 0.008948404341936111\n",
      "epoch: 3 step: 890, loss is 0.04425554722547531\n",
      "epoch: 3 step: 891, loss is 0.08754941821098328\n",
      "epoch: 3 step: 892, loss is 0.001774352160282433\n",
      "epoch: 3 step: 893, loss is 0.05140184238553047\n",
      "epoch: 3 step: 894, loss is 0.00986433681100607\n",
      "epoch: 3 step: 895, loss is 0.0040975818410515785\n",
      "epoch: 3 step: 896, loss is 0.0006461988086812198\n",
      "epoch: 3 step: 897, loss is 0.0033573561813682318\n",
      "epoch: 3 step: 898, loss is 0.08262674510478973\n",
      "epoch: 3 step: 899, loss is 0.043390870094299316\n",
      "epoch: 3 step: 900, loss is 0.04741467162966728\n",
      "epoch: 3 step: 901, loss is 0.0027167904190719128\n",
      "epoch: 3 step: 902, loss is 0.002286237897351384\n",
      "epoch: 3 step: 903, loss is 0.03193279355764389\n",
      "epoch: 3 step: 904, loss is 0.009316646493971348\n",
      "epoch: 3 step: 905, loss is 0.00022829223598819226\n",
      "epoch: 3 step: 906, loss is 0.0015730590093880892\n",
      "epoch: 3 step: 907, loss is 0.04952174797654152\n",
      "epoch: 3 step: 908, loss is 0.031589310616254807\n",
      "epoch: 3 step: 909, loss is 0.04156738147139549\n",
      "epoch: 3 step: 910, loss is 0.05848904699087143\n",
      "epoch: 3 step: 911, loss is 0.027931207790970802\n",
      "epoch: 3 step: 912, loss is 0.03842558711767197\n",
      "epoch: 3 step: 913, loss is 0.011722150258719921\n",
      "epoch: 3 step: 914, loss is 6.303568079601973e-05\n",
      "epoch: 3 step: 915, loss is 0.10130929201841354\n",
      "epoch: 3 step: 916, loss is 0.0013687761966139078\n",
      "epoch: 3 step: 917, loss is 0.06592632830142975\n",
      "epoch: 3 step: 918, loss is 0.003325244877487421\n",
      "epoch: 3 step: 919, loss is 0.15142877399921417\n",
      "epoch: 3 step: 920, loss is 0.3174245357513428\n",
      "epoch: 3 step: 921, loss is 0.00010720077261794358\n",
      "epoch: 3 step: 922, loss is 0.01952018402516842\n",
      "epoch: 3 step: 923, loss is 0.028835568577051163\n",
      "epoch: 3 step: 924, loss is 0.12483885139226913\n",
      "epoch: 3 step: 925, loss is 0.00979442335665226\n",
      "epoch: 3 step: 926, loss is 0.13215264678001404\n",
      "epoch: 3 step: 927, loss is 0.0005665903445333242\n",
      "epoch: 3 step: 928, loss is 0.00024050874344538897\n",
      "epoch: 3 step: 929, loss is 0.08592130988836288\n",
      "epoch: 3 step: 930, loss is 0.015066767111420631\n",
      "epoch: 3 step: 931, loss is 0.005617823451757431\n",
      "epoch: 3 step: 932, loss is 0.015655171126127243\n",
      "epoch: 3 step: 933, loss is 0.0023826174437999725\n",
      "epoch: 3 step: 934, loss is 0.1832122653722763\n",
      "epoch: 3 step: 935, loss is 0.0010345586342737079\n",
      "epoch: 3 step: 936, loss is 0.00988075789064169\n",
      "epoch: 3 step: 937, loss is 0.0005362363299354911\n",
      "epoch: 3 step: 938, loss is 0.0007927698316052556\n",
      "epoch: 3 step: 939, loss is 0.0017191217048093677\n",
      "epoch: 3 step: 940, loss is 0.0032564105931669474\n",
      "epoch: 3 step: 941, loss is 0.006207630038261414\n",
      "epoch: 3 step: 942, loss is 0.01396496593952179\n",
      "epoch: 3 step: 943, loss is 0.04471920430660248\n",
      "epoch: 3 step: 944, loss is 0.2780316472053528\n",
      "epoch: 3 step: 945, loss is 0.013163622468709946\n",
      "epoch: 3 step: 946, loss is 0.0036829852033406496\n",
      "epoch: 3 step: 947, loss is 0.06359009444713593\n",
      "epoch: 3 step: 948, loss is 0.001378642744384706\n",
      "epoch: 3 step: 949, loss is 0.011809032410383224\n",
      "epoch: 3 step: 950, loss is 0.09873834252357483\n",
      "epoch: 3 step: 951, loss is 0.059398479759693146\n",
      "epoch: 3 step: 952, loss is 0.02500488981604576\n",
      "epoch: 3 step: 953, loss is 0.002356206299737096\n",
      "epoch: 3 step: 954, loss is 0.0007250317139551044\n",
      "epoch: 3 step: 955, loss is 0.0154119823127985\n",
      "epoch: 3 step: 956, loss is 0.021325448527932167\n",
      "epoch: 3 step: 957, loss is 0.0013850592076778412\n",
      "epoch: 3 step: 958, loss is 0.02531375363469124\n",
      "epoch: 3 step: 959, loss is 0.1206325888633728\n",
      "epoch: 3 step: 960, loss is 0.004258500412106514\n",
      "epoch: 3 step: 961, loss is 0.011990154162049294\n",
      "epoch: 3 step: 962, loss is 0.003426646813750267\n",
      "epoch: 3 step: 963, loss is 0.004108580760657787\n",
      "epoch: 3 step: 964, loss is 0.0048868353478610516\n",
      "epoch: 3 step: 965, loss is 0.0036954267416149378\n",
      "epoch: 3 step: 966, loss is 0.0032493078615516424\n",
      "epoch: 3 step: 967, loss is 0.15960060060024261\n",
      "epoch: 3 step: 968, loss is 0.009606734849512577\n",
      "epoch: 3 step: 969, loss is 0.008655011653900146\n",
      "epoch: 3 step: 970, loss is 0.06432916224002838\n",
      "epoch: 3 step: 971, loss is 0.019916698336601257\n",
      "epoch: 3 step: 972, loss is 0.10608723014593124\n",
      "epoch: 3 step: 973, loss is 0.20158247649669647\n",
      "epoch: 3 step: 974, loss is 0.005402485374361277\n",
      "epoch: 3 step: 975, loss is 0.007635514251887798\n",
      "epoch: 3 step: 976, loss is 0.026035375893115997\n",
      "epoch: 3 step: 977, loss is 0.09935355186462402\n",
      "epoch: 3 step: 978, loss is 0.05240302532911301\n",
      "epoch: 3 step: 979, loss is 0.0017699896125122905\n",
      "epoch: 3 step: 980, loss is 0.019687462598085403\n",
      "epoch: 3 step: 981, loss is 0.02556203491985798\n",
      "epoch: 3 step: 982, loss is 0.19437739253044128\n",
      "epoch: 3 step: 983, loss is 0.0344560481607914\n",
      "epoch: 3 step: 984, loss is 0.03196856379508972\n",
      "epoch: 3 step: 985, loss is 0.006919932551681995\n",
      "epoch: 3 step: 986, loss is 0.0017118128016591072\n",
      "epoch: 3 step: 987, loss is 0.0020262738689780235\n",
      "epoch: 3 step: 988, loss is 0.10904347151517868\n",
      "epoch: 3 step: 989, loss is 0.009905451908707619\n",
      "epoch: 3 step: 990, loss is 0.0005158818094059825\n",
      "epoch: 3 step: 991, loss is 0.004470348358154297\n",
      "epoch: 3 step: 992, loss is 0.006808653939515352\n",
      "epoch: 3 step: 993, loss is 0.012041143141686916\n",
      "epoch: 3 step: 994, loss is 0.05675182491540909\n",
      "epoch: 3 step: 995, loss is 0.0025654544588178396\n",
      "epoch: 3 step: 996, loss is 0.005108855199068785\n",
      "epoch: 3 step: 997, loss is 0.012547417543828487\n",
      "epoch: 3 step: 998, loss is 0.0028070295229554176\n",
      "epoch: 3 step: 999, loss is 0.04141548275947571\n",
      "epoch: 3 step: 1000, loss is 0.014889678917825222\n",
      "epoch: 3 step: 1001, loss is 0.0013819800224155188\n",
      "epoch: 3 step: 1002, loss is 0.0110031021758914\n",
      "epoch: 3 step: 1003, loss is 0.0400911346077919\n",
      "epoch: 3 step: 1004, loss is 0.08616860210895538\n",
      "epoch: 3 step: 1005, loss is 0.003408218966796994\n",
      "epoch: 3 step: 1006, loss is 0.028594758361577988\n",
      "epoch: 3 step: 1007, loss is 0.002096395241096616\n",
      "epoch: 3 step: 1008, loss is 0.0005646503414027393\n",
      "epoch: 3 step: 1009, loss is 0.0355483777821064\n",
      "epoch: 3 step: 1010, loss is 0.05021640285849571\n",
      "epoch: 3 step: 1011, loss is 0.0004921481013298035\n",
      "epoch: 3 step: 1012, loss is 0.0013214829377830029\n",
      "epoch: 3 step: 1013, loss is 0.003946257755160332\n",
      "epoch: 3 step: 1014, loss is 0.034638650715351105\n",
      "epoch: 3 step: 1015, loss is 0.20642195641994476\n",
      "epoch: 3 step: 1016, loss is 0.0769687369465828\n",
      "epoch: 3 step: 1017, loss is 0.006109085399657488\n",
      "epoch: 3 step: 1018, loss is 0.13253605365753174\n",
      "epoch: 3 step: 1019, loss is 0.3032722771167755\n",
      "epoch: 3 step: 1020, loss is 0.08066119253635406\n",
      "epoch: 3 step: 1021, loss is 0.002974430099129677\n",
      "epoch: 3 step: 1022, loss is 0.0009785857982933521\n",
      "epoch: 3 step: 1023, loss is 0.0032474836334586143\n",
      "epoch: 3 step: 1024, loss is 0.00011363703379174694\n",
      "epoch: 3 step: 1025, loss is 0.007965762168169022\n",
      "epoch: 3 step: 1026, loss is 0.1319274604320526\n",
      "epoch: 3 step: 1027, loss is 0.09829869866371155\n",
      "epoch: 3 step: 1028, loss is 0.09273713082075119\n",
      "epoch: 3 step: 1029, loss is 0.013698359951376915\n",
      "epoch: 3 step: 1030, loss is 0.030011113733053207\n",
      "epoch: 3 step: 1031, loss is 0.0022192150354385376\n",
      "epoch: 3 step: 1032, loss is 0.00431218883022666\n",
      "epoch: 3 step: 1033, loss is 0.0030864335130900145\n",
      "epoch: 3 step: 1034, loss is 0.08413052558898926\n",
      "epoch: 3 step: 1035, loss is 0.060530636459589005\n",
      "epoch: 3 step: 1036, loss is 0.011230056174099445\n",
      "epoch: 3 step: 1037, loss is 0.14893865585327148\n",
      "epoch: 3 step: 1038, loss is 0.019560283049941063\n",
      "epoch: 3 step: 1039, loss is 0.0330398753285408\n",
      "epoch: 3 step: 1040, loss is 0.004632611759006977\n",
      "epoch: 3 step: 1041, loss is 0.0047803837805986404\n",
      "epoch: 3 step: 1042, loss is 0.02715352177619934\n",
      "epoch: 3 step: 1043, loss is 0.36349886655807495\n",
      "epoch: 3 step: 1044, loss is 0.01155918464064598\n",
      "epoch: 3 step: 1045, loss is 0.00914399791508913\n",
      "epoch: 3 step: 1046, loss is 0.0026934167835861444\n",
      "epoch: 3 step: 1047, loss is 0.2585097849369049\n",
      "epoch: 3 step: 1048, loss is 0.02488858625292778\n",
      "epoch: 3 step: 1049, loss is 0.033729225397109985\n",
      "epoch: 3 step: 1050, loss is 0.002467445796355605\n",
      "epoch: 3 step: 1051, loss is 0.00791839323937893\n",
      "epoch: 3 step: 1052, loss is 0.01034580823034048\n",
      "epoch: 3 step: 1053, loss is 0.06807803362607956\n",
      "epoch: 3 step: 1054, loss is 0.09524349868297577\n",
      "epoch: 3 step: 1055, loss is 0.1538110226392746\n",
      "epoch: 3 step: 1056, loss is 0.009150877594947815\n",
      "epoch: 3 step: 1057, loss is 0.07061940431594849\n",
      "epoch: 3 step: 1058, loss is 0.32297107577323914\n",
      "epoch: 3 step: 1059, loss is 0.006857546977698803\n",
      "epoch: 3 step: 1060, loss is 0.0011884598061442375\n",
      "epoch: 3 step: 1061, loss is 0.0020182901062071323\n",
      "epoch: 3 step: 1062, loss is 0.0009258314967155457\n",
      "epoch: 3 step: 1063, loss is 0.008142709732055664\n",
      "epoch: 3 step: 1064, loss is 0.011791089549660683\n",
      "epoch: 3 step: 1065, loss is 0.013931473717093468\n",
      "epoch: 3 step: 1066, loss is 0.08626577258110046\n",
      "epoch: 3 step: 1067, loss is 0.12748242914676666\n",
      "epoch: 3 step: 1068, loss is 0.021373210474848747\n",
      "epoch: 3 step: 1069, loss is 0.007086480036377907\n",
      "epoch: 3 step: 1070, loss is 0.016005320474505424\n",
      "epoch: 3 step: 1071, loss is 0.1561754196882248\n",
      "epoch: 3 step: 1072, loss is 0.016188081353902817\n",
      "epoch: 3 step: 1073, loss is 0.061695199459791183\n",
      "epoch: 3 step: 1074, loss is 0.005299034062772989\n",
      "epoch: 3 step: 1075, loss is 0.12330479919910431\n",
      "epoch: 3 step: 1076, loss is 0.0078046927228569984\n",
      "epoch: 3 step: 1077, loss is 0.0032342257909476757\n",
      "epoch: 3 step: 1078, loss is 0.003729870542883873\n",
      "epoch: 3 step: 1079, loss is 0.12988784909248352\n",
      "epoch: 3 step: 1080, loss is 0.09220799803733826\n",
      "epoch: 3 step: 1081, loss is 0.00172086909878999\n",
      "epoch: 3 step: 1082, loss is 0.003546515479683876\n",
      "epoch: 3 step: 1083, loss is 0.0013199923560023308\n",
      "epoch: 3 step: 1084, loss is 0.15716524422168732\n",
      "epoch: 3 step: 1085, loss is 0.006314965896308422\n",
      "epoch: 3 step: 1086, loss is 0.004358083475381136\n",
      "epoch: 3 step: 1087, loss is 0.030619647353887558\n",
      "epoch: 3 step: 1088, loss is 0.025138763710856438\n",
      "epoch: 3 step: 1089, loss is 0.10088510811328888\n",
      "epoch: 3 step: 1090, loss is 0.0012566061923280358\n",
      "epoch: 3 step: 1091, loss is 0.010473371483385563\n",
      "epoch: 3 step: 1092, loss is 0.0103464275598526\n",
      "epoch: 3 step: 1093, loss is 0.03925022855401039\n",
      "epoch: 3 step: 1094, loss is 0.05154240131378174\n",
      "epoch: 3 step: 1095, loss is 0.0034502625931054354\n",
      "epoch: 3 step: 1096, loss is 0.0032965107820928097\n",
      "epoch: 3 step: 1097, loss is 0.12486651539802551\n",
      "epoch: 3 step: 1098, loss is 0.002596522681415081\n",
      "epoch: 3 step: 1099, loss is 0.008040858432650566\n",
      "epoch: 3 step: 1100, loss is 0.0065918052569031715\n",
      "epoch: 3 step: 1101, loss is 0.11101680994033813\n",
      "epoch: 3 step: 1102, loss is 0.005936591420322657\n",
      "epoch: 3 step: 1103, loss is 0.04597446694970131\n",
      "epoch: 3 step: 1104, loss is 0.04790002107620239\n",
      "epoch: 3 step: 1105, loss is 0.11093053221702576\n",
      "epoch: 3 step: 1106, loss is 0.018434830009937286\n",
      "epoch: 3 step: 1107, loss is 0.01754028908908367\n",
      "epoch: 3 step: 1108, loss is 0.0017794661689549685\n",
      "epoch: 3 step: 1109, loss is 0.0755474716424942\n",
      "epoch: 3 step: 1110, loss is 0.03025626204907894\n",
      "epoch: 3 step: 1111, loss is 0.007531806360930204\n",
      "epoch: 3 step: 1112, loss is 0.009305527433753014\n",
      "epoch: 3 step: 1113, loss is 0.0032787970267236233\n",
      "epoch: 3 step: 1114, loss is 0.052837710827589035\n",
      "epoch: 3 step: 1115, loss is 0.15416200459003448\n",
      "epoch: 3 step: 1116, loss is 0.003247589338570833\n",
      "epoch: 3 step: 1117, loss is 0.0635237917304039\n",
      "epoch: 3 step: 1118, loss is 0.011386757716536522\n",
      "epoch: 3 step: 1119, loss is 0.0004687629407271743\n",
      "epoch: 3 step: 1120, loss is 0.03535770997405052\n",
      "epoch: 3 step: 1121, loss is 0.021921999752521515\n",
      "epoch: 3 step: 1122, loss is 0.0015873574884608388\n",
      "epoch: 3 step: 1123, loss is 0.0010409846436232328\n",
      "epoch: 3 step: 1124, loss is 0.0029704454354941845\n",
      "epoch: 3 step: 1125, loss is 0.0008732847636565566\n",
      "epoch: 3 step: 1126, loss is 0.20257559418678284\n",
      "epoch: 3 step: 1127, loss is 0.13683255016803741\n",
      "epoch: 3 step: 1128, loss is 0.11836956441402435\n",
      "epoch: 3 step: 1129, loss is 0.005333296488970518\n",
      "epoch: 3 step: 1130, loss is 0.0009329619351774454\n",
      "epoch: 3 step: 1131, loss is 0.0011630967492237687\n",
      "epoch: 3 step: 1132, loss is 0.016579847782850266\n",
      "epoch: 3 step: 1133, loss is 0.012126980349421501\n",
      "epoch: 3 step: 1134, loss is 0.0019064367515966296\n",
      "epoch: 3 step: 1135, loss is 0.002269202843308449\n",
      "epoch: 3 step: 1136, loss is 0.0283797774463892\n",
      "epoch: 3 step: 1137, loss is 0.012999129481613636\n",
      "epoch: 3 step: 1138, loss is 0.021761640906333923\n",
      "epoch: 3 step: 1139, loss is 0.013013624586164951\n",
      "epoch: 3 step: 1140, loss is 0.0026973902713507414\n",
      "epoch: 3 step: 1141, loss is 0.11050842702388763\n",
      "epoch: 3 step: 1142, loss is 0.0040177637711167336\n",
      "epoch: 3 step: 1143, loss is 0.015304298140108585\n",
      "epoch: 3 step: 1144, loss is 0.029678883031010628\n",
      "epoch: 3 step: 1145, loss is 0.002710883505642414\n",
      "epoch: 3 step: 1146, loss is 0.005691461265087128\n",
      "epoch: 3 step: 1147, loss is 0.003156862687319517\n",
      "epoch: 3 step: 1148, loss is 0.10692906379699707\n",
      "epoch: 3 step: 1149, loss is 0.4948965609073639\n",
      "epoch: 3 step: 1150, loss is 0.001576920272782445\n",
      "epoch: 3 step: 1151, loss is 0.007157113868743181\n",
      "epoch: 3 step: 1152, loss is 0.014720087870955467\n",
      "epoch: 3 step: 1153, loss is 0.01957162655889988\n",
      "epoch: 3 step: 1154, loss is 0.01251512672752142\n",
      "epoch: 3 step: 1155, loss is 0.08378547430038452\n",
      "epoch: 3 step: 1156, loss is 0.07370134443044662\n",
      "epoch: 3 step: 1157, loss is 0.016567982733249664\n",
      "epoch: 3 step: 1158, loss is 0.03311306983232498\n",
      "epoch: 3 step: 1159, loss is 0.29456302523612976\n",
      "epoch: 3 step: 1160, loss is 0.006026962772011757\n",
      "epoch: 3 step: 1161, loss is 0.02418161928653717\n",
      "epoch: 3 step: 1162, loss is 0.03630626201629639\n",
      "epoch: 3 step: 1163, loss is 0.0017328375251963735\n",
      "epoch: 3 step: 1164, loss is 0.09690605103969574\n",
      "epoch: 3 step: 1165, loss is 0.07977736741304398\n",
      "epoch: 3 step: 1166, loss is 0.00257879844866693\n",
      "epoch: 3 step: 1167, loss is 0.0917774885892868\n",
      "epoch: 3 step: 1168, loss is 0.030690602958202362\n",
      "epoch: 3 step: 1169, loss is 0.17784635722637177\n",
      "epoch: 3 step: 1170, loss is 0.16322939097881317\n",
      "epoch: 3 step: 1171, loss is 0.06717979162931442\n",
      "epoch: 3 step: 1172, loss is 0.03084232658147812\n",
      "epoch: 3 step: 1173, loss is 0.025699952617287636\n",
      "epoch: 3 step: 1174, loss is 0.009454494342207909\n",
      "epoch: 3 step: 1175, loss is 0.1804768592119217\n",
      "epoch: 3 step: 1176, loss is 0.013667247258126736\n",
      "epoch: 3 step: 1177, loss is 0.05758923292160034\n",
      "epoch: 3 step: 1178, loss is 0.004415170289576054\n",
      "epoch: 3 step: 1179, loss is 0.09117317944765091\n",
      "epoch: 3 step: 1180, loss is 0.3426550328731537\n",
      "epoch: 3 step: 1181, loss is 0.0027788528241217136\n",
      "epoch: 3 step: 1182, loss is 0.0497313067317009\n",
      "epoch: 3 step: 1183, loss is 0.09928537160158157\n",
      "epoch: 3 step: 1184, loss is 0.015917979180812836\n",
      "epoch: 3 step: 1185, loss is 0.01297045685350895\n",
      "epoch: 3 step: 1186, loss is 0.008803064003586769\n",
      "epoch: 3 step: 1187, loss is 0.30000248551368713\n",
      "epoch: 3 step: 1188, loss is 0.0037673667538911104\n",
      "epoch: 3 step: 1189, loss is 0.015436742454767227\n",
      "epoch: 3 step: 1190, loss is 0.028551790863275528\n",
      "epoch: 3 step: 1191, loss is 0.04389483854174614\n",
      "epoch: 3 step: 1192, loss is 0.003487270325422287\n",
      "epoch: 3 step: 1193, loss is 0.014603544026613235\n",
      "epoch: 3 step: 1194, loss is 0.015260778367519379\n",
      "epoch: 3 step: 1195, loss is 0.021312499418854713\n",
      "epoch: 3 step: 1196, loss is 0.38665494322776794\n",
      "epoch: 3 step: 1197, loss is 0.03824111074209213\n",
      "epoch: 3 step: 1198, loss is 0.03513870760798454\n",
      "epoch: 3 step: 1199, loss is 0.0355166457593441\n",
      "epoch: 3 step: 1200, loss is 0.023574113845825195\n",
      "epoch: 3 step: 1201, loss is 0.0014947399031370878\n",
      "epoch: 3 step: 1202, loss is 0.03572985157370567\n",
      "epoch: 3 step: 1203, loss is 0.07847034186124802\n",
      "epoch: 3 step: 1204, loss is 0.024021627381443977\n",
      "epoch: 3 step: 1205, loss is 0.014960850588977337\n",
      "epoch: 3 step: 1206, loss is 0.14620190858840942\n",
      "epoch: 3 step: 1207, loss is 0.19278235733509064\n",
      "epoch: 3 step: 1208, loss is 0.02362854592502117\n",
      "epoch: 3 step: 1209, loss is 0.0015091467648744583\n",
      "epoch: 3 step: 1210, loss is 0.026296647265553474\n",
      "epoch: 3 step: 1211, loss is 0.034901078790426254\n",
      "epoch: 3 step: 1212, loss is 0.018281318247318268\n",
      "epoch: 3 step: 1213, loss is 0.016533028334379196\n",
      "epoch: 3 step: 1214, loss is 0.11966916173696518\n",
      "epoch: 3 step: 1215, loss is 0.042182426899671555\n",
      "epoch: 3 step: 1216, loss is 0.005246873013675213\n",
      "epoch: 3 step: 1217, loss is 0.010815395973622799\n",
      "epoch: 3 step: 1218, loss is 0.0534309484064579\n",
      "epoch: 3 step: 1219, loss is 0.006410080473870039\n",
      "epoch: 3 step: 1220, loss is 0.06617612391710281\n",
      "epoch: 3 step: 1221, loss is 0.0162793081253767\n",
      "epoch: 3 step: 1222, loss is 0.014042317867279053\n",
      "epoch: 3 step: 1223, loss is 0.006121668964624405\n",
      "epoch: 3 step: 1224, loss is 0.004801267758011818\n",
      "epoch: 3 step: 1225, loss is 0.02818751335144043\n",
      "epoch: 3 step: 1226, loss is 0.02749066799879074\n",
      "epoch: 3 step: 1227, loss is 0.17832085490226746\n",
      "epoch: 3 step: 1228, loss is 0.01209110114723444\n",
      "epoch: 3 step: 1229, loss is 0.0018955653067678213\n",
      "epoch: 3 step: 1230, loss is 0.006307415664196014\n",
      "epoch: 3 step: 1231, loss is 0.004081275314092636\n",
      "epoch: 3 step: 1232, loss is 0.03736906126141548\n",
      "epoch: 3 step: 1233, loss is 0.10090053826570511\n",
      "epoch: 3 step: 1234, loss is 0.020724549889564514\n",
      "epoch: 3 step: 1235, loss is 0.13045082986354828\n",
      "epoch: 3 step: 1236, loss is 0.0015348333399742842\n",
      "epoch: 3 step: 1237, loss is 0.024969937279820442\n",
      "epoch: 3 step: 1238, loss is 0.010508261620998383\n",
      "epoch: 3 step: 1239, loss is 0.031003233045339584\n",
      "epoch: 3 step: 1240, loss is 0.005749482195824385\n",
      "epoch: 3 step: 1241, loss is 0.003447161288931966\n",
      "epoch: 3 step: 1242, loss is 0.022753454744815826\n",
      "epoch: 3 step: 1243, loss is 0.004622452426701784\n",
      "epoch: 3 step: 1244, loss is 0.01500134076923132\n",
      "epoch: 3 step: 1245, loss is 0.00039284012746065855\n",
      "epoch: 3 step: 1246, loss is 0.046852171421051025\n",
      "epoch: 3 step: 1247, loss is 0.00036576431011781096\n",
      "epoch: 3 step: 1248, loss is 0.0021966027561575174\n",
      "epoch: 3 step: 1249, loss is 0.10567218065261841\n",
      "epoch: 3 step: 1250, loss is 0.04095130413770676\n",
      "epoch: 3 step: 1251, loss is 0.0039751254953444\n",
      "epoch: 3 step: 1252, loss is 0.01095693651586771\n",
      "epoch: 3 step: 1253, loss is 0.011995763517916203\n",
      "epoch: 3 step: 1254, loss is 0.0015110900858417153\n",
      "epoch: 3 step: 1255, loss is 0.05617012828588486\n",
      "epoch: 3 step: 1256, loss is 0.06969325244426727\n",
      "epoch: 3 step: 1257, loss is 0.003713536076247692\n",
      "epoch: 3 step: 1258, loss is 0.0071404557675123215\n",
      "epoch: 3 step: 1259, loss is 0.06526083499193192\n",
      "epoch: 3 step: 1260, loss is 0.04897823929786682\n",
      "epoch: 3 step: 1261, loss is 0.09002397209405899\n",
      "epoch: 3 step: 1262, loss is 0.041172269731760025\n",
      "epoch: 3 step: 1263, loss is 0.01969265192747116\n",
      "epoch: 3 step: 1264, loss is 0.0003950706741306931\n",
      "epoch: 3 step: 1265, loss is 0.045524198561906815\n",
      "epoch: 3 step: 1266, loss is 0.33063897490501404\n",
      "epoch: 3 step: 1267, loss is 0.15325838327407837\n",
      "epoch: 3 step: 1268, loss is 0.17626342177391052\n",
      "epoch: 3 step: 1269, loss is 0.004281303379684687\n",
      "epoch: 3 step: 1270, loss is 0.28200146555900574\n",
      "epoch: 3 step: 1271, loss is 0.06423255801200867\n",
      "epoch: 3 step: 1272, loss is 0.037467435002326965\n",
      "epoch: 3 step: 1273, loss is 0.007448917720466852\n",
      "epoch: 3 step: 1274, loss is 0.06167279928922653\n",
      "epoch: 3 step: 1275, loss is 0.0051226308569312096\n",
      "epoch: 3 step: 1276, loss is 0.016161317005753517\n",
      "epoch: 3 step: 1277, loss is 0.06574644148349762\n",
      "epoch: 3 step: 1278, loss is 0.004408599343150854\n",
      "epoch: 3 step: 1279, loss is 0.014670484699308872\n",
      "epoch: 3 step: 1280, loss is 0.17277464270591736\n",
      "epoch: 3 step: 1281, loss is 0.0028483590576797724\n",
      "epoch: 3 step: 1282, loss is 0.04042183235287666\n",
      "epoch: 3 step: 1283, loss is 0.10949784517288208\n",
      "epoch: 3 step: 1284, loss is 0.00687072891741991\n",
      "epoch: 3 step: 1285, loss is 0.1603957712650299\n",
      "epoch: 3 step: 1286, loss is 0.012722997926175594\n",
      "epoch: 3 step: 1287, loss is 0.07174539566040039\n",
      "epoch: 3 step: 1288, loss is 0.009391551837325096\n",
      "epoch: 3 step: 1289, loss is 0.02737317606806755\n",
      "epoch: 3 step: 1290, loss is 0.00882889423519373\n",
      "epoch: 3 step: 1291, loss is 0.0032365366350859404\n",
      "epoch: 3 step: 1292, loss is 0.005950883496552706\n",
      "epoch: 3 step: 1293, loss is 0.0017049778252840042\n",
      "epoch: 3 step: 1294, loss is 0.15770931541919708\n",
      "epoch: 3 step: 1295, loss is 0.00401523569598794\n",
      "epoch: 3 step: 1296, loss is 0.0018329470185562968\n",
      "epoch: 3 step: 1297, loss is 0.006810425315052271\n",
      "epoch: 3 step: 1298, loss is 0.13946469128131866\n",
      "epoch: 3 step: 1299, loss is 0.018256900832057\n",
      "epoch: 3 step: 1300, loss is 0.04631952568888664\n",
      "epoch: 3 step: 1301, loss is 0.005777149926871061\n",
      "epoch: 3 step: 1302, loss is 0.0934608206152916\n",
      "epoch: 3 step: 1303, loss is 0.006476118694990873\n",
      "epoch: 3 step: 1304, loss is 0.0020156116224825382\n",
      "epoch: 3 step: 1305, loss is 0.0003651088336482644\n",
      "epoch: 3 step: 1306, loss is 0.020679941400885582\n",
      "epoch: 3 step: 1307, loss is 0.0201325174421072\n",
      "epoch: 3 step: 1308, loss is 0.020942846313118935\n",
      "epoch: 3 step: 1309, loss is 0.010965925641357899\n",
      "epoch: 3 step: 1310, loss is 0.02647796832025051\n",
      "epoch: 3 step: 1311, loss is 0.00594363221898675\n",
      "epoch: 3 step: 1312, loss is 0.0010717060649767518\n",
      "epoch: 3 step: 1313, loss is 0.00853399932384491\n",
      "epoch: 3 step: 1314, loss is 0.07775271683931351\n",
      "epoch: 3 step: 1315, loss is 0.0003582309582270682\n",
      "epoch: 3 step: 1316, loss is 0.0286196731030941\n",
      "epoch: 3 step: 1317, loss is 0.07542067021131516\n",
      "epoch: 3 step: 1318, loss is 0.016081994399428368\n",
      "epoch: 3 step: 1319, loss is 0.07339874655008316\n",
      "epoch: 3 step: 1320, loss is 0.08338472992181778\n",
      "epoch: 3 step: 1321, loss is 0.0373002253472805\n",
      "epoch: 3 step: 1322, loss is 0.07064671814441681\n",
      "epoch: 3 step: 1323, loss is 0.13492467999458313\n",
      "epoch: 3 step: 1324, loss is 0.13161636888980865\n",
      "epoch: 3 step: 1325, loss is 0.09986428916454315\n",
      "epoch: 3 step: 1326, loss is 0.0011048868764191866\n",
      "epoch: 3 step: 1327, loss is 0.0005264575011096895\n",
      "epoch: 3 step: 1328, loss is 0.013506289571523666\n",
      "epoch: 3 step: 1329, loss is 0.017103157937526703\n",
      "epoch: 3 step: 1330, loss is 0.027565406635403633\n",
      "epoch: 3 step: 1331, loss is 0.034388113766908646\n",
      "epoch: 3 step: 1332, loss is 0.0034848565701395273\n",
      "epoch: 3 step: 1333, loss is 0.010994967073202133\n",
      "epoch: 3 step: 1334, loss is 0.010079260915517807\n",
      "epoch: 3 step: 1335, loss is 0.010386805050075054\n",
      "epoch: 3 step: 1336, loss is 0.012598502449691296\n",
      "epoch: 3 step: 1337, loss is 0.3417288362979889\n",
      "epoch: 3 step: 1338, loss is 0.0006155124865472317\n",
      "epoch: 3 step: 1339, loss is 0.01218206062912941\n",
      "epoch: 3 step: 1340, loss is 0.00960709247738123\n",
      "epoch: 3 step: 1341, loss is 0.034875258803367615\n",
      "epoch: 3 step: 1342, loss is 0.012009230442345142\n",
      "epoch: 3 step: 1343, loss is 0.0002977977565024048\n",
      "epoch: 3 step: 1344, loss is 0.003696886822581291\n",
      "epoch: 3 step: 1345, loss is 0.02238987199962139\n",
      "epoch: 3 step: 1346, loss is 0.010782819241285324\n",
      "epoch: 3 step: 1347, loss is 0.01967920921742916\n",
      "epoch: 3 step: 1348, loss is 0.007484567351639271\n",
      "epoch: 3 step: 1349, loss is 0.005085633136332035\n",
      "epoch: 3 step: 1350, loss is 0.07037593424320221\n",
      "epoch: 3 step: 1351, loss is 0.0039006767328828573\n",
      "epoch: 3 step: 1352, loss is 0.0021010416094213724\n",
      "epoch: 3 step: 1353, loss is 0.061399269849061966\n",
      "epoch: 3 step: 1354, loss is 0.0004535866610240191\n",
      "epoch: 3 step: 1355, loss is 0.06773906201124191\n",
      "epoch: 3 step: 1356, loss is 0.0069816624745726585\n",
      "epoch: 3 step: 1357, loss is 0.04741325601935387\n",
      "epoch: 3 step: 1358, loss is 0.04419032484292984\n",
      "epoch: 3 step: 1359, loss is 0.10352719575166702\n",
      "epoch: 3 step: 1360, loss is 0.03476003184914589\n",
      "epoch: 3 step: 1361, loss is 0.0028022003825753927\n",
      "epoch: 3 step: 1362, loss is 0.0017153432127088308\n",
      "epoch: 3 step: 1363, loss is 0.0036212096456438303\n",
      "epoch: 3 step: 1364, loss is 0.03232528641819954\n",
      "epoch: 3 step: 1365, loss is 0.0852978527545929\n",
      "epoch: 3 step: 1366, loss is 0.011226104572415352\n",
      "epoch: 3 step: 1367, loss is 0.05071451514959335\n",
      "epoch: 3 step: 1368, loss is 0.014012746512889862\n",
      "epoch: 3 step: 1369, loss is 0.016720663756132126\n",
      "epoch: 3 step: 1370, loss is 0.03863735496997833\n",
      "epoch: 3 step: 1371, loss is 0.06027374789118767\n",
      "epoch: 3 step: 1372, loss is 0.019739879295229912\n",
      "epoch: 3 step: 1373, loss is 0.10152830183506012\n",
      "epoch: 3 step: 1374, loss is 0.05938258394598961\n",
      "epoch: 3 step: 1375, loss is 0.2506411671638489\n",
      "epoch: 3 step: 1376, loss is 0.047542352229356766\n",
      "epoch: 3 step: 1377, loss is 0.04438687860965729\n",
      "epoch: 3 step: 1378, loss is 0.018147066235542297\n",
      "epoch: 3 step: 1379, loss is 0.22395239770412445\n",
      "epoch: 3 step: 1380, loss is 0.17263317108154297\n",
      "epoch: 3 step: 1381, loss is 0.011337826028466225\n",
      "epoch: 3 step: 1382, loss is 0.04156101867556572\n",
      "epoch: 3 step: 1383, loss is 0.05025401711463928\n",
      "epoch: 3 step: 1384, loss is 0.016333015635609627\n",
      "epoch: 3 step: 1385, loss is 0.0010020866757258773\n",
      "epoch: 3 step: 1386, loss is 0.05717998743057251\n",
      "epoch: 3 step: 1387, loss is 0.0023815478198230267\n",
      "epoch: 3 step: 1388, loss is 0.2540833055973053\n",
      "epoch: 3 step: 1389, loss is 0.016903618350625038\n",
      "epoch: 3 step: 1390, loss is 0.001187097397632897\n",
      "epoch: 3 step: 1391, loss is 0.09600404649972916\n",
      "epoch: 3 step: 1392, loss is 0.16269974410533905\n",
      "epoch: 3 step: 1393, loss is 0.028374696150422096\n",
      "epoch: 3 step: 1394, loss is 0.10513237118721008\n",
      "epoch: 3 step: 1395, loss is 0.022713271901011467\n",
      "epoch: 3 step: 1396, loss is 0.059227366000413895\n",
      "epoch: 3 step: 1397, loss is 0.18726631999015808\n",
      "epoch: 3 step: 1398, loss is 0.0035492603201419115\n",
      "epoch: 3 step: 1399, loss is 0.003003128804266453\n",
      "epoch: 3 step: 1400, loss is 0.06967983394861221\n",
      "epoch: 3 step: 1401, loss is 0.02950654923915863\n",
      "epoch: 3 step: 1402, loss is 0.012107149697840214\n",
      "epoch: 3 step: 1403, loss is 0.0014940066030249\n",
      "epoch: 3 step: 1404, loss is 0.0637969896197319\n",
      "epoch: 3 step: 1405, loss is 0.14182858169078827\n",
      "epoch: 3 step: 1406, loss is 0.0032700609881430864\n",
      "epoch: 3 step: 1407, loss is 0.011282258667051792\n",
      "epoch: 3 step: 1408, loss is 0.003788401372730732\n",
      "epoch: 3 step: 1409, loss is 0.018103206530213356\n",
      "epoch: 3 step: 1410, loss is 0.005494099576026201\n",
      "epoch: 3 step: 1411, loss is 0.14387434720993042\n",
      "epoch: 3 step: 1412, loss is 0.004772929474711418\n",
      "epoch: 3 step: 1413, loss is 0.0688263401389122\n",
      "epoch: 3 step: 1414, loss is 0.048623502254486084\n",
      "epoch: 3 step: 1415, loss is 0.049119237810373306\n",
      "epoch: 3 step: 1416, loss is 0.04296482726931572\n",
      "epoch: 3 step: 1417, loss is 0.27114927768707275\n",
      "epoch: 3 step: 1418, loss is 0.22002650797367096\n",
      "epoch: 3 step: 1419, loss is 0.000600441126152873\n",
      "epoch: 3 step: 1420, loss is 0.1895279884338379\n",
      "epoch: 3 step: 1421, loss is 0.06375149637460709\n",
      "epoch: 3 step: 1422, loss is 0.0035923304967582226\n",
      "epoch: 3 step: 1423, loss is 0.026118461042642593\n",
      "epoch: 3 step: 1424, loss is 0.0742107480764389\n",
      "epoch: 3 step: 1425, loss is 0.08100128173828125\n",
      "epoch: 3 step: 1426, loss is 0.026823416352272034\n",
      "epoch: 3 step: 1427, loss is 0.1328742951154709\n",
      "epoch: 3 step: 1428, loss is 0.033768896013498306\n",
      "epoch: 3 step: 1429, loss is 0.0014016985660418868\n",
      "epoch: 3 step: 1430, loss is 0.04035847261548042\n",
      "epoch: 3 step: 1431, loss is 0.0022105216048657894\n",
      "epoch: 3 step: 1432, loss is 0.03342888876795769\n",
      "epoch: 3 step: 1433, loss is 0.1346096694469452\n",
      "epoch: 3 step: 1434, loss is 0.0008035966893658042\n",
      "epoch: 3 step: 1435, loss is 0.21887952089309692\n",
      "epoch: 3 step: 1436, loss is 0.004744758829474449\n",
      "epoch: 3 step: 1437, loss is 0.05255284160375595\n",
      "epoch: 3 step: 1438, loss is 0.05065956339240074\n",
      "epoch: 3 step: 1439, loss is 0.010465036146342754\n",
      "epoch: 3 step: 1440, loss is 0.00776277482509613\n",
      "epoch: 3 step: 1441, loss is 0.000936712312977761\n",
      "epoch: 3 step: 1442, loss is 0.07403092086315155\n",
      "epoch: 3 step: 1443, loss is 0.005819129757583141\n",
      "epoch: 3 step: 1444, loss is 0.0011505449656397104\n",
      "epoch: 3 step: 1445, loss is 0.007864110171794891\n",
      "epoch: 3 step: 1446, loss is 0.0021219616755843163\n",
      "epoch: 3 step: 1447, loss is 0.021052444353699684\n",
      "epoch: 3 step: 1448, loss is 0.21305717527866364\n",
      "epoch: 3 step: 1449, loss is 0.0024617891758680344\n",
      "epoch: 3 step: 1450, loss is 0.1401159018278122\n",
      "epoch: 3 step: 1451, loss is 0.007260345388203859\n",
      "epoch: 3 step: 1452, loss is 0.03843003511428833\n",
      "epoch: 3 step: 1453, loss is 0.0034995682071894407\n",
      "epoch: 3 step: 1454, loss is 0.007365468889474869\n",
      "epoch: 3 step: 1455, loss is 0.005425935611128807\n",
      "epoch: 3 step: 1456, loss is 0.08831319957971573\n",
      "epoch: 3 step: 1457, loss is 0.02127562277019024\n",
      "epoch: 3 step: 1458, loss is 0.025212429463863373\n",
      "epoch: 3 step: 1459, loss is 0.2659648656845093\n",
      "epoch: 3 step: 1460, loss is 0.013226327486336231\n",
      "epoch: 3 step: 1461, loss is 0.061781834810972214\n",
      "epoch: 3 step: 1462, loss is 0.0012760841054841876\n",
      "epoch: 3 step: 1463, loss is 0.007595300208777189\n",
      "epoch: 3 step: 1464, loss is 0.049417633563280106\n",
      "epoch: 3 step: 1465, loss is 0.10209004580974579\n",
      "epoch: 3 step: 1466, loss is 0.08219235390424728\n",
      "epoch: 3 step: 1467, loss is 0.019801480695605278\n",
      "epoch: 3 step: 1468, loss is 0.040316466242074966\n",
      "epoch: 3 step: 1469, loss is 0.00847834162414074\n",
      "epoch: 3 step: 1470, loss is 0.011208409443497658\n",
      "epoch: 3 step: 1471, loss is 0.09681444615125656\n",
      "epoch: 3 step: 1472, loss is 0.046875420957803726\n",
      "epoch: 3 step: 1473, loss is 0.0017564496956765652\n",
      "epoch: 3 step: 1474, loss is 0.1294746994972229\n",
      "epoch: 3 step: 1475, loss is 0.013379355892539024\n",
      "epoch: 3 step: 1476, loss is 0.004900665488094091\n",
      "epoch: 3 step: 1477, loss is 0.026726379990577698\n",
      "epoch: 3 step: 1478, loss is 0.0093421321362257\n",
      "epoch: 3 step: 1479, loss is 0.0420834943652153\n",
      "epoch: 3 step: 1480, loss is 0.05865449458360672\n",
      "epoch: 3 step: 1481, loss is 0.018778866156935692\n",
      "epoch: 3 step: 1482, loss is 0.02230190671980381\n",
      "epoch: 3 step: 1483, loss is 0.08300980180501938\n",
      "epoch: 3 step: 1484, loss is 0.016969524323940277\n",
      "epoch: 3 step: 1485, loss is 0.07834446430206299\n",
      "epoch: 3 step: 1486, loss is 0.0177476666867733\n",
      "epoch: 3 step: 1487, loss is 0.1325826495885849\n",
      "epoch: 3 step: 1488, loss is 0.0044805617071688175\n",
      "epoch: 3 step: 1489, loss is 0.09396284073591232\n",
      "epoch: 3 step: 1490, loss is 0.012613431550562382\n",
      "epoch: 3 step: 1491, loss is 0.0075330836698412895\n",
      "epoch: 3 step: 1492, loss is 0.13727916777133942\n",
      "epoch: 3 step: 1493, loss is 0.0032079259399324656\n",
      "epoch: 3 step: 1494, loss is 0.02192649245262146\n",
      "epoch: 3 step: 1495, loss is 0.08713992685079575\n",
      "epoch: 3 step: 1496, loss is 0.06792683154344559\n",
      "epoch: 3 step: 1497, loss is 0.013937680050730705\n",
      "epoch: 3 step: 1498, loss is 0.0037631664890795946\n",
      "epoch: 3 step: 1499, loss is 0.055649422109127045\n",
      "epoch: 3 step: 1500, loss is 0.005507087800651789\n",
      "epoch: 3 step: 1501, loss is 0.0345996730029583\n",
      "epoch: 3 step: 1502, loss is 0.0623420886695385\n",
      "epoch: 3 step: 1503, loss is 0.018337540328502655\n",
      "epoch: 3 step: 1504, loss is 0.009059126488864422\n",
      "epoch: 3 step: 1505, loss is 0.0020430651493370533\n",
      "epoch: 3 step: 1506, loss is 0.0676329955458641\n",
      "epoch: 3 step: 1507, loss is 0.0015831147320568562\n",
      "epoch: 3 step: 1508, loss is 0.14872664213180542\n",
      "epoch: 3 step: 1509, loss is 0.00283777411095798\n",
      "epoch: 3 step: 1510, loss is 0.004819531459361315\n",
      "epoch: 3 step: 1511, loss is 0.005888526793569326\n",
      "epoch: 3 step: 1512, loss is 0.008935200981795788\n",
      "epoch: 3 step: 1513, loss is 0.004488097503781319\n",
      "epoch: 3 step: 1514, loss is 0.09482099860906601\n",
      "epoch: 3 step: 1515, loss is 0.005846112500876188\n",
      "epoch: 3 step: 1516, loss is 0.03395075350999832\n",
      "epoch: 3 step: 1517, loss is 0.030983977019786835\n",
      "epoch: 3 step: 1518, loss is 0.026113256812095642\n",
      "epoch: 3 step: 1519, loss is 0.004229855258017778\n",
      "epoch: 3 step: 1520, loss is 0.10044559091329575\n",
      "epoch: 3 step: 1521, loss is 0.002238568849861622\n",
      "epoch: 3 step: 1522, loss is 0.001900296425446868\n",
      "epoch: 3 step: 1523, loss is 0.024969467893242836\n",
      "epoch: 3 step: 1524, loss is 0.012474843300879002\n",
      "epoch: 3 step: 1525, loss is 0.022080261260271072\n",
      "epoch: 3 step: 1526, loss is 0.00724857859313488\n",
      "epoch: 3 step: 1527, loss is 0.2626897990703583\n",
      "epoch: 3 step: 1528, loss is 0.10780700296163559\n",
      "epoch: 3 step: 1529, loss is 0.004970157984644175\n",
      "epoch: 3 step: 1530, loss is 0.01006024144589901\n",
      "epoch: 3 step: 1531, loss is 0.0014094033977016807\n",
      "epoch: 3 step: 1532, loss is 0.013011600822210312\n",
      "epoch: 3 step: 1533, loss is 0.019590428099036217\n",
      "epoch: 3 step: 1534, loss is 0.05018168315291405\n",
      "epoch: 3 step: 1535, loss is 0.004563945345580578\n",
      "epoch: 3 step: 1536, loss is 0.013441916555166245\n",
      "epoch: 3 step: 1537, loss is 0.1413099318742752\n",
      "epoch: 3 step: 1538, loss is 0.06085469573736191\n",
      "epoch: 3 step: 1539, loss is 0.029499933123588562\n",
      "epoch: 3 step: 1540, loss is 0.06859099119901657\n",
      "epoch: 3 step: 1541, loss is 0.041901107877492905\n",
      "epoch: 3 step: 1542, loss is 0.0038029595743864775\n",
      "epoch: 3 step: 1543, loss is 0.01384614035487175\n",
      "epoch: 3 step: 1544, loss is 0.003564801998436451\n",
      "epoch: 3 step: 1545, loss is 0.007727455347776413\n",
      "epoch: 3 step: 1546, loss is 0.008282829076051712\n",
      "epoch: 3 step: 1547, loss is 0.24603836238384247\n",
      "epoch: 3 step: 1548, loss is 0.006111623719334602\n",
      "epoch: 3 step: 1549, loss is 0.028908787295222282\n",
      "epoch: 3 step: 1550, loss is 0.003999647218734026\n",
      "epoch: 3 step: 1551, loss is 0.07093662768602371\n",
      "epoch: 3 step: 1552, loss is 0.0037601294461637735\n",
      "epoch: 3 step: 1553, loss is 0.029545066878199577\n",
      "epoch: 3 step: 1554, loss is 0.022517845034599304\n",
      "epoch: 3 step: 1555, loss is 0.039442408829927444\n",
      "epoch: 3 step: 1556, loss is 0.012604172341525555\n",
      "epoch: 3 step: 1557, loss is 0.03491849824786186\n",
      "epoch: 3 step: 1558, loss is 0.003915418870747089\n",
      "epoch: 3 step: 1559, loss is 0.00680889468640089\n",
      "epoch: 3 step: 1560, loss is 0.15032517910003662\n",
      "epoch: 3 step: 1561, loss is 0.000461497635114938\n",
      "epoch: 3 step: 1562, loss is 0.05136417970061302\n",
      "epoch: 3 step: 1563, loss is 0.006145920138806105\n",
      "epoch: 3 step: 1564, loss is 0.08348802477121353\n",
      "epoch: 3 step: 1565, loss is 0.002347661880776286\n",
      "epoch: 3 step: 1566, loss is 0.0010753748938441277\n",
      "epoch: 3 step: 1567, loss is 0.00806121714413166\n",
      "epoch: 3 step: 1568, loss is 0.00620712386444211\n",
      "epoch: 3 step: 1569, loss is 0.04079361632466316\n",
      "epoch: 3 step: 1570, loss is 0.02936253324151039\n",
      "epoch: 3 step: 1571, loss is 0.00390749704092741\n",
      "epoch: 3 step: 1572, loss is 0.001021043281070888\n",
      "epoch: 3 step: 1573, loss is 0.06577521562576294\n",
      "epoch: 3 step: 1574, loss is 0.15033602714538574\n",
      "epoch: 3 step: 1575, loss is 0.0039688097313046455\n",
      "epoch: 3 step: 1576, loss is 0.02928115613758564\n",
      "epoch: 3 step: 1577, loss is 0.11386106163263321\n",
      "epoch: 3 step: 1578, loss is 0.02574975974857807\n",
      "epoch: 3 step: 1579, loss is 0.007290576118975878\n",
      "epoch: 3 step: 1580, loss is 0.013655832968652248\n",
      "epoch: 3 step: 1581, loss is 0.06929852813482285\n",
      "epoch: 3 step: 1582, loss is 0.0723043754696846\n",
      "epoch: 3 step: 1583, loss is 0.02678200788795948\n",
      "epoch: 3 step: 1584, loss is 0.07750782370567322\n",
      "epoch: 3 step: 1585, loss is 0.014517849311232567\n",
      "epoch: 3 step: 1586, loss is 0.011575918644666672\n",
      "epoch: 3 step: 1587, loss is 0.34489795565605164\n",
      "epoch: 3 step: 1588, loss is 0.029635412618517876\n",
      "epoch: 3 step: 1589, loss is 0.04044221341609955\n",
      "epoch: 3 step: 1590, loss is 0.010020611807703972\n",
      "epoch: 3 step: 1591, loss is 0.0013923945371061563\n",
      "epoch: 3 step: 1592, loss is 0.013684839941561222\n",
      "epoch: 3 step: 1593, loss is 0.27993765473365784\n",
      "epoch: 3 step: 1594, loss is 0.09028971195220947\n",
      "epoch: 3 step: 1595, loss is 0.011574272997677326\n",
      "epoch: 3 step: 1596, loss is 0.059837572276592255\n",
      "epoch: 3 step: 1597, loss is 0.016695726662874222\n",
      "epoch: 3 step: 1598, loss is 0.13096311688423157\n",
      "epoch: 3 step: 1599, loss is 0.015810100361704826\n",
      "epoch: 3 step: 1600, loss is 0.09902134537696838\n",
      "epoch: 3 step: 1601, loss is 0.05038158595561981\n",
      "epoch: 3 step: 1602, loss is 0.005359819158911705\n",
      "epoch: 3 step: 1603, loss is 0.1768970489501953\n",
      "epoch: 3 step: 1604, loss is 0.14702117443084717\n",
      "epoch: 3 step: 1605, loss is 0.012080839835107327\n",
      "epoch: 3 step: 1606, loss is 0.040983546525239944\n",
      "epoch: 3 step: 1607, loss is 0.01905539631843567\n",
      "epoch: 3 step: 1608, loss is 0.06031041964888573\n",
      "epoch: 3 step: 1609, loss is 0.01583300158381462\n",
      "epoch: 3 step: 1610, loss is 0.0032985336147248745\n",
      "epoch: 3 step: 1611, loss is 0.016001060605049133\n",
      "epoch: 3 step: 1612, loss is 0.02255920320749283\n",
      "epoch: 3 step: 1613, loss is 0.10324718058109283\n",
      "epoch: 3 step: 1614, loss is 0.008589450269937515\n",
      "epoch: 3 step: 1615, loss is 0.1828967034816742\n",
      "epoch: 3 step: 1616, loss is 0.06113544851541519\n",
      "epoch: 3 step: 1617, loss is 0.25406357645988464\n",
      "epoch: 3 step: 1618, loss is 0.01953408122062683\n",
      "epoch: 3 step: 1619, loss is 0.008009928278625011\n",
      "epoch: 3 step: 1620, loss is 0.002671995433047414\n",
      "epoch: 3 step: 1621, loss is 0.03427981212735176\n",
      "epoch: 3 step: 1622, loss is 0.05760056525468826\n",
      "epoch: 3 step: 1623, loss is 0.004019135143607855\n",
      "epoch: 3 step: 1624, loss is 0.04469509422779083\n",
      "epoch: 3 step: 1625, loss is 0.006734221708029509\n",
      "epoch: 3 step: 1626, loss is 0.1485123187303543\n",
      "epoch: 3 step: 1627, loss is 0.001339794835075736\n",
      "epoch: 3 step: 1628, loss is 0.020095407962799072\n",
      "epoch: 3 step: 1629, loss is 0.0016492644790560007\n",
      "epoch: 3 step: 1630, loss is 0.01670212484896183\n",
      "epoch: 3 step: 1631, loss is 0.004841790534555912\n",
      "epoch: 3 step: 1632, loss is 0.0104757659137249\n",
      "epoch: 3 step: 1633, loss is 0.007894854061305523\n",
      "epoch: 3 step: 1634, loss is 0.06685801595449448\n",
      "epoch: 3 step: 1635, loss is 0.03486936539411545\n",
      "epoch: 3 step: 1636, loss is 0.0565929152071476\n",
      "epoch: 3 step: 1637, loss is 0.03849751502275467\n",
      "epoch: 3 step: 1638, loss is 0.016905417665839195\n",
      "epoch: 3 step: 1639, loss is 0.04310956597328186\n",
      "epoch: 3 step: 1640, loss is 0.08709268271923065\n",
      "epoch: 3 step: 1641, loss is 0.0039003239944577217\n",
      "epoch: 3 step: 1642, loss is 0.0111415795981884\n",
      "epoch: 3 step: 1643, loss is 0.0010706399334594607\n",
      "epoch: 3 step: 1644, loss is 0.01652667298913002\n",
      "epoch: 3 step: 1645, loss is 0.0026535887736827135\n",
      "epoch: 3 step: 1646, loss is 0.008424793370068073\n",
      "epoch: 3 step: 1647, loss is 0.007303696125745773\n",
      "epoch: 3 step: 1648, loss is 0.013416050001978874\n",
      "epoch: 3 step: 1649, loss is 0.008724141865968704\n",
      "epoch: 3 step: 1650, loss is 0.08898437023162842\n",
      "epoch: 3 step: 1651, loss is 0.0007244659354910254\n",
      "epoch: 3 step: 1652, loss is 0.5997312068939209\n",
      "epoch: 3 step: 1653, loss is 0.024346577003598213\n",
      "epoch: 3 step: 1654, loss is 0.05756611004471779\n",
      "epoch: 3 step: 1655, loss is 0.004843576345592737\n",
      "epoch: 3 step: 1656, loss is 0.004642884247004986\n",
      "epoch: 3 step: 1657, loss is 0.2723650634288788\n",
      "epoch: 3 step: 1658, loss is 0.005093702580779791\n",
      "epoch: 3 step: 1659, loss is 0.2389201819896698\n",
      "epoch: 3 step: 1660, loss is 0.018437592312693596\n",
      "epoch: 3 step: 1661, loss is 0.0032047478016465902\n",
      "epoch: 3 step: 1662, loss is 0.009854402393102646\n",
      "epoch: 3 step: 1663, loss is 0.00913182646036148\n",
      "epoch: 3 step: 1664, loss is 0.11670514941215515\n",
      "epoch: 3 step: 1665, loss is 0.017375444993376732\n",
      "epoch: 3 step: 1666, loss is 0.004515479784458876\n",
      "epoch: 3 step: 1667, loss is 0.007004583720117807\n",
      "epoch: 3 step: 1668, loss is 0.005609017796814442\n",
      "epoch: 3 step: 1669, loss is 0.013737877830862999\n",
      "epoch: 3 step: 1670, loss is 0.03129632771015167\n",
      "epoch: 3 step: 1671, loss is 0.006266899406909943\n",
      "epoch: 3 step: 1672, loss is 0.0019871655385941267\n",
      "epoch: 3 step: 1673, loss is 0.013056580908596516\n",
      "epoch: 3 step: 1674, loss is 0.008109081536531448\n",
      "epoch: 3 step: 1675, loss is 0.10352777689695358\n",
      "epoch: 3 step: 1676, loss is 0.007173933554440737\n",
      "epoch: 3 step: 1677, loss is 0.0037472122348845005\n",
      "epoch: 3 step: 1678, loss is 0.040869131684303284\n",
      "epoch: 3 step: 1679, loss is 0.000972346228081733\n",
      "epoch: 3 step: 1680, loss is 0.020523248240351677\n",
      "epoch: 3 step: 1681, loss is 0.1210581585764885\n",
      "epoch: 3 step: 1682, loss is 0.001643255352973938\n",
      "epoch: 3 step: 1683, loss is 0.0020574245136231184\n",
      "epoch: 3 step: 1684, loss is 0.022256232798099518\n",
      "epoch: 3 step: 1685, loss is 0.0803227424621582\n",
      "epoch: 3 step: 1686, loss is 0.07369707524776459\n",
      "epoch: 3 step: 1687, loss is 0.03760821372270584\n",
      "epoch: 3 step: 1688, loss is 0.0073668016120791435\n",
      "epoch: 3 step: 1689, loss is 0.21861349046230316\n",
      "epoch: 3 step: 1690, loss is 0.0007689998019486666\n",
      "epoch: 3 step: 1691, loss is 0.0007225159206427634\n",
      "epoch: 3 step: 1692, loss is 0.02033327706158161\n",
      "epoch: 3 step: 1693, loss is 0.0318293571472168\n",
      "epoch: 3 step: 1694, loss is 0.018795447424054146\n",
      "epoch: 3 step: 1695, loss is 0.01995871029794216\n",
      "epoch: 3 step: 1696, loss is 0.07509257644414902\n",
      "epoch: 3 step: 1697, loss is 0.0025909333489835262\n",
      "epoch: 3 step: 1698, loss is 0.012417877092957497\n",
      "epoch: 3 step: 1699, loss is 0.0005416299682110548\n",
      "epoch: 3 step: 1700, loss is 0.04418794438242912\n",
      "epoch: 3 step: 1701, loss is 0.008390507660806179\n",
      "epoch: 3 step: 1702, loss is 0.007351316977292299\n",
      "epoch: 3 step: 1703, loss is 0.08776824921369553\n",
      "epoch: 3 step: 1704, loss is 0.008596147410571575\n",
      "epoch: 3 step: 1705, loss is 0.029641717672348022\n",
      "epoch: 3 step: 1706, loss is 0.19205456972122192\n",
      "epoch: 3 step: 1707, loss is 0.0018489303765818477\n",
      "epoch: 3 step: 1708, loss is 0.002892228774726391\n",
      "epoch: 3 step: 1709, loss is 0.02037654258310795\n",
      "epoch: 3 step: 1710, loss is 0.006749866530299187\n",
      "epoch: 3 step: 1711, loss is 0.14514857530593872\n",
      "epoch: 3 step: 1712, loss is 0.008714779280126095\n",
      "epoch: 3 step: 1713, loss is 0.014655159786343575\n",
      "epoch: 3 step: 1714, loss is 0.010945207439363003\n",
      "epoch: 3 step: 1715, loss is 0.0004033789155073464\n",
      "epoch: 3 step: 1716, loss is 0.030560219660401344\n",
      "epoch: 3 step: 1717, loss is 0.007666997145861387\n",
      "epoch: 3 step: 1718, loss is 0.026358556002378464\n",
      "epoch: 3 step: 1719, loss is 0.11535786837339401\n",
      "epoch: 3 step: 1720, loss is 0.0629347488284111\n",
      "epoch: 3 step: 1721, loss is 0.031883519142866135\n",
      "epoch: 3 step: 1722, loss is 0.02212272398173809\n",
      "epoch: 3 step: 1723, loss is 0.0016363341128453612\n",
      "epoch: 3 step: 1724, loss is 0.04426763951778412\n",
      "epoch: 3 step: 1725, loss is 0.0022001201286911964\n",
      "epoch: 3 step: 1726, loss is 0.00034401981974951923\n",
      "epoch: 3 step: 1727, loss is 0.0026763996575027704\n",
      "epoch: 3 step: 1728, loss is 0.07981986552476883\n",
      "epoch: 3 step: 1729, loss is 0.06651248037815094\n",
      "epoch: 3 step: 1730, loss is 0.0004817213339265436\n",
      "epoch: 3 step: 1731, loss is 0.08143006265163422\n",
      "epoch: 3 step: 1732, loss is 0.005981473717838526\n",
      "epoch: 3 step: 1733, loss is 0.08310466259717941\n",
      "epoch: 3 step: 1734, loss is 0.05926235020160675\n",
      "epoch: 3 step: 1735, loss is 0.0025228685699403286\n",
      "epoch: 3 step: 1736, loss is 0.08584530651569366\n",
      "epoch: 3 step: 1737, loss is 0.03668544813990593\n",
      "epoch: 3 step: 1738, loss is 0.009114539250731468\n",
      "epoch: 3 step: 1739, loss is 0.0026070917956531048\n",
      "epoch: 3 step: 1740, loss is 0.011745074763894081\n",
      "epoch: 3 step: 1741, loss is 0.059498030692338943\n",
      "epoch: 3 step: 1742, loss is 0.03154601901769638\n",
      "epoch: 3 step: 1743, loss is 0.02828107960522175\n",
      "epoch: 3 step: 1744, loss is 0.0019385897321626544\n",
      "epoch: 3 step: 1745, loss is 0.03961346670985222\n",
      "epoch: 3 step: 1746, loss is 0.09825581312179565\n",
      "epoch: 3 step: 1747, loss is 0.0007502567605115473\n",
      "epoch: 3 step: 1748, loss is 0.01575208082795143\n",
      "epoch: 3 step: 1749, loss is 0.028260784223675728\n",
      "epoch: 3 step: 1750, loss is 0.022787287831306458\n",
      "epoch: 3 step: 1751, loss is 0.004400264006108046\n",
      "epoch: 3 step: 1752, loss is 0.0314132422208786\n",
      "epoch: 3 step: 1753, loss is 0.029184654355049133\n",
      "epoch: 3 step: 1754, loss is 0.025617778301239014\n",
      "epoch: 3 step: 1755, loss is 0.013180327601730824\n",
      "epoch: 3 step: 1756, loss is 0.0010566693963482976\n",
      "epoch: 3 step: 1757, loss is 0.006780901458114386\n",
      "epoch: 3 step: 1758, loss is 0.0013410542160272598\n",
      "epoch: 3 step: 1759, loss is 0.0011640334269031882\n",
      "epoch: 3 step: 1760, loss is 0.010166004300117493\n",
      "epoch: 3 step: 1761, loss is 0.000677785836160183\n",
      "epoch: 3 step: 1762, loss is 0.009300822392106056\n",
      "epoch: 3 step: 1763, loss is 0.10656540095806122\n",
      "epoch: 3 step: 1764, loss is 0.0033870069310069084\n",
      "epoch: 3 step: 1765, loss is 0.0015580083709210157\n",
      "epoch: 3 step: 1766, loss is 0.17447981238365173\n",
      "epoch: 3 step: 1767, loss is 0.003189260605722666\n",
      "epoch: 3 step: 1768, loss is 0.0015099402517080307\n",
      "epoch: 3 step: 1769, loss is 0.10453997552394867\n",
      "epoch: 3 step: 1770, loss is 0.012331832200288773\n",
      "epoch: 3 step: 1771, loss is 0.011463375762104988\n",
      "epoch: 3 step: 1772, loss is 0.1293697953224182\n",
      "epoch: 3 step: 1773, loss is 0.006425381638109684\n",
      "epoch: 3 step: 1774, loss is 0.02649199403822422\n",
      "epoch: 3 step: 1775, loss is 0.0090117696672678\n",
      "epoch: 3 step: 1776, loss is 0.009702544659376144\n",
      "epoch: 3 step: 1777, loss is 0.045945391058921814\n",
      "epoch: 3 step: 1778, loss is 0.13882242143154144\n",
      "epoch: 3 step: 1779, loss is 0.0020123727153986692\n",
      "epoch: 3 step: 1780, loss is 0.006459780968725681\n",
      "epoch: 3 step: 1781, loss is 0.004170302301645279\n",
      "epoch: 3 step: 1782, loss is 0.017216235399246216\n",
      "epoch: 3 step: 1783, loss is 0.04990168660879135\n",
      "epoch: 3 step: 1784, loss is 0.2175932675600052\n",
      "epoch: 3 step: 1785, loss is 0.0012073905672878027\n",
      "epoch: 3 step: 1786, loss is 0.17433951795101166\n",
      "epoch: 3 step: 1787, loss is 0.0007293993839994073\n",
      "epoch: 3 step: 1788, loss is 0.10976850241422653\n",
      "epoch: 3 step: 1789, loss is 0.011109697632491589\n",
      "epoch: 3 step: 1790, loss is 0.017777390778064728\n",
      "epoch: 3 step: 1791, loss is 0.14680029451847076\n",
      "epoch: 3 step: 1792, loss is 0.005602773278951645\n",
      "epoch: 3 step: 1793, loss is 0.003393983468413353\n",
      "epoch: 3 step: 1794, loss is 0.03391900286078453\n",
      "epoch: 3 step: 1795, loss is 0.059501975774765015\n",
      "epoch: 3 step: 1796, loss is 0.0026193310040980577\n",
      "epoch: 3 step: 1797, loss is 0.026783565059304237\n",
      "epoch: 3 step: 1798, loss is 0.09455127269029617\n",
      "epoch: 3 step: 1799, loss is 0.010375263169407845\n",
      "epoch: 3 step: 1800, loss is 0.05430123209953308\n",
      "epoch: 3 step: 1801, loss is 0.1708579659461975\n",
      "epoch: 3 step: 1802, loss is 0.11717142909765244\n",
      "epoch: 3 step: 1803, loss is 0.020217768847942352\n",
      "epoch: 3 step: 1804, loss is 0.008976302109658718\n",
      "epoch: 3 step: 1805, loss is 0.005537673830986023\n",
      "epoch: 3 step: 1806, loss is 0.1335967481136322\n",
      "epoch: 3 step: 1807, loss is 0.000405916478484869\n",
      "epoch: 3 step: 1808, loss is 0.0060497778467834\n",
      "epoch: 3 step: 1809, loss is 0.008276940323412418\n",
      "epoch: 3 step: 1810, loss is 0.017301131039857864\n",
      "epoch: 3 step: 1811, loss is 0.013227486982941628\n",
      "epoch: 3 step: 1812, loss is 0.0019300412386655807\n",
      "epoch: 3 step: 1813, loss is 0.036208853125572205\n",
      "epoch: 3 step: 1814, loss is 0.094483382999897\n",
      "epoch: 3 step: 1815, loss is 0.08181383460760117\n",
      "epoch: 3 step: 1816, loss is 0.0070256744511425495\n",
      "epoch: 3 step: 1817, loss is 0.05619460344314575\n",
      "epoch: 3 step: 1818, loss is 0.034550413489341736\n",
      "epoch: 3 step: 1819, loss is 0.04085884988307953\n",
      "epoch: 3 step: 1820, loss is 0.004411496687680483\n",
      "epoch: 3 step: 1821, loss is 0.11949925124645233\n",
      "epoch: 3 step: 1822, loss is 0.2194255143404007\n",
      "epoch: 3 step: 1823, loss is 0.0013042648788541555\n",
      "epoch: 3 step: 1824, loss is 0.028949491679668427\n",
      "epoch: 3 step: 1825, loss is 0.015391048043966293\n",
      "epoch: 3 step: 1826, loss is 0.0809633731842041\n",
      "epoch: 3 step: 1827, loss is 0.03521612286567688\n",
      "epoch: 3 step: 1828, loss is 0.03436923027038574\n",
      "epoch: 3 step: 1829, loss is 0.020807895809412003\n",
      "epoch: 3 step: 1830, loss is 0.10678651183843613\n",
      "epoch: 3 step: 1831, loss is 0.02639029733836651\n",
      "epoch: 3 step: 1832, loss is 0.001815156894735992\n",
      "epoch: 3 step: 1833, loss is 0.009290902875363827\n",
      "epoch: 3 step: 1834, loss is 0.02051992155611515\n",
      "epoch: 3 step: 1835, loss is 0.20656563341617584\n",
      "epoch: 3 step: 1836, loss is 0.014899815432727337\n",
      "epoch: 3 step: 1837, loss is 0.04291403293609619\n",
      "epoch: 3 step: 1838, loss is 0.18402060866355896\n",
      "epoch: 3 step: 1839, loss is 0.014688852243125439\n",
      "epoch: 3 step: 1840, loss is 0.10669215768575668\n",
      "epoch: 3 step: 1841, loss is 0.08108492940664291\n",
      "epoch: 3 step: 1842, loss is 0.006318940781056881\n",
      "epoch: 3 step: 1843, loss is 0.00767874950543046\n",
      "epoch: 3 step: 1844, loss is 0.0027113696560263634\n",
      "epoch: 3 step: 1845, loss is 0.002579161198809743\n",
      "epoch: 3 step: 1846, loss is 0.010334556922316551\n",
      "epoch: 3 step: 1847, loss is 0.0015232377918437123\n",
      "epoch: 3 step: 1848, loss is 0.01120363362133503\n",
      "epoch: 3 step: 1849, loss is 0.029351964592933655\n",
      "epoch: 3 step: 1850, loss is 0.005705530289560556\n",
      "epoch: 3 step: 1851, loss is 0.025011131539940834\n",
      "epoch: 3 step: 1852, loss is 0.07525888085365295\n",
      "epoch: 3 step: 1853, loss is 0.00397222675383091\n",
      "epoch: 3 step: 1854, loss is 0.01928591914474964\n",
      "epoch: 3 step: 1855, loss is 0.18218287825584412\n",
      "epoch: 3 step: 1856, loss is 0.028409354388713837\n",
      "epoch: 3 step: 1857, loss is 0.0171061884611845\n",
      "epoch: 3 step: 1858, loss is 0.15069758892059326\n",
      "epoch: 3 step: 1859, loss is 0.0035522673279047012\n",
      "epoch: 3 step: 1860, loss is 0.07241600751876831\n",
      "epoch: 3 step: 1861, loss is 0.003185421461239457\n",
      "epoch: 3 step: 1862, loss is 0.03210548311471939\n",
      "epoch: 3 step: 1863, loss is 0.002294770907610655\n",
      "epoch: 3 step: 1864, loss is 0.08191564679145813\n",
      "epoch: 3 step: 1865, loss is 0.0036084474995732307\n",
      "epoch: 3 step: 1866, loss is 0.010141727514564991\n",
      "epoch: 3 step: 1867, loss is 0.11313368380069733\n",
      "epoch: 3 step: 1868, loss is 0.08657874166965485\n",
      "epoch: 3 step: 1869, loss is 0.026213034987449646\n",
      "epoch: 3 step: 1870, loss is 0.034654296934604645\n",
      "epoch: 3 step: 1871, loss is 0.004616409074515104\n",
      "epoch: 3 step: 1872, loss is 0.21847662329673767\n",
      "epoch: 3 step: 1873, loss is 0.2768411636352539\n",
      "epoch: 3 step: 1874, loss is 0.0021333377808332443\n",
      "epoch: 3 step: 1875, loss is 0.19029588997364044\n",
      "epoch: 4 step: 1, loss is 0.002605727408081293\n",
      "epoch: 4 step: 2, loss is 0.0026439905632287264\n",
      "epoch: 4 step: 3, loss is 0.0012283389223739505\n",
      "epoch: 4 step: 4, loss is 0.002402891404926777\n",
      "epoch: 4 step: 5, loss is 0.024778330698609352\n",
      "epoch: 4 step: 6, loss is 0.06144777685403824\n",
      "epoch: 4 step: 7, loss is 0.07205769419670105\n",
      "epoch: 4 step: 8, loss is 0.04444567486643791\n",
      "epoch: 4 step: 9, loss is 0.034984610974788666\n",
      "epoch: 4 step: 10, loss is 0.009282369166612625\n",
      "epoch: 4 step: 11, loss is 0.009192652069032192\n",
      "epoch: 4 step: 12, loss is 0.08383333683013916\n",
      "epoch: 4 step: 13, loss is 0.011173906736075878\n",
      "epoch: 4 step: 14, loss is 0.007213343866169453\n",
      "epoch: 4 step: 15, loss is 0.12520374357700348\n",
      "epoch: 4 step: 16, loss is 0.00399659713730216\n",
      "epoch: 4 step: 17, loss is 0.022268161177635193\n",
      "epoch: 4 step: 18, loss is 0.0018848584732040763\n",
      "epoch: 4 step: 19, loss is 0.004083619453012943\n",
      "epoch: 4 step: 20, loss is 0.2110809087753296\n",
      "epoch: 4 step: 21, loss is 0.004739291500300169\n",
      "epoch: 4 step: 22, loss is 0.06276358664035797\n",
      "epoch: 4 step: 23, loss is 0.031553592532873154\n",
      "epoch: 4 step: 24, loss is 0.0016195587813854218\n",
      "epoch: 4 step: 25, loss is 0.014967964962124825\n",
      "epoch: 4 step: 26, loss is 0.005865009035915136\n",
      "epoch: 4 step: 27, loss is 0.046223513782024384\n",
      "epoch: 4 step: 28, loss is 0.0025811067316681147\n",
      "epoch: 4 step: 29, loss is 0.0018063513562083244\n",
      "epoch: 4 step: 30, loss is 0.048497799783945084\n",
      "epoch: 4 step: 31, loss is 0.014814428053796291\n",
      "epoch: 4 step: 32, loss is 0.03170177713036537\n",
      "epoch: 4 step: 33, loss is 0.0032229397911578417\n",
      "epoch: 4 step: 34, loss is 0.07179617881774902\n",
      "epoch: 4 step: 35, loss is 0.006893799640238285\n",
      "epoch: 4 step: 36, loss is 0.0006692903116345406\n",
      "epoch: 4 step: 37, loss is 0.04559476673603058\n",
      "epoch: 4 step: 38, loss is 0.0032848261762410402\n",
      "epoch: 4 step: 39, loss is 0.01976628042757511\n",
      "epoch: 4 step: 40, loss is 0.1243174746632576\n",
      "epoch: 4 step: 41, loss is 0.07032085210084915\n",
      "epoch: 4 step: 42, loss is 0.2781243622303009\n",
      "epoch: 4 step: 43, loss is 0.04774068668484688\n",
      "epoch: 4 step: 44, loss is 0.05355573445558548\n",
      "epoch: 4 step: 45, loss is 0.002762218937277794\n",
      "epoch: 4 step: 46, loss is 0.002430400811135769\n",
      "epoch: 4 step: 47, loss is 0.003168908879160881\n",
      "epoch: 4 step: 48, loss is 0.010841320268809795\n",
      "epoch: 4 step: 49, loss is 0.005433009006083012\n",
      "epoch: 4 step: 50, loss is 0.129912331700325\n",
      "epoch: 4 step: 51, loss is 0.027309028431773186\n",
      "epoch: 4 step: 52, loss is 0.06159695237874985\n",
      "epoch: 4 step: 53, loss is 0.004190974403172731\n",
      "epoch: 4 step: 54, loss is 0.12875379621982574\n",
      "epoch: 4 step: 55, loss is 0.22284631431102753\n",
      "epoch: 4 step: 56, loss is 0.011021915823221207\n",
      "epoch: 4 step: 57, loss is 0.0029621843714267015\n",
      "epoch: 4 step: 58, loss is 0.18337225914001465\n",
      "epoch: 4 step: 59, loss is 0.022778043523430824\n",
      "epoch: 4 step: 60, loss is 0.06183220073580742\n",
      "epoch: 4 step: 61, loss is 0.015379739925265312\n",
      "epoch: 4 step: 62, loss is 0.0691993460059166\n",
      "epoch: 4 step: 63, loss is 0.005505913868546486\n",
      "epoch: 4 step: 64, loss is 0.06882461160421371\n",
      "epoch: 4 step: 65, loss is 0.04003661498427391\n",
      "epoch: 4 step: 66, loss is 0.006058630999177694\n",
      "epoch: 4 step: 67, loss is 0.0019393520196899772\n",
      "epoch: 4 step: 68, loss is 0.09443795680999756\n",
      "epoch: 4 step: 69, loss is 0.006746223196387291\n",
      "epoch: 4 step: 70, loss is 0.0018852795474231243\n",
      "epoch: 4 step: 71, loss is 0.01326199434697628\n",
      "epoch: 4 step: 72, loss is 0.003025858663022518\n",
      "epoch: 4 step: 73, loss is 0.013349750079214573\n",
      "epoch: 4 step: 74, loss is 0.1506594568490982\n",
      "epoch: 4 step: 75, loss is 0.026306265965104103\n",
      "epoch: 4 step: 76, loss is 0.0552699901163578\n",
      "epoch: 4 step: 77, loss is 0.1697736531496048\n",
      "epoch: 4 step: 78, loss is 0.018566882237792015\n",
      "epoch: 4 step: 79, loss is 0.0011334065347909927\n",
      "epoch: 4 step: 80, loss is 0.009159698151051998\n",
      "epoch: 4 step: 81, loss is 0.015651987865567207\n",
      "epoch: 4 step: 82, loss is 0.014685281552374363\n",
      "epoch: 4 step: 83, loss is 0.2254805564880371\n",
      "epoch: 4 step: 84, loss is 0.00292973848991096\n",
      "epoch: 4 step: 85, loss is 0.0009225312387570739\n",
      "epoch: 4 step: 86, loss is 0.0012766810832545161\n",
      "epoch: 4 step: 87, loss is 0.021493876352906227\n",
      "epoch: 4 step: 88, loss is 0.005147410556674004\n",
      "epoch: 4 step: 89, loss is 0.0027869862969964743\n",
      "epoch: 4 step: 90, loss is 0.03226964548230171\n",
      "epoch: 4 step: 91, loss is 0.055420100688934326\n",
      "epoch: 4 step: 92, loss is 0.05645652860403061\n",
      "epoch: 4 step: 93, loss is 0.014987854287028313\n",
      "epoch: 4 step: 94, loss is 0.0024985456839203835\n",
      "epoch: 4 step: 95, loss is 0.015801141038537025\n",
      "epoch: 4 step: 96, loss is 0.004861654248088598\n",
      "epoch: 4 step: 97, loss is 0.037822335958480835\n",
      "epoch: 4 step: 98, loss is 0.02010505273938179\n",
      "epoch: 4 step: 99, loss is 0.06861112266778946\n",
      "epoch: 4 step: 100, loss is 0.027130354195833206\n",
      "epoch: 4 step: 101, loss is 0.006754153873771429\n",
      "epoch: 4 step: 102, loss is 0.031011300161480904\n",
      "epoch: 4 step: 103, loss is 0.013424591161310673\n",
      "epoch: 4 step: 104, loss is 0.015862036496400833\n",
      "epoch: 4 step: 105, loss is 0.001921190181747079\n",
      "epoch: 4 step: 106, loss is 0.0031990008428692818\n",
      "epoch: 4 step: 107, loss is 0.0685320496559143\n",
      "epoch: 4 step: 108, loss is 0.13715040683746338\n",
      "epoch: 4 step: 109, loss is 0.0066138701513409615\n",
      "epoch: 4 step: 110, loss is 0.008523968979716301\n",
      "epoch: 4 step: 111, loss is 0.0008469515014439821\n",
      "epoch: 4 step: 112, loss is 0.01093389093875885\n",
      "epoch: 4 step: 113, loss is 0.0020455813501030207\n",
      "epoch: 4 step: 114, loss is 0.002487733494490385\n",
      "epoch: 4 step: 115, loss is 0.006771043408662081\n",
      "epoch: 4 step: 116, loss is 0.01704976335167885\n",
      "epoch: 4 step: 117, loss is 0.001274009351618588\n",
      "epoch: 4 step: 118, loss is 0.051643256098032\n",
      "epoch: 4 step: 119, loss is 0.0029284278862178326\n",
      "epoch: 4 step: 120, loss is 0.01833569072186947\n",
      "epoch: 4 step: 121, loss is 0.0015394081128761172\n",
      "epoch: 4 step: 122, loss is 0.025656629353761673\n",
      "epoch: 4 step: 123, loss is 0.13328570127487183\n",
      "epoch: 4 step: 124, loss is 0.3450380861759186\n",
      "epoch: 4 step: 125, loss is 0.0017815643223002553\n",
      "epoch: 4 step: 126, loss is 0.03521968424320221\n",
      "epoch: 4 step: 127, loss is 0.0007249064510688186\n",
      "epoch: 4 step: 128, loss is 0.0006434686947613955\n",
      "epoch: 4 step: 129, loss is 0.08157455176115036\n",
      "epoch: 4 step: 130, loss is 0.006561890710145235\n",
      "epoch: 4 step: 131, loss is 0.03959563747048378\n",
      "epoch: 4 step: 132, loss is 0.018113143742084503\n",
      "epoch: 4 step: 133, loss is 0.018877658993005753\n",
      "epoch: 4 step: 134, loss is 0.002150243381038308\n",
      "epoch: 4 step: 135, loss is 0.0008481720578856766\n",
      "epoch: 4 step: 136, loss is 0.21205434203147888\n",
      "epoch: 4 step: 137, loss is 8.768722182139754e-05\n",
      "epoch: 4 step: 138, loss is 0.04390673711895943\n",
      "epoch: 4 step: 139, loss is 0.0011746528325602412\n",
      "epoch: 4 step: 140, loss is 0.006209590472280979\n",
      "epoch: 4 step: 141, loss is 0.00419169757515192\n",
      "epoch: 4 step: 142, loss is 0.0004806092183571309\n",
      "epoch: 4 step: 143, loss is 0.008537843823432922\n",
      "epoch: 4 step: 144, loss is 0.0857209637761116\n",
      "epoch: 4 step: 145, loss is 0.016507588326931\n",
      "epoch: 4 step: 146, loss is 0.024995462968945503\n",
      "epoch: 4 step: 147, loss is 0.00010284828022122383\n",
      "epoch: 4 step: 148, loss is 0.014469821006059647\n",
      "epoch: 4 step: 149, loss is 0.0012658453779295087\n",
      "epoch: 4 step: 150, loss is 0.006702125072479248\n",
      "epoch: 4 step: 151, loss is 0.031950756907463074\n",
      "epoch: 4 step: 152, loss is 0.010567976161837578\n",
      "epoch: 4 step: 153, loss is 0.005921703763306141\n",
      "epoch: 4 step: 154, loss is 0.14182227849960327\n",
      "epoch: 4 step: 155, loss is 0.0013319666031748056\n",
      "epoch: 4 step: 156, loss is 0.023851366713643074\n",
      "epoch: 4 step: 157, loss is 0.11887628585100174\n",
      "epoch: 4 step: 158, loss is 0.04988881200551987\n",
      "epoch: 4 step: 159, loss is 0.0006461794255301356\n",
      "epoch: 4 step: 160, loss is 0.005777889396995306\n",
      "epoch: 4 step: 161, loss is 0.015034614130854607\n",
      "epoch: 4 step: 162, loss is 0.03195231035351753\n",
      "epoch: 4 step: 163, loss is 0.10964754968881607\n",
      "epoch: 4 step: 164, loss is 0.02456437051296234\n",
      "epoch: 4 step: 165, loss is 0.05297639220952988\n",
      "epoch: 4 step: 166, loss is 0.15255415439605713\n",
      "epoch: 4 step: 167, loss is 0.0029439060017466545\n",
      "epoch: 4 step: 168, loss is 0.0011353290174156427\n",
      "epoch: 4 step: 169, loss is 0.013855983503162861\n",
      "epoch: 4 step: 170, loss is 0.0066226995550096035\n",
      "epoch: 4 step: 171, loss is 0.0663386881351471\n",
      "epoch: 4 step: 172, loss is 0.05170806497335434\n",
      "epoch: 4 step: 173, loss is 0.02763877622783184\n",
      "epoch: 4 step: 174, loss is 0.029223358258605003\n",
      "epoch: 4 step: 175, loss is 0.00043528646347112954\n",
      "epoch: 4 step: 176, loss is 0.006475506816059351\n",
      "epoch: 4 step: 177, loss is 0.054203812032938004\n",
      "epoch: 4 step: 178, loss is 0.002356024459004402\n",
      "epoch: 4 step: 179, loss is 0.0008573692757636309\n",
      "epoch: 4 step: 180, loss is 0.0038866414688527584\n",
      "epoch: 4 step: 181, loss is 0.03760036081075668\n",
      "epoch: 4 step: 182, loss is 0.004297126550227404\n",
      "epoch: 4 step: 183, loss is 0.0017856378108263016\n",
      "epoch: 4 step: 184, loss is 0.03402766212821007\n",
      "epoch: 4 step: 185, loss is 0.015687264502048492\n",
      "epoch: 4 step: 186, loss is 0.008008807897567749\n",
      "epoch: 4 step: 187, loss is 0.05853138864040375\n",
      "epoch: 4 step: 188, loss is 0.0014341662172228098\n",
      "epoch: 4 step: 189, loss is 0.07075946778059006\n",
      "epoch: 4 step: 190, loss is 0.00010515411850064993\n",
      "epoch: 4 step: 191, loss is 0.0010451065609231591\n",
      "epoch: 4 step: 192, loss is 0.009145883843302727\n",
      "epoch: 4 step: 193, loss is 0.012343230657279491\n",
      "epoch: 4 step: 194, loss is 0.1917739063501358\n",
      "epoch: 4 step: 195, loss is 0.05830221250653267\n",
      "epoch: 4 step: 196, loss is 0.007009059656411409\n",
      "epoch: 4 step: 197, loss is 0.017906254157423973\n",
      "epoch: 4 step: 198, loss is 0.0020282650366425514\n",
      "epoch: 4 step: 199, loss is 0.3347444236278534\n",
      "epoch: 4 step: 200, loss is 0.12208721786737442\n",
      "epoch: 4 step: 201, loss is 0.016063107177615166\n",
      "epoch: 4 step: 202, loss is 0.01926323026418686\n",
      "epoch: 4 step: 203, loss is 0.014458979479968548\n",
      "epoch: 4 step: 204, loss is 0.042294152081012726\n",
      "epoch: 4 step: 205, loss is 0.00014390319120138884\n",
      "epoch: 4 step: 206, loss is 0.019482091069221497\n",
      "epoch: 4 step: 207, loss is 0.03210868686437607\n",
      "epoch: 4 step: 208, loss is 0.0036351243034005165\n",
      "epoch: 4 step: 209, loss is 0.0016249144682660699\n",
      "epoch: 4 step: 210, loss is 0.04536127671599388\n",
      "epoch: 4 step: 211, loss is 0.0021244122181087732\n",
      "epoch: 4 step: 212, loss is 0.004373905248939991\n",
      "epoch: 4 step: 213, loss is 0.002956981072202325\n",
      "epoch: 4 step: 214, loss is 0.002972319955006242\n",
      "epoch: 4 step: 215, loss is 0.0017314652213826776\n",
      "epoch: 4 step: 216, loss is 0.0062217614613473415\n",
      "epoch: 4 step: 217, loss is 0.0004811710095964372\n",
      "epoch: 4 step: 218, loss is 0.0222634244710207\n",
      "epoch: 4 step: 219, loss is 0.0510520376265049\n",
      "epoch: 4 step: 220, loss is 0.0004691232752520591\n",
      "epoch: 4 step: 221, loss is 0.19803832471370697\n",
      "epoch: 4 step: 222, loss is 0.020361488685011864\n",
      "epoch: 4 step: 223, loss is 0.0007836166769266129\n",
      "epoch: 4 step: 224, loss is 0.024226516485214233\n",
      "epoch: 4 step: 225, loss is 0.049976181238889694\n",
      "epoch: 4 step: 226, loss is 0.053958963602781296\n",
      "epoch: 4 step: 227, loss is 0.004576695617288351\n",
      "epoch: 4 step: 228, loss is 0.005125822965055704\n",
      "epoch: 4 step: 229, loss is 0.007579824421554804\n",
      "epoch: 4 step: 230, loss is 0.028888940811157227\n",
      "epoch: 4 step: 231, loss is 0.058517906814813614\n",
      "epoch: 4 step: 232, loss is 0.013884039595723152\n",
      "epoch: 4 step: 233, loss is 0.08056794106960297\n",
      "epoch: 4 step: 234, loss is 0.008911163546144962\n",
      "epoch: 4 step: 235, loss is 0.05414455011487007\n",
      "epoch: 4 step: 236, loss is 0.0007766299531795084\n",
      "epoch: 4 step: 237, loss is 0.0005066405283287168\n",
      "epoch: 4 step: 238, loss is 0.0037947124801576138\n",
      "epoch: 4 step: 239, loss is 0.10177522897720337\n",
      "epoch: 4 step: 240, loss is 0.007752071134746075\n",
      "epoch: 4 step: 241, loss is 0.029024656862020493\n",
      "epoch: 4 step: 242, loss is 0.12919072806835175\n",
      "epoch: 4 step: 243, loss is 0.016017448157072067\n",
      "epoch: 4 step: 244, loss is 0.004549544304609299\n",
      "epoch: 4 step: 245, loss is 0.001965251751244068\n",
      "epoch: 4 step: 246, loss is 0.0710216537117958\n",
      "epoch: 4 step: 247, loss is 0.004711160436272621\n",
      "epoch: 4 step: 248, loss is 0.0012648850679397583\n",
      "epoch: 4 step: 249, loss is 0.02124396152794361\n",
      "epoch: 4 step: 250, loss is 0.0016647973097860813\n",
      "epoch: 4 step: 251, loss is 0.0024560238234698772\n",
      "epoch: 4 step: 252, loss is 0.003734809346497059\n",
      "epoch: 4 step: 253, loss is 0.08111249655485153\n",
      "epoch: 4 step: 254, loss is 0.02503945119678974\n",
      "epoch: 4 step: 255, loss is 0.002360015641897917\n",
      "epoch: 4 step: 256, loss is 0.04567139223217964\n",
      "epoch: 4 step: 257, loss is 0.05667703598737717\n",
      "epoch: 4 step: 258, loss is 0.04098042845726013\n",
      "epoch: 4 step: 259, loss is 0.000326622772263363\n",
      "epoch: 4 step: 260, loss is 0.0053757233545184135\n",
      "epoch: 4 step: 261, loss is 0.010206416249275208\n",
      "epoch: 4 step: 262, loss is 0.02130906656384468\n",
      "epoch: 4 step: 263, loss is 0.0012364988215267658\n",
      "epoch: 4 step: 264, loss is 0.007236851844936609\n",
      "epoch: 4 step: 265, loss is 0.008812371641397476\n",
      "epoch: 4 step: 266, loss is 0.320765882730484\n",
      "epoch: 4 step: 267, loss is 0.003969257697463036\n",
      "epoch: 4 step: 268, loss is 0.02132030948996544\n",
      "epoch: 4 step: 269, loss is 0.05618387833237648\n",
      "epoch: 4 step: 270, loss is 0.014263982884585857\n",
      "epoch: 4 step: 271, loss is 0.07182168960571289\n",
      "epoch: 4 step: 272, loss is 0.06892244517803192\n",
      "epoch: 4 step: 273, loss is 0.06489266455173492\n",
      "epoch: 4 step: 274, loss is 0.007002641912549734\n",
      "epoch: 4 step: 275, loss is 0.025988217443227768\n",
      "epoch: 4 step: 276, loss is 0.01137905940413475\n",
      "epoch: 4 step: 277, loss is 0.0026109032332897186\n",
      "epoch: 4 step: 278, loss is 0.04803042858839035\n",
      "epoch: 4 step: 279, loss is 0.007402602583169937\n",
      "epoch: 4 step: 280, loss is 0.005365164950489998\n",
      "epoch: 4 step: 281, loss is 0.012950260192155838\n",
      "epoch: 4 step: 282, loss is 0.017077116295695305\n",
      "epoch: 4 step: 283, loss is 0.0016844565980136395\n",
      "epoch: 4 step: 284, loss is 0.07836153358221054\n",
      "epoch: 4 step: 285, loss is 0.00042964733438566327\n",
      "epoch: 4 step: 286, loss is 0.04897436127066612\n",
      "epoch: 4 step: 287, loss is 0.06065450608730316\n",
      "epoch: 4 step: 288, loss is 0.016617875546216965\n",
      "epoch: 4 step: 289, loss is 0.0020442260429263115\n",
      "epoch: 4 step: 290, loss is 0.01226199883967638\n",
      "epoch: 4 step: 291, loss is 0.008209627121686935\n",
      "epoch: 4 step: 292, loss is 0.009262022562325\n",
      "epoch: 4 step: 293, loss is 0.09000276029109955\n",
      "epoch: 4 step: 294, loss is 0.0014922830741852522\n",
      "epoch: 4 step: 295, loss is 0.1035151332616806\n",
      "epoch: 4 step: 296, loss is 0.16359709203243256\n",
      "epoch: 4 step: 297, loss is 0.005021112505346537\n",
      "epoch: 4 step: 298, loss is 0.07246007025241852\n",
      "epoch: 4 step: 299, loss is 0.047052569687366486\n",
      "epoch: 4 step: 300, loss is 0.013107066974043846\n",
      "epoch: 4 step: 301, loss is 0.0005021955003030598\n",
      "epoch: 4 step: 302, loss is 0.017992090433835983\n",
      "epoch: 4 step: 303, loss is 0.0030275522731244564\n",
      "epoch: 4 step: 304, loss is 0.0016032728599384427\n",
      "epoch: 4 step: 305, loss is 0.0021130628883838654\n",
      "epoch: 4 step: 306, loss is 0.003335703397169709\n",
      "epoch: 4 step: 307, loss is 0.010764104314148426\n",
      "epoch: 4 step: 308, loss is 0.02048763819038868\n",
      "epoch: 4 step: 309, loss is 0.008804763667285442\n",
      "epoch: 4 step: 310, loss is 0.0004602252156473696\n",
      "epoch: 4 step: 311, loss is 0.005754908546805382\n",
      "epoch: 4 step: 312, loss is 0.020070543512701988\n",
      "epoch: 4 step: 313, loss is 0.001012230757623911\n",
      "epoch: 4 step: 314, loss is 0.003059433540329337\n",
      "epoch: 4 step: 315, loss is 0.006463646423071623\n",
      "epoch: 4 step: 316, loss is 0.006604422349482775\n",
      "epoch: 4 step: 317, loss is 0.0003494469274301082\n",
      "epoch: 4 step: 318, loss is 0.038295138627290726\n",
      "epoch: 4 step: 319, loss is 0.06796958297491074\n",
      "epoch: 4 step: 320, loss is 0.006653031799942255\n",
      "epoch: 4 step: 321, loss is 0.002759364200755954\n",
      "epoch: 4 step: 322, loss is 0.028848472982645035\n",
      "epoch: 4 step: 323, loss is 0.001476738601922989\n",
      "epoch: 4 step: 324, loss is 0.008893082849681377\n",
      "epoch: 4 step: 325, loss is 0.016046050935983658\n",
      "epoch: 4 step: 326, loss is 0.0013190116733312607\n",
      "epoch: 4 step: 327, loss is 0.007261867634952068\n",
      "epoch: 4 step: 328, loss is 0.0022886863444000483\n",
      "epoch: 4 step: 329, loss is 0.0005885704886168242\n",
      "epoch: 4 step: 330, loss is 0.00425788713619113\n",
      "epoch: 4 step: 331, loss is 0.0015411421190947294\n",
      "epoch: 4 step: 332, loss is 0.0007436791202053428\n",
      "epoch: 4 step: 333, loss is 0.003724723355844617\n",
      "epoch: 4 step: 334, loss is 0.006199117284268141\n",
      "epoch: 4 step: 335, loss is 0.0035987263545393944\n",
      "epoch: 4 step: 336, loss is 0.002851948607712984\n",
      "epoch: 4 step: 337, loss is 0.0026666191406548023\n",
      "epoch: 4 step: 338, loss is 0.14547039568424225\n",
      "epoch: 4 step: 339, loss is 0.002084162551909685\n",
      "epoch: 4 step: 340, loss is 0.0008178065763786435\n",
      "epoch: 4 step: 341, loss is 0.003680446185171604\n",
      "epoch: 4 step: 342, loss is 0.004940358456224203\n",
      "epoch: 4 step: 343, loss is 0.0020673908293247223\n",
      "epoch: 4 step: 344, loss is 0.002429128624498844\n",
      "epoch: 4 step: 345, loss is 0.2225029468536377\n",
      "epoch: 4 step: 346, loss is 0.0025114284362643957\n",
      "epoch: 4 step: 347, loss is 0.0012405142188072205\n",
      "epoch: 4 step: 348, loss is 0.0018242555670440197\n",
      "epoch: 4 step: 349, loss is 0.009512330405414104\n",
      "epoch: 4 step: 350, loss is 0.0012503188336268067\n",
      "epoch: 4 step: 351, loss is 0.055828336626291275\n",
      "epoch: 4 step: 352, loss is 0.05663684383034706\n",
      "epoch: 4 step: 353, loss is 0.01678488217294216\n",
      "epoch: 4 step: 354, loss is 0.0032859183847904205\n",
      "epoch: 4 step: 355, loss is 0.0004819633613806218\n",
      "epoch: 4 step: 356, loss is 0.000538645195774734\n",
      "epoch: 4 step: 357, loss is 0.02889999747276306\n",
      "epoch: 4 step: 358, loss is 0.00551953911781311\n",
      "epoch: 4 step: 359, loss is 0.0005229806993156672\n",
      "epoch: 4 step: 360, loss is 0.01102263480424881\n",
      "epoch: 4 step: 361, loss is 0.00018958409782499075\n",
      "epoch: 4 step: 362, loss is 0.1265844702720642\n",
      "epoch: 4 step: 363, loss is 0.0007651717169210315\n",
      "epoch: 4 step: 364, loss is 0.08675117790699005\n",
      "epoch: 4 step: 365, loss is 0.0022734918165951967\n",
      "epoch: 4 step: 366, loss is 0.10868224501609802\n",
      "epoch: 4 step: 367, loss is 0.0018539034062996507\n",
      "epoch: 4 step: 368, loss is 0.0006670267321169376\n",
      "epoch: 4 step: 369, loss is 0.00822517555207014\n",
      "epoch: 4 step: 370, loss is 0.06860847026109695\n",
      "epoch: 4 step: 371, loss is 0.003101274836808443\n",
      "epoch: 4 step: 372, loss is 0.008244172669947147\n",
      "epoch: 4 step: 373, loss is 0.014639935456216335\n",
      "epoch: 4 step: 374, loss is 0.0010592445032671094\n",
      "epoch: 4 step: 375, loss is 0.09053958207368851\n",
      "epoch: 4 step: 376, loss is 0.0016637437511235476\n",
      "epoch: 4 step: 377, loss is 0.005691831931471825\n",
      "epoch: 4 step: 378, loss is 0.3297175168991089\n",
      "epoch: 4 step: 379, loss is 0.00740926805883646\n",
      "epoch: 4 step: 380, loss is 0.0038314478006213903\n",
      "epoch: 4 step: 381, loss is 0.0017712826374918222\n",
      "epoch: 4 step: 382, loss is 0.002985396422445774\n",
      "epoch: 4 step: 383, loss is 0.028532136231660843\n",
      "epoch: 4 step: 384, loss is 0.0004091849841643125\n",
      "epoch: 4 step: 385, loss is 0.053068857640028\n",
      "epoch: 4 step: 386, loss is 0.020541740581393242\n",
      "epoch: 4 step: 387, loss is 0.005290496163070202\n",
      "epoch: 4 step: 388, loss is 0.02750147134065628\n",
      "epoch: 4 step: 389, loss is 0.020386023446917534\n",
      "epoch: 4 step: 390, loss is 0.07271915674209595\n",
      "epoch: 4 step: 391, loss is 0.07988384366035461\n",
      "epoch: 4 step: 392, loss is 0.015028499998152256\n",
      "epoch: 4 step: 393, loss is 0.008859870955348015\n",
      "epoch: 4 step: 394, loss is 0.054127030074596405\n",
      "epoch: 4 step: 395, loss is 0.00279748416505754\n",
      "epoch: 4 step: 396, loss is 0.016424288973212242\n",
      "epoch: 4 step: 397, loss is 0.09355417639017105\n",
      "epoch: 4 step: 398, loss is 0.15147006511688232\n",
      "epoch: 4 step: 399, loss is 0.01878684014081955\n",
      "epoch: 4 step: 400, loss is 0.009445964358747005\n",
      "epoch: 4 step: 401, loss is 0.001507091918028891\n",
      "epoch: 4 step: 402, loss is 0.004064721986651421\n",
      "epoch: 4 step: 403, loss is 0.010947356931865215\n",
      "epoch: 4 step: 404, loss is 0.004970413167029619\n",
      "epoch: 4 step: 405, loss is 0.0017764955991879106\n",
      "epoch: 4 step: 406, loss is 0.0013236519880592823\n",
      "epoch: 4 step: 407, loss is 0.04614650085568428\n",
      "epoch: 4 step: 408, loss is 0.002098785014823079\n",
      "epoch: 4 step: 409, loss is 0.0029378922190517187\n",
      "epoch: 4 step: 410, loss is 0.00473281042650342\n",
      "epoch: 4 step: 411, loss is 0.0009617528994567692\n",
      "epoch: 4 step: 412, loss is 0.00010125934932148084\n",
      "epoch: 4 step: 413, loss is 0.018721040338277817\n",
      "epoch: 4 step: 414, loss is 0.001159487059339881\n",
      "epoch: 4 step: 415, loss is 0.11756760627031326\n",
      "epoch: 4 step: 416, loss is 0.2733536958694458\n",
      "epoch: 4 step: 417, loss is 0.01183148194104433\n",
      "epoch: 4 step: 418, loss is 0.016830159351229668\n",
      "epoch: 4 step: 419, loss is 0.000502767157740891\n",
      "epoch: 4 step: 420, loss is 0.00028734863735735416\n",
      "epoch: 4 step: 421, loss is 0.03908330202102661\n",
      "epoch: 4 step: 422, loss is 0.00383745227009058\n",
      "epoch: 4 step: 423, loss is 0.002064474392682314\n",
      "epoch: 4 step: 424, loss is 0.024036094546318054\n",
      "epoch: 4 step: 425, loss is 0.023602455854415894\n",
      "epoch: 4 step: 426, loss is 0.4610634744167328\n",
      "epoch: 4 step: 427, loss is 0.007179198786616325\n",
      "epoch: 4 step: 428, loss is 0.004711557645350695\n",
      "epoch: 4 step: 429, loss is 0.03761197626590729\n",
      "epoch: 4 step: 430, loss is 0.003923614043742418\n",
      "epoch: 4 step: 431, loss is 0.032494720071554184\n",
      "epoch: 4 step: 432, loss is 0.027822230011224747\n",
      "epoch: 4 step: 433, loss is 0.4261809289455414\n",
      "epoch: 4 step: 434, loss is 0.07627398520708084\n",
      "epoch: 4 step: 435, loss is 0.04035895690321922\n",
      "epoch: 4 step: 436, loss is 0.10921338200569153\n",
      "epoch: 4 step: 437, loss is 0.0003617746406234801\n",
      "epoch: 4 step: 438, loss is 0.02125762775540352\n",
      "epoch: 4 step: 439, loss is 0.001399140921421349\n",
      "epoch: 4 step: 440, loss is 0.015150738880038261\n",
      "epoch: 4 step: 441, loss is 0.010041218250989914\n",
      "epoch: 4 step: 442, loss is 0.00577672990038991\n",
      "epoch: 4 step: 443, loss is 0.04819273203611374\n",
      "epoch: 4 step: 444, loss is 0.0025563500821590424\n",
      "epoch: 4 step: 445, loss is 0.00850552599877119\n",
      "epoch: 4 step: 446, loss is 0.0009912729728966951\n",
      "epoch: 4 step: 447, loss is 0.012548551894724369\n",
      "epoch: 4 step: 448, loss is 0.005074828397482634\n",
      "epoch: 4 step: 449, loss is 0.11737371981143951\n",
      "epoch: 4 step: 450, loss is 0.0430293083190918\n",
      "epoch: 4 step: 451, loss is 0.04395122453570366\n",
      "epoch: 4 step: 452, loss is 0.009327860549092293\n",
      "epoch: 4 step: 453, loss is 0.0010207837913185358\n",
      "epoch: 4 step: 454, loss is 0.003708701580762863\n",
      "epoch: 4 step: 455, loss is 0.06735873222351074\n",
      "epoch: 4 step: 456, loss is 0.10807783901691437\n",
      "epoch: 4 step: 457, loss is 0.0009694538312032819\n",
      "epoch: 4 step: 458, loss is 0.09300541132688522\n",
      "epoch: 4 step: 459, loss is 0.022379469126462936\n",
      "epoch: 4 step: 460, loss is 0.018406616523861885\n",
      "epoch: 4 step: 461, loss is 0.0068817464634776115\n",
      "epoch: 4 step: 462, loss is 0.018644260242581367\n",
      "epoch: 4 step: 463, loss is 0.01741461642086506\n",
      "epoch: 4 step: 464, loss is 0.00311886053532362\n",
      "epoch: 4 step: 465, loss is 0.06435514986515045\n",
      "epoch: 4 step: 466, loss is 0.0028724411968141794\n",
      "epoch: 4 step: 467, loss is 0.026917714625597\n",
      "epoch: 4 step: 468, loss is 0.001571047818288207\n",
      "epoch: 4 step: 469, loss is 0.014478246681392193\n",
      "epoch: 4 step: 470, loss is 0.051803749054670334\n",
      "epoch: 4 step: 471, loss is 0.0014883483527228236\n",
      "epoch: 4 step: 472, loss is 0.00456103915348649\n",
      "epoch: 4 step: 473, loss is 0.024856647476553917\n",
      "epoch: 4 step: 474, loss is 0.09374731034040451\n",
      "epoch: 4 step: 475, loss is 0.008095668628811836\n",
      "epoch: 4 step: 476, loss is 0.0018658025655895472\n",
      "epoch: 4 step: 477, loss is 0.0359695628285408\n",
      "epoch: 4 step: 478, loss is 0.0015273287426680326\n",
      "epoch: 4 step: 479, loss is 0.015662357211112976\n",
      "epoch: 4 step: 480, loss is 0.0035070935264229774\n",
      "epoch: 4 step: 481, loss is 0.1006801426410675\n",
      "epoch: 4 step: 482, loss is 0.0017614229582250118\n",
      "epoch: 4 step: 483, loss is 0.01858997717499733\n",
      "epoch: 4 step: 484, loss is 0.0015706681879237294\n",
      "epoch: 4 step: 485, loss is 0.004146907012909651\n",
      "epoch: 4 step: 486, loss is 0.0071160090155899525\n",
      "epoch: 4 step: 487, loss is 0.002129965927451849\n",
      "epoch: 4 step: 488, loss is 0.004464307334274054\n",
      "epoch: 4 step: 489, loss is 0.06309367716312408\n",
      "epoch: 4 step: 490, loss is 0.0004140637465752661\n",
      "epoch: 4 step: 491, loss is 0.0007256856188178062\n",
      "epoch: 4 step: 492, loss is 0.0027680424973368645\n",
      "epoch: 4 step: 493, loss is 0.005359293892979622\n",
      "epoch: 4 step: 494, loss is 0.0012354772770777345\n",
      "epoch: 4 step: 495, loss is 0.11374339461326599\n",
      "epoch: 4 step: 496, loss is 0.014846475794911385\n",
      "epoch: 4 step: 497, loss is 0.008048874326050282\n",
      "epoch: 4 step: 498, loss is 0.08753328770399094\n",
      "epoch: 4 step: 499, loss is 0.015413259156048298\n",
      "epoch: 4 step: 500, loss is 0.0011345649836584926\n",
      "epoch: 4 step: 501, loss is 0.012458298355340958\n",
      "epoch: 4 step: 502, loss is 0.005019029136747122\n",
      "epoch: 4 step: 503, loss is 0.0025930125266313553\n",
      "epoch: 4 step: 504, loss is 0.08980252593755722\n",
      "epoch: 4 step: 505, loss is 0.09943834692239761\n",
      "epoch: 4 step: 506, loss is 0.006729398388415575\n",
      "epoch: 4 step: 507, loss is 0.012423105537891388\n",
      "epoch: 4 step: 508, loss is 0.006489905063062906\n",
      "epoch: 4 step: 509, loss is 0.002955980831757188\n",
      "epoch: 4 step: 510, loss is 0.08352877199649811\n",
      "epoch: 4 step: 511, loss is 0.0709368959069252\n",
      "epoch: 4 step: 512, loss is 0.047386735677719116\n",
      "epoch: 4 step: 513, loss is 0.0013098003109917045\n",
      "epoch: 4 step: 514, loss is 0.1882701963186264\n",
      "epoch: 4 step: 515, loss is 0.016455095261335373\n",
      "epoch: 4 step: 516, loss is 0.01338050328195095\n",
      "epoch: 4 step: 517, loss is 0.0020134132355451584\n",
      "epoch: 4 step: 518, loss is 0.004533141851425171\n",
      "epoch: 4 step: 519, loss is 0.42441049218177795\n",
      "epoch: 4 step: 520, loss is 0.006119760684669018\n",
      "epoch: 4 step: 521, loss is 0.011167487129569054\n",
      "epoch: 4 step: 522, loss is 0.09733565151691437\n",
      "epoch: 4 step: 523, loss is 0.005728682968765497\n",
      "epoch: 4 step: 524, loss is 0.00998594705015421\n",
      "epoch: 4 step: 525, loss is 0.09005032479763031\n",
      "epoch: 4 step: 526, loss is 0.006841330323368311\n",
      "epoch: 4 step: 527, loss is 0.0034248800948262215\n",
      "epoch: 4 step: 528, loss is 0.002673740964382887\n",
      "epoch: 4 step: 529, loss is 0.049171581864356995\n",
      "epoch: 4 step: 530, loss is 0.00746120372787118\n",
      "epoch: 4 step: 531, loss is 0.0028327859472483397\n",
      "epoch: 4 step: 532, loss is 0.0036602786276489496\n",
      "epoch: 4 step: 533, loss is 0.13347838819026947\n",
      "epoch: 4 step: 534, loss is 0.14141884446144104\n",
      "epoch: 4 step: 535, loss is 0.04559165611863136\n",
      "epoch: 4 step: 536, loss is 0.0390251949429512\n",
      "epoch: 4 step: 537, loss is 0.013716291636228561\n",
      "epoch: 4 step: 538, loss is 0.06013869121670723\n",
      "epoch: 4 step: 539, loss is 0.049117691814899445\n",
      "epoch: 4 step: 540, loss is 0.05392943695187569\n",
      "epoch: 4 step: 541, loss is 0.009012024849653244\n",
      "epoch: 4 step: 542, loss is 0.003798008430749178\n",
      "epoch: 4 step: 543, loss is 0.07730112969875336\n",
      "epoch: 4 step: 544, loss is 0.006536292377859354\n",
      "epoch: 4 step: 545, loss is 0.005956936627626419\n",
      "epoch: 4 step: 546, loss is 0.0005384059622883797\n",
      "epoch: 4 step: 547, loss is 0.009384519420564175\n",
      "epoch: 4 step: 548, loss is 0.12940731644630432\n",
      "epoch: 4 step: 549, loss is 0.000697907293215394\n",
      "epoch: 4 step: 550, loss is 0.13028870522975922\n",
      "epoch: 4 step: 551, loss is 0.023298216983675957\n",
      "epoch: 4 step: 552, loss is 0.12887556850910187\n",
      "epoch: 4 step: 553, loss is 0.004489611834287643\n",
      "epoch: 4 step: 554, loss is 0.0033837344963103533\n",
      "epoch: 4 step: 555, loss is 0.00531437573954463\n",
      "epoch: 4 step: 556, loss is 0.004494821652770042\n",
      "epoch: 4 step: 557, loss is 0.008095514960587025\n",
      "epoch: 4 step: 558, loss is 0.049161721020936966\n",
      "epoch: 4 step: 559, loss is 0.03654125705361366\n",
      "epoch: 4 step: 560, loss is 0.08186402171850204\n",
      "epoch: 4 step: 561, loss is 0.23949360847473145\n",
      "epoch: 4 step: 562, loss is 0.005363827105611563\n",
      "epoch: 4 step: 563, loss is 0.006235250271856785\n",
      "epoch: 4 step: 564, loss is 0.03711697459220886\n",
      "epoch: 4 step: 565, loss is 0.000970761408098042\n",
      "epoch: 4 step: 566, loss is 0.024338003247976303\n",
      "epoch: 4 step: 567, loss is 0.002983029466122389\n",
      "epoch: 4 step: 568, loss is 0.04714259132742882\n",
      "epoch: 4 step: 569, loss is 0.024259258061647415\n",
      "epoch: 4 step: 570, loss is 0.007346159312874079\n",
      "epoch: 4 step: 571, loss is 0.011176135390996933\n",
      "epoch: 4 step: 572, loss is 0.012800626456737518\n",
      "epoch: 4 step: 573, loss is 0.0011551986681297421\n",
      "epoch: 4 step: 574, loss is 0.004656131379306316\n",
      "epoch: 4 step: 575, loss is 0.0009356241207569838\n",
      "epoch: 4 step: 576, loss is 0.0019875862635672092\n",
      "epoch: 4 step: 577, loss is 0.11324693262577057\n",
      "epoch: 4 step: 578, loss is 0.057619500905275345\n",
      "epoch: 4 step: 579, loss is 0.013294392265379429\n",
      "epoch: 4 step: 580, loss is 0.006392144598066807\n",
      "epoch: 4 step: 581, loss is 0.21434321999549866\n",
      "epoch: 4 step: 582, loss is 0.054680079221725464\n",
      "epoch: 4 step: 583, loss is 0.020063666626811028\n",
      "epoch: 4 step: 584, loss is 0.03326721489429474\n",
      "epoch: 4 step: 585, loss is 0.0016371230594813824\n",
      "epoch: 4 step: 586, loss is 0.0027768253348767757\n",
      "epoch: 4 step: 587, loss is 0.02383977547287941\n",
      "epoch: 4 step: 588, loss is 0.0846192017197609\n",
      "epoch: 4 step: 589, loss is 0.00034930434776470065\n",
      "epoch: 4 step: 590, loss is 0.002548531163483858\n",
      "epoch: 4 step: 591, loss is 0.0032835714519023895\n",
      "epoch: 4 step: 592, loss is 0.004886655602604151\n",
      "epoch: 4 step: 593, loss is 0.34551531076431274\n",
      "epoch: 4 step: 594, loss is 0.009658771567046642\n",
      "epoch: 4 step: 595, loss is 0.025690892711281776\n",
      "epoch: 4 step: 596, loss is 0.0005458551459014416\n",
      "epoch: 4 step: 597, loss is 0.01915663294494152\n",
      "epoch: 4 step: 598, loss is 0.07011745870113373\n",
      "epoch: 4 step: 599, loss is 0.024506203830242157\n",
      "epoch: 4 step: 600, loss is 0.0012041154550388455\n",
      "epoch: 4 step: 601, loss is 0.2060157209634781\n",
      "epoch: 4 step: 602, loss is 0.012849089689552784\n",
      "epoch: 4 step: 603, loss is 0.014217711053788662\n",
      "epoch: 4 step: 604, loss is 0.0020916378125548363\n",
      "epoch: 4 step: 605, loss is 0.0788208395242691\n",
      "epoch: 4 step: 606, loss is 0.026287689805030823\n",
      "epoch: 4 step: 607, loss is 0.005055820569396019\n",
      "epoch: 4 step: 608, loss is 0.03752993047237396\n",
      "epoch: 4 step: 609, loss is 0.027540791779756546\n",
      "epoch: 4 step: 610, loss is 0.03911513462662697\n",
      "epoch: 4 step: 611, loss is 0.06838853657245636\n",
      "epoch: 4 step: 612, loss is 0.058301664888858795\n",
      "epoch: 4 step: 613, loss is 0.002803423907607794\n",
      "epoch: 4 step: 614, loss is 0.01339531596750021\n",
      "epoch: 4 step: 615, loss is 0.012151980772614479\n",
      "epoch: 4 step: 616, loss is 0.01963891088962555\n",
      "epoch: 4 step: 617, loss is 0.000581665663048625\n",
      "epoch: 4 step: 618, loss is 0.002707375679165125\n",
      "epoch: 4 step: 619, loss is 0.014131320640444756\n",
      "epoch: 4 step: 620, loss is 0.04337410628795624\n",
      "epoch: 4 step: 621, loss is 0.062461599707603455\n",
      "epoch: 4 step: 622, loss is 0.0026532444171607494\n",
      "epoch: 4 step: 623, loss is 0.06567614525556564\n",
      "epoch: 4 step: 624, loss is 0.03683067485690117\n",
      "epoch: 4 step: 625, loss is 0.15732966363430023\n",
      "epoch: 4 step: 626, loss is 0.0013304559979587793\n",
      "epoch: 4 step: 627, loss is 0.03775673732161522\n",
      "epoch: 4 step: 628, loss is 0.0021294711623340845\n",
      "epoch: 4 step: 629, loss is 0.00022397238353732973\n",
      "epoch: 4 step: 630, loss is 0.007234134711325169\n",
      "epoch: 4 step: 631, loss is 0.05511057376861572\n",
      "epoch: 4 step: 632, loss is 0.0020305465441197157\n",
      "epoch: 4 step: 633, loss is 0.0031895721331238747\n",
      "epoch: 4 step: 634, loss is 0.02190990000963211\n",
      "epoch: 4 step: 635, loss is 0.015846336260437965\n",
      "epoch: 4 step: 636, loss is 0.0008508385508321226\n",
      "epoch: 4 step: 637, loss is 0.0024985126219689846\n",
      "epoch: 4 step: 638, loss is 0.018492184579372406\n",
      "epoch: 4 step: 639, loss is 0.001132465898990631\n",
      "epoch: 4 step: 640, loss is 0.029371606186032295\n",
      "epoch: 4 step: 641, loss is 0.002489755628630519\n",
      "epoch: 4 step: 642, loss is 0.002974550938233733\n",
      "epoch: 4 step: 643, loss is 0.03525877743959427\n",
      "epoch: 4 step: 644, loss is 0.014719657599925995\n",
      "epoch: 4 step: 645, loss is 0.04806148260831833\n",
      "epoch: 4 step: 646, loss is 0.003859566058963537\n",
      "epoch: 4 step: 647, loss is 0.00811845250427723\n",
      "epoch: 4 step: 648, loss is 0.001237145159393549\n",
      "epoch: 4 step: 649, loss is 0.1279943436384201\n",
      "epoch: 4 step: 650, loss is 0.00017198236309923232\n",
      "epoch: 4 step: 651, loss is 0.06131825968623161\n",
      "epoch: 4 step: 652, loss is 0.020211491733789444\n",
      "epoch: 4 step: 653, loss is 0.014349736273288727\n",
      "epoch: 4 step: 654, loss is 0.057259224355220795\n",
      "epoch: 4 step: 655, loss is 0.004608431831002235\n",
      "epoch: 4 step: 656, loss is 0.02178461290895939\n",
      "epoch: 4 step: 657, loss is 0.001602939679287374\n",
      "epoch: 4 step: 658, loss is 0.14383186399936676\n",
      "epoch: 4 step: 659, loss is 0.0028677494265139103\n",
      "epoch: 4 step: 660, loss is 0.0024787092115730047\n",
      "epoch: 4 step: 661, loss is 0.0012245244579389691\n",
      "epoch: 4 step: 662, loss is 0.003582907374948263\n",
      "epoch: 4 step: 663, loss is 0.17733334004878998\n",
      "epoch: 4 step: 664, loss is 0.002671563997864723\n",
      "epoch: 4 step: 665, loss is 0.052582550793886185\n",
      "epoch: 4 step: 666, loss is 0.007218954153358936\n",
      "epoch: 4 step: 667, loss is 0.003614872694015503\n",
      "epoch: 4 step: 668, loss is 0.015028576366603374\n",
      "epoch: 4 step: 669, loss is 0.00902186706662178\n",
      "epoch: 4 step: 670, loss is 0.0012857416877523065\n",
      "epoch: 4 step: 671, loss is 0.03813719004392624\n",
      "epoch: 4 step: 672, loss is 0.009312249720096588\n",
      "epoch: 4 step: 673, loss is 0.12504719197750092\n",
      "epoch: 4 step: 674, loss is 0.11400875449180603\n",
      "epoch: 4 step: 675, loss is 0.0029894032049924135\n",
      "epoch: 4 step: 676, loss is 0.1238381639122963\n",
      "epoch: 4 step: 677, loss is 0.07589969784021378\n",
      "epoch: 4 step: 678, loss is 0.08154667913913727\n",
      "epoch: 4 step: 679, loss is 0.009951162151992321\n",
      "epoch: 4 step: 680, loss is 0.007739235647022724\n",
      "epoch: 4 step: 681, loss is 0.009429357014596462\n",
      "epoch: 4 step: 682, loss is 0.009847377426922321\n",
      "epoch: 4 step: 683, loss is 0.22828999161720276\n",
      "epoch: 4 step: 684, loss is 0.056459471583366394\n",
      "epoch: 4 step: 685, loss is 0.006675086449831724\n",
      "epoch: 4 step: 686, loss is 0.0032586781308054924\n",
      "epoch: 4 step: 687, loss is 0.03586232289671898\n",
      "epoch: 4 step: 688, loss is 0.015036906115710735\n",
      "epoch: 4 step: 689, loss is 0.07320896536111832\n",
      "epoch: 4 step: 690, loss is 0.004129114560782909\n",
      "epoch: 4 step: 691, loss is 0.001427220762707293\n",
      "epoch: 4 step: 692, loss is 0.011700635775923729\n",
      "epoch: 4 step: 693, loss is 0.0015468393685296178\n",
      "epoch: 4 step: 694, loss is 0.013153214007616043\n",
      "epoch: 4 step: 695, loss is 0.001488740206696093\n",
      "epoch: 4 step: 696, loss is 0.010449452325701714\n",
      "epoch: 4 step: 697, loss is 0.0004556844069156796\n",
      "epoch: 4 step: 698, loss is 0.0018167117377743125\n",
      "epoch: 4 step: 699, loss is 0.0052931904792785645\n",
      "epoch: 4 step: 700, loss is 0.10946650803089142\n",
      "epoch: 4 step: 701, loss is 0.0072508566081523895\n",
      "epoch: 4 step: 702, loss is 0.020118778571486473\n",
      "epoch: 4 step: 703, loss is 0.021205008029937744\n",
      "epoch: 4 step: 704, loss is 0.002373772906139493\n",
      "epoch: 4 step: 705, loss is 0.0031797890551388264\n",
      "epoch: 4 step: 706, loss is 0.005574455484747887\n",
      "epoch: 4 step: 707, loss is 0.0064973668195307255\n",
      "epoch: 4 step: 708, loss is 0.001648697187192738\n",
      "epoch: 4 step: 709, loss is 0.10682743042707443\n",
      "epoch: 4 step: 710, loss is 0.023111533373594284\n",
      "epoch: 4 step: 711, loss is 0.0011037142248824239\n",
      "epoch: 4 step: 712, loss is 0.008764484897255898\n",
      "epoch: 4 step: 713, loss is 0.00067294598557055\n",
      "epoch: 4 step: 714, loss is 0.005990097299218178\n",
      "epoch: 4 step: 715, loss is 0.035895850509405136\n",
      "epoch: 4 step: 716, loss is 0.0007209045579656959\n",
      "epoch: 4 step: 717, loss is 0.04453246295452118\n",
      "epoch: 4 step: 718, loss is 0.11055637151002884\n",
      "epoch: 4 step: 719, loss is 0.03220631182193756\n",
      "epoch: 4 step: 720, loss is 0.0021340723615139723\n",
      "epoch: 4 step: 721, loss is 0.0450366847217083\n",
      "epoch: 4 step: 722, loss is 0.01052587665617466\n",
      "epoch: 4 step: 723, loss is 0.017091693356633186\n",
      "epoch: 4 step: 724, loss is 0.0009949184022843838\n",
      "epoch: 4 step: 725, loss is 0.011049184948205948\n",
      "epoch: 4 step: 726, loss is 0.0009754163329489529\n",
      "epoch: 4 step: 727, loss is 0.0004784619086422026\n",
      "epoch: 4 step: 728, loss is 0.1987966001033783\n",
      "epoch: 4 step: 729, loss is 0.016817184165120125\n",
      "epoch: 4 step: 730, loss is 0.04263117536902428\n",
      "epoch: 4 step: 731, loss is 0.0775756686925888\n",
      "epoch: 4 step: 732, loss is 0.0047797467559576035\n",
      "epoch: 4 step: 733, loss is 0.011530773714184761\n",
      "epoch: 4 step: 734, loss is 0.01728661172091961\n",
      "epoch: 4 step: 735, loss is 0.0038788963574916124\n",
      "epoch: 4 step: 736, loss is 0.0005712644779123366\n",
      "epoch: 4 step: 737, loss is 0.01688285358250141\n",
      "epoch: 4 step: 738, loss is 0.0009503390174359083\n",
      "epoch: 4 step: 739, loss is 0.0041586412116885185\n",
      "epoch: 4 step: 740, loss is 0.0011277308221906424\n",
      "epoch: 4 step: 741, loss is 0.0009035334223881364\n",
      "epoch: 4 step: 742, loss is 0.03786236792802811\n",
      "epoch: 4 step: 743, loss is 0.004784084856510162\n",
      "epoch: 4 step: 744, loss is 0.004726196639239788\n",
      "epoch: 4 step: 745, loss is 0.0013170285383239388\n",
      "epoch: 4 step: 746, loss is 0.0065195998176932335\n",
      "epoch: 4 step: 747, loss is 0.0041522919200360775\n",
      "epoch: 4 step: 748, loss is 0.07045500725507736\n",
      "epoch: 4 step: 749, loss is 0.005999886896461248\n",
      "epoch: 4 step: 750, loss is 0.02522934041917324\n",
      "epoch: 4 step: 751, loss is 0.004511459264904261\n",
      "epoch: 4 step: 752, loss is 0.02219206653535366\n",
      "epoch: 4 step: 753, loss is 0.0013387772487476468\n",
      "epoch: 4 step: 754, loss is 0.014427638612687588\n",
      "epoch: 4 step: 755, loss is 0.011103450320661068\n",
      "epoch: 4 step: 756, loss is 0.023625541478395462\n",
      "epoch: 4 step: 757, loss is 0.05398120358586311\n",
      "epoch: 4 step: 758, loss is 0.03681083023548126\n",
      "epoch: 4 step: 759, loss is 0.0014696161961182952\n",
      "epoch: 4 step: 760, loss is 0.00026580700068734586\n",
      "epoch: 4 step: 761, loss is 0.0049483152106404305\n",
      "epoch: 4 step: 762, loss is 0.008510962128639221\n",
      "epoch: 4 step: 763, loss is 0.005285902880132198\n",
      "epoch: 4 step: 764, loss is 0.1151183694601059\n",
      "epoch: 4 step: 765, loss is 0.04025137051939964\n",
      "epoch: 4 step: 766, loss is 0.23529775440692902\n",
      "epoch: 4 step: 767, loss is 0.0013182209804654121\n",
      "epoch: 4 step: 768, loss is 0.12666402757167816\n",
      "epoch: 4 step: 769, loss is 0.0030405025463551283\n",
      "epoch: 4 step: 770, loss is 0.022422319278120995\n",
      "epoch: 4 step: 771, loss is 0.02294488623738289\n",
      "epoch: 4 step: 772, loss is 0.00043804163578897715\n",
      "epoch: 4 step: 773, loss is 0.000674908165819943\n",
      "epoch: 4 step: 774, loss is 0.05222814902663231\n",
      "epoch: 4 step: 775, loss is 0.039534393697977066\n",
      "epoch: 4 step: 776, loss is 0.11427819728851318\n",
      "epoch: 4 step: 777, loss is 0.006331438664346933\n",
      "epoch: 4 step: 778, loss is 0.22056475281715393\n",
      "epoch: 4 step: 779, loss is 0.11520247161388397\n",
      "epoch: 4 step: 780, loss is 0.000614166259765625\n",
      "epoch: 4 step: 781, loss is 0.009014561772346497\n",
      "epoch: 4 step: 782, loss is 0.0027826291043311357\n",
      "epoch: 4 step: 783, loss is 0.046729933470487595\n",
      "epoch: 4 step: 784, loss is 0.009723644703626633\n",
      "epoch: 4 step: 785, loss is 0.11350090056657791\n",
      "epoch: 4 step: 786, loss is 0.001764081185683608\n",
      "epoch: 4 step: 787, loss is 0.0010593761689960957\n",
      "epoch: 4 step: 788, loss is 0.24891608953475952\n",
      "epoch: 4 step: 789, loss is 0.11146701127290726\n",
      "epoch: 4 step: 790, loss is 0.1484445333480835\n",
      "epoch: 4 step: 791, loss is 0.0038156509399414062\n",
      "epoch: 4 step: 792, loss is 0.009525933302938938\n",
      "epoch: 4 step: 793, loss is 0.055747080594301224\n",
      "epoch: 4 step: 794, loss is 0.05518452078104019\n",
      "epoch: 4 step: 795, loss is 0.019189495593309402\n",
      "epoch: 4 step: 796, loss is 0.009306179359555244\n",
      "epoch: 4 step: 797, loss is 0.00033563034958206117\n",
      "epoch: 4 step: 798, loss is 0.04020316153764725\n",
      "epoch: 4 step: 799, loss is 0.026558905839920044\n",
      "epoch: 4 step: 800, loss is 0.4208899438381195\n",
      "epoch: 4 step: 801, loss is 0.12334050238132477\n",
      "epoch: 4 step: 802, loss is 0.0013715571258217096\n",
      "epoch: 4 step: 803, loss is 0.0030732019804418087\n",
      "epoch: 4 step: 804, loss is 0.00026357738533988595\n",
      "epoch: 4 step: 805, loss is 9.854939708020538e-05\n",
      "epoch: 4 step: 806, loss is 0.00263965199701488\n",
      "epoch: 4 step: 807, loss is 0.0073542422614991665\n",
      "epoch: 4 step: 808, loss is 0.0599328987300396\n",
      "epoch: 4 step: 809, loss is 0.05580337345600128\n",
      "epoch: 4 step: 810, loss is 0.026696519926190376\n",
      "epoch: 4 step: 811, loss is 0.002263698261231184\n",
      "epoch: 4 step: 812, loss is 0.007555253338068724\n",
      "epoch: 4 step: 813, loss is 0.22268350422382355\n",
      "epoch: 4 step: 814, loss is 0.0013449328253045678\n",
      "epoch: 4 step: 815, loss is 0.015385358594357967\n",
      "epoch: 4 step: 816, loss is 0.0007444164366461337\n",
      "epoch: 4 step: 817, loss is 0.13053369522094727\n",
      "epoch: 4 step: 818, loss is 0.0005167735507711768\n",
      "epoch: 4 step: 819, loss is 0.09444621205329895\n",
      "epoch: 4 step: 820, loss is 0.005323838908225298\n",
      "epoch: 4 step: 821, loss is 0.001983861206099391\n",
      "epoch: 4 step: 822, loss is 0.0015273131430149078\n",
      "epoch: 4 step: 823, loss is 0.00134370569139719\n",
      "epoch: 4 step: 824, loss is 0.0034095216542482376\n",
      "epoch: 4 step: 825, loss is 0.008262479677796364\n",
      "epoch: 4 step: 826, loss is 0.2377457320690155\n",
      "epoch: 4 step: 827, loss is 0.00032779452158138156\n",
      "epoch: 4 step: 828, loss is 0.004030442796647549\n",
      "epoch: 4 step: 829, loss is 0.09931527823209763\n",
      "epoch: 4 step: 830, loss is 0.006651374977082014\n",
      "epoch: 4 step: 831, loss is 0.008742068894207478\n",
      "epoch: 4 step: 832, loss is 0.05003281310200691\n",
      "epoch: 4 step: 833, loss is 0.007267999462783337\n",
      "epoch: 4 step: 834, loss is 0.010691997595131397\n",
      "epoch: 4 step: 835, loss is 0.0023264396004378796\n",
      "epoch: 4 step: 836, loss is 0.2181469053030014\n",
      "epoch: 4 step: 837, loss is 0.00437313225120306\n",
      "epoch: 4 step: 838, loss is 0.004290305078029633\n",
      "epoch: 4 step: 839, loss is 0.13356328010559082\n",
      "epoch: 4 step: 840, loss is 0.14710791409015656\n",
      "epoch: 4 step: 841, loss is 0.00509090069681406\n",
      "epoch: 4 step: 842, loss is 0.0018210993148386478\n",
      "epoch: 4 step: 843, loss is 0.008305229246616364\n",
      "epoch: 4 step: 844, loss is 0.0033661730121821165\n",
      "epoch: 4 step: 845, loss is 0.034125689417123795\n",
      "epoch: 4 step: 846, loss is 0.042849428951740265\n",
      "epoch: 4 step: 847, loss is 0.002383127110078931\n",
      "epoch: 4 step: 848, loss is 0.005718539003282785\n",
      "epoch: 4 step: 849, loss is 0.017064526677131653\n",
      "epoch: 4 step: 850, loss is 0.1081894263625145\n",
      "epoch: 4 step: 851, loss is 0.003762277076020837\n",
      "epoch: 4 step: 852, loss is 0.003979111090302467\n",
      "epoch: 4 step: 853, loss is 0.014761118218302727\n",
      "epoch: 4 step: 854, loss is 0.1120954230427742\n",
      "epoch: 4 step: 855, loss is 0.01777835562825203\n",
      "epoch: 4 step: 856, loss is 0.004217008128762245\n",
      "epoch: 4 step: 857, loss is 0.008411046117544174\n",
      "epoch: 4 step: 858, loss is 0.26582467555999756\n",
      "epoch: 4 step: 859, loss is 0.06323441863059998\n",
      "epoch: 4 step: 860, loss is 0.016136545687913895\n",
      "epoch: 4 step: 861, loss is 0.005017021205276251\n",
      "epoch: 4 step: 862, loss is 0.07619430124759674\n",
      "epoch: 4 step: 863, loss is 0.006184034515172243\n",
      "epoch: 4 step: 864, loss is 0.013501870445907116\n",
      "epoch: 4 step: 865, loss is 0.013490955345332623\n",
      "epoch: 4 step: 866, loss is 0.019642218947410583\n",
      "epoch: 4 step: 867, loss is 0.000640222046058625\n",
      "epoch: 4 step: 868, loss is 0.04009481891989708\n",
      "epoch: 4 step: 869, loss is 0.09910332411527634\n",
      "epoch: 4 step: 870, loss is 0.03020671382546425\n",
      "epoch: 4 step: 871, loss is 0.10775177925825119\n",
      "epoch: 4 step: 872, loss is 0.0005357262562029064\n",
      "epoch: 4 step: 873, loss is 0.0032689105719327927\n",
      "epoch: 4 step: 874, loss is 0.013830994255840778\n",
      "epoch: 4 step: 875, loss is 0.1664133071899414\n",
      "epoch: 4 step: 876, loss is 0.04027251899242401\n",
      "epoch: 4 step: 877, loss is 0.029422834515571594\n",
      "epoch: 4 step: 878, loss is 0.08815426379442215\n",
      "epoch: 4 step: 879, loss is 0.07042507082223892\n",
      "epoch: 4 step: 880, loss is 0.0067713321186602116\n",
      "epoch: 4 step: 881, loss is 0.019746005535125732\n",
      "epoch: 4 step: 882, loss is 0.051388684660196304\n",
      "epoch: 4 step: 883, loss is 0.0023351856507360935\n",
      "epoch: 4 step: 884, loss is 0.011874645948410034\n",
      "epoch: 4 step: 885, loss is 0.12216919660568237\n",
      "epoch: 4 step: 886, loss is 0.04023685306310654\n",
      "epoch: 4 step: 887, loss is 0.0013758029090240598\n",
      "epoch: 4 step: 888, loss is 0.08734642714262009\n",
      "epoch: 4 step: 889, loss is 0.012060433626174927\n",
      "epoch: 4 step: 890, loss is 0.004881571512669325\n",
      "epoch: 4 step: 891, loss is 0.003788482863456011\n",
      "epoch: 4 step: 892, loss is 0.015472549013793468\n",
      "epoch: 4 step: 893, loss is 0.0030240542255342007\n",
      "epoch: 4 step: 894, loss is 0.0009742833208292723\n",
      "epoch: 4 step: 895, loss is 0.022929266095161438\n",
      "epoch: 4 step: 896, loss is 0.027382299304008484\n",
      "epoch: 4 step: 897, loss is 0.005350061692297459\n",
      "epoch: 4 step: 898, loss is 0.05508102849125862\n",
      "epoch: 4 step: 899, loss is 0.0012106141075491905\n",
      "epoch: 4 step: 900, loss is 0.006359803956001997\n",
      "epoch: 4 step: 901, loss is 0.0019375043921172619\n",
      "epoch: 4 step: 902, loss is 0.004528242163360119\n",
      "epoch: 4 step: 903, loss is 0.0029484897386282682\n",
      "epoch: 4 step: 904, loss is 0.014865239150822163\n",
      "epoch: 4 step: 905, loss is 0.0005254137213341892\n",
      "epoch: 4 step: 906, loss is 0.0002240268513560295\n",
      "epoch: 4 step: 907, loss is 0.004808368626981974\n",
      "epoch: 4 step: 908, loss is 0.0053969454020261765\n",
      "epoch: 4 step: 909, loss is 0.01935693249106407\n",
      "epoch: 4 step: 910, loss is 0.21531541645526886\n",
      "epoch: 4 step: 911, loss is 0.18484607338905334\n",
      "epoch: 4 step: 912, loss is 0.03277764841914177\n",
      "epoch: 4 step: 913, loss is 0.009110674262046814\n",
      "epoch: 4 step: 914, loss is 0.0029845244716852903\n",
      "epoch: 4 step: 915, loss is 0.007493908517062664\n",
      "epoch: 4 step: 916, loss is 0.0008568884804844856\n",
      "epoch: 4 step: 917, loss is 0.0060090250335633755\n",
      "epoch: 4 step: 918, loss is 0.009662644937634468\n",
      "epoch: 4 step: 919, loss is 0.005091050174087286\n",
      "epoch: 4 step: 920, loss is 0.02411932684481144\n",
      "epoch: 4 step: 921, loss is 0.0010852540144696832\n",
      "epoch: 4 step: 922, loss is 0.001258514472283423\n",
      "epoch: 4 step: 923, loss is 0.0019613613840192556\n",
      "epoch: 4 step: 924, loss is 0.005437657702714205\n",
      "epoch: 4 step: 925, loss is 0.011247864924371243\n",
      "epoch: 4 step: 926, loss is 0.0004822541668545455\n",
      "epoch: 4 step: 927, loss is 0.02783314883708954\n",
      "epoch: 4 step: 928, loss is 0.11141280084848404\n",
      "epoch: 4 step: 929, loss is 0.01245552022010088\n",
      "epoch: 4 step: 930, loss is 0.004572245758026838\n",
      "epoch: 4 step: 931, loss is 0.03690919652581215\n",
      "epoch: 4 step: 932, loss is 0.0012211085995659232\n",
      "epoch: 4 step: 933, loss is 0.0007796586723998189\n",
      "epoch: 4 step: 934, loss is 0.008741253986954689\n",
      "epoch: 4 step: 935, loss is 0.0017840940272435546\n",
      "epoch: 4 step: 936, loss is 0.0030139186419546604\n",
      "epoch: 4 step: 937, loss is 0.030482808127999306\n",
      "epoch: 4 step: 938, loss is 0.0007529159775003791\n",
      "epoch: 4 step: 939, loss is 0.005598926451057196\n",
      "epoch: 4 step: 940, loss is 0.002789953723549843\n",
      "epoch: 4 step: 941, loss is 0.026792330667376518\n",
      "epoch: 4 step: 942, loss is 0.030307859182357788\n",
      "epoch: 4 step: 943, loss is 0.14404714107513428\n",
      "epoch: 4 step: 944, loss is 0.03415556624531746\n",
      "epoch: 4 step: 945, loss is 0.0025321156717836857\n",
      "epoch: 4 step: 946, loss is 0.10789671540260315\n",
      "epoch: 4 step: 947, loss is 0.0024629738181829453\n",
      "epoch: 4 step: 948, loss is 0.0010436766315251589\n",
      "epoch: 4 step: 949, loss is 0.010404547676444054\n",
      "epoch: 4 step: 950, loss is 0.001436601160094142\n",
      "epoch: 4 step: 951, loss is 0.0032532208133488894\n",
      "epoch: 4 step: 952, loss is 0.009249246679246426\n",
      "epoch: 4 step: 953, loss is 0.0003311310720164329\n",
      "epoch: 4 step: 954, loss is 0.009136490523815155\n",
      "epoch: 4 step: 955, loss is 0.1299276202917099\n",
      "epoch: 4 step: 956, loss is 0.13351814448833466\n",
      "epoch: 4 step: 957, loss is 0.009135890752077103\n",
      "epoch: 4 step: 958, loss is 0.07399614155292511\n",
      "epoch: 4 step: 959, loss is 0.13699394464492798\n",
      "epoch: 4 step: 960, loss is 0.04279853031039238\n",
      "epoch: 4 step: 961, loss is 0.22128576040267944\n",
      "epoch: 4 step: 962, loss is 0.08374771475791931\n",
      "epoch: 4 step: 963, loss is 0.003752557560801506\n",
      "epoch: 4 step: 964, loss is 0.002036832273006439\n",
      "epoch: 4 step: 965, loss is 0.02673240751028061\n",
      "epoch: 4 step: 966, loss is 0.40724435448646545\n",
      "epoch: 4 step: 967, loss is 0.021983850747346878\n",
      "epoch: 4 step: 968, loss is 0.07372192293405533\n",
      "epoch: 4 step: 969, loss is 0.036372896283864975\n",
      "epoch: 4 step: 970, loss is 0.0029644786845892668\n",
      "epoch: 4 step: 971, loss is 0.03699515759944916\n",
      "epoch: 4 step: 972, loss is 0.005548258312046528\n",
      "epoch: 4 step: 973, loss is 0.1730521023273468\n",
      "epoch: 4 step: 974, loss is 0.01659488119184971\n",
      "epoch: 4 step: 975, loss is 0.04835997894406319\n",
      "epoch: 4 step: 976, loss is 0.0033808054868131876\n",
      "epoch: 4 step: 977, loss is 0.022121611982584\n",
      "epoch: 4 step: 978, loss is 0.01012248545885086\n",
      "epoch: 4 step: 979, loss is 0.047623272985219955\n",
      "epoch: 4 step: 980, loss is 0.012181909754872322\n",
      "epoch: 4 step: 981, loss is 0.01057121716439724\n",
      "epoch: 4 step: 982, loss is 0.0561048798263073\n",
      "epoch: 4 step: 983, loss is 0.024459581822156906\n",
      "epoch: 4 step: 984, loss is 0.010861233808100224\n",
      "epoch: 4 step: 985, loss is 0.16063709557056427\n",
      "epoch: 4 step: 986, loss is 0.0018465507309883833\n",
      "epoch: 4 step: 987, loss is 0.04880101606249809\n",
      "epoch: 4 step: 988, loss is 0.04043322429060936\n",
      "epoch: 4 step: 989, loss is 0.005076284985989332\n",
      "epoch: 4 step: 990, loss is 0.014284851029515266\n",
      "epoch: 4 step: 991, loss is 0.02046288177371025\n",
      "epoch: 4 step: 992, loss is 0.0010099951177835464\n",
      "epoch: 4 step: 993, loss is 0.005466487258672714\n",
      "epoch: 4 step: 994, loss is 0.0017175170360133052\n",
      "epoch: 4 step: 995, loss is 0.002377596916630864\n",
      "epoch: 4 step: 996, loss is 0.006191170774400234\n",
      "epoch: 4 step: 997, loss is 0.008240466006100178\n",
      "epoch: 4 step: 998, loss is 0.0026525496505200863\n",
      "epoch: 4 step: 999, loss is 0.0003949883976019919\n",
      "epoch: 4 step: 1000, loss is 0.03899657726287842\n",
      "epoch: 4 step: 1001, loss is 0.0007378616719506681\n",
      "epoch: 4 step: 1002, loss is 0.07803429663181305\n",
      "epoch: 4 step: 1003, loss is 0.00010099959763465449\n",
      "epoch: 4 step: 1004, loss is 0.017747309058904648\n",
      "epoch: 4 step: 1005, loss is 0.001450415002182126\n",
      "epoch: 4 step: 1006, loss is 0.0001167341397376731\n",
      "epoch: 4 step: 1007, loss is 0.029873505234718323\n",
      "epoch: 4 step: 1008, loss is 0.07527446001768112\n",
      "epoch: 4 step: 1009, loss is 0.017726726830005646\n",
      "epoch: 4 step: 1010, loss is 0.11951109766960144\n",
      "epoch: 4 step: 1011, loss is 0.006948612630367279\n",
      "epoch: 4 step: 1012, loss is 0.0006682941457256675\n",
      "epoch: 4 step: 1013, loss is 0.06570775806903839\n",
      "epoch: 4 step: 1014, loss is 0.01034839078783989\n",
      "epoch: 4 step: 1015, loss is 0.19935324788093567\n",
      "epoch: 4 step: 1016, loss is 0.0046963379718363285\n",
      "epoch: 4 step: 1017, loss is 0.019589439034461975\n",
      "epoch: 4 step: 1018, loss is 0.019807301461696625\n",
      "epoch: 4 step: 1019, loss is 0.004204736556857824\n",
      "epoch: 4 step: 1020, loss is 0.0046149687841534615\n",
      "epoch: 4 step: 1021, loss is 0.08230815827846527\n",
      "epoch: 4 step: 1022, loss is 0.008310765959322453\n",
      "epoch: 4 step: 1023, loss is 0.0071606626734137535\n",
      "epoch: 4 step: 1024, loss is 0.15213297307491302\n",
      "epoch: 4 step: 1025, loss is 0.1527089774608612\n",
      "epoch: 4 step: 1026, loss is 0.008744054473936558\n",
      "epoch: 4 step: 1027, loss is 0.01189925242215395\n",
      "epoch: 4 step: 1028, loss is 0.03841011971235275\n",
      "epoch: 4 step: 1029, loss is 0.03483284264802933\n",
      "epoch: 4 step: 1030, loss is 0.07798612862825394\n",
      "epoch: 4 step: 1031, loss is 0.0007835576543584466\n",
      "epoch: 4 step: 1032, loss is 0.008268688805401325\n",
      "epoch: 4 step: 1033, loss is 0.004819637630134821\n",
      "epoch: 4 step: 1034, loss is 0.00236328411847353\n",
      "epoch: 4 step: 1035, loss is 0.04113165661692619\n",
      "epoch: 4 step: 1036, loss is 0.0681128278374672\n",
      "epoch: 4 step: 1037, loss is 0.0064360396936535835\n",
      "epoch: 4 step: 1038, loss is 0.03064233437180519\n",
      "epoch: 4 step: 1039, loss is 0.011956590227782726\n",
      "epoch: 4 step: 1040, loss is 0.00854374561458826\n",
      "epoch: 4 step: 1041, loss is 0.008959034457802773\n",
      "epoch: 4 step: 1042, loss is 0.006759125739336014\n",
      "epoch: 4 step: 1043, loss is 0.006537341512739658\n",
      "epoch: 4 step: 1044, loss is 0.050277721136808395\n",
      "epoch: 4 step: 1045, loss is 0.034312378615140915\n",
      "epoch: 4 step: 1046, loss is 0.008140863850712776\n",
      "epoch: 4 step: 1047, loss is 0.0018264942336827517\n",
      "epoch: 4 step: 1048, loss is 0.05054689198732376\n",
      "epoch: 4 step: 1049, loss is 0.006858266424387693\n",
      "epoch: 4 step: 1050, loss is 0.016292814165353775\n",
      "epoch: 4 step: 1051, loss is 0.029623305425047874\n",
      "epoch: 4 step: 1052, loss is 0.07661722600460052\n",
      "epoch: 4 step: 1053, loss is 0.017724264413118362\n",
      "epoch: 4 step: 1054, loss is 0.0009485922055318952\n",
      "epoch: 4 step: 1055, loss is 0.0056846565566957\n",
      "epoch: 4 step: 1056, loss is 0.006015445105731487\n",
      "epoch: 4 step: 1057, loss is 0.2536855936050415\n",
      "epoch: 4 step: 1058, loss is 0.03452584892511368\n",
      "epoch: 4 step: 1059, loss is 0.00939592532813549\n",
      "epoch: 4 step: 1060, loss is 0.0590655542910099\n",
      "epoch: 4 step: 1061, loss is 0.0023153566289693117\n",
      "epoch: 4 step: 1062, loss is 0.0011197997955605388\n",
      "epoch: 4 step: 1063, loss is 0.0009911444503813982\n",
      "epoch: 4 step: 1064, loss is 0.24892139434814453\n",
      "epoch: 4 step: 1065, loss is 0.03211626410484314\n",
      "epoch: 4 step: 1066, loss is 0.00028308696346357465\n",
      "epoch: 4 step: 1067, loss is 0.003914218861609697\n",
      "epoch: 4 step: 1068, loss is 0.00423118332400918\n",
      "epoch: 4 step: 1069, loss is 0.004927957430481911\n",
      "epoch: 4 step: 1070, loss is 0.0042104655876755714\n",
      "epoch: 4 step: 1071, loss is 0.006023624911904335\n",
      "epoch: 4 step: 1072, loss is 0.09805022180080414\n",
      "epoch: 4 step: 1073, loss is 0.0845431461930275\n",
      "epoch: 4 step: 1074, loss is 0.01067968737334013\n",
      "epoch: 4 step: 1075, loss is 0.029037365689873695\n",
      "epoch: 4 step: 1076, loss is 0.008145024999976158\n",
      "epoch: 4 step: 1077, loss is 0.11244270950555801\n",
      "epoch: 4 step: 1078, loss is 0.1783990114927292\n",
      "epoch: 4 step: 1079, loss is 0.036143045872449875\n",
      "epoch: 4 step: 1080, loss is 0.0008114108350127935\n",
      "epoch: 4 step: 1081, loss is 0.2877141535282135\n",
      "epoch: 4 step: 1082, loss is 0.016359934583306313\n",
      "epoch: 4 step: 1083, loss is 0.012294583022594452\n",
      "epoch: 4 step: 1084, loss is 0.027816597372293472\n",
      "epoch: 4 step: 1085, loss is 0.002836033469066024\n",
      "epoch: 4 step: 1086, loss is 0.007497715298086405\n",
      "epoch: 4 step: 1087, loss is 0.026857662945985794\n",
      "epoch: 4 step: 1088, loss is 0.19248078763484955\n",
      "epoch: 4 step: 1089, loss is 0.007463006302714348\n",
      "epoch: 4 step: 1090, loss is 0.11168041825294495\n",
      "epoch: 4 step: 1091, loss is 0.0011533793294802308\n",
      "epoch: 4 step: 1092, loss is 0.010379452258348465\n",
      "epoch: 4 step: 1093, loss is 0.03982115164399147\n",
      "epoch: 4 step: 1094, loss is 0.0008490360341966152\n",
      "epoch: 4 step: 1095, loss is 0.017759261652827263\n",
      "epoch: 4 step: 1096, loss is 0.0008562623988837004\n",
      "epoch: 4 step: 1097, loss is 0.004908385686576366\n",
      "epoch: 4 step: 1098, loss is 0.11061662435531616\n",
      "epoch: 4 step: 1099, loss is 0.0542333647608757\n",
      "epoch: 4 step: 1100, loss is 0.016040636226534843\n",
      "epoch: 4 step: 1101, loss is 0.0015658416086807847\n",
      "epoch: 4 step: 1102, loss is 0.002980055520310998\n",
      "epoch: 4 step: 1103, loss is 0.008275605738162994\n",
      "epoch: 4 step: 1104, loss is 0.018430396914482117\n",
      "epoch: 4 step: 1105, loss is 0.004461850970983505\n",
      "epoch: 4 step: 1106, loss is 0.1648036539554596\n",
      "epoch: 4 step: 1107, loss is 0.02956404723227024\n",
      "epoch: 4 step: 1108, loss is 0.007817976176738739\n",
      "epoch: 4 step: 1109, loss is 0.008343062363564968\n",
      "epoch: 4 step: 1110, loss is 0.0032877696212381124\n",
      "epoch: 4 step: 1111, loss is 0.05408276244997978\n",
      "epoch: 4 step: 1112, loss is 0.03831274434924126\n",
      "epoch: 4 step: 1113, loss is 0.001330287428572774\n",
      "epoch: 4 step: 1114, loss is 0.015036698430776596\n",
      "epoch: 4 step: 1115, loss is 0.035922855138778687\n",
      "epoch: 4 step: 1116, loss is 0.0013371061068028212\n",
      "epoch: 4 step: 1117, loss is 0.04615814611315727\n",
      "epoch: 4 step: 1118, loss is 0.001177748548798263\n",
      "epoch: 4 step: 1119, loss is 0.06715327501296997\n",
      "epoch: 4 step: 1120, loss is 0.16742803156375885\n",
      "epoch: 4 step: 1121, loss is 0.0014610534999519587\n",
      "epoch: 4 step: 1122, loss is 0.0021840441040694714\n",
      "epoch: 4 step: 1123, loss is 0.014969580806791782\n",
      "epoch: 4 step: 1124, loss is 0.0590507909655571\n",
      "epoch: 4 step: 1125, loss is 0.02633187361061573\n",
      "epoch: 4 step: 1126, loss is 0.0011034463532269\n",
      "epoch: 4 step: 1127, loss is 0.0026952712796628475\n",
      "epoch: 4 step: 1128, loss is 0.0011242976179346442\n",
      "epoch: 4 step: 1129, loss is 0.003035965608432889\n",
      "epoch: 4 step: 1130, loss is 0.03817741572856903\n",
      "epoch: 4 step: 1131, loss is 0.0064234063029289246\n",
      "epoch: 4 step: 1132, loss is 0.12345234304666519\n",
      "epoch: 4 step: 1133, loss is 0.05871748551726341\n",
      "epoch: 4 step: 1134, loss is 0.0026968035381287336\n",
      "epoch: 4 step: 1135, loss is 0.11871292442083359\n",
      "epoch: 4 step: 1136, loss is 0.015927482396364212\n",
      "epoch: 4 step: 1137, loss is 0.029779892414808273\n",
      "epoch: 4 step: 1138, loss is 0.04388827830553055\n",
      "epoch: 4 step: 1139, loss is 0.003693806240335107\n",
      "epoch: 4 step: 1140, loss is 0.005743671674281359\n",
      "epoch: 4 step: 1141, loss is 0.04818470776081085\n",
      "epoch: 4 step: 1142, loss is 0.003405802184715867\n",
      "epoch: 4 step: 1143, loss is 0.009035933762788773\n",
      "epoch: 4 step: 1144, loss is 0.006093178875744343\n",
      "epoch: 4 step: 1145, loss is 0.004154165741056204\n",
      "epoch: 4 step: 1146, loss is 0.12940020859241486\n",
      "epoch: 4 step: 1147, loss is 0.00443051103502512\n",
      "epoch: 4 step: 1148, loss is 0.07742394506931305\n",
      "epoch: 4 step: 1149, loss is 0.0005592402303591371\n",
      "epoch: 4 step: 1150, loss is 0.024723336100578308\n",
      "epoch: 4 step: 1151, loss is 0.004596536047756672\n",
      "epoch: 4 step: 1152, loss is 0.3278448283672333\n",
      "epoch: 4 step: 1153, loss is 0.026069553568959236\n",
      "epoch: 4 step: 1154, loss is 0.027110105380415916\n",
      "epoch: 4 step: 1155, loss is 0.008768260478973389\n",
      "epoch: 4 step: 1156, loss is 0.003198545426130295\n",
      "epoch: 4 step: 1157, loss is 0.0013198736123740673\n",
      "epoch: 4 step: 1158, loss is 0.02499227039515972\n",
      "epoch: 4 step: 1159, loss is 0.018249036744236946\n",
      "epoch: 4 step: 1160, loss is 0.005277550779283047\n",
      "epoch: 4 step: 1161, loss is 0.05344223976135254\n",
      "epoch: 4 step: 1162, loss is 0.002107338048517704\n",
      "epoch: 4 step: 1163, loss is 0.014716090634465218\n",
      "epoch: 4 step: 1164, loss is 0.060334254056215286\n",
      "epoch: 4 step: 1165, loss is 0.13577350974082947\n",
      "epoch: 4 step: 1166, loss is 0.0151533093303442\n",
      "epoch: 4 step: 1167, loss is 0.06986679881811142\n",
      "epoch: 4 step: 1168, loss is 0.03467541188001633\n",
      "epoch: 4 step: 1169, loss is 0.004747073631733656\n",
      "epoch: 4 step: 1170, loss is 0.05088122561573982\n",
      "epoch: 4 step: 1171, loss is 0.0018041792791336775\n",
      "epoch: 4 step: 1172, loss is 0.02646431140601635\n",
      "epoch: 4 step: 1173, loss is 0.060984473675489426\n",
      "epoch: 4 step: 1174, loss is 0.018681053072214127\n",
      "epoch: 4 step: 1175, loss is 0.2064850926399231\n",
      "epoch: 4 step: 1176, loss is 0.033670347183942795\n",
      "epoch: 4 step: 1177, loss is 0.21037408709526062\n",
      "epoch: 4 step: 1178, loss is 0.0008429456502199173\n",
      "epoch: 4 step: 1179, loss is 0.0044645038433372974\n",
      "epoch: 4 step: 1180, loss is 0.007633088622242212\n",
      "epoch: 4 step: 1181, loss is 0.076889269053936\n",
      "epoch: 4 step: 1182, loss is 0.005102335941046476\n",
      "epoch: 4 step: 1183, loss is 0.06496291607618332\n",
      "epoch: 4 step: 1184, loss is 0.007558793295174837\n",
      "epoch: 4 step: 1185, loss is 0.0033568067010492086\n",
      "epoch: 4 step: 1186, loss is 0.16522496938705444\n",
      "epoch: 4 step: 1187, loss is 0.0036278385668992996\n",
      "epoch: 4 step: 1188, loss is 0.06601676344871521\n",
      "epoch: 4 step: 1189, loss is 0.14029206335544586\n",
      "epoch: 4 step: 1190, loss is 0.03430498391389847\n",
      "epoch: 4 step: 1191, loss is 0.0012083434266969562\n",
      "epoch: 4 step: 1192, loss is 0.08001773059368134\n",
      "epoch: 4 step: 1193, loss is 0.0032718561124056578\n",
      "epoch: 4 step: 1194, loss is 0.006329938303679228\n",
      "epoch: 4 step: 1195, loss is 0.0020963940769433975\n",
      "epoch: 4 step: 1196, loss is 0.0011208008509129286\n",
      "epoch: 4 step: 1197, loss is 0.0017507809679955244\n",
      "epoch: 4 step: 1198, loss is 0.025978388264775276\n",
      "epoch: 4 step: 1199, loss is 0.000567171024158597\n",
      "epoch: 4 step: 1200, loss is 0.11598892509937286\n",
      "epoch: 4 step: 1201, loss is 0.0057471017353236675\n",
      "epoch: 4 step: 1202, loss is 0.06996432691812515\n",
      "epoch: 4 step: 1203, loss is 0.005490599200129509\n",
      "epoch: 4 step: 1204, loss is 0.004355453886091709\n",
      "epoch: 4 step: 1205, loss is 0.005676664412021637\n",
      "epoch: 4 step: 1206, loss is 0.0027824072167277336\n",
      "epoch: 4 step: 1207, loss is 0.22462376952171326\n",
      "epoch: 4 step: 1208, loss is 0.11908044666051865\n",
      "epoch: 4 step: 1209, loss is 0.1554778665304184\n",
      "epoch: 4 step: 1210, loss is 0.0008594385581091046\n",
      "epoch: 4 step: 1211, loss is 0.018039602786302567\n",
      "epoch: 4 step: 1212, loss is 0.0068405005149543285\n",
      "epoch: 4 step: 1213, loss is 0.0004500039794947952\n",
      "epoch: 4 step: 1214, loss is 0.009262213483452797\n",
      "epoch: 4 step: 1215, loss is 0.025049105286598206\n",
      "epoch: 4 step: 1216, loss is 0.16817384958267212\n",
      "epoch: 4 step: 1217, loss is 0.23052813112735748\n",
      "epoch: 4 step: 1218, loss is 0.045438311994075775\n",
      "epoch: 4 step: 1219, loss is 0.17698301374912262\n",
      "epoch: 4 step: 1220, loss is 0.02295662835240364\n",
      "epoch: 4 step: 1221, loss is 0.00018813283531926572\n",
      "epoch: 4 step: 1222, loss is 0.026195356622338295\n",
      "epoch: 4 step: 1223, loss is 0.005046048201620579\n",
      "epoch: 4 step: 1224, loss is 0.03201087936758995\n",
      "epoch: 4 step: 1225, loss is 0.004298430401831865\n",
      "epoch: 4 step: 1226, loss is 0.03362472355365753\n",
      "epoch: 4 step: 1227, loss is 0.01743425987660885\n",
      "epoch: 4 step: 1228, loss is 0.3186998665332794\n",
      "epoch: 4 step: 1229, loss is 0.3599635660648346\n",
      "epoch: 4 step: 1230, loss is 0.01571579836308956\n",
      "epoch: 4 step: 1231, loss is 0.09329655021429062\n",
      "epoch: 4 step: 1232, loss is 0.0028976541943848133\n",
      "epoch: 4 step: 1233, loss is 0.1280546486377716\n",
      "epoch: 4 step: 1234, loss is 0.003608239581808448\n",
      "epoch: 4 step: 1235, loss is 0.0015184521907940507\n",
      "epoch: 4 step: 1236, loss is 0.006877358071506023\n",
      "epoch: 4 step: 1237, loss is 0.003008384956046939\n",
      "epoch: 4 step: 1238, loss is 0.23875530064105988\n",
      "epoch: 4 step: 1239, loss is 0.23422326147556305\n",
      "epoch: 4 step: 1240, loss is 0.002689645392820239\n",
      "epoch: 4 step: 1241, loss is 0.007197135128080845\n",
      "epoch: 4 step: 1242, loss is 0.013192535378038883\n",
      "epoch: 4 step: 1243, loss is 0.03319086506962776\n",
      "epoch: 4 step: 1244, loss is 0.05533819645643234\n",
      "epoch: 4 step: 1245, loss is 0.12400539964437485\n",
      "epoch: 4 step: 1246, loss is 0.009484164416790009\n",
      "epoch: 4 step: 1247, loss is 0.0006369408220052719\n",
      "epoch: 4 step: 1248, loss is 0.01630266383290291\n",
      "epoch: 4 step: 1249, loss is 0.0070659322664141655\n",
      "epoch: 4 step: 1250, loss is 0.0422871895134449\n",
      "epoch: 4 step: 1251, loss is 0.007493679411709309\n",
      "epoch: 4 step: 1252, loss is 0.00973938312381506\n",
      "epoch: 4 step: 1253, loss is 0.1981661319732666\n",
      "epoch: 4 step: 1254, loss is 0.08096443861722946\n",
      "epoch: 4 step: 1255, loss is 0.05567731335759163\n",
      "epoch: 4 step: 1256, loss is 0.03266109153628349\n",
      "epoch: 4 step: 1257, loss is 0.002728257095441222\n",
      "epoch: 4 step: 1258, loss is 0.02552487887442112\n",
      "epoch: 4 step: 1259, loss is 0.020661452785134315\n",
      "epoch: 4 step: 1260, loss is 0.0020698998123407364\n",
      "epoch: 4 step: 1261, loss is 0.08299824595451355\n",
      "epoch: 4 step: 1262, loss is 0.0013584191910922527\n",
      "epoch: 4 step: 1263, loss is 0.004932083189487457\n",
      "epoch: 4 step: 1264, loss is 0.1404934674501419\n",
      "epoch: 4 step: 1265, loss is 0.00522396806627512\n",
      "epoch: 4 step: 1266, loss is 0.03540223836898804\n",
      "epoch: 4 step: 1267, loss is 0.002435591071844101\n",
      "epoch: 4 step: 1268, loss is 0.012786781415343285\n",
      "epoch: 4 step: 1269, loss is 0.07656349986791611\n",
      "epoch: 4 step: 1270, loss is 0.16423219442367554\n",
      "epoch: 4 step: 1271, loss is 0.015591587871313095\n",
      "epoch: 4 step: 1272, loss is 0.01047656312584877\n",
      "epoch: 4 step: 1273, loss is 0.0041761924512684345\n",
      "epoch: 4 step: 1274, loss is 0.012814524583518505\n",
      "epoch: 4 step: 1275, loss is 0.006528234109282494\n",
      "epoch: 4 step: 1276, loss is 0.0043341913260519505\n",
      "epoch: 4 step: 1277, loss is 0.00036410888424143195\n",
      "epoch: 4 step: 1278, loss is 0.018790220841765404\n",
      "epoch: 4 step: 1279, loss is 0.006094698328524828\n",
      "epoch: 4 step: 1280, loss is 0.0749095007777214\n",
      "epoch: 4 step: 1281, loss is 0.002122007543221116\n",
      "epoch: 4 step: 1282, loss is 0.006358575075864792\n",
      "epoch: 4 step: 1283, loss is 0.019101949408650398\n",
      "epoch: 4 step: 1284, loss is 0.15968620777130127\n",
      "epoch: 4 step: 1285, loss is 0.003414554987102747\n",
      "epoch: 4 step: 1286, loss is 0.00045624939957633615\n",
      "epoch: 4 step: 1287, loss is 0.03903551027178764\n",
      "epoch: 4 step: 1288, loss is 0.1969878226518631\n",
      "epoch: 4 step: 1289, loss is 0.005915995221585035\n",
      "epoch: 4 step: 1290, loss is 0.02877858467400074\n",
      "epoch: 4 step: 1291, loss is 0.005941283889114857\n",
      "epoch: 4 step: 1292, loss is 0.0034597746562212706\n",
      "epoch: 4 step: 1293, loss is 0.007459288462996483\n",
      "epoch: 4 step: 1294, loss is 0.0017824331298470497\n",
      "epoch: 4 step: 1295, loss is 0.05979055166244507\n",
      "epoch: 4 step: 1296, loss is 0.0009681385708972812\n",
      "epoch: 4 step: 1297, loss is 0.0016358891734853387\n",
      "epoch: 4 step: 1298, loss is 0.001824917970225215\n",
      "epoch: 4 step: 1299, loss is 0.009569898247718811\n",
      "epoch: 4 step: 1300, loss is 0.004854334052652121\n",
      "epoch: 4 step: 1301, loss is 0.02075277455151081\n",
      "epoch: 4 step: 1302, loss is 0.0817733108997345\n",
      "epoch: 4 step: 1303, loss is 0.039684731513261795\n",
      "epoch: 4 step: 1304, loss is 0.0023415060713887215\n",
      "epoch: 4 step: 1305, loss is 0.024427535012364388\n",
      "epoch: 4 step: 1306, loss is 0.008671574294567108\n",
      "epoch: 4 step: 1307, loss is 0.005624482408165932\n",
      "epoch: 4 step: 1308, loss is 0.07439888268709183\n",
      "epoch: 4 step: 1309, loss is 0.003466739086434245\n",
      "epoch: 4 step: 1310, loss is 0.005141959059983492\n",
      "epoch: 4 step: 1311, loss is 0.04709000140428543\n",
      "epoch: 4 step: 1312, loss is 0.06425654143095016\n",
      "epoch: 4 step: 1313, loss is 0.053981054574251175\n",
      "epoch: 4 step: 1314, loss is 0.006001032888889313\n",
      "epoch: 4 step: 1315, loss is 0.0033436750527471304\n",
      "epoch: 4 step: 1316, loss is 0.04386349022388458\n",
      "epoch: 4 step: 1317, loss is 0.0031291197519749403\n",
      "epoch: 4 step: 1318, loss is 0.01987704448401928\n",
      "epoch: 4 step: 1319, loss is 0.00835146103054285\n",
      "epoch: 4 step: 1320, loss is 0.012507017701864243\n",
      "epoch: 4 step: 1321, loss is 0.17414526641368866\n",
      "epoch: 4 step: 1322, loss is 0.06527881324291229\n",
      "epoch: 4 step: 1323, loss is 0.004173056222498417\n",
      "epoch: 4 step: 1324, loss is 0.01754886470735073\n",
      "epoch: 4 step: 1325, loss is 0.010544021613895893\n",
      "epoch: 4 step: 1326, loss is 0.002317361766472459\n",
      "epoch: 4 step: 1327, loss is 0.0018505018670111895\n",
      "epoch: 4 step: 1328, loss is 0.02736867405474186\n",
      "epoch: 4 step: 1329, loss is 0.0044059716165065765\n",
      "epoch: 4 step: 1330, loss is 0.0012614399893209338\n",
      "epoch: 4 step: 1331, loss is 0.011831957846879959\n",
      "epoch: 4 step: 1332, loss is 0.051770810037851334\n",
      "epoch: 4 step: 1333, loss is 0.002281824592500925\n",
      "epoch: 4 step: 1334, loss is 0.0023280230816453695\n",
      "epoch: 4 step: 1335, loss is 0.006415107287466526\n",
      "epoch: 4 step: 1336, loss is 0.0018704109825193882\n",
      "epoch: 4 step: 1337, loss is 0.1939655989408493\n",
      "epoch: 4 step: 1338, loss is 0.0016385498456656933\n",
      "epoch: 4 step: 1339, loss is 0.001993580022826791\n",
      "epoch: 4 step: 1340, loss is 0.009616958908736706\n",
      "epoch: 4 step: 1341, loss is 0.008143030107021332\n",
      "epoch: 4 step: 1342, loss is 0.11723239719867706\n",
      "epoch: 4 step: 1343, loss is 0.056465018540620804\n",
      "epoch: 4 step: 1344, loss is 0.008531761355698109\n",
      "epoch: 4 step: 1345, loss is 0.010742690414190292\n",
      "epoch: 4 step: 1346, loss is 0.044702138751745224\n",
      "epoch: 4 step: 1347, loss is 0.09462840855121613\n",
      "epoch: 4 step: 1348, loss is 0.021170230582356453\n",
      "epoch: 4 step: 1349, loss is 0.15731331706047058\n",
      "epoch: 4 step: 1350, loss is 0.009092860855162144\n",
      "epoch: 4 step: 1351, loss is 0.025157978758215904\n",
      "epoch: 4 step: 1352, loss is 0.11126285791397095\n",
      "epoch: 4 step: 1353, loss is 0.005130168050527573\n",
      "epoch: 4 step: 1354, loss is 0.22295519709587097\n",
      "epoch: 4 step: 1355, loss is 0.016018567606806755\n",
      "epoch: 4 step: 1356, loss is 0.14722222089767456\n",
      "epoch: 4 step: 1357, loss is 0.00837577972561121\n",
      "epoch: 4 step: 1358, loss is 0.007146048825234175\n",
      "epoch: 4 step: 1359, loss is 0.0020494021009653807\n",
      "epoch: 4 step: 1360, loss is 0.12840008735656738\n",
      "epoch: 4 step: 1361, loss is 0.0009909601649269462\n",
      "epoch: 4 step: 1362, loss is 0.006750091910362244\n",
      "epoch: 4 step: 1363, loss is 0.1103285625576973\n",
      "epoch: 4 step: 1364, loss is 0.006717613432556391\n",
      "epoch: 4 step: 1365, loss is 0.003424709429964423\n",
      "epoch: 4 step: 1366, loss is 0.004756938200443983\n",
      "epoch: 4 step: 1367, loss is 0.0036195546854287386\n",
      "epoch: 4 step: 1368, loss is 0.002886441769078374\n",
      "epoch: 4 step: 1369, loss is 0.0418497659265995\n",
      "epoch: 4 step: 1370, loss is 0.024556145071983337\n",
      "epoch: 4 step: 1371, loss is 0.019096774980425835\n",
      "epoch: 4 step: 1372, loss is 0.0007991237798705697\n",
      "epoch: 4 step: 1373, loss is 0.008791469037532806\n",
      "epoch: 4 step: 1374, loss is 0.003999289125204086\n",
      "epoch: 4 step: 1375, loss is 0.02512810379266739\n",
      "epoch: 4 step: 1376, loss is 0.0017102722777053714\n",
      "epoch: 4 step: 1377, loss is 0.004887897986918688\n",
      "epoch: 4 step: 1378, loss is 0.001176097895950079\n",
      "epoch: 4 step: 1379, loss is 0.002391333691775799\n",
      "epoch: 4 step: 1380, loss is 0.047465089708566666\n",
      "epoch: 4 step: 1381, loss is 0.16349945962429047\n",
      "epoch: 4 step: 1382, loss is 0.00021844852017238736\n",
      "epoch: 4 step: 1383, loss is 0.057410094887018204\n",
      "epoch: 4 step: 1384, loss is 0.0050744554027915\n",
      "epoch: 4 step: 1385, loss is 0.0012867606710642576\n",
      "epoch: 4 step: 1386, loss is 0.009729940444231033\n",
      "epoch: 4 step: 1387, loss is 0.0056798821315169334\n",
      "epoch: 4 step: 1388, loss is 0.002010582946240902\n",
      "epoch: 4 step: 1389, loss is 0.0016429740935564041\n",
      "epoch: 4 step: 1390, loss is 0.021635983139276505\n",
      "epoch: 4 step: 1391, loss is 0.00040283225825987756\n",
      "epoch: 4 step: 1392, loss is 0.007127843331545591\n",
      "epoch: 4 step: 1393, loss is 0.006056913174688816\n",
      "epoch: 4 step: 1394, loss is 0.006250680424273014\n",
      "epoch: 4 step: 1395, loss is 0.005286618135869503\n",
      "epoch: 4 step: 1396, loss is 0.005461499560624361\n",
      "epoch: 4 step: 1397, loss is 0.061866313219070435\n",
      "epoch: 4 step: 1398, loss is 0.00030926227918826044\n",
      "epoch: 4 step: 1399, loss is 0.001589551568031311\n",
      "epoch: 4 step: 1400, loss is 0.003530400339514017\n",
      "epoch: 4 step: 1401, loss is 0.0047081708908081055\n",
      "epoch: 4 step: 1402, loss is 0.0013253212673589587\n",
      "epoch: 4 step: 1403, loss is 0.03261927142739296\n",
      "epoch: 4 step: 1404, loss is 0.0002458814706187695\n",
      "epoch: 4 step: 1405, loss is 0.00029830358107574284\n",
      "epoch: 4 step: 1406, loss is 0.010528444312512875\n",
      "epoch: 4 step: 1407, loss is 0.0064065223559737206\n",
      "epoch: 4 step: 1408, loss is 0.00889249611645937\n",
      "epoch: 4 step: 1409, loss is 0.007783412002027035\n",
      "epoch: 4 step: 1410, loss is 0.0015958631411194801\n",
      "epoch: 4 step: 1411, loss is 0.000604637956712395\n",
      "epoch: 4 step: 1412, loss is 0.0011955847730860114\n",
      "epoch: 4 step: 1413, loss is 0.0011627161875367165\n",
      "epoch: 4 step: 1414, loss is 0.0002982611767947674\n",
      "epoch: 4 step: 1415, loss is 0.20301498472690582\n",
      "epoch: 4 step: 1416, loss is 0.0029912181198596954\n",
      "epoch: 4 step: 1417, loss is 0.00043936673318967223\n",
      "epoch: 4 step: 1418, loss is 0.03929881379008293\n",
      "epoch: 4 step: 1419, loss is 0.03607584163546562\n",
      "epoch: 4 step: 1420, loss is 0.002690035616979003\n",
      "epoch: 4 step: 1421, loss is 0.008665956556797028\n",
      "epoch: 4 step: 1422, loss is 0.017651807516813278\n",
      "epoch: 4 step: 1423, loss is 0.03790539875626564\n",
      "epoch: 4 step: 1424, loss is 0.07560916244983673\n",
      "epoch: 4 step: 1425, loss is 0.017919274047017097\n",
      "epoch: 4 step: 1426, loss is 0.017308032140135765\n",
      "epoch: 4 step: 1427, loss is 0.03006606176495552\n",
      "epoch: 4 step: 1428, loss is 0.003024754114449024\n",
      "epoch: 4 step: 1429, loss is 0.0013118364149704576\n",
      "epoch: 4 step: 1430, loss is 0.00016692362260073423\n",
      "epoch: 4 step: 1431, loss is 0.0017430851003155112\n",
      "epoch: 4 step: 1432, loss is 0.0008739919867366552\n",
      "epoch: 4 step: 1433, loss is 0.003262600628659129\n",
      "epoch: 4 step: 1434, loss is 0.04156062379479408\n",
      "epoch: 4 step: 1435, loss is 0.1627538800239563\n",
      "epoch: 4 step: 1436, loss is 0.07228639721870422\n",
      "epoch: 4 step: 1437, loss is 0.00268977670930326\n",
      "epoch: 4 step: 1438, loss is 0.006769660860300064\n",
      "epoch: 4 step: 1439, loss is 0.04509977623820305\n",
      "epoch: 4 step: 1440, loss is 0.004500728566199541\n",
      "epoch: 4 step: 1441, loss is 0.0067522041499614716\n",
      "epoch: 4 step: 1442, loss is 0.00808751955628395\n",
      "epoch: 4 step: 1443, loss is 0.009720827452838421\n",
      "epoch: 4 step: 1444, loss is 0.050281185656785965\n",
      "epoch: 4 step: 1445, loss is 0.006390785798430443\n",
      "epoch: 4 step: 1446, loss is 0.0015553048579022288\n",
      "epoch: 4 step: 1447, loss is 0.00447640847414732\n",
      "epoch: 4 step: 1448, loss is 0.00686847185716033\n",
      "epoch: 4 step: 1449, loss is 0.00352654536254704\n",
      "epoch: 4 step: 1450, loss is 0.020193025469779968\n",
      "epoch: 4 step: 1451, loss is 0.017226120457053185\n",
      "epoch: 4 step: 1452, loss is 0.0005209261435084045\n",
      "epoch: 4 step: 1453, loss is 0.0077170454896986485\n",
      "epoch: 4 step: 1454, loss is 0.004885481204837561\n",
      "epoch: 4 step: 1455, loss is 0.06534203886985779\n",
      "epoch: 4 step: 1456, loss is 0.004026639275252819\n",
      "epoch: 4 step: 1457, loss is 0.0032950981985777617\n",
      "epoch: 4 step: 1458, loss is 0.0071930051781237125\n",
      "epoch: 4 step: 1459, loss is 0.0006412210641428828\n",
      "epoch: 4 step: 1460, loss is 0.15674808621406555\n",
      "epoch: 4 step: 1461, loss is 0.029316553846001625\n",
      "epoch: 4 step: 1462, loss is 0.0104459123685956\n",
      "epoch: 4 step: 1463, loss is 0.0826745256781578\n",
      "epoch: 4 step: 1464, loss is 0.0020561821293085814\n",
      "epoch: 4 step: 1465, loss is 0.17765633761882782\n",
      "epoch: 4 step: 1466, loss is 0.03439132869243622\n",
      "epoch: 4 step: 1467, loss is 0.006753295660018921\n",
      "epoch: 4 step: 1468, loss is 0.06744690984487534\n",
      "epoch: 4 step: 1469, loss is 0.00039895004010759294\n",
      "epoch: 4 step: 1470, loss is 0.16150608658790588\n",
      "epoch: 4 step: 1471, loss is 0.13514813780784607\n",
      "epoch: 4 step: 1472, loss is 0.0005391113227233291\n",
      "epoch: 4 step: 1473, loss is 0.20500776171684265\n",
      "epoch: 4 step: 1474, loss is 0.002537833759561181\n",
      "epoch: 4 step: 1475, loss is 0.08460589498281479\n",
      "epoch: 4 step: 1476, loss is 0.11699458211660385\n",
      "epoch: 4 step: 1477, loss is 0.0004295420949347317\n",
      "epoch: 4 step: 1478, loss is 0.000750498496927321\n",
      "epoch: 4 step: 1479, loss is 0.0038767452351748943\n",
      "epoch: 4 step: 1480, loss is 0.0034658450167626143\n",
      "epoch: 4 step: 1481, loss is 0.08408185094594955\n",
      "epoch: 4 step: 1482, loss is 0.026305677369236946\n",
      "epoch: 4 step: 1483, loss is 0.009260056540369987\n",
      "epoch: 4 step: 1484, loss is 0.11053884774446487\n",
      "epoch: 4 step: 1485, loss is 0.004554505925625563\n",
      "epoch: 4 step: 1486, loss is 0.1354644000530243\n",
      "epoch: 4 step: 1487, loss is 0.002417780691757798\n",
      "epoch: 4 step: 1488, loss is 0.002304350957274437\n",
      "epoch: 4 step: 1489, loss is 0.0031886701472103596\n",
      "epoch: 4 step: 1490, loss is 0.23186755180358887\n",
      "epoch: 4 step: 1491, loss is 0.1070195734500885\n",
      "epoch: 4 step: 1492, loss is 0.052520401775836945\n",
      "epoch: 4 step: 1493, loss is 0.03270060941576958\n",
      "epoch: 4 step: 1494, loss is 0.0032064332626760006\n",
      "epoch: 4 step: 1495, loss is 0.030994713306427002\n",
      "epoch: 4 step: 1496, loss is 0.12970086932182312\n",
      "epoch: 4 step: 1497, loss is 0.046243827790021896\n",
      "epoch: 4 step: 1498, loss is 0.0076856049709022045\n",
      "epoch: 4 step: 1499, loss is 0.013280784711241722\n",
      "epoch: 4 step: 1500, loss is 0.00023903726832941175\n",
      "epoch: 4 step: 1501, loss is 0.02516288124024868\n",
      "epoch: 4 step: 1502, loss is 0.039163630455732346\n",
      "epoch: 4 step: 1503, loss is 0.0014054797356948256\n",
      "epoch: 4 step: 1504, loss is 0.0034097256138920784\n",
      "epoch: 4 step: 1505, loss is 0.02747713401913643\n",
      "epoch: 4 step: 1506, loss is 0.04414645954966545\n",
      "epoch: 4 step: 1507, loss is 0.0006536009022966027\n",
      "epoch: 4 step: 1508, loss is 0.0009647490223869681\n",
      "epoch: 4 step: 1509, loss is 0.002643672050908208\n",
      "epoch: 4 step: 1510, loss is 0.04377596080303192\n",
      "epoch: 4 step: 1511, loss is 0.047309037297964096\n",
      "epoch: 4 step: 1512, loss is 0.0006313670892268419\n",
      "epoch: 4 step: 1513, loss is 0.005196143873035908\n",
      "epoch: 4 step: 1514, loss is 0.0033844702411442995\n",
      "epoch: 4 step: 1515, loss is 0.0011597475968301296\n",
      "epoch: 4 step: 1516, loss is 0.006699454039335251\n",
      "epoch: 4 step: 1517, loss is 0.014891603961586952\n",
      "epoch: 4 step: 1518, loss is 0.024758730083703995\n",
      "epoch: 4 step: 1519, loss is 0.08552388846874237\n",
      "epoch: 4 step: 1520, loss is 0.02701268531382084\n",
      "epoch: 4 step: 1521, loss is 0.0019781955052167177\n",
      "epoch: 4 step: 1522, loss is 0.0010742362355813384\n",
      "epoch: 4 step: 1523, loss is 0.01933400146663189\n",
      "epoch: 4 step: 1524, loss is 0.1039418950676918\n",
      "epoch: 4 step: 1525, loss is 0.004794010426849127\n",
      "epoch: 4 step: 1526, loss is 0.0037080515176057816\n",
      "epoch: 4 step: 1527, loss is 0.018023746088147163\n",
      "epoch: 4 step: 1528, loss is 0.0008605788461863995\n",
      "epoch: 4 step: 1529, loss is 0.003445586422458291\n",
      "epoch: 4 step: 1530, loss is 0.0019781882874667645\n",
      "epoch: 4 step: 1531, loss is 0.10585864633321762\n",
      "epoch: 4 step: 1532, loss is 0.05379795655608177\n",
      "epoch: 4 step: 1533, loss is 0.1847972571849823\n",
      "epoch: 4 step: 1534, loss is 0.0008669078233651817\n",
      "epoch: 4 step: 1535, loss is 0.032463181763887405\n",
      "epoch: 4 step: 1536, loss is 0.0012456594267860055\n",
      "epoch: 4 step: 1537, loss is 0.0001645366137381643\n",
      "epoch: 4 step: 1538, loss is 0.009337563998997211\n",
      "epoch: 4 step: 1539, loss is 0.0010000288020819426\n",
      "epoch: 4 step: 1540, loss is 0.0004762775788549334\n",
      "epoch: 4 step: 1541, loss is 0.14347009360790253\n",
      "epoch: 4 step: 1542, loss is 0.01007332094013691\n",
      "epoch: 4 step: 1543, loss is 0.0021939557045698166\n",
      "epoch: 4 step: 1544, loss is 0.013188772834837437\n",
      "epoch: 4 step: 1545, loss is 0.011345301754772663\n",
      "epoch: 4 step: 1546, loss is 0.01768590323626995\n",
      "epoch: 4 step: 1547, loss is 0.07005039602518082\n",
      "epoch: 4 step: 1548, loss is 0.004157357383519411\n",
      "epoch: 4 step: 1549, loss is 0.04439983144402504\n",
      "epoch: 4 step: 1550, loss is 0.0025402549654245377\n",
      "epoch: 4 step: 1551, loss is 0.024356666952371597\n",
      "epoch: 4 step: 1552, loss is 0.015700889751315117\n",
      "epoch: 4 step: 1553, loss is 0.010508628562092781\n",
      "epoch: 4 step: 1554, loss is 0.00034613427123986185\n",
      "epoch: 4 step: 1555, loss is 0.2332025021314621\n",
      "epoch: 4 step: 1556, loss is 0.035226356238126755\n",
      "epoch: 4 step: 1557, loss is 0.0031901139300316572\n",
      "epoch: 4 step: 1558, loss is 0.017206275835633278\n",
      "epoch: 4 step: 1559, loss is 0.0016623089322820306\n",
      "epoch: 4 step: 1560, loss is 0.0004166749131400138\n",
      "epoch: 4 step: 1561, loss is 0.009845204651355743\n",
      "epoch: 4 step: 1562, loss is 0.008010521531105042\n",
      "epoch: 4 step: 1563, loss is 0.019114861264824867\n",
      "epoch: 4 step: 1564, loss is 0.008503698743879795\n",
      "epoch: 4 step: 1565, loss is 0.06972797960042953\n",
      "epoch: 4 step: 1566, loss is 0.05272730067372322\n",
      "epoch: 4 step: 1567, loss is 0.02797667309641838\n",
      "epoch: 4 step: 1568, loss is 0.015009782277047634\n",
      "epoch: 4 step: 1569, loss is 0.12036330252885818\n",
      "epoch: 4 step: 1570, loss is 0.1361939162015915\n",
      "epoch: 4 step: 1571, loss is 0.002852379810065031\n",
      "epoch: 4 step: 1572, loss is 0.00374863063916564\n",
      "epoch: 4 step: 1573, loss is 0.0020098418463021517\n",
      "epoch: 4 step: 1574, loss is 0.002649208065122366\n",
      "epoch: 4 step: 1575, loss is 0.04930237680673599\n",
      "epoch: 4 step: 1576, loss is 0.039478231221437454\n",
      "epoch: 4 step: 1577, loss is 0.43825528025627136\n",
      "epoch: 4 step: 1578, loss is 0.00013281444262247533\n",
      "epoch: 4 step: 1579, loss is 0.0011508907191455364\n",
      "epoch: 4 step: 1580, loss is 0.08045592159032822\n",
      "epoch: 4 step: 1581, loss is 0.0015044942265376449\n",
      "epoch: 4 step: 1582, loss is 0.03743606433272362\n",
      "epoch: 4 step: 1583, loss is 0.02618025243282318\n",
      "epoch: 4 step: 1584, loss is 0.01902441680431366\n",
      "epoch: 4 step: 1585, loss is 0.011017770506441593\n",
      "epoch: 4 step: 1586, loss is 0.03779812157154083\n",
      "epoch: 4 step: 1587, loss is 0.005850554909557104\n",
      "epoch: 4 step: 1588, loss is 0.013705207034945488\n",
      "epoch: 4 step: 1589, loss is 0.001893207081593573\n",
      "epoch: 4 step: 1590, loss is 0.09658381342887878\n",
      "epoch: 4 step: 1591, loss is 0.00039566392661072314\n",
      "epoch: 4 step: 1592, loss is 0.27853789925575256\n",
      "epoch: 4 step: 1593, loss is 0.003149858908727765\n",
      "epoch: 4 step: 1594, loss is 0.07524507492780685\n",
      "epoch: 4 step: 1595, loss is 0.001635395223274827\n",
      "epoch: 4 step: 1596, loss is 0.0037136320024728775\n",
      "epoch: 4 step: 1597, loss is 0.11032824218273163\n",
      "epoch: 4 step: 1598, loss is 0.0388496033847332\n",
      "epoch: 4 step: 1599, loss is 0.038080375641584396\n",
      "epoch: 4 step: 1600, loss is 0.26666173338890076\n",
      "epoch: 4 step: 1601, loss is 0.006766076665371656\n",
      "epoch: 4 step: 1602, loss is 0.0037738000974059105\n",
      "epoch: 4 step: 1603, loss is 0.1337577849626541\n",
      "epoch: 4 step: 1604, loss is 0.012962338514626026\n",
      "epoch: 4 step: 1605, loss is 0.013554465025663376\n",
      "epoch: 4 step: 1606, loss is 0.0019195512868463993\n",
      "epoch: 4 step: 1607, loss is 0.008531330153346062\n",
      "epoch: 4 step: 1608, loss is 0.027059387415647507\n",
      "epoch: 4 step: 1609, loss is 0.08042716979980469\n",
      "epoch: 4 step: 1610, loss is 0.04799022525548935\n",
      "epoch: 4 step: 1611, loss is 0.06614813208580017\n",
      "epoch: 4 step: 1612, loss is 0.06033678725361824\n",
      "epoch: 4 step: 1613, loss is 0.0037655970081686974\n",
      "epoch: 4 step: 1614, loss is 0.0005764898960478604\n",
      "epoch: 4 step: 1615, loss is 0.06058725714683533\n",
      "epoch: 4 step: 1616, loss is 0.004556445870548487\n",
      "epoch: 4 step: 1617, loss is 0.06022285670042038\n",
      "epoch: 4 step: 1618, loss is 0.008413681760430336\n",
      "epoch: 4 step: 1619, loss is 0.029464581981301308\n",
      "epoch: 4 step: 1620, loss is 0.07277771830558777\n",
      "epoch: 4 step: 1621, loss is 0.014003828167915344\n",
      "epoch: 4 step: 1622, loss is 0.0223519466817379\n",
      "epoch: 4 step: 1623, loss is 0.0015450363280251622\n",
      "epoch: 4 step: 1624, loss is 0.001791923539713025\n",
      "epoch: 4 step: 1625, loss is 0.07173492014408112\n",
      "epoch: 4 step: 1626, loss is 0.023658446967601776\n",
      "epoch: 4 step: 1627, loss is 0.006588053423911333\n",
      "epoch: 4 step: 1628, loss is 0.008102948777377605\n",
      "epoch: 4 step: 1629, loss is 0.006027217023074627\n",
      "epoch: 4 step: 1630, loss is 0.0009757226798683405\n",
      "epoch: 4 step: 1631, loss is 0.06250090152025223\n",
      "epoch: 4 step: 1632, loss is 0.010832544416189194\n",
      "epoch: 4 step: 1633, loss is 0.004252823535352945\n",
      "epoch: 4 step: 1634, loss is 0.04285241663455963\n",
      "epoch: 4 step: 1635, loss is 0.002288667019456625\n",
      "epoch: 4 step: 1636, loss is 0.00780413206666708\n",
      "epoch: 4 step: 1637, loss is 0.008034717291593552\n",
      "epoch: 4 step: 1638, loss is 0.0786486268043518\n",
      "epoch: 4 step: 1639, loss is 0.03632235527038574\n",
      "epoch: 4 step: 1640, loss is 0.18628643453121185\n",
      "epoch: 4 step: 1641, loss is 0.36882373690605164\n",
      "epoch: 4 step: 1642, loss is 0.004864660557359457\n",
      "epoch: 4 step: 1643, loss is 0.00012957329454366118\n",
      "epoch: 4 step: 1644, loss is 0.0010791337117552757\n",
      "epoch: 4 step: 1645, loss is 0.03295060619711876\n",
      "epoch: 4 step: 1646, loss is 0.0002622774918563664\n",
      "epoch: 4 step: 1647, loss is 0.09210184216499329\n",
      "epoch: 4 step: 1648, loss is 0.23147214949131012\n",
      "epoch: 4 step: 1649, loss is 0.03917800262570381\n",
      "epoch: 4 step: 1650, loss is 0.0019679286051541567\n",
      "epoch: 4 step: 1651, loss is 0.10817496478557587\n",
      "epoch: 4 step: 1652, loss is 0.05286706984043121\n",
      "epoch: 4 step: 1653, loss is 0.021823890507221222\n",
      "epoch: 4 step: 1654, loss is 0.0007174380007199943\n",
      "epoch: 4 step: 1655, loss is 0.001964487601071596\n",
      "epoch: 4 step: 1656, loss is 0.0019851960241794586\n",
      "epoch: 4 step: 1657, loss is 0.08122509717941284\n",
      "epoch: 4 step: 1658, loss is 0.023166874423623085\n",
      "epoch: 4 step: 1659, loss is 0.001309215440414846\n",
      "epoch: 4 step: 1660, loss is 0.008926118724048138\n",
      "epoch: 4 step: 1661, loss is 0.010776661336421967\n",
      "epoch: 4 step: 1662, loss is 0.0028876238502562046\n",
      "epoch: 4 step: 1663, loss is 0.17925302684307098\n",
      "epoch: 4 step: 1664, loss is 0.0009509279043413699\n",
      "epoch: 4 step: 1665, loss is 0.010478662326931953\n",
      "epoch: 4 step: 1666, loss is 0.0039687794633209705\n",
      "epoch: 4 step: 1667, loss is 0.04445513337850571\n",
      "epoch: 4 step: 1668, loss is 0.010719106532633305\n",
      "epoch: 4 step: 1669, loss is 0.01221186202019453\n",
      "epoch: 4 step: 1670, loss is 0.000781923474278301\n",
      "epoch: 4 step: 1671, loss is 0.046395234763622284\n",
      "epoch: 4 step: 1672, loss is 0.03953631594777107\n",
      "epoch: 4 step: 1673, loss is 0.023488476872444153\n",
      "epoch: 4 step: 1674, loss is 0.07043886184692383\n",
      "epoch: 4 step: 1675, loss is 0.014343883842229843\n",
      "epoch: 4 step: 1676, loss is 0.0200651902705431\n",
      "epoch: 4 step: 1677, loss is 0.004214068409055471\n",
      "epoch: 4 step: 1678, loss is 0.0020831956062465906\n",
      "epoch: 4 step: 1679, loss is 0.01564144156873226\n",
      "epoch: 4 step: 1680, loss is 0.017252089455723763\n",
      "epoch: 4 step: 1681, loss is 0.001294377725571394\n",
      "epoch: 4 step: 1682, loss is 0.026908203959465027\n",
      "epoch: 4 step: 1683, loss is 0.00834207609295845\n",
      "epoch: 4 step: 1684, loss is 0.062325190752744675\n",
      "epoch: 4 step: 1685, loss is 0.07291708886623383\n",
      "epoch: 4 step: 1686, loss is 0.006518908776342869\n",
      "epoch: 4 step: 1687, loss is 0.01167112123221159\n",
      "epoch: 4 step: 1688, loss is 0.044379767030477524\n",
      "epoch: 4 step: 1689, loss is 0.006033766083419323\n",
      "epoch: 4 step: 1690, loss is 0.04191038757562637\n",
      "epoch: 4 step: 1691, loss is 0.05418097972869873\n",
      "epoch: 4 step: 1692, loss is 0.010658753104507923\n",
      "epoch: 4 step: 1693, loss is 0.016665471717715263\n",
      "epoch: 4 step: 1694, loss is 0.003534014569595456\n",
      "epoch: 4 step: 1695, loss is 0.024194873869419098\n",
      "epoch: 4 step: 1696, loss is 0.0007342927856370807\n",
      "epoch: 4 step: 1697, loss is 0.008344178088009357\n",
      "epoch: 4 step: 1698, loss is 0.003458998166024685\n",
      "epoch: 4 step: 1699, loss is 0.0012589993420988321\n",
      "epoch: 4 step: 1700, loss is 0.17504633963108063\n",
      "epoch: 4 step: 1701, loss is 0.00048347061965614557\n",
      "epoch: 4 step: 1702, loss is 0.0002952491049654782\n",
      "epoch: 4 step: 1703, loss is 0.012732146307826042\n",
      "epoch: 4 step: 1704, loss is 0.012144933454692364\n",
      "epoch: 4 step: 1705, loss is 0.00043121763155795634\n",
      "epoch: 4 step: 1706, loss is 0.18676982820034027\n",
      "epoch: 4 step: 1707, loss is 0.019910264760255814\n",
      "epoch: 4 step: 1708, loss is 0.04194637015461922\n",
      "epoch: 4 step: 1709, loss is 0.001969838747754693\n",
      "epoch: 4 step: 1710, loss is 0.0005331695429049432\n",
      "epoch: 4 step: 1711, loss is 0.03975951299071312\n",
      "epoch: 4 step: 1712, loss is 0.0011692977277562022\n",
      "epoch: 4 step: 1713, loss is 0.030520807951688766\n",
      "epoch: 4 step: 1714, loss is 0.00012311438331380486\n",
      "epoch: 4 step: 1715, loss is 0.10797062516212463\n",
      "epoch: 4 step: 1716, loss is 0.05124708265066147\n",
      "epoch: 4 step: 1717, loss is 0.0005963276489637792\n",
      "epoch: 4 step: 1718, loss is 0.004891374614089727\n",
      "epoch: 4 step: 1719, loss is 0.04330716282129288\n",
      "epoch: 4 step: 1720, loss is 0.1022389829158783\n",
      "epoch: 4 step: 1721, loss is 0.05454014614224434\n",
      "epoch: 4 step: 1722, loss is 0.002163412980735302\n",
      "epoch: 4 step: 1723, loss is 0.010380255989730358\n",
      "epoch: 4 step: 1724, loss is 0.00640589976683259\n",
      "epoch: 4 step: 1725, loss is 0.02308957464993\n",
      "epoch: 4 step: 1726, loss is 0.04656672477722168\n",
      "epoch: 4 step: 1727, loss is 0.027880504727363586\n",
      "epoch: 4 step: 1728, loss is 0.0007674943772144616\n",
      "epoch: 4 step: 1729, loss is 0.01594392955303192\n",
      "epoch: 4 step: 1730, loss is 0.026705212891101837\n",
      "epoch: 4 step: 1731, loss is 0.014371159486472607\n",
      "epoch: 4 step: 1732, loss is 0.015774572268128395\n",
      "epoch: 4 step: 1733, loss is 0.008854038082063198\n",
      "epoch: 4 step: 1734, loss is 0.0008588070049881935\n",
      "epoch: 4 step: 1735, loss is 0.0073051354847848415\n",
      "epoch: 4 step: 1736, loss is 0.008631604723632336\n",
      "epoch: 4 step: 1737, loss is 0.06045319885015488\n",
      "epoch: 4 step: 1738, loss is 0.030804801732301712\n",
      "epoch: 4 step: 1739, loss is 0.02955355867743492\n",
      "epoch: 4 step: 1740, loss is 0.009680784307420254\n",
      "epoch: 4 step: 1741, loss is 0.01279442198574543\n",
      "epoch: 4 step: 1742, loss is 0.0077717662788927555\n",
      "epoch: 4 step: 1743, loss is 0.0028791206423193216\n",
      "epoch: 4 step: 1744, loss is 0.03044239431619644\n",
      "epoch: 4 step: 1745, loss is 0.03124721720814705\n",
      "epoch: 4 step: 1746, loss is 0.003241488942876458\n",
      "epoch: 4 step: 1747, loss is 0.007263627834618092\n",
      "epoch: 4 step: 1748, loss is 0.13172121345996857\n",
      "epoch: 4 step: 1749, loss is 0.10070717334747314\n",
      "epoch: 4 step: 1750, loss is 0.0004883385263383389\n",
      "epoch: 4 step: 1751, loss is 0.0027766197454184294\n",
      "epoch: 4 step: 1752, loss is 0.0018511550733819604\n",
      "epoch: 4 step: 1753, loss is 0.08741818368434906\n",
      "epoch: 4 step: 1754, loss is 0.0006930379313416779\n",
      "epoch: 4 step: 1755, loss is 0.09153137356042862\n",
      "epoch: 4 step: 1756, loss is 0.022328009828925133\n",
      "epoch: 4 step: 1757, loss is 0.005852258764207363\n",
      "epoch: 4 step: 1758, loss is 0.05949150770902634\n",
      "epoch: 4 step: 1759, loss is 0.1231188029050827\n",
      "epoch: 4 step: 1760, loss is 0.01325849536806345\n",
      "epoch: 4 step: 1761, loss is 0.05948676913976669\n",
      "epoch: 4 step: 1762, loss is 0.0006029735668562353\n",
      "epoch: 4 step: 1763, loss is 0.002881032647565007\n",
      "epoch: 4 step: 1764, loss is 0.0416237972676754\n",
      "epoch: 4 step: 1765, loss is 0.016990283504128456\n",
      "epoch: 4 step: 1766, loss is 0.012409880757331848\n",
      "epoch: 4 step: 1767, loss is 0.003444123547524214\n",
      "epoch: 4 step: 1768, loss is 0.0021657708566635847\n",
      "epoch: 4 step: 1769, loss is 0.004157978110015392\n",
      "epoch: 4 step: 1770, loss is 0.003030339954420924\n",
      "epoch: 4 step: 1771, loss is 0.00023893668549135327\n",
      "epoch: 4 step: 1772, loss is 0.0025612281169742346\n",
      "epoch: 4 step: 1773, loss is 0.047532886266708374\n",
      "epoch: 4 step: 1774, loss is 0.07670575380325317\n",
      "epoch: 4 step: 1775, loss is 0.032253194600343704\n",
      "epoch: 4 step: 1776, loss is 0.15797823667526245\n",
      "epoch: 4 step: 1777, loss is 0.003627417143434286\n",
      "epoch: 4 step: 1778, loss is 0.002283578272908926\n",
      "epoch: 4 step: 1779, loss is 0.004608247894793749\n",
      "epoch: 4 step: 1780, loss is 0.020154181867837906\n",
      "epoch: 4 step: 1781, loss is 0.0004594054480548948\n",
      "epoch: 4 step: 1782, loss is 0.005580966826528311\n",
      "epoch: 4 step: 1783, loss is 0.007406726013869047\n",
      "epoch: 4 step: 1784, loss is 0.006628344301134348\n",
      "epoch: 4 step: 1785, loss is 0.004414475057274103\n",
      "epoch: 4 step: 1786, loss is 0.00038568474701605737\n",
      "epoch: 4 step: 1787, loss is 0.2199295610189438\n",
      "epoch: 4 step: 1788, loss is 0.03992971405386925\n",
      "epoch: 4 step: 1789, loss is 0.0020343095529824495\n",
      "epoch: 4 step: 1790, loss is 0.0007521405350416899\n",
      "epoch: 4 step: 1791, loss is 0.0014890501042827964\n",
      "epoch: 4 step: 1792, loss is 0.0014414499746635556\n",
      "epoch: 4 step: 1793, loss is 0.03168630599975586\n",
      "epoch: 4 step: 1794, loss is 0.007243441417813301\n",
      "epoch: 4 step: 1795, loss is 0.12076175212860107\n",
      "epoch: 4 step: 1796, loss is 0.16662009060382843\n",
      "epoch: 4 step: 1797, loss is 0.0017004897817969322\n",
      "epoch: 4 step: 1798, loss is 0.0021106707863509655\n",
      "epoch: 4 step: 1799, loss is 0.0038665414322167635\n",
      "epoch: 4 step: 1800, loss is 0.008887163363397121\n",
      "epoch: 4 step: 1801, loss is 0.008318830281496048\n",
      "epoch: 4 step: 1802, loss is 0.006743639707565308\n",
      "epoch: 4 step: 1803, loss is 0.1478285938501358\n",
      "epoch: 4 step: 1804, loss is 0.023621084168553352\n",
      "epoch: 4 step: 1805, loss is 0.005502337124198675\n",
      "epoch: 4 step: 1806, loss is 0.07904968410730362\n",
      "epoch: 4 step: 1807, loss is 0.00012490789231378585\n",
      "epoch: 4 step: 1808, loss is 0.02250470593571663\n",
      "epoch: 4 step: 1809, loss is 0.0007227087044157088\n",
      "epoch: 4 step: 1810, loss is 0.005113739054650068\n",
      "epoch: 4 step: 1811, loss is 0.016009652987122536\n",
      "epoch: 4 step: 1812, loss is 0.09844667464494705\n",
      "epoch: 4 step: 1813, loss is 0.005049851723015308\n",
      "epoch: 4 step: 1814, loss is 0.0009139585890807211\n",
      "epoch: 4 step: 1815, loss is 0.08794862776994705\n",
      "epoch: 4 step: 1816, loss is 0.001822213176637888\n",
      "epoch: 4 step: 1817, loss is 0.00027298470376990736\n",
      "epoch: 4 step: 1818, loss is 0.036682773381471634\n",
      "epoch: 4 step: 1819, loss is 0.11595712602138519\n",
      "epoch: 4 step: 1820, loss is 0.04471995308995247\n",
      "epoch: 4 step: 1821, loss is 0.0006317229708656669\n",
      "epoch: 4 step: 1822, loss is 0.001975118648260832\n",
      "epoch: 4 step: 1823, loss is 0.002420097589492798\n",
      "epoch: 4 step: 1824, loss is 0.0056482465006411076\n",
      "epoch: 4 step: 1825, loss is 0.16200411319732666\n",
      "epoch: 4 step: 1826, loss is 0.0036650942638516426\n",
      "epoch: 4 step: 1827, loss is 0.038413211703300476\n",
      "epoch: 4 step: 1828, loss is 0.00378700764849782\n",
      "epoch: 4 step: 1829, loss is 0.11910887062549591\n",
      "epoch: 4 step: 1830, loss is 0.008042339235544205\n",
      "epoch: 4 step: 1831, loss is 0.062377214431762695\n",
      "epoch: 4 step: 1832, loss is 0.009316337294876575\n",
      "epoch: 4 step: 1833, loss is 0.004389253444969654\n",
      "epoch: 4 step: 1834, loss is 0.06511562317609787\n",
      "epoch: 4 step: 1835, loss is 0.01309317909181118\n",
      "epoch: 4 step: 1836, loss is 0.12567871809005737\n",
      "epoch: 4 step: 1837, loss is 0.11300001293420792\n",
      "epoch: 4 step: 1838, loss is 0.005569715052843094\n",
      "epoch: 4 step: 1839, loss is 0.002664947649464011\n",
      "epoch: 4 step: 1840, loss is 0.07049158960580826\n",
      "epoch: 4 step: 1841, loss is 0.07175777107477188\n",
      "epoch: 4 step: 1842, loss is 0.07009045034646988\n",
      "epoch: 4 step: 1843, loss is 0.13389261066913605\n",
      "epoch: 4 step: 1844, loss is 0.0010856636799871922\n",
      "epoch: 4 step: 1845, loss is 0.024003200232982635\n",
      "epoch: 4 step: 1846, loss is 0.1796882301568985\n",
      "epoch: 4 step: 1847, loss is 0.008644035086035728\n",
      "epoch: 4 step: 1848, loss is 0.008451925590634346\n",
      "epoch: 4 step: 1849, loss is 0.005288204178214073\n",
      "epoch: 4 step: 1850, loss is 0.012898847460746765\n",
      "epoch: 4 step: 1851, loss is 0.006005796138197184\n",
      "epoch: 4 step: 1852, loss is 0.009216506965458393\n",
      "epoch: 4 step: 1853, loss is 0.019311929121613503\n",
      "epoch: 4 step: 1854, loss is 0.0011213395046070218\n",
      "epoch: 4 step: 1855, loss is 0.042824048548936844\n",
      "epoch: 4 step: 1856, loss is 0.005439890082925558\n",
      "epoch: 4 step: 1857, loss is 0.006113073788583279\n",
      "epoch: 4 step: 1858, loss is 0.007071881555020809\n",
      "epoch: 4 step: 1859, loss is 0.020907079800963402\n",
      "epoch: 4 step: 1860, loss is 0.013676325790584087\n",
      "epoch: 4 step: 1861, loss is 0.001284216996282339\n",
      "epoch: 4 step: 1862, loss is 0.0016643463168293238\n",
      "epoch: 4 step: 1863, loss is 0.017960621044039726\n",
      "epoch: 4 step: 1864, loss is 0.0018994674319401383\n",
      "epoch: 4 step: 1865, loss is 0.04508793726563454\n",
      "epoch: 4 step: 1866, loss is 0.17521022260189056\n",
      "epoch: 4 step: 1867, loss is 0.021312320604920387\n",
      "epoch: 4 step: 1868, loss is 0.0016913233557716012\n",
      "epoch: 4 step: 1869, loss is 0.006739120930433273\n",
      "epoch: 4 step: 1870, loss is 0.003281439421698451\n",
      "epoch: 4 step: 1871, loss is 0.015978362411260605\n",
      "epoch: 4 step: 1872, loss is 0.0024948909413069487\n",
      "epoch: 4 step: 1873, loss is 0.012357802130281925\n",
      "epoch: 4 step: 1874, loss is 0.028410600498318672\n",
      "epoch: 4 step: 1875, loss is 0.22265563905239105\n",
      "epoch: 5 step: 1, loss is 0.0005941971321590245\n",
      "epoch: 5 step: 2, loss is 0.025704745203256607\n",
      "epoch: 5 step: 3, loss is 0.0055290875025093555\n",
      "epoch: 5 step: 4, loss is 0.0032256394624710083\n",
      "epoch: 5 step: 5, loss is 0.07770774513483047\n",
      "epoch: 5 step: 6, loss is 0.008489815518260002\n",
      "epoch: 5 step: 7, loss is 0.0006732948240824044\n",
      "epoch: 5 step: 8, loss is 0.0007251722272485495\n",
      "epoch: 5 step: 9, loss is 0.03438284993171692\n",
      "epoch: 5 step: 10, loss is 0.033836495131254196\n",
      "epoch: 5 step: 11, loss is 0.04388423636555672\n",
      "epoch: 5 step: 12, loss is 0.031075621023774147\n",
      "epoch: 5 step: 13, loss is 0.07126542925834656\n",
      "epoch: 5 step: 14, loss is 0.0009918641299009323\n",
      "epoch: 5 step: 15, loss is 0.011003212071955204\n",
      "epoch: 5 step: 16, loss is 0.013569916598498821\n",
      "epoch: 5 step: 17, loss is 0.002977689728140831\n",
      "epoch: 5 step: 18, loss is 0.006068017333745956\n",
      "epoch: 5 step: 19, loss is 0.0008958338294178247\n",
      "epoch: 5 step: 20, loss is 0.008295697160065174\n",
      "epoch: 5 step: 21, loss is 0.013804478570818901\n",
      "epoch: 5 step: 22, loss is 0.0026133274659514427\n",
      "epoch: 5 step: 23, loss is 0.0008933473727665842\n",
      "epoch: 5 step: 24, loss is 0.0028459280729293823\n",
      "epoch: 5 step: 25, loss is 0.13415507972240448\n",
      "epoch: 5 step: 26, loss is 0.0009405987220816314\n",
      "epoch: 5 step: 27, loss is 0.0494205467402935\n",
      "epoch: 5 step: 28, loss is 0.0032008972484618425\n",
      "epoch: 5 step: 29, loss is 0.0005736321327276528\n",
      "epoch: 5 step: 30, loss is 0.03068862482905388\n",
      "epoch: 5 step: 31, loss is 0.023545725271105766\n",
      "epoch: 5 step: 32, loss is 0.027481749653816223\n",
      "epoch: 5 step: 33, loss is 0.0004318967694416642\n",
      "epoch: 5 step: 34, loss is 0.0007844847859814763\n",
      "epoch: 5 step: 35, loss is 0.009811270982027054\n",
      "epoch: 5 step: 36, loss is 0.00034489089739508927\n",
      "epoch: 5 step: 37, loss is 0.010390514507889748\n",
      "epoch: 5 step: 38, loss is 0.023395342752337456\n",
      "epoch: 5 step: 39, loss is 0.00017641267913859338\n",
      "epoch: 5 step: 40, loss is 0.01582011952996254\n",
      "epoch: 5 step: 41, loss is 0.0024275495670735836\n",
      "epoch: 5 step: 42, loss is 0.012748930603265762\n",
      "epoch: 5 step: 43, loss is 0.0008272017003037035\n",
      "epoch: 5 step: 44, loss is 0.003187908325344324\n",
      "epoch: 5 step: 45, loss is 0.00256129028275609\n",
      "epoch: 5 step: 46, loss is 0.0003910113009624183\n",
      "epoch: 5 step: 47, loss is 0.010157440789043903\n",
      "epoch: 5 step: 48, loss is 0.004910790361464024\n",
      "epoch: 5 step: 49, loss is 0.006125096697360277\n",
      "epoch: 5 step: 50, loss is 0.005316397175192833\n",
      "epoch: 5 step: 51, loss is 0.1251009851694107\n",
      "epoch: 5 step: 52, loss is 0.04721265658736229\n",
      "epoch: 5 step: 53, loss is 0.017210382968187332\n",
      "epoch: 5 step: 54, loss is 0.024144036695361137\n",
      "epoch: 5 step: 55, loss is 0.006593606900423765\n",
      "epoch: 5 step: 56, loss is 0.011767429299652576\n",
      "epoch: 5 step: 57, loss is 0.0012253052555024624\n",
      "epoch: 5 step: 58, loss is 0.0018202201463282108\n",
      "epoch: 5 step: 59, loss is 0.04004930332303047\n",
      "epoch: 5 step: 60, loss is 0.015459791757166386\n",
      "epoch: 5 step: 61, loss is 0.002459291135892272\n",
      "epoch: 5 step: 62, loss is 0.0006485172780230641\n",
      "epoch: 5 step: 63, loss is 0.0009177658939734101\n",
      "epoch: 5 step: 64, loss is 0.0019893331918865442\n",
      "epoch: 5 step: 65, loss is 0.014349599368870258\n",
      "epoch: 5 step: 66, loss is 0.003311415435746312\n",
      "epoch: 5 step: 67, loss is 0.002141970908269286\n",
      "epoch: 5 step: 68, loss is 0.00029464976978488266\n",
      "epoch: 5 step: 69, loss is 0.03587469086050987\n",
      "epoch: 5 step: 70, loss is 0.1633729785680771\n",
      "epoch: 5 step: 71, loss is 0.009733948856592178\n",
      "epoch: 5 step: 72, loss is 0.0001855518639786169\n",
      "epoch: 5 step: 73, loss is 0.047747544944286346\n",
      "epoch: 5 step: 74, loss is 0.0008463424746878445\n",
      "epoch: 5 step: 75, loss is 0.009164983406662941\n",
      "epoch: 5 step: 76, loss is 0.013377061113715172\n",
      "epoch: 5 step: 77, loss is 0.0007816481520421803\n",
      "epoch: 5 step: 78, loss is 0.00022922824427951127\n",
      "epoch: 5 step: 79, loss is 0.052021387964487076\n",
      "epoch: 5 step: 80, loss is 0.1282881498336792\n",
      "epoch: 5 step: 81, loss is 0.004967810586094856\n",
      "epoch: 5 step: 82, loss is 0.01866692304611206\n",
      "epoch: 5 step: 83, loss is 0.0005524023436009884\n",
      "epoch: 5 step: 84, loss is 0.0010791548993438482\n",
      "epoch: 5 step: 85, loss is 0.00030046625761315227\n",
      "epoch: 5 step: 86, loss is 0.03133856877684593\n",
      "epoch: 5 step: 87, loss is 0.006980028934776783\n",
      "epoch: 5 step: 88, loss is 0.0063654277473688126\n",
      "epoch: 5 step: 89, loss is 0.0003689814475364983\n",
      "epoch: 5 step: 90, loss is 0.08974578231573105\n",
      "epoch: 5 step: 91, loss is 0.0005447459989227355\n",
      "epoch: 5 step: 92, loss is 0.02879152074456215\n",
      "epoch: 5 step: 93, loss is 0.014080684632062912\n",
      "epoch: 5 step: 94, loss is 0.12067986279726028\n",
      "epoch: 5 step: 95, loss is 0.003701242385432124\n",
      "epoch: 5 step: 96, loss is 0.09530728310346603\n",
      "epoch: 5 step: 97, loss is 0.004327519331127405\n",
      "epoch: 5 step: 98, loss is 0.24466083943843842\n",
      "epoch: 5 step: 99, loss is 0.0338144525885582\n",
      "epoch: 5 step: 100, loss is 0.0047450256533920765\n",
      "epoch: 5 step: 101, loss is 0.00021586663206107914\n",
      "epoch: 5 step: 102, loss is 0.0007888377294875681\n",
      "epoch: 5 step: 103, loss is 0.008640792220830917\n",
      "epoch: 5 step: 104, loss is 0.0009710388840176165\n",
      "epoch: 5 step: 105, loss is 0.0033908593468368053\n",
      "epoch: 5 step: 106, loss is 0.014938009902834892\n",
      "epoch: 5 step: 107, loss is 0.0175606869161129\n",
      "epoch: 5 step: 108, loss is 0.001219856203533709\n",
      "epoch: 5 step: 109, loss is 0.0047670393250882626\n",
      "epoch: 5 step: 110, loss is 0.0008347683469764888\n",
      "epoch: 5 step: 111, loss is 0.0014663191977888346\n",
      "epoch: 5 step: 112, loss is 0.03058360144495964\n",
      "epoch: 5 step: 113, loss is 0.0046185776591300964\n",
      "epoch: 5 step: 114, loss is 0.0028249770402908325\n",
      "epoch: 5 step: 115, loss is 0.001456023077480495\n",
      "epoch: 5 step: 116, loss is 0.0011334456503391266\n",
      "epoch: 5 step: 117, loss is 0.060946274548769\n",
      "epoch: 5 step: 118, loss is 5.3956602641846985e-05\n",
      "epoch: 5 step: 119, loss is 0.08838837593793869\n",
      "epoch: 5 step: 120, loss is 0.0031981938518583775\n",
      "epoch: 5 step: 121, loss is 0.07606427371501923\n",
      "epoch: 5 step: 122, loss is 0.00841754861176014\n",
      "epoch: 5 step: 123, loss is 0.0017723817145451903\n",
      "epoch: 5 step: 124, loss is 0.013717427849769592\n",
      "epoch: 5 step: 125, loss is 0.016534050926566124\n",
      "epoch: 5 step: 126, loss is 0.008417275734245777\n",
      "epoch: 5 step: 127, loss is 0.06350671499967575\n",
      "epoch: 5 step: 128, loss is 0.02856418304145336\n",
      "epoch: 5 step: 129, loss is 0.029342258349061012\n",
      "epoch: 5 step: 130, loss is 0.014819711446762085\n",
      "epoch: 5 step: 131, loss is 0.013292450457811356\n",
      "epoch: 5 step: 132, loss is 0.0022334239911288023\n",
      "epoch: 5 step: 133, loss is 0.22342318296432495\n",
      "epoch: 5 step: 134, loss is 0.00872124545276165\n",
      "epoch: 5 step: 135, loss is 0.00017300563922617584\n",
      "epoch: 5 step: 136, loss is 0.006696358323097229\n",
      "epoch: 5 step: 137, loss is 0.001418201718479395\n",
      "epoch: 5 step: 138, loss is 0.06063375994563103\n",
      "epoch: 5 step: 139, loss is 0.016748981550335884\n",
      "epoch: 5 step: 140, loss is 0.0013005544897168875\n",
      "epoch: 5 step: 141, loss is 0.0008582433802075684\n",
      "epoch: 5 step: 142, loss is 0.07479962706565857\n",
      "epoch: 5 step: 143, loss is 0.0026608523912727833\n",
      "epoch: 5 step: 144, loss is 0.020002439618110657\n",
      "epoch: 5 step: 145, loss is 0.0072655570693314075\n",
      "epoch: 5 step: 146, loss is 0.04364345967769623\n",
      "epoch: 5 step: 147, loss is 0.0024275355972349644\n",
      "epoch: 5 step: 148, loss is 0.0011062530102208257\n",
      "epoch: 5 step: 149, loss is 0.01290728896856308\n",
      "epoch: 5 step: 150, loss is 0.12822185456752777\n",
      "epoch: 5 step: 151, loss is 0.006324217189103365\n",
      "epoch: 5 step: 152, loss is 0.09798969328403473\n",
      "epoch: 5 step: 153, loss is 0.0033557419665157795\n",
      "epoch: 5 step: 154, loss is 0.21180842816829681\n",
      "epoch: 5 step: 155, loss is 0.038621269166469574\n",
      "epoch: 5 step: 156, loss is 0.016378434374928474\n",
      "epoch: 5 step: 157, loss is 0.01145688071846962\n",
      "epoch: 5 step: 158, loss is 0.04910694807767868\n",
      "epoch: 5 step: 159, loss is 0.00785564724355936\n",
      "epoch: 5 step: 160, loss is 0.0029191290959715843\n",
      "epoch: 5 step: 161, loss is 7.423012721119449e-05\n",
      "epoch: 5 step: 162, loss is 0.3271195888519287\n",
      "epoch: 5 step: 163, loss is 0.0016208988381549716\n",
      "epoch: 5 step: 164, loss is 0.00771362753584981\n",
      "epoch: 5 step: 165, loss is 0.0005030913162045181\n",
      "epoch: 5 step: 166, loss is 0.002633477095514536\n",
      "epoch: 5 step: 167, loss is 0.021436993032693863\n",
      "epoch: 5 step: 168, loss is 0.00182920356746763\n",
      "epoch: 5 step: 169, loss is 0.12192041426897049\n",
      "epoch: 5 step: 170, loss is 0.010661095380783081\n",
      "epoch: 5 step: 171, loss is 0.015896396711468697\n",
      "epoch: 5 step: 172, loss is 0.05505713075399399\n",
      "epoch: 5 step: 173, loss is 0.002593334298580885\n",
      "epoch: 5 step: 174, loss is 0.006763350684195757\n",
      "epoch: 5 step: 175, loss is 0.031364478170871735\n",
      "epoch: 5 step: 176, loss is 0.002379780635237694\n",
      "epoch: 5 step: 177, loss is 0.1082533448934555\n",
      "epoch: 5 step: 178, loss is 0.0016100198263302445\n",
      "epoch: 5 step: 179, loss is 0.001064129057340324\n",
      "epoch: 5 step: 180, loss is 0.0024807113222777843\n",
      "epoch: 5 step: 181, loss is 0.019380886107683182\n",
      "epoch: 5 step: 182, loss is 0.0120411841198802\n",
      "epoch: 5 step: 183, loss is 0.00047183327842503786\n",
      "epoch: 5 step: 184, loss is 0.006070111412554979\n",
      "epoch: 5 step: 185, loss is 0.1378086656332016\n",
      "epoch: 5 step: 186, loss is 0.003906901925802231\n",
      "epoch: 5 step: 187, loss is 0.0025471122935414314\n",
      "epoch: 5 step: 188, loss is 0.059737835079431534\n",
      "epoch: 5 step: 189, loss is 0.23847410082817078\n",
      "epoch: 5 step: 190, loss is 0.0017777429893612862\n",
      "epoch: 5 step: 191, loss is 0.0018273487221449614\n",
      "epoch: 5 step: 192, loss is 0.031910400837659836\n",
      "epoch: 5 step: 193, loss is 0.011101201176643372\n",
      "epoch: 5 step: 194, loss is 0.02059490792453289\n",
      "epoch: 5 step: 195, loss is 0.0005587408086284995\n",
      "epoch: 5 step: 196, loss is 0.015027042478322983\n",
      "epoch: 5 step: 197, loss is 0.02308281883597374\n",
      "epoch: 5 step: 198, loss is 0.019872577860951424\n",
      "epoch: 5 step: 199, loss is 0.02995396964251995\n",
      "epoch: 5 step: 200, loss is 0.056694526225328445\n",
      "epoch: 5 step: 201, loss is 0.07850232720375061\n",
      "epoch: 5 step: 202, loss is 0.14668205380439758\n",
      "epoch: 5 step: 203, loss is 0.000525784445926547\n",
      "epoch: 5 step: 204, loss is 0.0031574892345815897\n",
      "epoch: 5 step: 205, loss is 0.2363431602716446\n",
      "epoch: 5 step: 206, loss is 0.004197538364678621\n",
      "epoch: 5 step: 207, loss is 0.0011687674559652805\n",
      "epoch: 5 step: 208, loss is 0.0028728374745696783\n",
      "epoch: 5 step: 209, loss is 0.0018390556797385216\n",
      "epoch: 5 step: 210, loss is 0.10042903572320938\n",
      "epoch: 5 step: 211, loss is 0.004930357448756695\n",
      "epoch: 5 step: 212, loss is 0.005758612882345915\n",
      "epoch: 5 step: 213, loss is 0.011459816247224808\n",
      "epoch: 5 step: 214, loss is 0.004174661822617054\n",
      "epoch: 5 step: 215, loss is 0.03767641633749008\n",
      "epoch: 5 step: 216, loss is 0.002921939827501774\n",
      "epoch: 5 step: 217, loss is 0.0016310353530570865\n",
      "epoch: 5 step: 218, loss is 0.05452842637896538\n",
      "epoch: 5 step: 219, loss is 0.0032722705509513617\n",
      "epoch: 5 step: 220, loss is 0.0017458649817854166\n",
      "epoch: 5 step: 221, loss is 0.0039949798956513405\n",
      "epoch: 5 step: 222, loss is 0.0008139124838635325\n",
      "epoch: 5 step: 223, loss is 0.017617400735616684\n",
      "epoch: 5 step: 224, loss is 0.000668258115183562\n",
      "epoch: 5 step: 225, loss is 0.002440942917019129\n",
      "epoch: 5 step: 226, loss is 0.001138306688517332\n",
      "epoch: 5 step: 227, loss is 0.06787097454071045\n",
      "epoch: 5 step: 228, loss is 0.0027244531083852053\n",
      "epoch: 5 step: 229, loss is 0.005596364848315716\n",
      "epoch: 5 step: 230, loss is 0.011490417644381523\n",
      "epoch: 5 step: 231, loss is 0.034873951226472855\n",
      "epoch: 5 step: 232, loss is 0.004511778708547354\n",
      "epoch: 5 step: 233, loss is 0.006697756703943014\n",
      "epoch: 5 step: 234, loss is 0.006702188868075609\n",
      "epoch: 5 step: 235, loss is 0.012271687388420105\n",
      "epoch: 5 step: 236, loss is 0.0011384740937501192\n",
      "epoch: 5 step: 237, loss is 0.09852337092161179\n",
      "epoch: 5 step: 238, loss is 0.0006275353371165693\n",
      "epoch: 5 step: 239, loss is 0.00013008438691031188\n",
      "epoch: 5 step: 240, loss is 0.004172696266323328\n",
      "epoch: 5 step: 241, loss is 0.033243946731090546\n",
      "epoch: 5 step: 242, loss is 0.0009736121282912791\n",
      "epoch: 5 step: 243, loss is 0.0034240444656461477\n",
      "epoch: 5 step: 244, loss is 0.009653345681726933\n",
      "epoch: 5 step: 245, loss is 0.007623694371432066\n",
      "epoch: 5 step: 246, loss is 0.023195942863821983\n",
      "epoch: 5 step: 247, loss is 0.003007137682288885\n",
      "epoch: 5 step: 248, loss is 0.09139633178710938\n",
      "epoch: 5 step: 249, loss is 0.27620500326156616\n",
      "epoch: 5 step: 250, loss is 0.010687571950256824\n",
      "epoch: 5 step: 251, loss is 5.885418795514852e-05\n",
      "epoch: 5 step: 252, loss is 0.06560496985912323\n",
      "epoch: 5 step: 253, loss is 0.07363086193799973\n",
      "epoch: 5 step: 254, loss is 0.05545079708099365\n",
      "epoch: 5 step: 255, loss is 0.013265914283692837\n",
      "epoch: 5 step: 256, loss is 0.00025964167434722185\n",
      "epoch: 5 step: 257, loss is 0.025623783469200134\n",
      "epoch: 5 step: 258, loss is 0.0026110413018614054\n",
      "epoch: 5 step: 259, loss is 0.4397308826446533\n",
      "epoch: 5 step: 260, loss is 0.0030297318007797003\n",
      "epoch: 5 step: 261, loss is 0.033302925527095795\n",
      "epoch: 5 step: 262, loss is 0.0018579962197691202\n",
      "epoch: 5 step: 263, loss is 0.03683143109083176\n",
      "epoch: 5 step: 264, loss is 0.006250767502933741\n",
      "epoch: 5 step: 265, loss is 0.003045732155442238\n",
      "epoch: 5 step: 266, loss is 0.003278410993516445\n",
      "epoch: 5 step: 267, loss is 0.0039046427700668573\n",
      "epoch: 5 step: 268, loss is 0.00010042685607913882\n",
      "epoch: 5 step: 269, loss is 0.0025993732269853354\n",
      "epoch: 5 step: 270, loss is 0.04424455016851425\n",
      "epoch: 5 step: 271, loss is 0.007781602442264557\n",
      "epoch: 5 step: 272, loss is 0.0028364164754748344\n",
      "epoch: 5 step: 273, loss is 0.018017536029219627\n",
      "epoch: 5 step: 274, loss is 0.000172852844116278\n",
      "epoch: 5 step: 275, loss is 0.0021034146193414927\n",
      "epoch: 5 step: 276, loss is 0.009516156278550625\n",
      "epoch: 5 step: 277, loss is 0.004753118846565485\n",
      "epoch: 5 step: 278, loss is 0.11458051949739456\n",
      "epoch: 5 step: 279, loss is 0.00011160408757859841\n",
      "epoch: 5 step: 280, loss is 0.07519309222698212\n",
      "epoch: 5 step: 281, loss is 0.0005458843661472201\n",
      "epoch: 5 step: 282, loss is 0.17128415405750275\n",
      "epoch: 5 step: 283, loss is 0.000884378096088767\n",
      "epoch: 5 step: 284, loss is 0.002890457632020116\n",
      "epoch: 5 step: 285, loss is 0.0005527125904336572\n",
      "epoch: 5 step: 286, loss is 0.023615457117557526\n",
      "epoch: 5 step: 287, loss is 0.08169082552194595\n",
      "epoch: 5 step: 288, loss is 0.013617157936096191\n",
      "epoch: 5 step: 289, loss is 0.04103068262338638\n",
      "epoch: 5 step: 290, loss is 0.008468825370073318\n",
      "epoch: 5 step: 291, loss is 0.05951346084475517\n",
      "epoch: 5 step: 292, loss is 0.0019146037520840764\n",
      "epoch: 5 step: 293, loss is 0.009165152907371521\n",
      "epoch: 5 step: 294, loss is 0.05886928737163544\n",
      "epoch: 5 step: 295, loss is 0.0013098623603582382\n",
      "epoch: 5 step: 296, loss is 0.00556906359270215\n",
      "epoch: 5 step: 297, loss is 0.0038152458146214485\n",
      "epoch: 5 step: 298, loss is 0.008261198177933693\n",
      "epoch: 5 step: 299, loss is 0.031362392008304596\n",
      "epoch: 5 step: 300, loss is 0.007765921764075756\n",
      "epoch: 5 step: 301, loss is 0.0011149258352816105\n",
      "epoch: 5 step: 302, loss is 0.03710160404443741\n",
      "epoch: 5 step: 303, loss is 0.04949730262160301\n",
      "epoch: 5 step: 304, loss is 0.09864526242017746\n",
      "epoch: 5 step: 305, loss is 0.02095424383878708\n",
      "epoch: 5 step: 306, loss is 0.000986341037787497\n",
      "epoch: 5 step: 307, loss is 0.06752213835716248\n",
      "epoch: 5 step: 308, loss is 0.0026775901205837727\n",
      "epoch: 5 step: 309, loss is 0.001163769164122641\n",
      "epoch: 5 step: 310, loss is 0.0005143132875673473\n",
      "epoch: 5 step: 311, loss is 0.013633925467729568\n",
      "epoch: 5 step: 312, loss is 0.001005930476821959\n",
      "epoch: 5 step: 313, loss is 0.012733962386846542\n",
      "epoch: 5 step: 314, loss is 0.016341842710971832\n",
      "epoch: 5 step: 315, loss is 0.044829368591308594\n",
      "epoch: 5 step: 316, loss is 0.0067414045333862305\n",
      "epoch: 5 step: 317, loss is 0.006313474848866463\n",
      "epoch: 5 step: 318, loss is 0.0002479956892784685\n",
      "epoch: 5 step: 319, loss is 0.11290135979652405\n",
      "epoch: 5 step: 320, loss is 0.04968823492527008\n",
      "epoch: 5 step: 321, loss is 0.0006663526874035597\n",
      "epoch: 5 step: 322, loss is 0.038767289370298386\n",
      "epoch: 5 step: 323, loss is 0.0018739579245448112\n",
      "epoch: 5 step: 324, loss is 0.011874737218022346\n",
      "epoch: 5 step: 325, loss is 0.006497174501419067\n",
      "epoch: 5 step: 326, loss is 0.005933492444455624\n",
      "epoch: 5 step: 327, loss is 0.0032247407361865044\n",
      "epoch: 5 step: 328, loss is 0.00269906735047698\n",
      "epoch: 5 step: 329, loss is 0.010335679166018963\n",
      "epoch: 5 step: 330, loss is 0.0008784222300164402\n",
      "epoch: 5 step: 331, loss is 0.14986765384674072\n",
      "epoch: 5 step: 332, loss is 0.01567939855158329\n",
      "epoch: 5 step: 333, loss is 0.004426951985806227\n",
      "epoch: 5 step: 334, loss is 0.0011629092041403055\n",
      "epoch: 5 step: 335, loss is 0.021844476461410522\n",
      "epoch: 5 step: 336, loss is 0.0003847341286018491\n",
      "epoch: 5 step: 337, loss is 0.00180104561150074\n",
      "epoch: 5 step: 338, loss is 0.00086694594938308\n",
      "epoch: 5 step: 339, loss is 0.01320232916623354\n",
      "epoch: 5 step: 340, loss is 0.0011705089127644897\n",
      "epoch: 5 step: 341, loss is 0.0031718090176582336\n",
      "epoch: 5 step: 342, loss is 0.0019141871016472578\n",
      "epoch: 5 step: 343, loss is 0.025100957602262497\n",
      "epoch: 5 step: 344, loss is 0.0013972976012155414\n",
      "epoch: 5 step: 345, loss is 0.015777865424752235\n",
      "epoch: 5 step: 346, loss is 0.0003468730137683451\n",
      "epoch: 5 step: 347, loss is 0.0017538502579554915\n",
      "epoch: 5 step: 348, loss is 0.05590590834617615\n",
      "epoch: 5 step: 349, loss is 0.0021551894024014473\n",
      "epoch: 5 step: 350, loss is 0.1727597713470459\n",
      "epoch: 5 step: 351, loss is 0.000468259648187086\n",
      "epoch: 5 step: 352, loss is 0.001847707200795412\n",
      "epoch: 5 step: 353, loss is 0.016974177211523056\n",
      "epoch: 5 step: 354, loss is 0.0011260147439315915\n",
      "epoch: 5 step: 355, loss is 0.0019415462156757712\n",
      "epoch: 5 step: 356, loss is 0.07735461741685867\n",
      "epoch: 5 step: 357, loss is 0.0006573080900125206\n",
      "epoch: 5 step: 358, loss is 0.0053286077454686165\n",
      "epoch: 5 step: 359, loss is 0.0009041990851983428\n",
      "epoch: 5 step: 360, loss is 4.9128000682685524e-05\n",
      "epoch: 5 step: 361, loss is 0.012181040830910206\n",
      "epoch: 5 step: 362, loss is 0.003943544812500477\n",
      "epoch: 5 step: 363, loss is 0.0014576606918126345\n",
      "epoch: 5 step: 364, loss is 0.02540692314505577\n",
      "epoch: 5 step: 365, loss is 0.013073482550680637\n",
      "epoch: 5 step: 366, loss is 0.09263323247432709\n",
      "epoch: 5 step: 367, loss is 0.011525467969477177\n",
      "epoch: 5 step: 368, loss is 0.005209716968238354\n",
      "epoch: 5 step: 369, loss is 0.07888808101415634\n",
      "epoch: 5 step: 370, loss is 0.04563925787806511\n",
      "epoch: 5 step: 371, loss is 0.0006788712926208973\n",
      "epoch: 5 step: 372, loss is 0.060192715376615524\n",
      "epoch: 5 step: 373, loss is 0.01455902773886919\n",
      "epoch: 5 step: 374, loss is 0.00022891871049068868\n",
      "epoch: 5 step: 375, loss is 0.006363327614963055\n",
      "epoch: 5 step: 376, loss is 0.0004394589923322201\n",
      "epoch: 5 step: 377, loss is 0.024110382422804832\n",
      "epoch: 5 step: 378, loss is 0.047163140028715134\n",
      "epoch: 5 step: 379, loss is 0.007177549879997969\n",
      "epoch: 5 step: 380, loss is 0.05855141580104828\n",
      "epoch: 5 step: 381, loss is 0.09329410642385483\n",
      "epoch: 5 step: 382, loss is 0.0009283567196689546\n",
      "epoch: 5 step: 383, loss is 0.015162937343120575\n",
      "epoch: 5 step: 384, loss is 0.0074865370988845825\n",
      "epoch: 5 step: 385, loss is 0.00941319577395916\n",
      "epoch: 5 step: 386, loss is 0.0015683507081121206\n",
      "epoch: 5 step: 387, loss is 0.004386520944535732\n",
      "epoch: 5 step: 388, loss is 0.02596435509622097\n",
      "epoch: 5 step: 389, loss is 0.0016683329595252872\n",
      "epoch: 5 step: 390, loss is 0.045737139880657196\n",
      "epoch: 5 step: 391, loss is 0.019857576116919518\n",
      "epoch: 5 step: 392, loss is 0.003721651853993535\n",
      "epoch: 5 step: 393, loss is 0.003411402925848961\n",
      "epoch: 5 step: 394, loss is 0.02313873916864395\n",
      "epoch: 5 step: 395, loss is 0.005584402941167355\n",
      "epoch: 5 step: 396, loss is 0.00022929685655981302\n",
      "epoch: 5 step: 397, loss is 0.001124038826674223\n",
      "epoch: 5 step: 398, loss is 0.007981427945196629\n",
      "epoch: 5 step: 399, loss is 5.373543535824865e-05\n",
      "epoch: 5 step: 400, loss is 0.00024459947599098086\n",
      "epoch: 5 step: 401, loss is 0.004028964322060347\n",
      "epoch: 5 step: 402, loss is 0.0012616552412509918\n",
      "epoch: 5 step: 403, loss is 0.02933730185031891\n",
      "epoch: 5 step: 404, loss is 0.007027000188827515\n",
      "epoch: 5 step: 405, loss is 0.0031290671322494745\n",
      "epoch: 5 step: 406, loss is 0.03540031239390373\n",
      "epoch: 5 step: 407, loss is 0.0023301823530346155\n",
      "epoch: 5 step: 408, loss is 0.23757824301719666\n",
      "epoch: 5 step: 409, loss is 0.01105009950697422\n",
      "epoch: 5 step: 410, loss is 0.0010592632461339235\n",
      "epoch: 5 step: 411, loss is 0.16006648540496826\n",
      "epoch: 5 step: 412, loss is 0.14235079288482666\n",
      "epoch: 5 step: 413, loss is 0.00044664775487035513\n",
      "epoch: 5 step: 414, loss is 0.0012777236988767982\n",
      "epoch: 5 step: 415, loss is 0.0006942712934687734\n",
      "epoch: 5 step: 416, loss is 0.007219569757580757\n",
      "epoch: 5 step: 417, loss is 0.0018901517614722252\n",
      "epoch: 5 step: 418, loss is 0.003351811785250902\n",
      "epoch: 5 step: 419, loss is 0.017876526340842247\n",
      "epoch: 5 step: 420, loss is 0.0037161174695938826\n",
      "epoch: 5 step: 421, loss is 0.22354908287525177\n",
      "epoch: 5 step: 422, loss is 0.033801332116127014\n",
      "epoch: 5 step: 423, loss is 0.015984416007995605\n",
      "epoch: 5 step: 424, loss is 0.000326144159771502\n",
      "epoch: 5 step: 425, loss is 0.00902253296226263\n",
      "epoch: 5 step: 426, loss is 0.0034473733976483345\n",
      "epoch: 5 step: 427, loss is 0.0004425248480401933\n",
      "epoch: 5 step: 428, loss is 0.012011208571493626\n",
      "epoch: 5 step: 429, loss is 0.000206077063921839\n",
      "epoch: 5 step: 430, loss is 0.01779668778181076\n",
      "epoch: 5 step: 431, loss is 0.07017317414283752\n",
      "epoch: 5 step: 432, loss is 0.09194595366716385\n",
      "epoch: 5 step: 433, loss is 0.01936662569642067\n",
      "epoch: 5 step: 434, loss is 0.012864775955677032\n",
      "epoch: 5 step: 435, loss is 0.014134898781776428\n",
      "epoch: 5 step: 436, loss is 0.03663886711001396\n",
      "epoch: 5 step: 437, loss is 0.0008157453848980367\n",
      "epoch: 5 step: 438, loss is 0.003060871735215187\n",
      "epoch: 5 step: 439, loss is 0.1888912320137024\n",
      "epoch: 5 step: 440, loss is 0.016262991353869438\n",
      "epoch: 5 step: 441, loss is 0.0031983242370188236\n",
      "epoch: 5 step: 442, loss is 0.0017425969708710909\n",
      "epoch: 5 step: 443, loss is 0.0010320735163986683\n",
      "epoch: 5 step: 444, loss is 0.00040823285235092044\n",
      "epoch: 5 step: 445, loss is 0.0019104600651189685\n",
      "epoch: 5 step: 446, loss is 0.058423638343811035\n",
      "epoch: 5 step: 447, loss is 0.0163408312946558\n",
      "epoch: 5 step: 448, loss is 0.0004780579765792936\n",
      "epoch: 5 step: 449, loss is 0.010653347708284855\n",
      "epoch: 5 step: 450, loss is 0.008760528638958931\n",
      "epoch: 5 step: 451, loss is 0.054295215755701065\n",
      "epoch: 5 step: 452, loss is 0.0005341671640053391\n",
      "epoch: 5 step: 453, loss is 0.009391168132424355\n",
      "epoch: 5 step: 454, loss is 0.02265317738056183\n",
      "epoch: 5 step: 455, loss is 0.006454748101532459\n",
      "epoch: 5 step: 456, loss is 0.021055910736322403\n",
      "epoch: 5 step: 457, loss is 0.00024339482479263097\n",
      "epoch: 5 step: 458, loss is 0.00270734541118145\n",
      "epoch: 5 step: 459, loss is 0.0438050776720047\n",
      "epoch: 5 step: 460, loss is 0.018324797973036766\n",
      "epoch: 5 step: 461, loss is 0.0019516304600983858\n",
      "epoch: 5 step: 462, loss is 0.0008470214088447392\n",
      "epoch: 5 step: 463, loss is 6.959632446523756e-05\n",
      "epoch: 5 step: 464, loss is 0.0128999724984169\n",
      "epoch: 5 step: 465, loss is 0.01096316333860159\n",
      "epoch: 5 step: 466, loss is 0.002805241383612156\n",
      "epoch: 5 step: 467, loss is 0.05140066519379616\n",
      "epoch: 5 step: 468, loss is 0.0018039598362520337\n",
      "epoch: 5 step: 469, loss is 0.01348208449780941\n",
      "epoch: 5 step: 470, loss is 0.06414201855659485\n",
      "epoch: 5 step: 471, loss is 0.17664794623851776\n",
      "epoch: 5 step: 472, loss is 0.004481656942516565\n",
      "epoch: 5 step: 473, loss is 0.0021760202944278717\n",
      "epoch: 5 step: 474, loss is 0.02703862451016903\n",
      "epoch: 5 step: 475, loss is 0.004070120397955179\n",
      "epoch: 5 step: 476, loss is 0.00023836432956159115\n",
      "epoch: 5 step: 477, loss is 0.0005212771939113736\n",
      "epoch: 5 step: 478, loss is 0.00922130886465311\n",
      "epoch: 5 step: 479, loss is 0.009100627154111862\n",
      "epoch: 5 step: 480, loss is 0.01622460037469864\n",
      "epoch: 5 step: 481, loss is 0.01020757481455803\n",
      "epoch: 5 step: 482, loss is 0.0009706316632218659\n",
      "epoch: 5 step: 483, loss is 0.01524980366230011\n",
      "epoch: 5 step: 484, loss is 0.0008771516149863601\n",
      "epoch: 5 step: 485, loss is 0.0004074490861967206\n",
      "epoch: 5 step: 486, loss is 0.0025235244538635015\n",
      "epoch: 5 step: 487, loss is 0.015319643542170525\n",
      "epoch: 5 step: 488, loss is 0.004281302448362112\n",
      "epoch: 5 step: 489, loss is 0.0093281464651227\n",
      "epoch: 5 step: 490, loss is 0.003735491307452321\n",
      "epoch: 5 step: 491, loss is 0.002366165164858103\n",
      "epoch: 5 step: 492, loss is 0.003961775917559862\n",
      "epoch: 5 step: 493, loss is 0.0011523340363055468\n",
      "epoch: 5 step: 494, loss is 0.0001976963394554332\n",
      "epoch: 5 step: 495, loss is 0.0374283641576767\n",
      "epoch: 5 step: 496, loss is 0.10232643783092499\n",
      "epoch: 5 step: 497, loss is 0.14108264446258545\n",
      "epoch: 5 step: 498, loss is 0.06321144849061966\n",
      "epoch: 5 step: 499, loss is 0.0004654519143514335\n",
      "epoch: 5 step: 500, loss is 0.002088181208819151\n",
      "epoch: 5 step: 501, loss is 0.0007369217928498983\n",
      "epoch: 5 step: 502, loss is 0.002587511669844389\n",
      "epoch: 5 step: 503, loss is 0.08794823288917542\n",
      "epoch: 5 step: 504, loss is 0.020684633404016495\n",
      "epoch: 5 step: 505, loss is 0.023208580911159515\n",
      "epoch: 5 step: 506, loss is 0.012495961971580982\n",
      "epoch: 5 step: 507, loss is 0.0005668313824571669\n",
      "epoch: 5 step: 508, loss is 0.012911701574921608\n",
      "epoch: 5 step: 509, loss is 0.018088921904563904\n",
      "epoch: 5 step: 510, loss is 0.001275532995350659\n",
      "epoch: 5 step: 511, loss is 0.02599024772644043\n",
      "epoch: 5 step: 512, loss is 0.057694390416145325\n",
      "epoch: 5 step: 513, loss is 0.00062249117763713\n",
      "epoch: 5 step: 514, loss is 0.004511912353336811\n",
      "epoch: 5 step: 515, loss is 0.019837670028209686\n",
      "epoch: 5 step: 516, loss is 0.0024495969992130995\n",
      "epoch: 5 step: 517, loss is 0.0012820017291232944\n",
      "epoch: 5 step: 518, loss is 0.03772640973329544\n",
      "epoch: 5 step: 519, loss is 0.003706029849126935\n",
      "epoch: 5 step: 520, loss is 0.002538492437452078\n",
      "epoch: 5 step: 521, loss is 0.000265980779659003\n",
      "epoch: 5 step: 522, loss is 8.113643707474694e-05\n",
      "epoch: 5 step: 523, loss is 0.011565230786800385\n",
      "epoch: 5 step: 524, loss is 0.1214514672756195\n",
      "epoch: 5 step: 525, loss is 0.000852737226523459\n",
      "epoch: 5 step: 526, loss is 0.004779510200023651\n",
      "epoch: 5 step: 527, loss is 0.0006205622339621186\n",
      "epoch: 5 step: 528, loss is 0.018157850950956345\n",
      "epoch: 5 step: 529, loss is 0.0006531645776703954\n",
      "epoch: 5 step: 530, loss is 0.004832036793231964\n",
      "epoch: 5 step: 531, loss is 0.03057154081761837\n",
      "epoch: 5 step: 532, loss is 0.09654755890369415\n",
      "epoch: 5 step: 533, loss is 0.010329652577638626\n",
      "epoch: 5 step: 534, loss is 0.005150795914232731\n",
      "epoch: 5 step: 535, loss is 0.002212521154433489\n",
      "epoch: 5 step: 536, loss is 0.03075156733393669\n",
      "epoch: 5 step: 537, loss is 5.14205421495717e-05\n",
      "epoch: 5 step: 538, loss is 0.13359211385250092\n",
      "epoch: 5 step: 539, loss is 0.010188999585807323\n",
      "epoch: 5 step: 540, loss is 0.025317832827568054\n",
      "epoch: 5 step: 541, loss is 0.003390975296497345\n",
      "epoch: 5 step: 542, loss is 0.1723625510931015\n",
      "epoch: 5 step: 543, loss is 0.0007724558236077428\n",
      "epoch: 5 step: 544, loss is 0.0009173601283691823\n",
      "epoch: 5 step: 545, loss is 0.002015255857259035\n",
      "epoch: 5 step: 546, loss is 0.005087088793516159\n",
      "epoch: 5 step: 547, loss is 0.15692046284675598\n",
      "epoch: 5 step: 548, loss is 0.015591581352055073\n",
      "epoch: 5 step: 549, loss is 0.021002748981118202\n",
      "epoch: 5 step: 550, loss is 0.09638582170009613\n",
      "epoch: 5 step: 551, loss is 0.028805628418922424\n",
      "epoch: 5 step: 552, loss is 0.01502379309386015\n",
      "epoch: 5 step: 553, loss is 0.00012661470100283623\n",
      "epoch: 5 step: 554, loss is 0.00030605768552049994\n",
      "epoch: 5 step: 555, loss is 0.01340425293892622\n",
      "epoch: 5 step: 556, loss is 0.0863754004240036\n",
      "epoch: 5 step: 557, loss is 0.0013727156911045313\n",
      "epoch: 5 step: 558, loss is 0.0412590391933918\n",
      "epoch: 5 step: 559, loss is 0.007142907474189997\n",
      "epoch: 5 step: 560, loss is 0.0014577727997675538\n",
      "epoch: 5 step: 561, loss is 0.0005889164167456329\n",
      "epoch: 5 step: 562, loss is 0.06911250948905945\n",
      "epoch: 5 step: 563, loss is 0.032270535826683044\n",
      "epoch: 5 step: 564, loss is 0.0034362266305834055\n",
      "epoch: 5 step: 565, loss is 0.003019866766408086\n",
      "epoch: 5 step: 566, loss is 0.0012053776299580932\n",
      "epoch: 5 step: 567, loss is 0.0015093404799699783\n",
      "epoch: 5 step: 568, loss is 0.0019095316529273987\n",
      "epoch: 5 step: 569, loss is 0.0869174599647522\n",
      "epoch: 5 step: 570, loss is 0.015075378119945526\n",
      "epoch: 5 step: 571, loss is 0.016285961493849754\n",
      "epoch: 5 step: 572, loss is 0.002149777254089713\n",
      "epoch: 5 step: 573, loss is 0.009198971092700958\n",
      "epoch: 5 step: 574, loss is 0.0005285928491503\n",
      "epoch: 5 step: 575, loss is 0.07071851193904877\n",
      "epoch: 5 step: 576, loss is 0.08584615588188171\n",
      "epoch: 5 step: 577, loss is 0.005381145980209112\n",
      "epoch: 5 step: 578, loss is 0.026913123205304146\n",
      "epoch: 5 step: 579, loss is 0.06884648650884628\n",
      "epoch: 5 step: 580, loss is 0.0007064956007525325\n",
      "epoch: 5 step: 581, loss is 0.01125817745923996\n",
      "epoch: 5 step: 582, loss is 0.014752771705389023\n",
      "epoch: 5 step: 583, loss is 0.0028544138185679913\n",
      "epoch: 5 step: 584, loss is 0.009941826574504375\n",
      "epoch: 5 step: 585, loss is 0.009502718225121498\n",
      "epoch: 5 step: 586, loss is 0.006528699770569801\n",
      "epoch: 5 step: 587, loss is 0.07084299623966217\n",
      "epoch: 5 step: 588, loss is 0.00199399353004992\n",
      "epoch: 5 step: 589, loss is 0.04535025358200073\n",
      "epoch: 5 step: 590, loss is 0.002104436280205846\n",
      "epoch: 5 step: 591, loss is 0.004796118941158056\n",
      "epoch: 5 step: 592, loss is 0.0001571195462020114\n",
      "epoch: 5 step: 593, loss is 4.784261909662746e-05\n",
      "epoch: 5 step: 594, loss is 0.01883939653635025\n",
      "epoch: 5 step: 595, loss is 0.01619432307779789\n",
      "epoch: 5 step: 596, loss is 0.0174630768597126\n",
      "epoch: 5 step: 597, loss is 0.23481526970863342\n",
      "epoch: 5 step: 598, loss is 0.11853494495153427\n",
      "epoch: 5 step: 599, loss is 0.0004869759432040155\n",
      "epoch: 5 step: 600, loss is 0.01774982549250126\n",
      "epoch: 5 step: 601, loss is 0.00260542007163167\n",
      "epoch: 5 step: 602, loss is 0.005551205016672611\n",
      "epoch: 5 step: 603, loss is 0.0003308607265353203\n",
      "epoch: 5 step: 604, loss is 0.0038237469270825386\n",
      "epoch: 5 step: 605, loss is 0.006642147898674011\n",
      "epoch: 5 step: 606, loss is 0.005056148394942284\n",
      "epoch: 5 step: 607, loss is 0.0008685223292559385\n",
      "epoch: 5 step: 608, loss is 0.01197970099747181\n",
      "epoch: 5 step: 609, loss is 0.0034418252762407064\n",
      "epoch: 5 step: 610, loss is 0.020930100232362747\n",
      "epoch: 5 step: 611, loss is 0.002067758934572339\n",
      "epoch: 5 step: 612, loss is 0.009244140237569809\n",
      "epoch: 5 step: 613, loss is 0.00015614884614478797\n",
      "epoch: 5 step: 614, loss is 0.0028952716384083033\n",
      "epoch: 5 step: 615, loss is 0.005575852934271097\n",
      "epoch: 5 step: 616, loss is 0.00031070641125552356\n",
      "epoch: 5 step: 617, loss is 0.00010205285798292607\n",
      "epoch: 5 step: 618, loss is 0.002424867358058691\n",
      "epoch: 5 step: 619, loss is 0.0015203137882053852\n",
      "epoch: 5 step: 620, loss is 0.0017085984582081437\n",
      "epoch: 5 step: 621, loss is 0.020108425989747047\n",
      "epoch: 5 step: 622, loss is 0.0011685839854180813\n",
      "epoch: 5 step: 623, loss is 0.012475916184484959\n",
      "epoch: 5 step: 624, loss is 0.01958213746547699\n",
      "epoch: 5 step: 625, loss is 0.0024691501166671515\n",
      "epoch: 5 step: 626, loss is 0.007645866833627224\n",
      "epoch: 5 step: 627, loss is 0.012688067741692066\n",
      "epoch: 5 step: 628, loss is 0.001024352852255106\n",
      "epoch: 5 step: 629, loss is 0.004279846791177988\n",
      "epoch: 5 step: 630, loss is 3.8782949559390545e-05\n",
      "epoch: 5 step: 631, loss is 0.0398210771381855\n",
      "epoch: 5 step: 632, loss is 0.003615591675043106\n",
      "epoch: 5 step: 633, loss is 0.001118143554776907\n",
      "epoch: 5 step: 634, loss is 0.012873596511781216\n",
      "epoch: 5 step: 635, loss is 0.0013462206115946174\n",
      "epoch: 5 step: 636, loss is 0.0015184945659711957\n",
      "epoch: 5 step: 637, loss is 0.03196708485484123\n",
      "epoch: 5 step: 638, loss is 0.007357188034802675\n",
      "epoch: 5 step: 639, loss is 0.018838440999388695\n",
      "epoch: 5 step: 640, loss is 0.007011951878666878\n",
      "epoch: 5 step: 641, loss is 0.009793155826628208\n",
      "epoch: 5 step: 642, loss is 0.03466826304793358\n",
      "epoch: 5 step: 643, loss is 0.0008175232796929777\n",
      "epoch: 5 step: 644, loss is 0.0057563260197639465\n",
      "epoch: 5 step: 645, loss is 0.0012417449615895748\n",
      "epoch: 5 step: 646, loss is 0.020897898823022842\n",
      "epoch: 5 step: 647, loss is 0.0027121601160615683\n",
      "epoch: 5 step: 648, loss is 0.05020321533083916\n",
      "epoch: 5 step: 649, loss is 0.001105144969187677\n",
      "epoch: 5 step: 650, loss is 0.000101915851701051\n",
      "epoch: 5 step: 651, loss is 0.0015101414173841476\n",
      "epoch: 5 step: 652, loss is 0.05171359330415726\n",
      "epoch: 5 step: 653, loss is 0.00834394060075283\n",
      "epoch: 5 step: 654, loss is 0.0002718227915465832\n",
      "epoch: 5 step: 655, loss is 0.0001728538773022592\n",
      "epoch: 5 step: 656, loss is 0.0007765860063955188\n",
      "epoch: 5 step: 657, loss is 0.012020299211144447\n",
      "epoch: 5 step: 658, loss is 0.2254122495651245\n",
      "epoch: 5 step: 659, loss is 0.027982957661151886\n",
      "epoch: 5 step: 660, loss is 0.0014347200049087405\n",
      "epoch: 5 step: 661, loss is 0.0006018588901497424\n",
      "epoch: 5 step: 662, loss is 0.0025393199175596237\n",
      "epoch: 5 step: 663, loss is 0.014035722240805626\n",
      "epoch: 5 step: 664, loss is 0.0027922203298658133\n",
      "epoch: 5 step: 665, loss is 5.647356738336384e-05\n",
      "epoch: 5 step: 666, loss is 0.01000476349145174\n",
      "epoch: 5 step: 667, loss is 0.10776303708553314\n",
      "epoch: 5 step: 668, loss is 0.00040754766087047756\n",
      "epoch: 5 step: 669, loss is 0.002953852294012904\n",
      "epoch: 5 step: 670, loss is 0.00023882962705101818\n",
      "epoch: 5 step: 671, loss is 0.0007310132496058941\n",
      "epoch: 5 step: 672, loss is 0.017039146274328232\n",
      "epoch: 5 step: 673, loss is 0.0017219869187101722\n",
      "epoch: 5 step: 674, loss is 0.2921845316886902\n",
      "epoch: 5 step: 675, loss is 0.0013507634866982698\n",
      "epoch: 5 step: 676, loss is 0.010500870645046234\n",
      "epoch: 5 step: 677, loss is 0.00036225724034011364\n",
      "epoch: 5 step: 678, loss is 0.01631627045571804\n",
      "epoch: 5 step: 679, loss is 0.005900233052670956\n",
      "epoch: 5 step: 680, loss is 0.010298901237547398\n",
      "epoch: 5 step: 681, loss is 0.003295756410807371\n",
      "epoch: 5 step: 682, loss is 0.03329339623451233\n",
      "epoch: 5 step: 683, loss is 0.09005008637905121\n",
      "epoch: 5 step: 684, loss is 0.060381628572940826\n",
      "epoch: 5 step: 685, loss is 0.058280326426029205\n",
      "epoch: 5 step: 686, loss is 0.001445992267690599\n",
      "epoch: 5 step: 687, loss is 0.15418219566345215\n",
      "epoch: 5 step: 688, loss is 0.011010821908712387\n",
      "epoch: 5 step: 689, loss is 9.613728616386652e-05\n",
      "epoch: 5 step: 690, loss is 0.040909282863140106\n",
      "epoch: 5 step: 691, loss is 0.004648786969482899\n",
      "epoch: 5 step: 692, loss is 0.0013672318309545517\n",
      "epoch: 5 step: 693, loss is 0.0008967012399807572\n",
      "epoch: 5 step: 694, loss is 0.0005597969866357744\n",
      "epoch: 5 step: 695, loss is 0.013360218144953251\n",
      "epoch: 5 step: 696, loss is 0.00948894489556551\n",
      "epoch: 5 step: 697, loss is 0.040894556790590286\n",
      "epoch: 5 step: 698, loss is 0.10722555965185165\n",
      "epoch: 5 step: 699, loss is 0.2811598479747772\n",
      "epoch: 5 step: 700, loss is 0.0008767871768213809\n",
      "epoch: 5 step: 701, loss is 0.0017050805035978556\n",
      "epoch: 5 step: 702, loss is 0.0001517676137154922\n",
      "epoch: 5 step: 703, loss is 0.02844071015715599\n",
      "epoch: 5 step: 704, loss is 0.08153194189071655\n",
      "epoch: 5 step: 705, loss is 0.005359371658414602\n",
      "epoch: 5 step: 706, loss is 0.0022874935530126095\n",
      "epoch: 5 step: 707, loss is 0.020297212526202202\n",
      "epoch: 5 step: 708, loss is 0.022506745532155037\n",
      "epoch: 5 step: 709, loss is 0.0025080640334635973\n",
      "epoch: 5 step: 710, loss is 0.010728000663220882\n",
      "epoch: 5 step: 711, loss is 0.06729673594236374\n",
      "epoch: 5 step: 712, loss is 0.00020974544167984277\n",
      "epoch: 5 step: 713, loss is 0.17901456356048584\n",
      "epoch: 5 step: 714, loss is 0.0009331871988251805\n",
      "epoch: 5 step: 715, loss is 0.07039047032594681\n",
      "epoch: 5 step: 716, loss is 0.005922622978687286\n",
      "epoch: 5 step: 717, loss is 0.002129875821992755\n",
      "epoch: 5 step: 718, loss is 0.005239490885287523\n",
      "epoch: 5 step: 719, loss is 0.19074246287345886\n",
      "epoch: 5 step: 720, loss is 0.0005492171039804816\n",
      "epoch: 5 step: 721, loss is 0.0017092123162001371\n",
      "epoch: 5 step: 722, loss is 0.006647563073784113\n",
      "epoch: 5 step: 723, loss is 0.04788527637720108\n",
      "epoch: 5 step: 724, loss is 0.01636369898915291\n",
      "epoch: 5 step: 725, loss is 0.0012142218183726072\n",
      "epoch: 5 step: 726, loss is 0.017306730151176453\n",
      "epoch: 5 step: 727, loss is 0.00447507482022047\n",
      "epoch: 5 step: 728, loss is 0.0046528917737305164\n",
      "epoch: 5 step: 729, loss is 0.0005537105025723577\n",
      "epoch: 5 step: 730, loss is 0.193941131234169\n",
      "epoch: 5 step: 731, loss is 0.008275050669908524\n",
      "epoch: 5 step: 732, loss is 0.009578418917953968\n",
      "epoch: 5 step: 733, loss is 0.005621044896543026\n",
      "epoch: 5 step: 734, loss is 0.001363352406769991\n",
      "epoch: 5 step: 735, loss is 0.002209998434409499\n",
      "epoch: 5 step: 736, loss is 0.01187361590564251\n",
      "epoch: 5 step: 737, loss is 0.007309214677661657\n",
      "epoch: 5 step: 738, loss is 0.018410254269838333\n",
      "epoch: 5 step: 739, loss is 0.012486494146287441\n",
      "epoch: 5 step: 740, loss is 0.0016200703103095293\n",
      "epoch: 5 step: 741, loss is 0.015261680819094181\n",
      "epoch: 5 step: 742, loss is 0.019451605156064034\n",
      "epoch: 5 step: 743, loss is 0.003929623402655125\n",
      "epoch: 5 step: 744, loss is 0.0031521033961325884\n",
      "epoch: 5 step: 745, loss is 0.008670706301927567\n",
      "epoch: 5 step: 746, loss is 0.053129974752664566\n",
      "epoch: 5 step: 747, loss is 0.0034170718863606453\n",
      "epoch: 5 step: 748, loss is 0.12633341550827026\n",
      "epoch: 5 step: 749, loss is 0.04637223482131958\n",
      "epoch: 5 step: 750, loss is 0.040409382432699203\n",
      "epoch: 5 step: 751, loss is 0.004490791354328394\n",
      "epoch: 5 step: 752, loss is 0.03264543041586876\n",
      "epoch: 5 step: 753, loss is 0.06771648675203323\n",
      "epoch: 5 step: 754, loss is 0.005145394708961248\n",
      "epoch: 5 step: 755, loss is 0.007366903591901064\n",
      "epoch: 5 step: 756, loss is 0.13709647953510284\n",
      "epoch: 5 step: 757, loss is 0.046588581055402756\n",
      "epoch: 5 step: 758, loss is 0.01839659921824932\n",
      "epoch: 5 step: 759, loss is 0.00235889689065516\n",
      "epoch: 5 step: 760, loss is 0.08133187890052795\n",
      "epoch: 5 step: 761, loss is 0.12926092743873596\n",
      "epoch: 5 step: 762, loss is 0.028731154277920723\n",
      "epoch: 5 step: 763, loss is 0.026124048978090286\n",
      "epoch: 5 step: 764, loss is 0.000598363927565515\n",
      "epoch: 5 step: 765, loss is 0.0625557079911232\n",
      "epoch: 5 step: 766, loss is 0.0021351126488298178\n",
      "epoch: 5 step: 767, loss is 0.0012087965151295066\n",
      "epoch: 5 step: 768, loss is 0.005635833367705345\n",
      "epoch: 5 step: 769, loss is 0.0006850233767181635\n",
      "epoch: 5 step: 770, loss is 0.0006420469144359231\n",
      "epoch: 5 step: 771, loss is 0.004921800922602415\n",
      "epoch: 5 step: 772, loss is 0.011557082645595074\n",
      "epoch: 5 step: 773, loss is 0.020232483744621277\n",
      "epoch: 5 step: 774, loss is 0.0009078697767108679\n",
      "epoch: 5 step: 775, loss is 0.003417950589209795\n",
      "epoch: 5 step: 776, loss is 0.0030230742413550615\n",
      "epoch: 5 step: 777, loss is 0.031118959188461304\n",
      "epoch: 5 step: 778, loss is 0.02371816150844097\n",
      "epoch: 5 step: 779, loss is 0.017744777724146843\n",
      "epoch: 5 step: 780, loss is 0.0005793324089609087\n",
      "epoch: 5 step: 781, loss is 0.010181666351854801\n",
      "epoch: 5 step: 782, loss is 0.0037205666303634644\n",
      "epoch: 5 step: 783, loss is 0.04956676438450813\n",
      "epoch: 5 step: 784, loss is 0.002772635780274868\n",
      "epoch: 5 step: 785, loss is 0.0018268442945554852\n",
      "epoch: 5 step: 786, loss is 0.22836825251579285\n",
      "epoch: 5 step: 787, loss is 0.030318934470415115\n",
      "epoch: 5 step: 788, loss is 0.013868276961147785\n",
      "epoch: 5 step: 789, loss is 0.042073048651218414\n",
      "epoch: 5 step: 790, loss is 0.0008628957439213991\n",
      "epoch: 5 step: 791, loss is 0.00011964913574047387\n",
      "epoch: 5 step: 792, loss is 0.0004259837733116001\n",
      "epoch: 5 step: 793, loss is 0.0016794686671346426\n",
      "epoch: 5 step: 794, loss is 0.003831173526123166\n",
      "epoch: 5 step: 795, loss is 0.002232105005532503\n",
      "epoch: 5 step: 796, loss is 0.19598808884620667\n",
      "epoch: 5 step: 797, loss is 0.001642376882955432\n",
      "epoch: 5 step: 798, loss is 0.011643667705357075\n",
      "epoch: 5 step: 799, loss is 0.0006707412539981306\n",
      "epoch: 5 step: 800, loss is 0.0015003788284957409\n",
      "epoch: 5 step: 801, loss is 0.17787691950798035\n",
      "epoch: 5 step: 802, loss is 0.02333456464111805\n",
      "epoch: 5 step: 803, loss is 0.0004126302956137806\n",
      "epoch: 5 step: 804, loss is 0.03742134943604469\n",
      "epoch: 5 step: 805, loss is 0.047511469572782516\n",
      "epoch: 5 step: 806, loss is 0.019667955115437508\n",
      "epoch: 5 step: 807, loss is 0.07723665982484818\n",
      "epoch: 5 step: 808, loss is 0.0017821070505306125\n",
      "epoch: 5 step: 809, loss is 0.003051700070500374\n",
      "epoch: 5 step: 810, loss is 0.051497768610715866\n",
      "epoch: 5 step: 811, loss is 0.014584163203835487\n",
      "epoch: 5 step: 812, loss is 0.0018813377246260643\n",
      "epoch: 5 step: 813, loss is 0.003550907364115119\n",
      "epoch: 5 step: 814, loss is 0.00045296584721654654\n",
      "epoch: 5 step: 815, loss is 4.1661623981781304e-05\n",
      "epoch: 5 step: 816, loss is 0.05477702617645264\n",
      "epoch: 5 step: 817, loss is 0.07441335171461105\n",
      "epoch: 5 step: 818, loss is 0.006775802932679653\n",
      "epoch: 5 step: 819, loss is 0.03203053027391434\n",
      "epoch: 5 step: 820, loss is 0.20995627343654633\n",
      "epoch: 5 step: 821, loss is 0.21310614049434662\n",
      "epoch: 5 step: 822, loss is 0.0019099301425740123\n",
      "epoch: 5 step: 823, loss is 0.002049267292022705\n",
      "epoch: 5 step: 824, loss is 0.001841367338784039\n",
      "epoch: 5 step: 825, loss is 0.0008683555643074214\n",
      "epoch: 5 step: 826, loss is 0.05277257412672043\n",
      "epoch: 5 step: 827, loss is 0.024328332394361496\n",
      "epoch: 5 step: 828, loss is 0.0039318762719631195\n",
      "epoch: 5 step: 829, loss is 0.010171402245759964\n",
      "epoch: 5 step: 830, loss is 0.0017295627621933818\n",
      "epoch: 5 step: 831, loss is 0.0021468501072376966\n",
      "epoch: 5 step: 832, loss is 0.005304145161062479\n",
      "epoch: 5 step: 833, loss is 0.0004903228837065399\n",
      "epoch: 5 step: 834, loss is 0.03399526700377464\n",
      "epoch: 5 step: 835, loss is 0.0035980595275759697\n",
      "epoch: 5 step: 836, loss is 0.15876418352127075\n",
      "epoch: 5 step: 837, loss is 0.008974192664027214\n",
      "epoch: 5 step: 838, loss is 0.09650208801031113\n",
      "epoch: 5 step: 839, loss is 0.007757720071822405\n",
      "epoch: 5 step: 840, loss is 0.07669077813625336\n",
      "epoch: 5 step: 841, loss is 0.0005349604180082679\n",
      "epoch: 5 step: 842, loss is 0.0005134273669682443\n",
      "epoch: 5 step: 843, loss is 0.07676802575588226\n",
      "epoch: 5 step: 844, loss is 0.007956737652420998\n",
      "epoch: 5 step: 845, loss is 0.2071755826473236\n",
      "epoch: 5 step: 846, loss is 0.004021194763481617\n",
      "epoch: 5 step: 847, loss is 0.12142569571733475\n",
      "epoch: 5 step: 848, loss is 0.000997520750388503\n",
      "epoch: 5 step: 849, loss is 0.01615883782505989\n",
      "epoch: 5 step: 850, loss is 0.0034900037571787834\n",
      "epoch: 5 step: 851, loss is 0.0017047792207449675\n",
      "epoch: 5 step: 852, loss is 0.018400942906737328\n",
      "epoch: 5 step: 853, loss is 0.11846847832202911\n",
      "epoch: 5 step: 854, loss is 0.007214553654193878\n",
      "epoch: 5 step: 855, loss is 0.0037398547865450382\n",
      "epoch: 5 step: 856, loss is 0.0009141190093941987\n",
      "epoch: 5 step: 857, loss is 0.04040095955133438\n",
      "epoch: 5 step: 858, loss is 0.0013692124048247933\n",
      "epoch: 5 step: 859, loss is 0.14449746906757355\n",
      "epoch: 5 step: 860, loss is 0.018585072830319405\n",
      "epoch: 5 step: 861, loss is 0.0046685319393873215\n",
      "epoch: 5 step: 862, loss is 0.000798012362793088\n",
      "epoch: 5 step: 863, loss is 0.0031173033639788628\n",
      "epoch: 5 step: 864, loss is 0.05095898360013962\n",
      "epoch: 5 step: 865, loss is 0.019731583073735237\n",
      "epoch: 5 step: 866, loss is 0.002671646187081933\n",
      "epoch: 5 step: 867, loss is 0.012982656247913837\n",
      "epoch: 5 step: 868, loss is 0.0004562201793305576\n",
      "epoch: 5 step: 869, loss is 0.018609585240483284\n",
      "epoch: 5 step: 870, loss is 0.014245226047933102\n",
      "epoch: 5 step: 871, loss is 0.04771482199430466\n",
      "epoch: 5 step: 872, loss is 0.0508832111954689\n",
      "epoch: 5 step: 873, loss is 0.006802195683121681\n",
      "epoch: 5 step: 874, loss is 0.00462305499240756\n",
      "epoch: 5 step: 875, loss is 0.0003935142303816974\n",
      "epoch: 5 step: 876, loss is 0.046157050877809525\n",
      "epoch: 5 step: 877, loss is 0.0671074166893959\n",
      "epoch: 5 step: 878, loss is 0.024747993797063828\n",
      "epoch: 5 step: 879, loss is 0.03269382193684578\n",
      "epoch: 5 step: 880, loss is 0.5418906211853027\n",
      "epoch: 5 step: 881, loss is 0.000440857169451192\n",
      "epoch: 5 step: 882, loss is 0.0007065895479172468\n",
      "epoch: 5 step: 883, loss is 0.011413054540753365\n",
      "epoch: 5 step: 884, loss is 0.0018804511055350304\n",
      "epoch: 5 step: 885, loss is 0.027085794135928154\n",
      "epoch: 5 step: 886, loss is 0.0019133262103423476\n",
      "epoch: 5 step: 887, loss is 0.0010331595549359918\n",
      "epoch: 5 step: 888, loss is 0.006165033206343651\n",
      "epoch: 5 step: 889, loss is 0.05058828368782997\n",
      "epoch: 5 step: 890, loss is 0.0015876381658017635\n",
      "epoch: 5 step: 891, loss is 0.18838146328926086\n",
      "epoch: 5 step: 892, loss is 0.0063880253583192825\n",
      "epoch: 5 step: 893, loss is 0.26003220677375793\n",
      "epoch: 5 step: 894, loss is 0.004738479852676392\n",
      "epoch: 5 step: 895, loss is 0.0007764470647089183\n",
      "epoch: 5 step: 896, loss is 0.0060137975960969925\n",
      "epoch: 5 step: 897, loss is 0.02086678519845009\n",
      "epoch: 5 step: 898, loss is 0.08947968482971191\n",
      "epoch: 5 step: 899, loss is 0.002863840665668249\n",
      "epoch: 5 step: 900, loss is 0.20643438398838043\n",
      "epoch: 5 step: 901, loss is 0.012912525795400143\n",
      "epoch: 5 step: 902, loss is 0.0032981724943965673\n",
      "epoch: 5 step: 903, loss is 0.019319018349051476\n",
      "epoch: 5 step: 904, loss is 0.005882388446480036\n",
      "epoch: 5 step: 905, loss is 0.010349842719733715\n",
      "epoch: 5 step: 906, loss is 0.0019541902001947165\n",
      "epoch: 5 step: 907, loss is 0.02639947272837162\n",
      "epoch: 5 step: 908, loss is 0.003207672853022814\n",
      "epoch: 5 step: 909, loss is 0.0184748787432909\n",
      "epoch: 5 step: 910, loss is 0.01939667947590351\n",
      "epoch: 5 step: 911, loss is 0.006122718099504709\n",
      "epoch: 5 step: 912, loss is 0.03178870305418968\n",
      "epoch: 5 step: 913, loss is 0.005480623338371515\n",
      "epoch: 5 step: 914, loss is 0.001959765562787652\n",
      "epoch: 5 step: 915, loss is 0.02436969429254532\n",
      "epoch: 5 step: 916, loss is 0.019501203671097755\n",
      "epoch: 5 step: 917, loss is 0.059587977826595306\n",
      "epoch: 5 step: 918, loss is 0.00849928893148899\n",
      "epoch: 5 step: 919, loss is 0.041730429977178574\n",
      "epoch: 5 step: 920, loss is 0.10441397875547409\n",
      "epoch: 5 step: 921, loss is 0.14603684842586517\n",
      "epoch: 5 step: 922, loss is 0.052099455147981644\n",
      "epoch: 5 step: 923, loss is 0.026187945157289505\n",
      "epoch: 5 step: 924, loss is 0.0014708411181345582\n",
      "epoch: 5 step: 925, loss is 0.0007352454122155905\n",
      "epoch: 5 step: 926, loss is 0.02284325659275055\n",
      "epoch: 5 step: 927, loss is 0.03526587784290314\n",
      "epoch: 5 step: 928, loss is 0.05777337774634361\n",
      "epoch: 5 step: 929, loss is 0.0008484602440148592\n",
      "epoch: 5 step: 930, loss is 0.16840367019176483\n",
      "epoch: 5 step: 931, loss is 0.030399009585380554\n",
      "epoch: 5 step: 932, loss is 0.0005648722872138023\n",
      "epoch: 5 step: 933, loss is 0.000474428350571543\n",
      "epoch: 5 step: 934, loss is 0.0004104271938558668\n",
      "epoch: 5 step: 935, loss is 0.06454182416200638\n",
      "epoch: 5 step: 936, loss is 0.014478444121778011\n",
      "epoch: 5 step: 937, loss is 0.020217537879943848\n",
      "epoch: 5 step: 938, loss is 0.0009805206209421158\n",
      "epoch: 5 step: 939, loss is 0.0002841906389221549\n",
      "epoch: 5 step: 940, loss is 0.02500193938612938\n",
      "epoch: 5 step: 941, loss is 0.042054805904626846\n",
      "epoch: 5 step: 942, loss is 0.005913998000323772\n",
      "epoch: 5 step: 943, loss is 0.0007136254571378231\n",
      "epoch: 5 step: 944, loss is 0.00037291657645255327\n",
      "epoch: 5 step: 945, loss is 0.008603204041719437\n",
      "epoch: 5 step: 946, loss is 0.12753987312316895\n",
      "epoch: 5 step: 947, loss is 0.0009197350591421127\n",
      "epoch: 5 step: 948, loss is 0.0885363444685936\n",
      "epoch: 5 step: 949, loss is 0.0009197668405249715\n",
      "epoch: 5 step: 950, loss is 0.00744622154161334\n",
      "epoch: 5 step: 951, loss is 0.027884988114237785\n",
      "epoch: 5 step: 952, loss is 0.08004870265722275\n",
      "epoch: 5 step: 953, loss is 0.03171307593584061\n",
      "epoch: 5 step: 954, loss is 0.0018027262995019555\n",
      "epoch: 5 step: 955, loss is 0.027000850066542625\n",
      "epoch: 5 step: 956, loss is 0.09196213632822037\n",
      "epoch: 5 step: 957, loss is 0.051153816282749176\n",
      "epoch: 5 step: 958, loss is 0.0141388438642025\n",
      "epoch: 5 step: 959, loss is 0.010803806595504284\n",
      "epoch: 5 step: 960, loss is 0.0011668121442198753\n",
      "epoch: 5 step: 961, loss is 0.07032042741775513\n",
      "epoch: 5 step: 962, loss is 0.0011241583852097392\n",
      "epoch: 5 step: 963, loss is 0.0771801769733429\n",
      "epoch: 5 step: 964, loss is 0.004176144953817129\n",
      "epoch: 5 step: 965, loss is 0.09311435371637344\n",
      "epoch: 5 step: 966, loss is 0.0029452794697135687\n",
      "epoch: 5 step: 967, loss is 0.4382590651512146\n",
      "epoch: 5 step: 968, loss is 0.26786211133003235\n",
      "epoch: 5 step: 969, loss is 0.014363707043230534\n",
      "epoch: 5 step: 970, loss is 0.0010861019836738706\n",
      "epoch: 5 step: 971, loss is 0.0026589527260512114\n",
      "epoch: 5 step: 972, loss is 0.006745007820427418\n",
      "epoch: 5 step: 973, loss is 0.02401355840265751\n",
      "epoch: 5 step: 974, loss is 0.02436058223247528\n",
      "epoch: 5 step: 975, loss is 0.009924744255840778\n",
      "epoch: 5 step: 976, loss is 0.03265686333179474\n",
      "epoch: 5 step: 977, loss is 0.05028478428721428\n",
      "epoch: 5 step: 978, loss is 0.0026771482080221176\n",
      "epoch: 5 step: 979, loss is 0.008025454357266426\n",
      "epoch: 5 step: 980, loss is 0.01443469524383545\n",
      "epoch: 5 step: 981, loss is 0.013774300925433636\n",
      "epoch: 5 step: 982, loss is 0.039259735494852066\n",
      "epoch: 5 step: 983, loss is 0.08680510520935059\n",
      "epoch: 5 step: 984, loss is 0.016068726778030396\n",
      "epoch: 5 step: 985, loss is 0.012829139828681946\n",
      "epoch: 5 step: 986, loss is 0.051396600902080536\n",
      "epoch: 5 step: 987, loss is 0.006308612413704395\n",
      "epoch: 5 step: 988, loss is 0.11922959238290787\n",
      "epoch: 5 step: 989, loss is 0.009333545342087746\n",
      "epoch: 5 step: 990, loss is 0.016707029193639755\n",
      "epoch: 5 step: 991, loss is 0.05300252512097359\n",
      "epoch: 5 step: 992, loss is 0.004286042880266905\n",
      "epoch: 5 step: 993, loss is 0.08882824331521988\n",
      "epoch: 5 step: 994, loss is 0.0027400758117437363\n",
      "epoch: 5 step: 995, loss is 0.00922065507620573\n",
      "epoch: 5 step: 996, loss is 0.02245006337761879\n",
      "epoch: 5 step: 997, loss is 0.007344698067754507\n",
      "epoch: 5 step: 998, loss is 0.000281511340290308\n",
      "epoch: 5 step: 999, loss is 0.004901757463812828\n",
      "epoch: 5 step: 1000, loss is 0.10088930279016495\n",
      "epoch: 5 step: 1001, loss is 0.000850444077514112\n",
      "epoch: 5 step: 1002, loss is 0.036210477352142334\n",
      "epoch: 5 step: 1003, loss is 0.027245502918958664\n",
      "epoch: 5 step: 1004, loss is 0.0038453321903944016\n",
      "epoch: 5 step: 1005, loss is 0.03958610072731972\n",
      "epoch: 5 step: 1006, loss is 0.026465071365237236\n",
      "epoch: 5 step: 1007, loss is 0.031571149826049805\n",
      "epoch: 5 step: 1008, loss is 0.0072953286580741405\n",
      "epoch: 5 step: 1009, loss is 0.08924300968647003\n",
      "epoch: 5 step: 1010, loss is 0.04395514354109764\n",
      "epoch: 5 step: 1011, loss is 0.012904300354421139\n",
      "epoch: 5 step: 1012, loss is 0.008722403086721897\n",
      "epoch: 5 step: 1013, loss is 0.0016674869693815708\n",
      "epoch: 5 step: 1014, loss is 0.10715402662754059\n",
      "epoch: 5 step: 1015, loss is 0.0004866765521001071\n",
      "epoch: 5 step: 1016, loss is 0.0018286738777533174\n",
      "epoch: 5 step: 1017, loss is 0.07490695267915726\n",
      "epoch: 5 step: 1018, loss is 0.007585788145661354\n",
      "epoch: 5 step: 1019, loss is 0.01230185478925705\n",
      "epoch: 5 step: 1020, loss is 0.08897759765386581\n",
      "epoch: 5 step: 1021, loss is 0.02018146403133869\n",
      "epoch: 5 step: 1022, loss is 0.0006649616407230496\n",
      "epoch: 5 step: 1023, loss is 0.04775732383131981\n",
      "epoch: 5 step: 1024, loss is 0.02439909614622593\n",
      "epoch: 5 step: 1025, loss is 0.002387311076745391\n",
      "epoch: 5 step: 1026, loss is 0.14000600576400757\n",
      "epoch: 5 step: 1027, loss is 0.01798763871192932\n",
      "epoch: 5 step: 1028, loss is 0.0023034089244902134\n",
      "epoch: 5 step: 1029, loss is 0.0038484514225274324\n",
      "epoch: 5 step: 1030, loss is 0.0011678639566525817\n",
      "epoch: 5 step: 1031, loss is 0.012844352051615715\n",
      "epoch: 5 step: 1032, loss is 0.07156777381896973\n",
      "epoch: 5 step: 1033, loss is 0.0029472631867974997\n",
      "epoch: 5 step: 1034, loss is 0.15198679268360138\n",
      "epoch: 5 step: 1035, loss is 0.049667246639728546\n",
      "epoch: 5 step: 1036, loss is 0.11896665394306183\n",
      "epoch: 5 step: 1037, loss is 0.011020919308066368\n",
      "epoch: 5 step: 1038, loss is 0.14816860854625702\n",
      "epoch: 5 step: 1039, loss is 0.07306002825498581\n",
      "epoch: 5 step: 1040, loss is 0.0391937680542469\n",
      "epoch: 5 step: 1041, loss is 0.08043503761291504\n",
      "epoch: 5 step: 1042, loss is 0.0008231704123318195\n",
      "epoch: 5 step: 1043, loss is 0.002818117616698146\n",
      "epoch: 5 step: 1044, loss is 0.03136591240763664\n",
      "epoch: 5 step: 1045, loss is 0.0010993859032168984\n",
      "epoch: 5 step: 1046, loss is 0.4201123118400574\n",
      "epoch: 5 step: 1047, loss is 0.0034014261327683926\n",
      "epoch: 5 step: 1048, loss is 0.0004854347789660096\n",
      "epoch: 5 step: 1049, loss is 0.0163840614259243\n",
      "epoch: 5 step: 1050, loss is 0.02069723792374134\n",
      "epoch: 5 step: 1051, loss is 0.02886703610420227\n",
      "epoch: 5 step: 1052, loss is 0.04359876736998558\n",
      "epoch: 5 step: 1053, loss is 0.011433102190494537\n",
      "epoch: 5 step: 1054, loss is 0.0324457585811615\n",
      "epoch: 5 step: 1055, loss is 0.023992309346795082\n",
      "epoch: 5 step: 1056, loss is 0.08807220309972763\n",
      "epoch: 5 step: 1057, loss is 0.023481406271457672\n",
      "epoch: 5 step: 1058, loss is 0.0026669162325561047\n",
      "epoch: 5 step: 1059, loss is 0.03072722814977169\n",
      "epoch: 5 step: 1060, loss is 0.020133864134550095\n",
      "epoch: 5 step: 1061, loss is 0.004852148238569498\n",
      "epoch: 5 step: 1062, loss is 0.006675227079540491\n",
      "epoch: 5 step: 1063, loss is 0.013643129728734493\n",
      "epoch: 5 step: 1064, loss is 0.0022728305775672197\n",
      "epoch: 5 step: 1065, loss is 0.0070733241736888885\n",
      "epoch: 5 step: 1066, loss is 0.029387356713414192\n",
      "epoch: 5 step: 1067, loss is 0.01309181284159422\n",
      "epoch: 5 step: 1068, loss is 0.07101405411958694\n",
      "epoch: 5 step: 1069, loss is 0.014349309727549553\n",
      "epoch: 5 step: 1070, loss is 0.0070755816996097565\n",
      "epoch: 5 step: 1071, loss is 0.0015430401545017958\n",
      "epoch: 5 step: 1072, loss is 0.010304192081093788\n",
      "epoch: 5 step: 1073, loss is 0.0025272974744439125\n",
      "epoch: 5 step: 1074, loss is 0.012304174713790417\n",
      "epoch: 5 step: 1075, loss is 0.0006428404012694955\n",
      "epoch: 5 step: 1076, loss is 0.006870632525533438\n",
      "epoch: 5 step: 1077, loss is 0.012462235055863857\n",
      "epoch: 5 step: 1078, loss is 0.002439575269818306\n",
      "epoch: 5 step: 1079, loss is 0.004827938042581081\n",
      "epoch: 5 step: 1080, loss is 0.07322929054498672\n",
      "epoch: 5 step: 1081, loss is 0.001321893185377121\n",
      "epoch: 5 step: 1082, loss is 0.0012382566928863525\n",
      "epoch: 5 step: 1083, loss is 0.020919835194945335\n",
      "epoch: 5 step: 1084, loss is 0.07572327554225922\n",
      "epoch: 5 step: 1085, loss is 0.0009639606578275561\n",
      "epoch: 5 step: 1086, loss is 0.0009952939581125975\n",
      "epoch: 5 step: 1087, loss is 0.0011804207460954785\n",
      "epoch: 5 step: 1088, loss is 0.012080085463821888\n",
      "epoch: 5 step: 1089, loss is 0.0004679069679696113\n",
      "epoch: 5 step: 1090, loss is 0.0006190540152601898\n",
      "epoch: 5 step: 1091, loss is 0.04867260158061981\n",
      "epoch: 5 step: 1092, loss is 0.02359393611550331\n",
      "epoch: 5 step: 1093, loss is 0.0002983382437378168\n",
      "epoch: 5 step: 1094, loss is 0.23876246809959412\n",
      "epoch: 5 step: 1095, loss is 0.019771864637732506\n",
      "epoch: 5 step: 1096, loss is 0.0598982572555542\n",
      "epoch: 5 step: 1097, loss is 0.011367141269147396\n",
      "epoch: 5 step: 1098, loss is 0.00612153485417366\n",
      "epoch: 5 step: 1099, loss is 0.0028916392475366592\n",
      "epoch: 5 step: 1100, loss is 0.00894956011325121\n",
      "epoch: 5 step: 1101, loss is 0.003367564408108592\n",
      "epoch: 5 step: 1102, loss is 0.00044222071301192045\n",
      "epoch: 5 step: 1103, loss is 0.12379477918148041\n",
      "epoch: 5 step: 1104, loss is 0.08401565253734589\n",
      "epoch: 5 step: 1105, loss is 0.005184496752917767\n",
      "epoch: 5 step: 1106, loss is 0.0030568966176360846\n",
      "epoch: 5 step: 1107, loss is 0.013971151784062386\n",
      "epoch: 5 step: 1108, loss is 0.0006123415078036487\n",
      "epoch: 5 step: 1109, loss is 0.001588774030096829\n",
      "epoch: 5 step: 1110, loss is 0.01142856478691101\n",
      "epoch: 5 step: 1111, loss is 0.00046125860535539687\n",
      "epoch: 5 step: 1112, loss is 0.0354389064013958\n",
      "epoch: 5 step: 1113, loss is 0.028781695291399956\n",
      "epoch: 5 step: 1114, loss is 0.025178441777825356\n",
      "epoch: 5 step: 1115, loss is 0.003099544905126095\n",
      "epoch: 5 step: 1116, loss is 0.00065424240892753\n",
      "epoch: 5 step: 1117, loss is 0.00017413213208783418\n",
      "epoch: 5 step: 1118, loss is 0.021982677280902863\n",
      "epoch: 5 step: 1119, loss is 0.029850458726286888\n",
      "epoch: 5 step: 1120, loss is 0.00405415752902627\n",
      "epoch: 5 step: 1121, loss is 0.008934005163609982\n",
      "epoch: 5 step: 1122, loss is 0.025095805525779724\n",
      "epoch: 5 step: 1123, loss is 0.01708734966814518\n",
      "epoch: 5 step: 1124, loss is 0.0013423528289422393\n",
      "epoch: 5 step: 1125, loss is 0.0008706296212039888\n",
      "epoch: 5 step: 1126, loss is 0.002174608875066042\n",
      "epoch: 5 step: 1127, loss is 0.021374361589550972\n",
      "epoch: 5 step: 1128, loss is 0.21207062900066376\n",
      "epoch: 5 step: 1129, loss is 0.013358401134610176\n",
      "epoch: 5 step: 1130, loss is 0.0019112199079245329\n",
      "epoch: 5 step: 1131, loss is 0.007417017128318548\n",
      "epoch: 5 step: 1132, loss is 0.0006129493121989071\n",
      "epoch: 5 step: 1133, loss is 0.07350320369005203\n",
      "epoch: 5 step: 1134, loss is 0.002967623993754387\n",
      "epoch: 5 step: 1135, loss is 0.24265258014202118\n",
      "epoch: 5 step: 1136, loss is 0.006552075035870075\n",
      "epoch: 5 step: 1137, loss is 0.0656740814447403\n",
      "epoch: 5 step: 1138, loss is 0.00030164449708536267\n",
      "epoch: 5 step: 1139, loss is 0.007144012954086065\n",
      "epoch: 5 step: 1140, loss is 0.04002038389444351\n",
      "epoch: 5 step: 1141, loss is 0.0010782952886074781\n",
      "epoch: 5 step: 1142, loss is 0.005515879485756159\n",
      "epoch: 5 step: 1143, loss is 0.050631504505872726\n",
      "epoch: 5 step: 1144, loss is 0.002765215467661619\n",
      "epoch: 5 step: 1145, loss is 0.20634883642196655\n",
      "epoch: 5 step: 1146, loss is 0.06343981623649597\n",
      "epoch: 5 step: 1147, loss is 0.005613969173282385\n",
      "epoch: 5 step: 1148, loss is 0.007032046094536781\n",
      "epoch: 5 step: 1149, loss is 0.025460660457611084\n",
      "epoch: 5 step: 1150, loss is 0.020572861656546593\n",
      "epoch: 5 step: 1151, loss is 0.003399740671738982\n",
      "epoch: 5 step: 1152, loss is 0.02066025510430336\n",
      "epoch: 5 step: 1153, loss is 0.031101465225219727\n",
      "epoch: 5 step: 1154, loss is 0.12377378344535828\n",
      "epoch: 5 step: 1155, loss is 0.08467035740613937\n",
      "epoch: 5 step: 1156, loss is 0.039661020040512085\n",
      "epoch: 5 step: 1157, loss is 0.02380877546966076\n",
      "epoch: 5 step: 1158, loss is 2.1977535652695224e-05\n",
      "epoch: 5 step: 1159, loss is 0.001645868644118309\n",
      "epoch: 5 step: 1160, loss is 0.004496318753808737\n",
      "epoch: 5 step: 1161, loss is 0.016811950132250786\n",
      "epoch: 5 step: 1162, loss is 0.06289467215538025\n",
      "epoch: 5 step: 1163, loss is 0.061567023396492004\n",
      "epoch: 5 step: 1164, loss is 0.1515401154756546\n",
      "epoch: 5 step: 1165, loss is 0.000726289872545749\n",
      "epoch: 5 step: 1166, loss is 0.0015979076270014048\n",
      "epoch: 5 step: 1167, loss is 0.006542104296386242\n",
      "epoch: 5 step: 1168, loss is 0.04783042520284653\n",
      "epoch: 5 step: 1169, loss is 0.00043146664393134415\n",
      "epoch: 5 step: 1170, loss is 0.05588134750723839\n",
      "epoch: 5 step: 1171, loss is 0.010316211730241776\n",
      "epoch: 5 step: 1172, loss is 0.015313992276787758\n",
      "epoch: 5 step: 1173, loss is 0.16442112624645233\n",
      "epoch: 5 step: 1174, loss is 0.006285932380706072\n",
      "epoch: 5 step: 1175, loss is 0.02991044707596302\n",
      "epoch: 5 step: 1176, loss is 0.024416200816631317\n",
      "epoch: 5 step: 1177, loss is 0.002684898441657424\n",
      "epoch: 5 step: 1178, loss is 0.0006833720835857093\n",
      "epoch: 5 step: 1179, loss is 0.07305349409580231\n",
      "epoch: 5 step: 1180, loss is 0.11670450866222382\n",
      "epoch: 5 step: 1181, loss is 0.300370991230011\n",
      "epoch: 5 step: 1182, loss is 0.0007192155462689698\n",
      "epoch: 5 step: 1183, loss is 0.01897033117711544\n",
      "epoch: 5 step: 1184, loss is 0.002482636598870158\n",
      "epoch: 5 step: 1185, loss is 0.004914707969874144\n",
      "epoch: 5 step: 1186, loss is 0.001211385359056294\n",
      "epoch: 5 step: 1187, loss is 0.04023633897304535\n",
      "epoch: 5 step: 1188, loss is 0.029141871258616447\n",
      "epoch: 5 step: 1189, loss is 0.0031618685461580753\n",
      "epoch: 5 step: 1190, loss is 0.1304606944322586\n",
      "epoch: 5 step: 1191, loss is 0.001953698229044676\n",
      "epoch: 5 step: 1192, loss is 0.05727303773164749\n",
      "epoch: 5 step: 1193, loss is 0.005847721826285124\n",
      "epoch: 5 step: 1194, loss is 0.18202781677246094\n",
      "epoch: 5 step: 1195, loss is 0.03081737458705902\n",
      "epoch: 5 step: 1196, loss is 0.00015283151878975332\n",
      "epoch: 5 step: 1197, loss is 0.02368811145424843\n",
      "epoch: 5 step: 1198, loss is 0.0028984129894524813\n",
      "epoch: 5 step: 1199, loss is 0.11053263396024704\n",
      "epoch: 5 step: 1200, loss is 0.022568246349692345\n",
      "epoch: 5 step: 1201, loss is 0.0248072799295187\n",
      "epoch: 5 step: 1202, loss is 0.011928192339837551\n",
      "epoch: 5 step: 1203, loss is 0.0007934444583952427\n",
      "epoch: 5 step: 1204, loss is 0.0192278865724802\n",
      "epoch: 5 step: 1205, loss is 0.006953187752515078\n",
      "epoch: 5 step: 1206, loss is 0.015559980645775795\n",
      "epoch: 5 step: 1207, loss is 0.00046989534166641533\n",
      "epoch: 5 step: 1208, loss is 0.0028991415165364742\n",
      "epoch: 5 step: 1209, loss is 0.012085868045687675\n",
      "epoch: 5 step: 1210, loss is 0.002336266217753291\n",
      "epoch: 5 step: 1211, loss is 0.21562950313091278\n",
      "epoch: 5 step: 1212, loss is 0.10885101556777954\n",
      "epoch: 5 step: 1213, loss is 0.12341419607400894\n",
      "epoch: 5 step: 1214, loss is 0.001378608518280089\n",
      "epoch: 5 step: 1215, loss is 0.03213096410036087\n",
      "epoch: 5 step: 1216, loss is 0.00014312718121800572\n",
      "epoch: 5 step: 1217, loss is 0.18156953155994415\n",
      "epoch: 5 step: 1218, loss is 0.0010844357311725616\n",
      "epoch: 5 step: 1219, loss is 0.0011182386660948396\n",
      "epoch: 5 step: 1220, loss is 0.003332914784550667\n",
      "epoch: 5 step: 1221, loss is 0.015969691798090935\n",
      "epoch: 5 step: 1222, loss is 0.0005411082529462874\n",
      "epoch: 5 step: 1223, loss is 0.018306102603673935\n",
      "epoch: 5 step: 1224, loss is 0.0289979986846447\n",
      "epoch: 5 step: 1225, loss is 0.0017576562240719795\n",
      "epoch: 5 step: 1226, loss is 0.006961063481867313\n",
      "epoch: 5 step: 1227, loss is 0.002917218254879117\n",
      "epoch: 5 step: 1228, loss is 0.30655238032341003\n",
      "epoch: 5 step: 1229, loss is 0.0023394159507006407\n",
      "epoch: 5 step: 1230, loss is 0.005408633034676313\n",
      "epoch: 5 step: 1231, loss is 0.013082904741168022\n",
      "epoch: 5 step: 1232, loss is 0.056854549795389175\n",
      "epoch: 5 step: 1233, loss is 0.015388470143079758\n",
      "epoch: 5 step: 1234, loss is 0.08846605569124222\n",
      "epoch: 5 step: 1235, loss is 0.008825412951409817\n",
      "epoch: 5 step: 1236, loss is 0.23339898884296417\n",
      "epoch: 5 step: 1237, loss is 0.0006414723466150463\n",
      "epoch: 5 step: 1238, loss is 0.027566466480493546\n",
      "epoch: 5 step: 1239, loss is 0.0061797890812158585\n",
      "epoch: 5 step: 1240, loss is 0.00600932352244854\n",
      "epoch: 5 step: 1241, loss is 0.0024448868352919817\n",
      "epoch: 5 step: 1242, loss is 0.05454770848155022\n",
      "epoch: 5 step: 1243, loss is 0.14055149257183075\n",
      "epoch: 5 step: 1244, loss is 0.01390685886144638\n",
      "epoch: 5 step: 1245, loss is 0.05030280724167824\n",
      "epoch: 5 step: 1246, loss is 0.0014974656514823437\n",
      "epoch: 5 step: 1247, loss is 0.007981926202774048\n",
      "epoch: 5 step: 1248, loss is 0.006251895800232887\n",
      "epoch: 5 step: 1249, loss is 0.12581001222133636\n",
      "epoch: 5 step: 1250, loss is 0.0007141298265196383\n",
      "epoch: 5 step: 1251, loss is 0.03462531045079231\n",
      "epoch: 5 step: 1252, loss is 0.006647605448961258\n",
      "epoch: 5 step: 1253, loss is 0.0028473064303398132\n",
      "epoch: 5 step: 1254, loss is 0.02961174212396145\n",
      "epoch: 5 step: 1255, loss is 0.008547068573534489\n",
      "epoch: 5 step: 1256, loss is 0.004273174796253443\n",
      "epoch: 5 step: 1257, loss is 0.004638598300516605\n",
      "epoch: 5 step: 1258, loss is 0.00923039298504591\n",
      "epoch: 5 step: 1259, loss is 0.0034463470801711082\n",
      "epoch: 5 step: 1260, loss is 0.08619698137044907\n",
      "epoch: 5 step: 1261, loss is 0.04508088901638985\n",
      "epoch: 5 step: 1262, loss is 0.002087981440126896\n",
      "epoch: 5 step: 1263, loss is 0.10503366589546204\n",
      "epoch: 5 step: 1264, loss is 0.001130357850342989\n",
      "epoch: 5 step: 1265, loss is 0.005381190218031406\n",
      "epoch: 5 step: 1266, loss is 0.04124261066317558\n",
      "epoch: 5 step: 1267, loss is 0.002404153347015381\n",
      "epoch: 5 step: 1268, loss is 0.00669765193015337\n",
      "epoch: 5 step: 1269, loss is 0.02640467695891857\n",
      "epoch: 5 step: 1270, loss is 0.012488140724599361\n",
      "epoch: 5 step: 1271, loss is 0.002972461050376296\n",
      "epoch: 5 step: 1272, loss is 0.0035516340285539627\n",
      "epoch: 5 step: 1273, loss is 0.022362925112247467\n",
      "epoch: 5 step: 1274, loss is 0.002483511809259653\n",
      "epoch: 5 step: 1275, loss is 0.06225845217704773\n",
      "epoch: 5 step: 1276, loss is 0.019349176436662674\n",
      "epoch: 5 step: 1277, loss is 0.03895558789372444\n",
      "epoch: 5 step: 1278, loss is 0.06258020550012589\n",
      "epoch: 5 step: 1279, loss is 0.02200903370976448\n",
      "epoch: 5 step: 1280, loss is 0.002402326324954629\n",
      "epoch: 5 step: 1281, loss is 0.02123844437301159\n",
      "epoch: 5 step: 1282, loss is 0.002587958239018917\n",
      "epoch: 5 step: 1283, loss is 0.0005916200461797416\n",
      "epoch: 5 step: 1284, loss is 0.00026556209195405245\n",
      "epoch: 5 step: 1285, loss is 0.0003947052173316479\n",
      "epoch: 5 step: 1286, loss is 0.019914617761969566\n",
      "epoch: 5 step: 1287, loss is 0.01852356269955635\n",
      "epoch: 5 step: 1288, loss is 0.012436386197805405\n",
      "epoch: 5 step: 1289, loss is 0.006029650568962097\n",
      "epoch: 5 step: 1290, loss is 0.01153231505304575\n",
      "epoch: 5 step: 1291, loss is 0.002974009606987238\n",
      "epoch: 5 step: 1292, loss is 0.0009010959765873849\n",
      "epoch: 5 step: 1293, loss is 0.016244878992438316\n",
      "epoch: 5 step: 1294, loss is 0.00665358966216445\n",
      "epoch: 5 step: 1295, loss is 0.0004040605272166431\n",
      "epoch: 5 step: 1296, loss is 0.0003487832145765424\n",
      "epoch: 5 step: 1297, loss is 0.0018388698808848858\n",
      "epoch: 5 step: 1298, loss is 0.05650598183274269\n",
      "epoch: 5 step: 1299, loss is 0.0009155680891126394\n",
      "epoch: 5 step: 1300, loss is 0.0347263477742672\n",
      "epoch: 5 step: 1301, loss is 0.0012562039773911238\n",
      "epoch: 5 step: 1302, loss is 0.009546470828354359\n",
      "epoch: 5 step: 1303, loss is 0.0005814089672639966\n",
      "epoch: 5 step: 1304, loss is 0.09015747904777527\n",
      "epoch: 5 step: 1305, loss is 0.0015218071639537811\n",
      "epoch: 5 step: 1306, loss is 0.01826867088675499\n",
      "epoch: 5 step: 1307, loss is 0.003794935764744878\n",
      "epoch: 5 step: 1308, loss is 0.001445750705897808\n",
      "epoch: 5 step: 1309, loss is 0.003974708262830973\n",
      "epoch: 5 step: 1310, loss is 0.000735699140932411\n",
      "epoch: 5 step: 1311, loss is 0.00031661323737353086\n",
      "epoch: 5 step: 1312, loss is 0.005588909611105919\n",
      "epoch: 5 step: 1313, loss is 0.003795310854911804\n",
      "epoch: 5 step: 1314, loss is 0.004178136587142944\n",
      "epoch: 5 step: 1315, loss is 0.0026135030202567577\n",
      "epoch: 5 step: 1316, loss is 0.032832663506269455\n",
      "epoch: 5 step: 1317, loss is 0.00012254608736839145\n",
      "epoch: 5 step: 1318, loss is 0.00609535863623023\n",
      "epoch: 5 step: 1319, loss is 0.005904748570173979\n",
      "epoch: 5 step: 1320, loss is 0.004104201216250658\n",
      "epoch: 5 step: 1321, loss is 0.026928331702947617\n",
      "epoch: 5 step: 1322, loss is 0.005678114481270313\n",
      "epoch: 5 step: 1323, loss is 0.0010811557294800878\n",
      "epoch: 5 step: 1324, loss is 0.000510601734276861\n",
      "epoch: 5 step: 1325, loss is 0.053206123411655426\n",
      "epoch: 5 step: 1326, loss is 0.0002503630530554801\n",
      "epoch: 5 step: 1327, loss is 0.011211203411221504\n",
      "epoch: 5 step: 1328, loss is 0.06342713534832001\n",
      "epoch: 5 step: 1329, loss is 0.0193964671343565\n",
      "epoch: 5 step: 1330, loss is 0.004474524408578873\n",
      "epoch: 5 step: 1331, loss is 0.007952533662319183\n",
      "epoch: 5 step: 1332, loss is 0.0029103432316333055\n",
      "epoch: 5 step: 1333, loss is 0.008471887558698654\n",
      "epoch: 5 step: 1334, loss is 0.00011361956421751529\n",
      "epoch: 5 step: 1335, loss is 0.22291891276836395\n",
      "epoch: 5 step: 1336, loss is 0.0007952206069603562\n",
      "epoch: 5 step: 1337, loss is 0.0006602006033062935\n",
      "epoch: 5 step: 1338, loss is 0.005774358287453651\n",
      "epoch: 5 step: 1339, loss is 0.013441324234008789\n",
      "epoch: 5 step: 1340, loss is 0.022149737924337387\n",
      "epoch: 5 step: 1341, loss is 0.005742140579968691\n",
      "epoch: 5 step: 1342, loss is 0.0012344487477093935\n",
      "epoch: 5 step: 1343, loss is 0.0020603155717253685\n",
      "epoch: 5 step: 1344, loss is 0.003846436273306608\n",
      "epoch: 5 step: 1345, loss is 0.001609442406333983\n",
      "epoch: 5 step: 1346, loss is 0.012751635164022446\n",
      "epoch: 5 step: 1347, loss is 0.004419196862727404\n",
      "epoch: 5 step: 1348, loss is 0.002090885303914547\n",
      "epoch: 5 step: 1349, loss is 0.012832380831241608\n",
      "epoch: 5 step: 1350, loss is 0.03702103719115257\n",
      "epoch: 5 step: 1351, loss is 0.00015784482820890844\n",
      "epoch: 5 step: 1352, loss is 0.00892296340316534\n",
      "epoch: 5 step: 1353, loss is 0.0013887290842831135\n",
      "epoch: 5 step: 1354, loss is 0.0053451466374099255\n",
      "epoch: 5 step: 1355, loss is 0.008648298680782318\n",
      "epoch: 5 step: 1356, loss is 0.0019724001176655293\n",
      "epoch: 5 step: 1357, loss is 0.00046707948786206543\n",
      "epoch: 5 step: 1358, loss is 0.28755658864974976\n",
      "epoch: 5 step: 1359, loss is 0.004182935226708651\n",
      "epoch: 5 step: 1360, loss is 0.023800082504749298\n",
      "epoch: 5 step: 1361, loss is 0.22245818376541138\n",
      "epoch: 5 step: 1362, loss is 0.1688922643661499\n",
      "epoch: 5 step: 1363, loss is 0.0030622254125773907\n",
      "epoch: 5 step: 1364, loss is 1.6652120393700898e-06\n",
      "epoch: 5 step: 1365, loss is 0.0004921132931485772\n",
      "epoch: 5 step: 1366, loss is 0.0001615595683688298\n",
      "epoch: 5 step: 1367, loss is 0.011213569901883602\n",
      "epoch: 5 step: 1368, loss is 0.012010512873530388\n",
      "epoch: 5 step: 1369, loss is 0.021244684234261513\n",
      "epoch: 5 step: 1370, loss is 0.08823828399181366\n",
      "epoch: 5 step: 1371, loss is 0.05304165929555893\n",
      "epoch: 5 step: 1372, loss is 0.002099422039464116\n",
      "epoch: 5 step: 1373, loss is 0.007485568523406982\n",
      "epoch: 5 step: 1374, loss is 0.029774438589811325\n",
      "epoch: 5 step: 1375, loss is 0.033915262669324875\n",
      "epoch: 5 step: 1376, loss is 0.011192480102181435\n",
      "epoch: 5 step: 1377, loss is 0.0008038384839892387\n",
      "epoch: 5 step: 1378, loss is 0.0015515650156885386\n",
      "epoch: 5 step: 1379, loss is 0.1278185099363327\n",
      "epoch: 5 step: 1380, loss is 0.04749796539545059\n",
      "epoch: 5 step: 1381, loss is 0.0038533061742782593\n",
      "epoch: 5 step: 1382, loss is 0.0018755439668893814\n",
      "epoch: 5 step: 1383, loss is 0.02163967676460743\n",
      "epoch: 5 step: 1384, loss is 0.019425010308623314\n",
      "epoch: 5 step: 1385, loss is 0.003180470084771514\n",
      "epoch: 5 step: 1386, loss is 0.15840712189674377\n",
      "epoch: 5 step: 1387, loss is 0.01705756038427353\n",
      "epoch: 5 step: 1388, loss is 0.0001842834026319906\n",
      "epoch: 5 step: 1389, loss is 0.00029362522764131427\n",
      "epoch: 5 step: 1390, loss is 0.0018989871023222804\n",
      "epoch: 5 step: 1391, loss is 0.0002969949273392558\n",
      "epoch: 5 step: 1392, loss is 0.011610317975282669\n",
      "epoch: 5 step: 1393, loss is 0.0008255835855379701\n",
      "epoch: 5 step: 1394, loss is 0.17692823708057404\n",
      "epoch: 5 step: 1395, loss is 0.004564298782497644\n",
      "epoch: 5 step: 1396, loss is 0.009694813750684261\n",
      "epoch: 5 step: 1397, loss is 0.00984437670558691\n",
      "epoch: 5 step: 1398, loss is 0.0023485838901251554\n",
      "epoch: 5 step: 1399, loss is 0.010707897134125233\n",
      "epoch: 5 step: 1400, loss is 0.015611318871378899\n",
      "epoch: 5 step: 1401, loss is 0.02970409020781517\n",
      "epoch: 5 step: 1402, loss is 0.001699234708212316\n",
      "epoch: 5 step: 1403, loss is 0.022060614079236984\n",
      "epoch: 5 step: 1404, loss is 0.007341831922531128\n",
      "epoch: 5 step: 1405, loss is 0.0006433730013668537\n",
      "epoch: 5 step: 1406, loss is 0.0005258178571239114\n",
      "epoch: 5 step: 1407, loss is 0.0091926921159029\n",
      "epoch: 5 step: 1408, loss is 0.0474369153380394\n",
      "epoch: 5 step: 1409, loss is 0.0014701460022479296\n",
      "epoch: 5 step: 1410, loss is 0.001975746126845479\n",
      "epoch: 5 step: 1411, loss is 0.06924891471862793\n",
      "epoch: 5 step: 1412, loss is 0.0010451549896970391\n",
      "epoch: 5 step: 1413, loss is 0.06462091207504272\n",
      "epoch: 5 step: 1414, loss is 0.000340226455591619\n",
      "epoch: 5 step: 1415, loss is 0.024845825508236885\n",
      "epoch: 5 step: 1416, loss is 0.3971082270145416\n",
      "epoch: 5 step: 1417, loss is 0.014238565228879452\n",
      "epoch: 5 step: 1418, loss is 0.006173752248287201\n",
      "epoch: 5 step: 1419, loss is 0.0020963610149919987\n",
      "epoch: 5 step: 1420, loss is 0.02819175086915493\n",
      "epoch: 5 step: 1421, loss is 0.008455046452581882\n",
      "epoch: 5 step: 1422, loss is 0.00048678595339879394\n",
      "epoch: 5 step: 1423, loss is 0.0023590230848640203\n",
      "epoch: 5 step: 1424, loss is 0.0006383029976859689\n",
      "epoch: 5 step: 1425, loss is 0.0020004725083708763\n",
      "epoch: 5 step: 1426, loss is 0.01566474884748459\n",
      "epoch: 5 step: 1427, loss is 0.007822916842997074\n",
      "epoch: 5 step: 1428, loss is 0.08305049687623978\n",
      "epoch: 5 step: 1429, loss is 0.11770049482584\n",
      "epoch: 5 step: 1430, loss is 0.01334198098629713\n",
      "epoch: 5 step: 1431, loss is 0.0008600460132583976\n",
      "epoch: 5 step: 1432, loss is 0.06588184833526611\n",
      "epoch: 5 step: 1433, loss is 0.002569695236161351\n",
      "epoch: 5 step: 1434, loss is 0.0214934591203928\n",
      "epoch: 5 step: 1435, loss is 0.00310926022939384\n",
      "epoch: 5 step: 1436, loss is 0.02499493770301342\n",
      "epoch: 5 step: 1437, loss is 0.016154199838638306\n",
      "epoch: 5 step: 1438, loss is 0.013025923632085323\n",
      "epoch: 5 step: 1439, loss is 0.08353682607412338\n",
      "epoch: 5 step: 1440, loss is 0.1706106811761856\n",
      "epoch: 5 step: 1441, loss is 0.0007868952816352248\n",
      "epoch: 5 step: 1442, loss is 0.005981729365885258\n",
      "epoch: 5 step: 1443, loss is 0.014801900833845139\n",
      "epoch: 5 step: 1444, loss is 0.0022829545196145773\n",
      "epoch: 5 step: 1445, loss is 0.0026718173176050186\n",
      "epoch: 5 step: 1446, loss is 0.04212784767150879\n",
      "epoch: 5 step: 1447, loss is 0.012136534787714481\n",
      "epoch: 5 step: 1448, loss is 0.04166414961218834\n",
      "epoch: 5 step: 1449, loss is 0.01811947673559189\n",
      "epoch: 5 step: 1450, loss is 0.06164124980568886\n",
      "epoch: 5 step: 1451, loss is 0.023958873003721237\n",
      "epoch: 5 step: 1452, loss is 0.07190744578838348\n",
      "epoch: 5 step: 1453, loss is 0.006936077494174242\n",
      "epoch: 5 step: 1454, loss is 0.02412065491080284\n",
      "epoch: 5 step: 1455, loss is 0.051762405782938004\n",
      "epoch: 5 step: 1456, loss is 0.003141043009236455\n",
      "epoch: 5 step: 1457, loss is 0.0880521759390831\n",
      "epoch: 5 step: 1458, loss is 0.00018402881687507033\n",
      "epoch: 5 step: 1459, loss is 0.0014467587461695075\n",
      "epoch: 5 step: 1460, loss is 0.06897792220115662\n",
      "epoch: 5 step: 1461, loss is 0.051849666982889175\n",
      "epoch: 5 step: 1462, loss is 0.002337539102882147\n",
      "epoch: 5 step: 1463, loss is 0.0024480654392391443\n",
      "epoch: 5 step: 1464, loss is 0.0011666616192087531\n",
      "epoch: 5 step: 1465, loss is 0.001377380103804171\n",
      "epoch: 5 step: 1466, loss is 0.003597037401050329\n",
      "epoch: 5 step: 1467, loss is 0.0056486171670258045\n",
      "epoch: 5 step: 1468, loss is 0.010365381836891174\n",
      "epoch: 5 step: 1469, loss is 0.18221378326416016\n",
      "epoch: 5 step: 1470, loss is 0.0014523094287142158\n",
      "epoch: 5 step: 1471, loss is 0.005395836662501097\n",
      "epoch: 5 step: 1472, loss is 0.00044251937652006745\n",
      "epoch: 5 step: 1473, loss is 0.005380366463214159\n",
      "epoch: 5 step: 1474, loss is 0.033153146505355835\n",
      "epoch: 5 step: 1475, loss is 0.05151386931538582\n",
      "epoch: 5 step: 1476, loss is 0.001668343087658286\n",
      "epoch: 5 step: 1477, loss is 0.004048643168061972\n",
      "epoch: 5 step: 1478, loss is 0.12042403966188431\n",
      "epoch: 5 step: 1479, loss is 0.04653983563184738\n",
      "epoch: 5 step: 1480, loss is 0.040197793394327164\n",
      "epoch: 5 step: 1481, loss is 0.0006154995644465089\n",
      "epoch: 5 step: 1482, loss is 0.008482593111693859\n",
      "epoch: 5 step: 1483, loss is 0.0030353881884366274\n",
      "epoch: 5 step: 1484, loss is 0.0652007907629013\n",
      "epoch: 5 step: 1485, loss is 0.026933159679174423\n",
      "epoch: 5 step: 1486, loss is 0.06461050361394882\n",
      "epoch: 5 step: 1487, loss is 0.03024625964462757\n",
      "epoch: 5 step: 1488, loss is 0.021487025544047356\n",
      "epoch: 5 step: 1489, loss is 0.0004325320478528738\n",
      "epoch: 5 step: 1490, loss is 0.020927974954247475\n",
      "epoch: 5 step: 1491, loss is 0.017071031033992767\n",
      "epoch: 5 step: 1492, loss is 0.0006684567197225988\n",
      "epoch: 5 step: 1493, loss is 0.004297957289963961\n",
      "epoch: 5 step: 1494, loss is 0.07818210124969482\n",
      "epoch: 5 step: 1495, loss is 0.1297069787979126\n",
      "epoch: 5 step: 1496, loss is 0.0019622966647148132\n",
      "epoch: 5 step: 1497, loss is 0.0011403335956856608\n",
      "epoch: 5 step: 1498, loss is 0.06328102946281433\n",
      "epoch: 5 step: 1499, loss is 0.0006435609539039433\n",
      "epoch: 5 step: 1500, loss is 0.0009497564751654863\n",
      "epoch: 5 step: 1501, loss is 0.005988186225295067\n",
      "epoch: 5 step: 1502, loss is 0.002446728525683284\n",
      "epoch: 5 step: 1503, loss is 0.00014532737259287387\n",
      "epoch: 5 step: 1504, loss is 0.1138930544257164\n",
      "epoch: 5 step: 1505, loss is 0.0017160145798698068\n",
      "epoch: 5 step: 1506, loss is 0.13121382892131805\n",
      "epoch: 5 step: 1507, loss is 0.0013250581687316298\n",
      "epoch: 5 step: 1508, loss is 0.0047055939212441444\n",
      "epoch: 5 step: 1509, loss is 0.0019785393960773945\n",
      "epoch: 5 step: 1510, loss is 0.003874605754390359\n",
      "epoch: 5 step: 1511, loss is 0.000802999478764832\n",
      "epoch: 5 step: 1512, loss is 0.0004005560476798564\n",
      "epoch: 5 step: 1513, loss is 0.013617923483252525\n",
      "epoch: 5 step: 1514, loss is 0.3428521752357483\n",
      "epoch: 5 step: 1515, loss is 0.14412184059619904\n",
      "epoch: 5 step: 1516, loss is 0.031617652624845505\n",
      "epoch: 5 step: 1517, loss is 0.042103350162506104\n",
      "epoch: 5 step: 1518, loss is 0.14812852442264557\n",
      "epoch: 5 step: 1519, loss is 0.03400500863790512\n",
      "epoch: 5 step: 1520, loss is 0.0006425894098356366\n",
      "epoch: 5 step: 1521, loss is 0.02128750830888748\n",
      "epoch: 5 step: 1522, loss is 0.006963224615901709\n",
      "epoch: 5 step: 1523, loss is 0.03262944519519806\n",
      "epoch: 5 step: 1524, loss is 0.07110034674406052\n",
      "epoch: 5 step: 1525, loss is 0.0016635055653750896\n",
      "epoch: 5 step: 1526, loss is 0.007195411249995232\n",
      "epoch: 5 step: 1527, loss is 0.012945065274834633\n",
      "epoch: 5 step: 1528, loss is 0.05043477937579155\n",
      "epoch: 5 step: 1529, loss is 0.0011210122611373663\n",
      "epoch: 5 step: 1530, loss is 0.021920928731560707\n",
      "epoch: 5 step: 1531, loss is 0.021572919562458992\n",
      "epoch: 5 step: 1532, loss is 0.006024779286235571\n",
      "epoch: 5 step: 1533, loss is 0.04225775599479675\n",
      "epoch: 5 step: 1534, loss is 0.0009098284645006061\n",
      "epoch: 5 step: 1535, loss is 0.001172877149656415\n",
      "epoch: 5 step: 1536, loss is 0.09876862168312073\n",
      "epoch: 5 step: 1537, loss is 0.015534449368715286\n",
      "epoch: 5 step: 1538, loss is 0.07243068516254425\n",
      "epoch: 5 step: 1539, loss is 0.012323034927248955\n",
      "epoch: 5 step: 1540, loss is 0.003331620479002595\n",
      "epoch: 5 step: 1541, loss is 0.1274963617324829\n",
      "epoch: 5 step: 1542, loss is 0.0810401663184166\n",
      "epoch: 5 step: 1543, loss is 0.12612004578113556\n",
      "epoch: 5 step: 1544, loss is 0.048914097249507904\n",
      "epoch: 5 step: 1545, loss is 0.002742271637544036\n",
      "epoch: 5 step: 1546, loss is 0.05767856538295746\n",
      "epoch: 5 step: 1547, loss is 0.014469847083091736\n",
      "epoch: 5 step: 1548, loss is 0.009008239954710007\n",
      "epoch: 5 step: 1549, loss is 0.01039165910333395\n",
      "epoch: 5 step: 1550, loss is 0.0010256343521177769\n",
      "epoch: 5 step: 1551, loss is 0.09561929851770401\n",
      "epoch: 5 step: 1552, loss is 0.020388098433613777\n",
      "epoch: 5 step: 1553, loss is 0.2686430811882019\n",
      "epoch: 5 step: 1554, loss is 0.0008727585664018989\n",
      "epoch: 5 step: 1555, loss is 0.0013718560803681612\n",
      "epoch: 5 step: 1556, loss is 0.02037767320871353\n",
      "epoch: 5 step: 1557, loss is 0.11823268979787827\n",
      "epoch: 5 step: 1558, loss is 0.0029976111836731434\n",
      "epoch: 5 step: 1559, loss is 0.002843867288902402\n",
      "epoch: 5 step: 1560, loss is 0.010726051405072212\n",
      "epoch: 5 step: 1561, loss is 0.13351258635520935\n",
      "epoch: 5 step: 1562, loss is 0.0007408006931655109\n",
      "epoch: 5 step: 1563, loss is 0.005096885375678539\n",
      "epoch: 5 step: 1564, loss is 0.0011329527478665113\n",
      "epoch: 5 step: 1565, loss is 0.01856232061982155\n",
      "epoch: 5 step: 1566, loss is 0.056704793125391006\n",
      "epoch: 5 step: 1567, loss is 0.022805597633123398\n",
      "epoch: 5 step: 1568, loss is 0.0008472507470287383\n",
      "epoch: 5 step: 1569, loss is 0.0009247451671399176\n",
      "epoch: 5 step: 1570, loss is 0.0016397385625168681\n",
      "epoch: 5 step: 1571, loss is 0.01053108274936676\n",
      "epoch: 5 step: 1572, loss is 0.012908495962619781\n",
      "epoch: 5 step: 1573, loss is 0.050745148211717606\n",
      "epoch: 5 step: 1574, loss is 0.0025164380203932524\n",
      "epoch: 5 step: 1575, loss is 0.06761807203292847\n",
      "epoch: 5 step: 1576, loss is 0.00041996638174168766\n",
      "epoch: 5 step: 1577, loss is 0.06722614914178848\n",
      "epoch: 5 step: 1578, loss is 0.003602517768740654\n",
      "epoch: 5 step: 1579, loss is 0.0031149943824857473\n",
      "epoch: 5 step: 1580, loss is 0.08730080723762512\n",
      "epoch: 5 step: 1581, loss is 0.012773196212947369\n",
      "epoch: 5 step: 1582, loss is 0.0013587784487754107\n",
      "epoch: 5 step: 1583, loss is 0.12925828993320465\n",
      "epoch: 5 step: 1584, loss is 0.006344112101942301\n",
      "epoch: 5 step: 1585, loss is 0.012004256248474121\n",
      "epoch: 5 step: 1586, loss is 0.023011479526758194\n",
      "epoch: 5 step: 1587, loss is 0.0028558438643813133\n",
      "epoch: 5 step: 1588, loss is 0.017774689942598343\n",
      "epoch: 5 step: 1589, loss is 0.035477716475725174\n",
      "epoch: 5 step: 1590, loss is 0.020574143156409264\n",
      "epoch: 5 step: 1591, loss is 0.002074428368359804\n",
      "epoch: 5 step: 1592, loss is 0.002243980998173356\n",
      "epoch: 5 step: 1593, loss is 0.0008119418635033071\n",
      "epoch: 5 step: 1594, loss is 0.0006334963836707175\n",
      "epoch: 5 step: 1595, loss is 0.024767499417066574\n",
      "epoch: 5 step: 1596, loss is 0.02563277631998062\n",
      "epoch: 5 step: 1597, loss is 0.00047254585660994053\n",
      "epoch: 5 step: 1598, loss is 0.2059219628572464\n",
      "epoch: 5 step: 1599, loss is 0.008978606201708317\n",
      "epoch: 5 step: 1600, loss is 0.00678824819624424\n",
      "epoch: 5 step: 1601, loss is 0.0571448877453804\n",
      "epoch: 5 step: 1602, loss is 0.002272564684972167\n",
      "epoch: 5 step: 1603, loss is 0.19555257260799408\n",
      "epoch: 5 step: 1604, loss is 0.001603475073352456\n",
      "epoch: 5 step: 1605, loss is 0.01105380617082119\n",
      "epoch: 5 step: 1606, loss is 0.031152047216892242\n",
      "epoch: 5 step: 1607, loss is 0.007868288084864616\n",
      "epoch: 5 step: 1608, loss is 0.00037760913255624473\n",
      "epoch: 5 step: 1609, loss is 0.019240155816078186\n",
      "epoch: 5 step: 1610, loss is 0.05548009276390076\n",
      "epoch: 5 step: 1611, loss is 0.10433517396450043\n",
      "epoch: 5 step: 1612, loss is 0.25017669796943665\n",
      "epoch: 5 step: 1613, loss is 0.09582620859146118\n",
      "epoch: 5 step: 1614, loss is 0.0027356287464499474\n",
      "epoch: 5 step: 1615, loss is 0.001573691377416253\n",
      "epoch: 5 step: 1616, loss is 0.0015937781427055597\n",
      "epoch: 5 step: 1617, loss is 0.05035959929227829\n",
      "epoch: 5 step: 1618, loss is 0.06703367084264755\n",
      "epoch: 5 step: 1619, loss is 0.014748955145478249\n",
      "epoch: 5 step: 1620, loss is 0.01966806687414646\n",
      "epoch: 5 step: 1621, loss is 0.008499094285070896\n",
      "epoch: 5 step: 1622, loss is 0.0046167694963514805\n",
      "epoch: 5 step: 1623, loss is 0.03514021262526512\n",
      "epoch: 5 step: 1624, loss is 0.005486348643898964\n",
      "epoch: 5 step: 1625, loss is 0.0016249610343948007\n",
      "epoch: 5 step: 1626, loss is 0.0395810529589653\n",
      "epoch: 5 step: 1627, loss is 0.003796920645982027\n",
      "epoch: 5 step: 1628, loss is 0.0231584794819355\n",
      "epoch: 5 step: 1629, loss is 0.008449036628007889\n",
      "epoch: 5 step: 1630, loss is 0.041876304894685745\n",
      "epoch: 5 step: 1631, loss is 0.0025317466352134943\n",
      "epoch: 5 step: 1632, loss is 0.003962378017604351\n",
      "epoch: 5 step: 1633, loss is 0.0023389810230582952\n",
      "epoch: 5 step: 1634, loss is 0.0001651972415857017\n",
      "epoch: 5 step: 1635, loss is 0.006364911794662476\n",
      "epoch: 5 step: 1636, loss is 0.04904462769627571\n",
      "epoch: 5 step: 1637, loss is 0.015528086572885513\n",
      "epoch: 5 step: 1638, loss is 0.045640092343091965\n",
      "epoch: 5 step: 1639, loss is 0.019799185916781425\n",
      "epoch: 5 step: 1640, loss is 0.01042845193296671\n",
      "epoch: 5 step: 1641, loss is 0.0016634039347991347\n",
      "epoch: 5 step: 1642, loss is 0.055645838379859924\n",
      "epoch: 5 step: 1643, loss is 0.002556789433583617\n",
      "epoch: 5 step: 1644, loss is 0.0018684940878301859\n",
      "epoch: 5 step: 1645, loss is 0.009290996007621288\n",
      "epoch: 5 step: 1646, loss is 0.0021834149956703186\n",
      "epoch: 5 step: 1647, loss is 0.0034466003999114037\n",
      "epoch: 5 step: 1648, loss is 0.003369380487129092\n",
      "epoch: 5 step: 1649, loss is 0.06106424704194069\n",
      "epoch: 5 step: 1650, loss is 0.000246446521487087\n",
      "epoch: 5 step: 1651, loss is 0.10815588384866714\n",
      "epoch: 5 step: 1652, loss is 0.026605939492583275\n",
      "epoch: 5 step: 1653, loss is 0.0019518741173669696\n",
      "epoch: 5 step: 1654, loss is 0.005884892772883177\n",
      "epoch: 5 step: 1655, loss is 0.0004253760853316635\n",
      "epoch: 5 step: 1656, loss is 0.025068223476409912\n",
      "epoch: 5 step: 1657, loss is 0.00342109939083457\n",
      "epoch: 5 step: 1658, loss is 0.0003396670799702406\n",
      "epoch: 5 step: 1659, loss is 0.0015091568930074573\n",
      "epoch: 5 step: 1660, loss is 0.0013726446777582169\n",
      "epoch: 5 step: 1661, loss is 0.0011644846526905894\n",
      "epoch: 5 step: 1662, loss is 0.0010798037983477116\n",
      "epoch: 5 step: 1663, loss is 0.0005121859721839428\n",
      "epoch: 5 step: 1664, loss is 0.0021065003238618374\n",
      "epoch: 5 step: 1665, loss is 0.10280229151248932\n",
      "epoch: 5 step: 1666, loss is 0.04388297349214554\n",
      "epoch: 5 step: 1667, loss is 0.0018917805282399058\n",
      "epoch: 5 step: 1668, loss is 0.05930094048380852\n",
      "epoch: 5 step: 1669, loss is 0.003638190682977438\n",
      "epoch: 5 step: 1670, loss is 0.0018526965286582708\n",
      "epoch: 5 step: 1671, loss is 0.0011452733306214213\n",
      "epoch: 5 step: 1672, loss is 0.01092162262648344\n",
      "epoch: 5 step: 1673, loss is 0.004740473348647356\n",
      "epoch: 5 step: 1674, loss is 0.04635823518037796\n",
      "epoch: 5 step: 1675, loss is 0.04572637751698494\n",
      "epoch: 5 step: 1676, loss is 0.11589254438877106\n",
      "epoch: 5 step: 1677, loss is 0.050902996212244034\n",
      "epoch: 5 step: 1678, loss is 0.025190411135554314\n",
      "epoch: 5 step: 1679, loss is 0.0011764277005568147\n",
      "epoch: 5 step: 1680, loss is 0.0005407834541983902\n",
      "epoch: 5 step: 1681, loss is 0.04404694214463234\n",
      "epoch: 5 step: 1682, loss is 0.012510666623711586\n",
      "epoch: 5 step: 1683, loss is 0.00334915891289711\n",
      "epoch: 5 step: 1684, loss is 0.0043465010821819305\n",
      "epoch: 5 step: 1685, loss is 0.03951144963502884\n",
      "epoch: 5 step: 1686, loss is 0.10929781943559647\n",
      "epoch: 5 step: 1687, loss is 0.06852181255817413\n",
      "epoch: 5 step: 1688, loss is 0.001595541019923985\n",
      "epoch: 5 step: 1689, loss is 0.019259223714470863\n",
      "epoch: 5 step: 1690, loss is 6.022010711603798e-05\n",
      "epoch: 5 step: 1691, loss is 0.001797012286260724\n",
      "epoch: 5 step: 1692, loss is 0.0001593916822457686\n",
      "epoch: 5 step: 1693, loss is 0.0038215338718146086\n",
      "epoch: 5 step: 1694, loss is 0.038415390998125076\n",
      "epoch: 5 step: 1695, loss is 0.00033275692840106785\n",
      "epoch: 5 step: 1696, loss is 0.0002074481890304014\n",
      "epoch: 5 step: 1697, loss is 0.04967688024044037\n",
      "epoch: 5 step: 1698, loss is 0.0462118536233902\n",
      "epoch: 5 step: 1699, loss is 0.000251428980845958\n",
      "epoch: 5 step: 1700, loss is 0.0016223739366978407\n",
      "epoch: 5 step: 1701, loss is 0.013131420128047466\n",
      "epoch: 5 step: 1702, loss is 0.009746449999511242\n",
      "epoch: 5 step: 1703, loss is 0.0017600225983187556\n",
      "epoch: 5 step: 1704, loss is 0.0009879979770630598\n",
      "epoch: 5 step: 1705, loss is 0.04106457531452179\n",
      "epoch: 5 step: 1706, loss is 0.00018404406728222966\n",
      "epoch: 5 step: 1707, loss is 0.00020811331341974437\n",
      "epoch: 5 step: 1708, loss is 0.0003468968207016587\n",
      "epoch: 5 step: 1709, loss is 0.008685385808348656\n",
      "epoch: 5 step: 1710, loss is 0.0027718583587557077\n",
      "epoch: 5 step: 1711, loss is 0.0005881813704036176\n",
      "epoch: 5 step: 1712, loss is 0.11197315156459808\n",
      "epoch: 5 step: 1713, loss is 0.015336005948483944\n",
      "epoch: 5 step: 1714, loss is 0.03467458486557007\n",
      "epoch: 5 step: 1715, loss is 0.0005118536064401269\n",
      "epoch: 5 step: 1716, loss is 0.0730956643819809\n",
      "epoch: 5 step: 1717, loss is 0.008390866219997406\n",
      "epoch: 5 step: 1718, loss is 0.0003977688029408455\n",
      "epoch: 5 step: 1719, loss is 0.004607595503330231\n",
      "epoch: 5 step: 1720, loss is 0.008934387005865574\n",
      "epoch: 5 step: 1721, loss is 0.00012197418254800141\n",
      "epoch: 5 step: 1722, loss is 0.0026674368418753147\n",
      "epoch: 5 step: 1723, loss is 0.001704924157820642\n",
      "epoch: 5 step: 1724, loss is 0.18735992908477783\n",
      "epoch: 5 step: 1725, loss is 0.003759204875677824\n",
      "epoch: 5 step: 1726, loss is 0.0565621480345726\n",
      "epoch: 5 step: 1727, loss is 0.005772602278739214\n",
      "epoch: 5 step: 1728, loss is 0.012389679439365864\n",
      "epoch: 5 step: 1729, loss is 0.007737128064036369\n",
      "epoch: 5 step: 1730, loss is 0.0013929246924817562\n",
      "epoch: 5 step: 1731, loss is 0.00012586539378389716\n",
      "epoch: 5 step: 1732, loss is 0.0003313783381599933\n",
      "epoch: 5 step: 1733, loss is 0.00028433394618332386\n",
      "epoch: 5 step: 1734, loss is 0.00041709354263730347\n",
      "epoch: 5 step: 1735, loss is 0.0007503439555875957\n",
      "epoch: 5 step: 1736, loss is 0.06296852976083755\n",
      "epoch: 5 step: 1737, loss is 0.15453261137008667\n",
      "epoch: 5 step: 1738, loss is 0.005686616059392691\n",
      "epoch: 5 step: 1739, loss is 0.042134981602430344\n",
      "epoch: 5 step: 1740, loss is 0.0008500390686094761\n",
      "epoch: 5 step: 1741, loss is 0.0048684654757380486\n",
      "epoch: 5 step: 1742, loss is 0.003247101092711091\n",
      "epoch: 5 step: 1743, loss is 0.006764733698219061\n",
      "epoch: 5 step: 1744, loss is 0.0012092238757759333\n",
      "epoch: 5 step: 1745, loss is 0.0013469569385051727\n",
      "epoch: 5 step: 1746, loss is 0.0006840996793471277\n",
      "epoch: 5 step: 1747, loss is 0.002535636071115732\n",
      "epoch: 5 step: 1748, loss is 0.0025015263818204403\n",
      "epoch: 5 step: 1749, loss is 0.03380867466330528\n",
      "epoch: 5 step: 1750, loss is 0.021923797205090523\n",
      "epoch: 5 step: 1751, loss is 0.002267284318804741\n",
      "epoch: 5 step: 1752, loss is 0.0001261454017367214\n",
      "epoch: 5 step: 1753, loss is 0.017849421128630638\n",
      "epoch: 5 step: 1754, loss is 0.0002903641725424677\n",
      "epoch: 5 step: 1755, loss is 0.0021202429197728634\n",
      "epoch: 5 step: 1756, loss is 0.003475425997748971\n",
      "epoch: 5 step: 1757, loss is 0.0055105541832745075\n",
      "epoch: 5 step: 1758, loss is 0.005890389904379845\n",
      "epoch: 5 step: 1759, loss is 0.0003263839171268046\n",
      "epoch: 5 step: 1760, loss is 0.052000850439071655\n",
      "epoch: 5 step: 1761, loss is 0.10412914305925369\n",
      "epoch: 5 step: 1762, loss is 0.019287699833512306\n",
      "epoch: 5 step: 1763, loss is 0.002858102321624756\n",
      "epoch: 5 step: 1764, loss is 0.00021886856120545417\n",
      "epoch: 5 step: 1765, loss is 0.006685364991426468\n",
      "epoch: 5 step: 1766, loss is 0.00011900539539055899\n",
      "epoch: 5 step: 1767, loss is 0.004525788594037294\n",
      "epoch: 5 step: 1768, loss is 0.003488910850137472\n",
      "epoch: 5 step: 1769, loss is 0.004568157717585564\n",
      "epoch: 5 step: 1770, loss is 0.006718136835843325\n",
      "epoch: 5 step: 1771, loss is 0.00247355573810637\n",
      "epoch: 5 step: 1772, loss is 0.045947249978780746\n",
      "epoch: 5 step: 1773, loss is 0.014966879040002823\n",
      "epoch: 5 step: 1774, loss is 0.004374704789370298\n",
      "epoch: 5 step: 1775, loss is 0.0872715562582016\n",
      "epoch: 5 step: 1776, loss is 0.001696043647825718\n",
      "epoch: 5 step: 1777, loss is 0.01891748607158661\n",
      "epoch: 5 step: 1778, loss is 0.05070987343788147\n",
      "epoch: 5 step: 1779, loss is 0.012875515967607498\n",
      "epoch: 5 step: 1780, loss is 0.19837437570095062\n",
      "epoch: 5 step: 1781, loss is 0.001035268884152174\n",
      "epoch: 5 step: 1782, loss is 0.05078544095158577\n",
      "epoch: 5 step: 1783, loss is 0.0048966482281684875\n",
      "epoch: 5 step: 1784, loss is 0.002407954540103674\n",
      "epoch: 5 step: 1785, loss is 0.0009128327947109938\n",
      "epoch: 5 step: 1786, loss is 0.14496248960494995\n",
      "epoch: 5 step: 1787, loss is 0.0024935263209044933\n",
      "epoch: 5 step: 1788, loss is 0.0004378912562970072\n",
      "epoch: 5 step: 1789, loss is 0.07501669228076935\n",
      "epoch: 5 step: 1790, loss is 0.0009952657856047153\n",
      "epoch: 5 step: 1791, loss is 0.011780628003180027\n",
      "epoch: 5 step: 1792, loss is 0.11581983417272568\n",
      "epoch: 5 step: 1793, loss is 0.08527487516403198\n",
      "epoch: 5 step: 1794, loss is 0.10854344815015793\n",
      "epoch: 5 step: 1795, loss is 0.021959170699119568\n",
      "epoch: 5 step: 1796, loss is 0.02626834250986576\n",
      "epoch: 5 step: 1797, loss is 0.0019437960581853986\n",
      "epoch: 5 step: 1798, loss is 0.03595345467329025\n",
      "epoch: 5 step: 1799, loss is 0.16447210311889648\n",
      "epoch: 5 step: 1800, loss is 0.04831160977482796\n",
      "epoch: 5 step: 1801, loss is 0.03912382200360298\n",
      "epoch: 5 step: 1802, loss is 0.001155304373241961\n",
      "epoch: 5 step: 1803, loss is 0.0400700680911541\n",
      "epoch: 5 step: 1804, loss is 0.009036979638040066\n",
      "epoch: 5 step: 1805, loss is 0.014396664686501026\n",
      "epoch: 5 step: 1806, loss is 0.0004397410084493458\n",
      "epoch: 5 step: 1807, loss is 0.019962014630436897\n",
      "epoch: 5 step: 1808, loss is 0.0009382645948790014\n",
      "epoch: 5 step: 1809, loss is 0.007528627756983042\n",
      "epoch: 5 step: 1810, loss is 0.042471133172512054\n",
      "epoch: 5 step: 1811, loss is 0.001647020922973752\n",
      "epoch: 5 step: 1812, loss is 0.3025320768356323\n",
      "epoch: 5 step: 1813, loss is 0.000591411255300045\n",
      "epoch: 5 step: 1814, loss is 0.002255824627354741\n",
      "epoch: 5 step: 1815, loss is 0.008190635591745377\n",
      "epoch: 5 step: 1816, loss is 0.002173256129026413\n",
      "epoch: 5 step: 1817, loss is 0.2435527890920639\n",
      "epoch: 5 step: 1818, loss is 0.003242629114538431\n",
      "epoch: 5 step: 1819, loss is 0.08363695442676544\n",
      "epoch: 5 step: 1820, loss is 0.04319918528199196\n",
      "epoch: 5 step: 1821, loss is 0.21536855399608612\n",
      "epoch: 5 step: 1822, loss is 0.009652368724346161\n",
      "epoch: 5 step: 1823, loss is 0.014027732424438\n",
      "epoch: 5 step: 1824, loss is 0.023637237027287483\n",
      "epoch: 5 step: 1825, loss is 0.027911847457289696\n",
      "epoch: 5 step: 1826, loss is 0.004199882969260216\n",
      "epoch: 5 step: 1827, loss is 0.0404927022755146\n",
      "epoch: 5 step: 1828, loss is 0.010174266993999481\n",
      "epoch: 5 step: 1829, loss is 0.025614934042096138\n",
      "epoch: 5 step: 1830, loss is 0.23010604083538055\n",
      "epoch: 5 step: 1831, loss is 0.07221408933401108\n",
      "epoch: 5 step: 1832, loss is 0.001242776750586927\n",
      "epoch: 5 step: 1833, loss is 0.0017869641305878758\n",
      "epoch: 5 step: 1834, loss is 0.039164017885923386\n",
      "epoch: 5 step: 1835, loss is 0.11488256603479385\n",
      "epoch: 5 step: 1836, loss is 0.001746663823723793\n",
      "epoch: 5 step: 1837, loss is 0.09514736384153366\n",
      "epoch: 5 step: 1838, loss is 0.005609884858131409\n",
      "epoch: 5 step: 1839, loss is 0.1199343129992485\n",
      "epoch: 5 step: 1840, loss is 0.02134666219353676\n",
      "epoch: 5 step: 1841, loss is 0.006881884299218655\n",
      "epoch: 5 step: 1842, loss is 0.014487197622656822\n",
      "epoch: 5 step: 1843, loss is 0.04316354915499687\n",
      "epoch: 5 step: 1844, loss is 0.1002441868185997\n",
      "epoch: 5 step: 1845, loss is 0.0010968775022774935\n",
      "epoch: 5 step: 1846, loss is 0.007546300068497658\n",
      "epoch: 5 step: 1847, loss is 0.027539756149053574\n",
      "epoch: 5 step: 1848, loss is 0.0115915322676301\n",
      "epoch: 5 step: 1849, loss is 0.047577355057001114\n",
      "epoch: 5 step: 1850, loss is 0.008221892639994621\n",
      "epoch: 5 step: 1851, loss is 0.04138870909810066\n",
      "epoch: 5 step: 1852, loss is 0.013832607306540012\n",
      "epoch: 5 step: 1853, loss is 0.002218464855104685\n",
      "epoch: 5 step: 1854, loss is 0.005955919157713652\n",
      "epoch: 5 step: 1855, loss is 0.00046138392644934356\n",
      "epoch: 5 step: 1856, loss is 0.03733494132757187\n",
      "epoch: 5 step: 1857, loss is 0.0009206639369949698\n",
      "epoch: 5 step: 1858, loss is 0.0013114549219608307\n",
      "epoch: 5 step: 1859, loss is 0.014254031702876091\n",
      "epoch: 5 step: 1860, loss is 0.00511020515114069\n",
      "epoch: 5 step: 1861, loss is 0.0008325543603859842\n",
      "epoch: 5 step: 1862, loss is 0.006547994911670685\n",
      "epoch: 5 step: 1863, loss is 0.000551135279238224\n",
      "epoch: 5 step: 1864, loss is 0.04368847236037254\n",
      "epoch: 5 step: 1865, loss is 0.004755777306854725\n",
      "epoch: 5 step: 1866, loss is 0.07479099929332733\n",
      "epoch: 5 step: 1867, loss is 0.08674962818622589\n",
      "epoch: 5 step: 1868, loss is 0.0040783449076116085\n",
      "epoch: 5 step: 1869, loss is 0.0018384229624643922\n",
      "epoch: 5 step: 1870, loss is 0.003862001933157444\n",
      "epoch: 5 step: 1871, loss is 0.0019154929323121905\n",
      "epoch: 5 step: 1872, loss is 0.01858535408973694\n",
      "epoch: 5 step: 1873, loss is 0.03574587404727936\n",
      "epoch: 5 step: 1874, loss is 0.019914839416742325\n",
      "epoch: 5 step: 1875, loss is 0.00448474008589983\n",
      "epoch: 6 step: 1, loss is 0.0030961378943175077\n",
      "epoch: 6 step: 2, loss is 0.005584478843957186\n",
      "epoch: 6 step: 3, loss is 0.0010696370154619217\n",
      "epoch: 6 step: 4, loss is 0.0003385428572073579\n",
      "epoch: 6 step: 5, loss is 0.011418557725846767\n",
      "epoch: 6 step: 6, loss is 0.0030611094553023577\n",
      "epoch: 6 step: 7, loss is 0.0012417163234204054\n",
      "epoch: 6 step: 8, loss is 0.001555691473186016\n",
      "epoch: 6 step: 9, loss is 0.0026768301613628864\n",
      "epoch: 6 step: 10, loss is 0.000414024805650115\n",
      "epoch: 6 step: 11, loss is 0.0652252584695816\n",
      "epoch: 6 step: 12, loss is 0.005834898445755243\n",
      "epoch: 6 step: 13, loss is 0.0005154028185643256\n",
      "epoch: 6 step: 14, loss is 0.05521674081683159\n",
      "epoch: 6 step: 15, loss is 0.0010060612112283707\n",
      "epoch: 6 step: 16, loss is 0.02196948416531086\n",
      "epoch: 6 step: 17, loss is 0.0005052181077189744\n",
      "epoch: 6 step: 18, loss is 0.00861615501344204\n",
      "epoch: 6 step: 19, loss is 0.008711329661309719\n",
      "epoch: 6 step: 20, loss is 0.0040585678070783615\n",
      "epoch: 6 step: 21, loss is 0.0017728450475260615\n",
      "epoch: 6 step: 22, loss is 0.004917589016258717\n",
      "epoch: 6 step: 23, loss is 0.06597985327243805\n",
      "epoch: 6 step: 24, loss is 0.00226710457354784\n",
      "epoch: 6 step: 25, loss is 0.02843044139444828\n",
      "epoch: 6 step: 26, loss is 0.0009093931876122952\n",
      "epoch: 6 step: 27, loss is 0.003981331363320351\n",
      "epoch: 6 step: 28, loss is 0.01730569265782833\n",
      "epoch: 6 step: 29, loss is 0.045133717358112335\n",
      "epoch: 6 step: 30, loss is 0.07029235363006592\n",
      "epoch: 6 step: 31, loss is 0.007589543703943491\n",
      "epoch: 6 step: 32, loss is 0.0013968050479888916\n",
      "epoch: 6 step: 33, loss is 0.0004622235137503594\n",
      "epoch: 6 step: 34, loss is 0.002320225816220045\n",
      "epoch: 6 step: 35, loss is 0.011630970053374767\n",
      "epoch: 6 step: 36, loss is 0.0039167567156255245\n",
      "epoch: 6 step: 37, loss is 0.02439447119832039\n",
      "epoch: 6 step: 38, loss is 0.024055540561676025\n",
      "epoch: 6 step: 39, loss is 0.006037593819200993\n",
      "epoch: 6 step: 40, loss is 0.026800375431776047\n",
      "epoch: 6 step: 41, loss is 0.017609376460313797\n",
      "epoch: 6 step: 42, loss is 0.03315402939915657\n",
      "epoch: 6 step: 43, loss is 0.0008525439188815653\n",
      "epoch: 6 step: 44, loss is 0.002973073860630393\n",
      "epoch: 6 step: 45, loss is 0.016334019601345062\n",
      "epoch: 6 step: 46, loss is 0.23449452221393585\n",
      "epoch: 6 step: 47, loss is 0.002864247653633356\n",
      "epoch: 6 step: 48, loss is 0.005308745428919792\n",
      "epoch: 6 step: 49, loss is 0.003529542125761509\n",
      "epoch: 6 step: 50, loss is 0.109798364341259\n",
      "epoch: 6 step: 51, loss is 0.0744568258523941\n",
      "epoch: 6 step: 52, loss is 0.039688125252723694\n",
      "epoch: 6 step: 53, loss is 0.004718177020549774\n",
      "epoch: 6 step: 54, loss is 0.007233579643070698\n",
      "epoch: 6 step: 55, loss is 0.0004974798066541553\n",
      "epoch: 6 step: 56, loss is 0.0036325794644653797\n",
      "epoch: 6 step: 57, loss is 0.001959412358701229\n",
      "epoch: 6 step: 58, loss is 0.0003894488327205181\n",
      "epoch: 6 step: 59, loss is 0.0033761197701096535\n",
      "epoch: 6 step: 60, loss is 0.0012246394762769341\n",
      "epoch: 6 step: 61, loss is 0.008044937625527382\n",
      "epoch: 6 step: 62, loss is 0.04718107730150223\n",
      "epoch: 6 step: 63, loss is 0.001940890564583242\n",
      "epoch: 6 step: 64, loss is 0.0002875200589187443\n",
      "epoch: 6 step: 65, loss is 0.021814530715346336\n",
      "epoch: 6 step: 66, loss is 0.002151403110474348\n",
      "epoch: 6 step: 67, loss is 0.0009092898690141737\n",
      "epoch: 6 step: 68, loss is 0.008425390347838402\n",
      "epoch: 6 step: 69, loss is 0.0013344277394935489\n",
      "epoch: 6 step: 70, loss is 0.008391328155994415\n",
      "epoch: 6 step: 71, loss is 0.007936911657452583\n",
      "epoch: 6 step: 72, loss is 0.0003178643819410354\n",
      "epoch: 6 step: 73, loss is 0.003473870223388076\n",
      "epoch: 6 step: 74, loss is 0.0015827625757083297\n",
      "epoch: 6 step: 75, loss is 0.0011436158092692494\n",
      "epoch: 6 step: 76, loss is 0.0062490226700901985\n",
      "epoch: 6 step: 77, loss is 0.0002522698196116835\n",
      "epoch: 6 step: 78, loss is 0.0015669603599235415\n",
      "epoch: 6 step: 79, loss is 0.0031900564208626747\n",
      "epoch: 6 step: 80, loss is 0.00032834179000929\n",
      "epoch: 6 step: 81, loss is 0.0037832939997315407\n",
      "epoch: 6 step: 82, loss is 0.00048788401181809604\n",
      "epoch: 6 step: 83, loss is 0.003776692086830735\n",
      "epoch: 6 step: 84, loss is 0.013016866520047188\n",
      "epoch: 6 step: 85, loss is 0.12611407041549683\n",
      "epoch: 6 step: 86, loss is 0.0008440383244305849\n",
      "epoch: 6 step: 87, loss is 0.0004575501661747694\n",
      "epoch: 6 step: 88, loss is 0.004592908080667257\n",
      "epoch: 6 step: 89, loss is 0.004643724765628576\n",
      "epoch: 6 step: 90, loss is 0.14151306450366974\n",
      "epoch: 6 step: 91, loss is 0.0010775879491120577\n",
      "epoch: 6 step: 92, loss is 0.0008315373561345041\n",
      "epoch: 6 step: 93, loss is 0.009275015443563461\n",
      "epoch: 6 step: 94, loss is 0.004458956886082888\n",
      "epoch: 6 step: 95, loss is 0.0850820541381836\n",
      "epoch: 6 step: 96, loss is 0.005922955460846424\n",
      "epoch: 6 step: 97, loss is 0.0013100309297442436\n",
      "epoch: 6 step: 98, loss is 0.008868829347193241\n",
      "epoch: 6 step: 99, loss is 0.02075992152094841\n",
      "epoch: 6 step: 100, loss is 0.01635340414941311\n",
      "epoch: 6 step: 101, loss is 0.008268551900982857\n",
      "epoch: 6 step: 102, loss is 0.0035476330667734146\n",
      "epoch: 6 step: 103, loss is 0.0006164158112369478\n",
      "epoch: 6 step: 104, loss is 0.0008124418091028929\n",
      "epoch: 6 step: 105, loss is 0.02908642403781414\n",
      "epoch: 6 step: 106, loss is 0.15846577286720276\n",
      "epoch: 6 step: 107, loss is 0.03390121087431908\n",
      "epoch: 6 step: 108, loss is 0.047689639031887054\n",
      "epoch: 6 step: 109, loss is 0.0010693358490243554\n",
      "epoch: 6 step: 110, loss is 0.006749700289219618\n",
      "epoch: 6 step: 111, loss is 0.0004811911494471133\n",
      "epoch: 6 step: 112, loss is 0.0004045028181280941\n",
      "epoch: 6 step: 113, loss is 0.016231724992394447\n",
      "epoch: 6 step: 114, loss is 0.0010045348899438977\n",
      "epoch: 6 step: 115, loss is 0.010528210550546646\n",
      "epoch: 6 step: 116, loss is 0.051445066928863525\n",
      "epoch: 6 step: 117, loss is 0.0017796213505789638\n",
      "epoch: 6 step: 118, loss is 0.15230673551559448\n",
      "epoch: 6 step: 119, loss is 0.0034231196623295546\n",
      "epoch: 6 step: 120, loss is 0.0787903442978859\n",
      "epoch: 6 step: 121, loss is 0.0014701955951750278\n",
      "epoch: 6 step: 122, loss is 0.031356081366539\n",
      "epoch: 6 step: 123, loss is 0.00033206737134605646\n",
      "epoch: 6 step: 124, loss is 0.0035130002070218325\n",
      "epoch: 6 step: 125, loss is 0.0011091649066656828\n",
      "epoch: 6 step: 126, loss is 0.001268433639779687\n",
      "epoch: 6 step: 127, loss is 0.000736834539566189\n",
      "epoch: 6 step: 128, loss is 0.008935254998505116\n",
      "epoch: 6 step: 129, loss is 0.0023183152079582214\n",
      "epoch: 6 step: 130, loss is 0.0025662556290626526\n",
      "epoch: 6 step: 131, loss is 0.0007392123807221651\n",
      "epoch: 6 step: 132, loss is 0.00031491610570810735\n",
      "epoch: 6 step: 133, loss is 0.0677812471985817\n",
      "epoch: 6 step: 134, loss is 0.008963179774582386\n",
      "epoch: 6 step: 135, loss is 0.0007742996676824987\n",
      "epoch: 6 step: 136, loss is 0.00031573663000017405\n",
      "epoch: 6 step: 137, loss is 0.0072243474423885345\n",
      "epoch: 6 step: 138, loss is 0.029799234122037888\n",
      "epoch: 6 step: 139, loss is 0.0002480347466189414\n",
      "epoch: 6 step: 140, loss is 0.030683616176247597\n",
      "epoch: 6 step: 141, loss is 0.00021369790192693472\n",
      "epoch: 6 step: 142, loss is 0.020806631073355675\n",
      "epoch: 6 step: 143, loss is 0.0009764321730472147\n",
      "epoch: 6 step: 144, loss is 0.0007307628984563053\n",
      "epoch: 6 step: 145, loss is 0.0004906778340227902\n",
      "epoch: 6 step: 146, loss is 0.0013726563192903996\n",
      "epoch: 6 step: 147, loss is 0.004384227097034454\n",
      "epoch: 6 step: 148, loss is 0.03255964443087578\n",
      "epoch: 6 step: 149, loss is 0.00020865138503722847\n",
      "epoch: 6 step: 150, loss is 0.0031659482046961784\n",
      "epoch: 6 step: 151, loss is 0.002047316636890173\n",
      "epoch: 6 step: 152, loss is 0.016103658825159073\n",
      "epoch: 6 step: 153, loss is 0.025104006752371788\n",
      "epoch: 6 step: 154, loss is 0.002197690075263381\n",
      "epoch: 6 step: 155, loss is 0.0014152961084619164\n",
      "epoch: 6 step: 156, loss is 0.0017320308834314346\n",
      "epoch: 6 step: 157, loss is 0.0023794511798769236\n",
      "epoch: 6 step: 158, loss is 0.0018602662021294236\n",
      "epoch: 6 step: 159, loss is 0.00855210144072771\n",
      "epoch: 6 step: 160, loss is 0.00046448700595647097\n",
      "epoch: 6 step: 161, loss is 0.00030305824475362897\n",
      "epoch: 6 step: 162, loss is 0.002922773826867342\n",
      "epoch: 6 step: 163, loss is 0.0034051742404699326\n",
      "epoch: 6 step: 164, loss is 0.00784433912485838\n",
      "epoch: 6 step: 165, loss is 0.026196714490652084\n",
      "epoch: 6 step: 166, loss is 0.007862593978643417\n",
      "epoch: 6 step: 167, loss is 0.008060874417424202\n",
      "epoch: 6 step: 168, loss is 0.00037918437737971544\n",
      "epoch: 6 step: 169, loss is 0.00041863168007694185\n",
      "epoch: 6 step: 170, loss is 0.07598934322595596\n",
      "epoch: 6 step: 171, loss is 0.0006020588334649801\n",
      "epoch: 6 step: 172, loss is 0.0005547191831283271\n",
      "epoch: 6 step: 173, loss is 0.0001576530485181138\n",
      "epoch: 6 step: 174, loss is 0.0010890769772231579\n",
      "epoch: 6 step: 175, loss is 0.04380208998918533\n",
      "epoch: 6 step: 176, loss is 6.122906052041799e-05\n",
      "epoch: 6 step: 177, loss is 0.0056841326877474785\n",
      "epoch: 6 step: 178, loss is 0.009992459788918495\n",
      "epoch: 6 step: 179, loss is 0.0049467748031020164\n",
      "epoch: 6 step: 180, loss is 0.047680992633104324\n",
      "epoch: 6 step: 181, loss is 0.0010489829583093524\n",
      "epoch: 6 step: 182, loss is 0.057588670402765274\n",
      "epoch: 6 step: 183, loss is 0.0001146908980445005\n",
      "epoch: 6 step: 184, loss is 0.00042046603630296886\n",
      "epoch: 6 step: 185, loss is 0.00020567404862958938\n",
      "epoch: 6 step: 186, loss is 0.002831046935170889\n",
      "epoch: 6 step: 187, loss is 0.00019769763457588851\n",
      "epoch: 6 step: 188, loss is 0.01155941840261221\n",
      "epoch: 6 step: 189, loss is 0.002296680584549904\n",
      "epoch: 6 step: 190, loss is 0.018946250900626183\n",
      "epoch: 6 step: 191, loss is 0.0035389852710068226\n",
      "epoch: 6 step: 192, loss is 0.17904722690582275\n",
      "epoch: 6 step: 193, loss is 6.751078035449609e-05\n",
      "epoch: 6 step: 194, loss is 0.0325654037296772\n",
      "epoch: 6 step: 195, loss is 0.0980447456240654\n",
      "epoch: 6 step: 196, loss is 0.016720259562134743\n",
      "epoch: 6 step: 197, loss is 0.00031362881418317556\n",
      "epoch: 6 step: 198, loss is 0.019735194742679596\n",
      "epoch: 6 step: 199, loss is 0.0004413062706589699\n",
      "epoch: 6 step: 200, loss is 0.0008368910639546812\n",
      "epoch: 6 step: 201, loss is 0.0053292992524802685\n",
      "epoch: 6 step: 202, loss is 0.48772358894348145\n",
      "epoch: 6 step: 203, loss is 0.10229595750570297\n",
      "epoch: 6 step: 204, loss is 0.0028944555670022964\n",
      "epoch: 6 step: 205, loss is 0.0033621005713939667\n",
      "epoch: 6 step: 206, loss is 0.0016107718693092465\n",
      "epoch: 6 step: 207, loss is 0.00023403413069900125\n",
      "epoch: 6 step: 208, loss is 0.0993674248456955\n",
      "epoch: 6 step: 209, loss is 0.0007289857021532953\n",
      "epoch: 6 step: 210, loss is 0.001563546946272254\n",
      "epoch: 6 step: 211, loss is 0.023286791518330574\n",
      "epoch: 6 step: 212, loss is 0.009579332545399666\n",
      "epoch: 6 step: 213, loss is 0.0007776071433909237\n",
      "epoch: 6 step: 214, loss is 0.002113248221576214\n",
      "epoch: 6 step: 215, loss is 0.011461008340120316\n",
      "epoch: 6 step: 216, loss is 0.0039046411402523518\n",
      "epoch: 6 step: 217, loss is 0.026855502277612686\n",
      "epoch: 6 step: 218, loss is 0.0008726896485313773\n",
      "epoch: 6 step: 219, loss is 0.014580183662474155\n",
      "epoch: 6 step: 220, loss is 0.0012672314187511802\n",
      "epoch: 6 step: 221, loss is 0.0007387060904875398\n",
      "epoch: 6 step: 222, loss is 0.0005011933972127736\n",
      "epoch: 6 step: 223, loss is 0.02141108177602291\n",
      "epoch: 6 step: 224, loss is 0.006026242394000292\n",
      "epoch: 6 step: 225, loss is 0.0004870564443990588\n",
      "epoch: 6 step: 226, loss is 0.0037357674445956945\n",
      "epoch: 6 step: 227, loss is 0.012476722709834576\n",
      "epoch: 6 step: 228, loss is 0.0011037084041163325\n",
      "epoch: 6 step: 229, loss is 0.00034471857361495495\n",
      "epoch: 6 step: 230, loss is 0.00025418392033316195\n",
      "epoch: 6 step: 231, loss is 0.0006547585362568498\n",
      "epoch: 6 step: 232, loss is 0.0008242878830060363\n",
      "epoch: 6 step: 233, loss is 0.004182731732726097\n",
      "epoch: 6 step: 234, loss is 0.04130043089389801\n",
      "epoch: 6 step: 235, loss is 0.003286849707365036\n",
      "epoch: 6 step: 236, loss is 0.005344488192349672\n",
      "epoch: 6 step: 237, loss is 0.001607174752280116\n",
      "epoch: 6 step: 238, loss is 0.12649860978126526\n",
      "epoch: 6 step: 239, loss is 0.01814563199877739\n",
      "epoch: 6 step: 240, loss is 0.0012810654006898403\n",
      "epoch: 6 step: 241, loss is 0.0030814248602837324\n",
      "epoch: 6 step: 242, loss is 0.0023272631224244833\n",
      "epoch: 6 step: 243, loss is 0.002841709880158305\n",
      "epoch: 6 step: 244, loss is 0.005070628132671118\n",
      "epoch: 6 step: 245, loss is 0.005871642846614122\n",
      "epoch: 6 step: 246, loss is 0.0002829178993124515\n",
      "epoch: 6 step: 247, loss is 0.001332439249381423\n",
      "epoch: 6 step: 248, loss is 0.0014712971169501543\n",
      "epoch: 6 step: 249, loss is 0.015923354774713516\n",
      "epoch: 6 step: 250, loss is 0.0005526823806576431\n",
      "epoch: 6 step: 251, loss is 0.004797500092536211\n",
      "epoch: 6 step: 252, loss is 0.05054587870836258\n",
      "epoch: 6 step: 253, loss is 0.0005555771058425307\n",
      "epoch: 6 step: 254, loss is 0.00043558693141676486\n",
      "epoch: 6 step: 255, loss is 0.004663792904466391\n",
      "epoch: 6 step: 256, loss is 0.0018029166385531425\n",
      "epoch: 6 step: 257, loss is 0.05805964022874832\n",
      "epoch: 6 step: 258, loss is 0.005116910673677921\n",
      "epoch: 6 step: 259, loss is 0.04328970983624458\n",
      "epoch: 6 step: 260, loss is 0.0031956264283508062\n",
      "epoch: 6 step: 261, loss is 0.1741451919078827\n",
      "epoch: 6 step: 262, loss is 0.06112207844853401\n",
      "epoch: 6 step: 263, loss is 0.005484535824507475\n",
      "epoch: 6 step: 264, loss is 0.023089883849024773\n",
      "epoch: 6 step: 265, loss is 0.028032230213284492\n",
      "epoch: 6 step: 266, loss is 0.00027951697120442986\n",
      "epoch: 6 step: 267, loss is 0.003988675773143768\n",
      "epoch: 6 step: 268, loss is 0.050442345440387726\n",
      "epoch: 6 step: 269, loss is 0.036781758069992065\n",
      "epoch: 6 step: 270, loss is 0.11136218905448914\n",
      "epoch: 6 step: 271, loss is 0.03950894623994827\n",
      "epoch: 6 step: 272, loss is 0.04742046818137169\n",
      "epoch: 6 step: 273, loss is 0.1287343055009842\n",
      "epoch: 6 step: 274, loss is 0.009985120967030525\n",
      "epoch: 6 step: 275, loss is 0.004019655287265778\n",
      "epoch: 6 step: 276, loss is 0.09335317462682724\n",
      "epoch: 6 step: 277, loss is 0.0015385078731924295\n",
      "epoch: 6 step: 278, loss is 0.006901601329445839\n",
      "epoch: 6 step: 279, loss is 0.025634365156292915\n",
      "epoch: 6 step: 280, loss is 0.01371212862432003\n",
      "epoch: 6 step: 281, loss is 0.002223765943199396\n",
      "epoch: 6 step: 282, loss is 0.0016761611914262176\n",
      "epoch: 6 step: 283, loss is 0.002204765332862735\n",
      "epoch: 6 step: 284, loss is 0.007633269298821688\n",
      "epoch: 6 step: 285, loss is 0.0007930519059300423\n",
      "epoch: 6 step: 286, loss is 0.012832180596888065\n",
      "epoch: 6 step: 287, loss is 0.006994644645601511\n",
      "epoch: 6 step: 288, loss is 0.010590104386210442\n",
      "epoch: 6 step: 289, loss is 0.0007598240044899285\n",
      "epoch: 6 step: 290, loss is 0.003780609928071499\n",
      "epoch: 6 step: 291, loss is 0.0014126086607575417\n",
      "epoch: 6 step: 292, loss is 0.004996227566152811\n",
      "epoch: 6 step: 293, loss is 0.0008925438742153347\n",
      "epoch: 6 step: 294, loss is 0.0720829963684082\n",
      "epoch: 6 step: 295, loss is 0.00546729750931263\n",
      "epoch: 6 step: 296, loss is 0.011411866173148155\n",
      "epoch: 6 step: 297, loss is 0.0023950973991304636\n",
      "epoch: 6 step: 298, loss is 0.014502187259495258\n",
      "epoch: 6 step: 299, loss is 0.0003043925389647484\n",
      "epoch: 6 step: 300, loss is 0.0003325528814457357\n",
      "epoch: 6 step: 301, loss is 0.00878550112247467\n",
      "epoch: 6 step: 302, loss is 0.1092718243598938\n",
      "epoch: 6 step: 303, loss is 0.12646670639514923\n",
      "epoch: 6 step: 304, loss is 0.0011101445415988564\n",
      "epoch: 6 step: 305, loss is 0.11011285334825516\n",
      "epoch: 6 step: 306, loss is 0.0015043767634779215\n",
      "epoch: 6 step: 307, loss is 0.005559283774346113\n",
      "epoch: 6 step: 308, loss is 0.014094577170908451\n",
      "epoch: 6 step: 309, loss is 0.0022628551814705133\n",
      "epoch: 6 step: 310, loss is 0.0002687240776140243\n",
      "epoch: 6 step: 311, loss is 0.004700859542936087\n",
      "epoch: 6 step: 312, loss is 0.025871966034173965\n",
      "epoch: 6 step: 313, loss is 0.009774207137525082\n",
      "epoch: 6 step: 314, loss is 0.09073500335216522\n",
      "epoch: 6 step: 315, loss is 0.002301419386640191\n",
      "epoch: 6 step: 316, loss is 0.06963464617729187\n",
      "epoch: 6 step: 317, loss is 0.006365033332258463\n",
      "epoch: 6 step: 318, loss is 0.0015845810994505882\n",
      "epoch: 6 step: 319, loss is 0.004956376273185015\n",
      "epoch: 6 step: 320, loss is 0.0009630962740629911\n",
      "epoch: 6 step: 321, loss is 0.0018309924053028226\n",
      "epoch: 6 step: 322, loss is 0.003357141511514783\n",
      "epoch: 6 step: 323, loss is 0.017734145745635033\n",
      "epoch: 6 step: 324, loss is 0.00041121372487396\n",
      "epoch: 6 step: 325, loss is 0.03322477638721466\n",
      "epoch: 6 step: 326, loss is 0.005858178250491619\n",
      "epoch: 6 step: 327, loss is 0.021816570311784744\n",
      "epoch: 6 step: 328, loss is 0.0041125682182610035\n",
      "epoch: 6 step: 329, loss is 0.0013105097459629178\n",
      "epoch: 6 step: 330, loss is 0.009372761473059654\n",
      "epoch: 6 step: 331, loss is 0.0011563739972189069\n",
      "epoch: 6 step: 332, loss is 0.02561318501830101\n",
      "epoch: 6 step: 333, loss is 0.11173642426729202\n",
      "epoch: 6 step: 334, loss is 0.008287499658763409\n",
      "epoch: 6 step: 335, loss is 0.0018524195766076446\n",
      "epoch: 6 step: 336, loss is 0.0036194243002682924\n",
      "epoch: 6 step: 337, loss is 0.00984362605959177\n",
      "epoch: 6 step: 338, loss is 0.003461045678704977\n",
      "epoch: 6 step: 339, loss is 0.00090206996537745\n",
      "epoch: 6 step: 340, loss is 0.0008811968727968633\n",
      "epoch: 6 step: 341, loss is 0.015405034646391869\n",
      "epoch: 6 step: 342, loss is 0.005744397174566984\n",
      "epoch: 6 step: 343, loss is 0.002695380477234721\n",
      "epoch: 6 step: 344, loss is 0.10653644055128098\n",
      "epoch: 6 step: 345, loss is 0.08652367442846298\n",
      "epoch: 6 step: 346, loss is 0.001918543130159378\n",
      "epoch: 6 step: 347, loss is 0.023009900003671646\n",
      "epoch: 6 step: 348, loss is 0.012086236849427223\n",
      "epoch: 6 step: 349, loss is 0.007556874305009842\n",
      "epoch: 6 step: 350, loss is 0.0228292103856802\n",
      "epoch: 6 step: 351, loss is 0.03834746032953262\n",
      "epoch: 6 step: 352, loss is 0.012320324778556824\n",
      "epoch: 6 step: 353, loss is 0.007188575807958841\n",
      "epoch: 6 step: 354, loss is 0.00016915032756514847\n",
      "epoch: 6 step: 355, loss is 0.00016607607540208846\n",
      "epoch: 6 step: 356, loss is 0.0022156264167279005\n",
      "epoch: 6 step: 357, loss is 0.0006663614185526967\n",
      "epoch: 6 step: 358, loss is 0.023594047874212265\n",
      "epoch: 6 step: 359, loss is 0.0005371324368752539\n",
      "epoch: 6 step: 360, loss is 4.701669604401104e-05\n",
      "epoch: 6 step: 361, loss is 0.0012132531264796853\n",
      "epoch: 6 step: 362, loss is 0.04262946546077728\n",
      "epoch: 6 step: 363, loss is 0.004239964298903942\n",
      "epoch: 6 step: 364, loss is 0.02261696383357048\n",
      "epoch: 6 step: 365, loss is 0.008083594962954521\n",
      "epoch: 6 step: 366, loss is 0.003159043611958623\n",
      "epoch: 6 step: 367, loss is 0.0005158600397408009\n",
      "epoch: 6 step: 368, loss is 0.011763077229261398\n",
      "epoch: 6 step: 369, loss is 0.200560063123703\n",
      "epoch: 6 step: 370, loss is 8.331393473781645e-05\n",
      "epoch: 6 step: 371, loss is 0.003240699879825115\n",
      "epoch: 6 step: 372, loss is 0.0007771773962303996\n",
      "epoch: 6 step: 373, loss is 0.037366174161434174\n",
      "epoch: 6 step: 374, loss is 0.0014164939057081938\n",
      "epoch: 6 step: 375, loss is 0.0012380986008793116\n",
      "epoch: 6 step: 376, loss is 0.06926664710044861\n",
      "epoch: 6 step: 377, loss is 0.00799548625946045\n",
      "epoch: 6 step: 378, loss is 0.01583399623632431\n",
      "epoch: 6 step: 379, loss is 0.027870161458849907\n",
      "epoch: 6 step: 380, loss is 0.1191687062382698\n",
      "epoch: 6 step: 381, loss is 0.0007650801562704146\n",
      "epoch: 6 step: 382, loss is 0.0019617807120084763\n",
      "epoch: 6 step: 383, loss is 0.008376662619411945\n",
      "epoch: 6 step: 384, loss is 0.014273117296397686\n",
      "epoch: 6 step: 385, loss is 0.004316548351198435\n",
      "epoch: 6 step: 386, loss is 0.00413398165255785\n",
      "epoch: 6 step: 387, loss is 0.002560504712164402\n",
      "epoch: 6 step: 388, loss is 0.013324527069926262\n",
      "epoch: 6 step: 389, loss is 0.0007358778966590762\n",
      "epoch: 6 step: 390, loss is 0.0004927513073198497\n",
      "epoch: 6 step: 391, loss is 0.13106216490268707\n",
      "epoch: 6 step: 392, loss is 0.001889181905426085\n",
      "epoch: 6 step: 393, loss is 0.05750458687543869\n",
      "epoch: 6 step: 394, loss is 0.022763680666685104\n",
      "epoch: 6 step: 395, loss is 0.05838970094919205\n",
      "epoch: 6 step: 396, loss is 0.0028207125142216682\n",
      "epoch: 6 step: 397, loss is 0.005145237315446138\n",
      "epoch: 6 step: 398, loss is 0.0015936262207105756\n",
      "epoch: 6 step: 399, loss is 0.004785000346601009\n",
      "epoch: 6 step: 400, loss is 0.12538665533065796\n",
      "epoch: 6 step: 401, loss is 0.00021906662732362747\n",
      "epoch: 6 step: 402, loss is 0.0025528245605528355\n",
      "epoch: 6 step: 403, loss is 0.0009204057860188186\n",
      "epoch: 6 step: 404, loss is 0.18671609461307526\n",
      "epoch: 6 step: 405, loss is 0.0005453399498946965\n",
      "epoch: 6 step: 406, loss is 0.000733019202016294\n",
      "epoch: 6 step: 407, loss is 0.021915487945079803\n",
      "epoch: 6 step: 408, loss is 0.001087911194190383\n",
      "epoch: 6 step: 409, loss is 0.004424636252224445\n",
      "epoch: 6 step: 410, loss is 0.0026338035240769386\n",
      "epoch: 6 step: 411, loss is 0.09414470195770264\n",
      "epoch: 6 step: 412, loss is 0.042087770998477936\n",
      "epoch: 6 step: 413, loss is 0.001133266370743513\n",
      "epoch: 6 step: 414, loss is 0.011124132201075554\n",
      "epoch: 6 step: 415, loss is 0.0004574325284920633\n",
      "epoch: 6 step: 416, loss is 0.003531280905008316\n",
      "epoch: 6 step: 417, loss is 0.01963013969361782\n",
      "epoch: 6 step: 418, loss is 0.00043480750173330307\n",
      "epoch: 6 step: 419, loss is 0.005958799738436937\n",
      "epoch: 6 step: 420, loss is 0.02875444106757641\n",
      "epoch: 6 step: 421, loss is 0.007934968918561935\n",
      "epoch: 6 step: 422, loss is 0.002807439537718892\n",
      "epoch: 6 step: 423, loss is 0.008198547177016735\n",
      "epoch: 6 step: 424, loss is 0.005647851154208183\n",
      "epoch: 6 step: 425, loss is 0.02979600802063942\n",
      "epoch: 6 step: 426, loss is 0.0009565458749420941\n",
      "epoch: 6 step: 427, loss is 0.004059522412717342\n",
      "epoch: 6 step: 428, loss is 0.09633005410432816\n",
      "epoch: 6 step: 429, loss is 0.0001445265515940264\n",
      "epoch: 6 step: 430, loss is 0.021259622648358345\n",
      "epoch: 6 step: 431, loss is 0.008024889044463634\n",
      "epoch: 6 step: 432, loss is 0.003184742294251919\n",
      "epoch: 6 step: 433, loss is 0.1733432114124298\n",
      "epoch: 6 step: 434, loss is 0.0002661727194208652\n",
      "epoch: 6 step: 435, loss is 0.005538532044738531\n",
      "epoch: 6 step: 436, loss is 0.027044590562582016\n",
      "epoch: 6 step: 437, loss is 0.003059548558667302\n",
      "epoch: 6 step: 438, loss is 0.0006368256872519851\n",
      "epoch: 6 step: 439, loss is 0.001292945584282279\n",
      "epoch: 6 step: 440, loss is 0.08365216851234436\n",
      "epoch: 6 step: 441, loss is 0.0006146831437945366\n",
      "epoch: 6 step: 442, loss is 0.0006012967205606401\n",
      "epoch: 6 step: 443, loss is 0.009818877093493938\n",
      "epoch: 6 step: 444, loss is 0.059978947043418884\n",
      "epoch: 6 step: 445, loss is 0.0367356576025486\n",
      "epoch: 6 step: 446, loss is 0.033709730952978134\n",
      "epoch: 6 step: 447, loss is 0.00013096527254674584\n",
      "epoch: 6 step: 448, loss is 0.008399102836847305\n",
      "epoch: 6 step: 449, loss is 0.002724225400015712\n",
      "epoch: 6 step: 450, loss is 0.006843753159046173\n",
      "epoch: 6 step: 451, loss is 0.0016777075361460447\n",
      "epoch: 6 step: 452, loss is 0.0022141519002616405\n",
      "epoch: 6 step: 453, loss is 0.16163547337055206\n",
      "epoch: 6 step: 454, loss is 0.008683241903781891\n",
      "epoch: 6 step: 455, loss is 0.003933590836822987\n",
      "epoch: 6 step: 456, loss is 0.020295126363635063\n",
      "epoch: 6 step: 457, loss is 0.10234344750642776\n",
      "epoch: 6 step: 458, loss is 0.0009029725333675742\n",
      "epoch: 6 step: 459, loss is 0.0005442183464765549\n",
      "epoch: 6 step: 460, loss is 0.0015966104110702872\n",
      "epoch: 6 step: 461, loss is 0.11020465195178986\n",
      "epoch: 6 step: 462, loss is 0.001383454306051135\n",
      "epoch: 6 step: 463, loss is 0.01700485311448574\n",
      "epoch: 6 step: 464, loss is 0.024145830422639847\n",
      "epoch: 6 step: 465, loss is 0.08116026222705841\n",
      "epoch: 6 step: 466, loss is 0.0001877125760074705\n",
      "epoch: 6 step: 467, loss is 0.0009640807402320206\n",
      "epoch: 6 step: 468, loss is 0.0002000068488996476\n",
      "epoch: 6 step: 469, loss is 0.000726750586181879\n",
      "epoch: 6 step: 470, loss is 0.0001345847558695823\n",
      "epoch: 6 step: 471, loss is 0.16594943404197693\n",
      "epoch: 6 step: 472, loss is 0.0013811306562274694\n",
      "epoch: 6 step: 473, loss is 0.016486655920743942\n",
      "epoch: 6 step: 474, loss is 0.0005397070199251175\n",
      "epoch: 6 step: 475, loss is 0.026245832443237305\n",
      "epoch: 6 step: 476, loss is 0.0032430451828986406\n",
      "epoch: 6 step: 477, loss is 0.002979003358632326\n",
      "epoch: 6 step: 478, loss is 0.0007376035791821778\n",
      "epoch: 6 step: 479, loss is 0.06051035597920418\n",
      "epoch: 6 step: 480, loss is 0.0742725357413292\n",
      "epoch: 6 step: 481, loss is 0.00021084814216010273\n",
      "epoch: 6 step: 482, loss is 0.0002843911643140018\n",
      "epoch: 6 step: 483, loss is 0.04369612783193588\n",
      "epoch: 6 step: 484, loss is 0.00016368716023862362\n",
      "epoch: 6 step: 485, loss is 0.007310639135539532\n",
      "epoch: 6 step: 486, loss is 0.0611826591193676\n",
      "epoch: 6 step: 487, loss is 0.002619262086227536\n",
      "epoch: 6 step: 488, loss is 0.020056216046214104\n",
      "epoch: 6 step: 489, loss is 0.011729356832802296\n",
      "epoch: 6 step: 490, loss is 0.0023044361732900143\n",
      "epoch: 6 step: 491, loss is 0.013523461297154427\n",
      "epoch: 6 step: 492, loss is 0.0021131362300366163\n",
      "epoch: 6 step: 493, loss is 0.34584319591522217\n",
      "epoch: 6 step: 494, loss is 0.000235314728342928\n",
      "epoch: 6 step: 495, loss is 0.00036132030072622\n",
      "epoch: 6 step: 496, loss is 0.0005517767858691514\n",
      "epoch: 6 step: 497, loss is 0.002306760288774967\n",
      "epoch: 6 step: 498, loss is 0.022249935194849968\n",
      "epoch: 6 step: 499, loss is 0.0036971839144825935\n",
      "epoch: 6 step: 500, loss is 0.0003996973973698914\n",
      "epoch: 6 step: 501, loss is 0.0353628471493721\n",
      "epoch: 6 step: 502, loss is 0.04027421027421951\n",
      "epoch: 6 step: 503, loss is 0.0008970199851319194\n",
      "epoch: 6 step: 504, loss is 0.02156057581305504\n",
      "epoch: 6 step: 505, loss is 0.01466921903192997\n",
      "epoch: 6 step: 506, loss is 0.02372795343399048\n",
      "epoch: 6 step: 507, loss is 0.003975083120167255\n",
      "epoch: 6 step: 508, loss is 0.004231689032167196\n",
      "epoch: 6 step: 509, loss is 0.0006190140265971422\n",
      "epoch: 6 step: 510, loss is 0.008010469377040863\n",
      "epoch: 6 step: 511, loss is 0.12463711947202682\n",
      "epoch: 6 step: 512, loss is 0.004125980660319328\n",
      "epoch: 6 step: 513, loss is 0.008550387807190418\n",
      "epoch: 6 step: 514, loss is 0.0183245912194252\n",
      "epoch: 6 step: 515, loss is 0.017739467322826385\n",
      "epoch: 6 step: 516, loss is 0.02977445349097252\n",
      "epoch: 6 step: 517, loss is 0.13202302157878876\n",
      "epoch: 6 step: 518, loss is 0.01837872341275215\n",
      "epoch: 6 step: 519, loss is 0.0010106090921908617\n",
      "epoch: 6 step: 520, loss is 0.004254280589520931\n",
      "epoch: 6 step: 521, loss is 0.006377363111823797\n",
      "epoch: 6 step: 522, loss is 0.0007801884785294533\n",
      "epoch: 6 step: 523, loss is 0.0006738583906553686\n",
      "epoch: 6 step: 524, loss is 0.0006707556894980371\n",
      "epoch: 6 step: 525, loss is 0.03250515088438988\n",
      "epoch: 6 step: 526, loss is 0.002208797261118889\n",
      "epoch: 6 step: 527, loss is 0.0028598264325410128\n",
      "epoch: 6 step: 528, loss is 0.0017626727931201458\n",
      "epoch: 6 step: 529, loss is 0.0007494229357689619\n",
      "epoch: 6 step: 530, loss is 0.00034515041625127196\n",
      "epoch: 6 step: 531, loss is 0.0016368023352697492\n",
      "epoch: 6 step: 532, loss is 0.0006089901435188949\n",
      "epoch: 6 step: 533, loss is 0.0205092690885067\n",
      "epoch: 6 step: 534, loss is 0.007876989431679249\n",
      "epoch: 6 step: 535, loss is 0.00768602080643177\n",
      "epoch: 6 step: 536, loss is 0.0004103322571609169\n",
      "epoch: 6 step: 537, loss is 0.00020661059534177184\n",
      "epoch: 6 step: 538, loss is 0.09364058077335358\n",
      "epoch: 6 step: 539, loss is 0.0003099665336776525\n",
      "epoch: 6 step: 540, loss is 0.00151088647544384\n",
      "epoch: 6 step: 541, loss is 0.0006156255840323865\n",
      "epoch: 6 step: 542, loss is 0.0007735165418125689\n",
      "epoch: 6 step: 543, loss is 0.00315263494849205\n",
      "epoch: 6 step: 544, loss is 0.00034011690877377987\n",
      "epoch: 6 step: 545, loss is 0.0018216288881376386\n",
      "epoch: 6 step: 546, loss is 0.0293723251670599\n",
      "epoch: 6 step: 547, loss is 0.00040842799353413284\n",
      "epoch: 6 step: 548, loss is 2.5396186174475588e-05\n",
      "epoch: 6 step: 549, loss is 0.00903920829296112\n",
      "epoch: 6 step: 550, loss is 0.011817002668976784\n",
      "epoch: 6 step: 551, loss is 0.0006972227129153907\n",
      "epoch: 6 step: 552, loss is 0.000262494693743065\n",
      "epoch: 6 step: 553, loss is 0.00010435104195494205\n",
      "epoch: 6 step: 554, loss is 0.010186118073761463\n",
      "epoch: 6 step: 555, loss is 0.00015148238162510097\n",
      "epoch: 6 step: 556, loss is 0.0018118006410077214\n",
      "epoch: 6 step: 557, loss is 0.0004421306657604873\n",
      "epoch: 6 step: 558, loss is 0.014998051337897778\n",
      "epoch: 6 step: 559, loss is 0.03084537945687771\n",
      "epoch: 6 step: 560, loss is 0.005152957513928413\n",
      "epoch: 6 step: 561, loss is 0.016997821629047394\n",
      "epoch: 6 step: 562, loss is 0.04745400696992874\n",
      "epoch: 6 step: 563, loss is 0.0005986986216157675\n",
      "epoch: 6 step: 564, loss is 0.0050005861558020115\n",
      "epoch: 6 step: 565, loss is 0.02948644384741783\n",
      "epoch: 6 step: 566, loss is 0.0066367085091769695\n",
      "epoch: 6 step: 567, loss is 0.005828114692121744\n",
      "epoch: 6 step: 568, loss is 0.027579588815569878\n",
      "epoch: 6 step: 569, loss is 0.0007116742781363428\n",
      "epoch: 6 step: 570, loss is 0.00036796991480514407\n",
      "epoch: 6 step: 571, loss is 0.0031556368339806795\n",
      "epoch: 6 step: 572, loss is 0.0002837886568158865\n",
      "epoch: 6 step: 573, loss is 0.011635756120085716\n",
      "epoch: 6 step: 574, loss is 3.3137159334728494e-05\n",
      "epoch: 6 step: 575, loss is 0.0027157743461430073\n",
      "epoch: 6 step: 576, loss is 0.032292064279317856\n",
      "epoch: 6 step: 577, loss is 0.024646107107400894\n",
      "epoch: 6 step: 578, loss is 0.014776840806007385\n",
      "epoch: 6 step: 579, loss is 0.0027081009466201067\n",
      "epoch: 6 step: 580, loss is 0.0010214414214715362\n",
      "epoch: 6 step: 581, loss is 0.0014344181399792433\n",
      "epoch: 6 step: 582, loss is 0.0012016671244055033\n",
      "epoch: 6 step: 583, loss is 0.0014005206758156419\n",
      "epoch: 6 step: 584, loss is 0.004511912353336811\n",
      "epoch: 6 step: 585, loss is 0.035248514264822006\n",
      "epoch: 6 step: 586, loss is 0.002202983247116208\n",
      "epoch: 6 step: 587, loss is 0.0006318814121186733\n",
      "epoch: 6 step: 588, loss is 0.13323959708213806\n",
      "epoch: 6 step: 589, loss is 0.0387408584356308\n",
      "epoch: 6 step: 590, loss is 0.0003185416280757636\n",
      "epoch: 6 step: 591, loss is 0.0006514774868264794\n",
      "epoch: 6 step: 592, loss is 0.014052286744117737\n",
      "epoch: 6 step: 593, loss is 0.0038681405130773783\n",
      "epoch: 6 step: 594, loss is 0.0031927244272083044\n",
      "epoch: 6 step: 595, loss is 0.002476152265444398\n",
      "epoch: 6 step: 596, loss is 0.0752141997218132\n",
      "epoch: 6 step: 597, loss is 0.01894565299153328\n",
      "epoch: 6 step: 598, loss is 0.10079367458820343\n",
      "epoch: 6 step: 599, loss is 0.0003306376747786999\n",
      "epoch: 6 step: 600, loss is 0.0010829524835571647\n",
      "epoch: 6 step: 601, loss is 0.01358306035399437\n",
      "epoch: 6 step: 602, loss is 0.0003088798839598894\n",
      "epoch: 6 step: 603, loss is 0.05401130020618439\n",
      "epoch: 6 step: 604, loss is 0.022968797013163567\n",
      "epoch: 6 step: 605, loss is 0.005133590195327997\n",
      "epoch: 6 step: 606, loss is 0.003947536926716566\n",
      "epoch: 6 step: 607, loss is 0.001116336788982153\n",
      "epoch: 6 step: 608, loss is 0.00043182773515582085\n",
      "epoch: 6 step: 609, loss is 0.0004560774250421673\n",
      "epoch: 6 step: 610, loss is 0.00029674460529349744\n",
      "epoch: 6 step: 611, loss is 0.0031950846314430237\n",
      "epoch: 6 step: 612, loss is 0.0075137740932404995\n",
      "epoch: 6 step: 613, loss is 0.0002856649807654321\n",
      "epoch: 6 step: 614, loss is 0.021393775939941406\n",
      "epoch: 6 step: 615, loss is 0.1877344697713852\n",
      "epoch: 6 step: 616, loss is 0.006886407267302275\n",
      "epoch: 6 step: 617, loss is 0.010794440284371376\n",
      "epoch: 6 step: 618, loss is 0.006301676854491234\n",
      "epoch: 6 step: 619, loss is 0.020896589383482933\n",
      "epoch: 6 step: 620, loss is 0.002788498066365719\n",
      "epoch: 6 step: 621, loss is 0.007857516407966614\n",
      "epoch: 6 step: 622, loss is 0.01344335824251175\n",
      "epoch: 6 step: 623, loss is 0.3524984121322632\n",
      "epoch: 6 step: 624, loss is 0.0019176161149516702\n",
      "epoch: 6 step: 625, loss is 0.003960001282393932\n",
      "epoch: 6 step: 626, loss is 0.03814290836453438\n",
      "epoch: 6 step: 627, loss is 0.0012215025490149856\n",
      "epoch: 6 step: 628, loss is 0.008868684992194176\n",
      "epoch: 6 step: 629, loss is 0.054825566709041595\n",
      "epoch: 6 step: 630, loss is 0.004335013683885336\n",
      "epoch: 6 step: 631, loss is 0.003848123364150524\n",
      "epoch: 6 step: 632, loss is 0.0026118773967027664\n",
      "epoch: 6 step: 633, loss is 0.0028491413686424494\n",
      "epoch: 6 step: 634, loss is 0.0017913305200636387\n",
      "epoch: 6 step: 635, loss is 0.08612588793039322\n",
      "epoch: 6 step: 636, loss is 0.016522789373993874\n",
      "epoch: 6 step: 637, loss is 0.0009091144893318415\n",
      "epoch: 6 step: 638, loss is 0.08742552995681763\n",
      "epoch: 6 step: 639, loss is 0.02611861377954483\n",
      "epoch: 6 step: 640, loss is 0.000608431757427752\n",
      "epoch: 6 step: 641, loss is 0.0012366787996143103\n",
      "epoch: 6 step: 642, loss is 0.0029262062162160873\n",
      "epoch: 6 step: 643, loss is 0.07412251085042953\n",
      "epoch: 6 step: 644, loss is 0.014879459515213966\n",
      "epoch: 6 step: 645, loss is 0.0017625578911975026\n",
      "epoch: 6 step: 646, loss is 0.001485537039116025\n",
      "epoch: 6 step: 647, loss is 0.011871684342622757\n",
      "epoch: 6 step: 648, loss is 0.007475140504539013\n",
      "epoch: 6 step: 649, loss is 0.005477164871990681\n",
      "epoch: 6 step: 650, loss is 0.0014258387964218855\n",
      "epoch: 6 step: 651, loss is 0.00293688103556633\n",
      "epoch: 6 step: 652, loss is 0.000648025656118989\n",
      "epoch: 6 step: 653, loss is 0.0008905051508918405\n",
      "epoch: 6 step: 654, loss is 0.0019374125404283404\n",
      "epoch: 6 step: 655, loss is 0.09330897778272629\n",
      "epoch: 6 step: 656, loss is 0.01331824716180563\n",
      "epoch: 6 step: 657, loss is 0.00019025307847186923\n",
      "epoch: 6 step: 658, loss is 0.0029839896596968174\n",
      "epoch: 6 step: 659, loss is 0.002133822999894619\n",
      "epoch: 6 step: 660, loss is 0.08942731469869614\n",
      "epoch: 6 step: 661, loss is 0.0017120531992986798\n",
      "epoch: 6 step: 662, loss is 0.008054274134337902\n",
      "epoch: 6 step: 663, loss is 0.0034356541000306606\n",
      "epoch: 6 step: 664, loss is 0.00026014866307377815\n",
      "epoch: 6 step: 665, loss is 0.001547300023958087\n",
      "epoch: 6 step: 666, loss is 0.14049267768859863\n",
      "epoch: 6 step: 667, loss is 0.000685754872392863\n",
      "epoch: 6 step: 668, loss is 0.01773667521774769\n",
      "epoch: 6 step: 669, loss is 0.04303198680281639\n",
      "epoch: 6 step: 670, loss is 0.0006336913793347776\n",
      "epoch: 6 step: 671, loss is 0.0023575129453092813\n",
      "epoch: 6 step: 672, loss is 0.0023423221427947283\n",
      "epoch: 6 step: 673, loss is 0.0006157392635941505\n",
      "epoch: 6 step: 674, loss is 0.024817071855068207\n",
      "epoch: 6 step: 675, loss is 0.05663231015205383\n",
      "epoch: 6 step: 676, loss is 0.0009298261720687151\n",
      "epoch: 6 step: 677, loss is 0.001998316962271929\n",
      "epoch: 6 step: 678, loss is 0.013187761418521404\n",
      "epoch: 6 step: 679, loss is 0.0006519188173115253\n",
      "epoch: 6 step: 680, loss is 0.0018077859422191978\n",
      "epoch: 6 step: 681, loss is 0.0005772861768491566\n",
      "epoch: 6 step: 682, loss is 0.07437969744205475\n",
      "epoch: 6 step: 683, loss is 0.17819474637508392\n",
      "epoch: 6 step: 684, loss is 0.0011566388420760632\n",
      "epoch: 6 step: 685, loss is 0.004140282515436411\n",
      "epoch: 6 step: 686, loss is 0.02092665247619152\n",
      "epoch: 6 step: 687, loss is 0.0010977090569213033\n",
      "epoch: 6 step: 688, loss is 7.87667086115107e-05\n",
      "epoch: 6 step: 689, loss is 0.0007891446002759039\n",
      "epoch: 6 step: 690, loss is 0.04099442809820175\n",
      "epoch: 6 step: 691, loss is 0.00035480360384099185\n",
      "epoch: 6 step: 692, loss is 0.0034656997304409742\n",
      "epoch: 6 step: 693, loss is 0.0003368729376234114\n",
      "epoch: 6 step: 694, loss is 0.003969140816479921\n",
      "epoch: 6 step: 695, loss is 6.40412763459608e-05\n",
      "epoch: 6 step: 696, loss is 5.7738026953302324e-05\n",
      "epoch: 6 step: 697, loss is 0.0010793968103826046\n",
      "epoch: 6 step: 698, loss is 0.001037520240060985\n",
      "epoch: 6 step: 699, loss is 0.003126435447484255\n",
      "epoch: 6 step: 700, loss is 0.00020357302855700254\n",
      "epoch: 6 step: 701, loss is 0.0016874020220711827\n",
      "epoch: 6 step: 702, loss is 0.004661947954446077\n",
      "epoch: 6 step: 703, loss is 0.18819241225719452\n",
      "epoch: 6 step: 704, loss is 0.00027284957468509674\n",
      "epoch: 6 step: 705, loss is 0.0017783602233976126\n",
      "epoch: 6 step: 706, loss is 0.009907846339046955\n",
      "epoch: 6 step: 707, loss is 0.0054698060266673565\n",
      "epoch: 6 step: 708, loss is 0.09652913361787796\n",
      "epoch: 6 step: 709, loss is 0.012128780595958233\n",
      "epoch: 6 step: 710, loss is 0.0320076085627079\n",
      "epoch: 6 step: 711, loss is 0.008407751098275185\n",
      "epoch: 6 step: 712, loss is 0.014089321717619896\n",
      "epoch: 6 step: 713, loss is 0.13102319836616516\n",
      "epoch: 6 step: 714, loss is 0.0012858412228524685\n",
      "epoch: 6 step: 715, loss is 0.00065491849090904\n",
      "epoch: 6 step: 716, loss is 0.04094880074262619\n",
      "epoch: 6 step: 717, loss is 0.0002089645859086886\n",
      "epoch: 6 step: 718, loss is 0.0013566240668296814\n",
      "epoch: 6 step: 719, loss is 0.001310531748458743\n",
      "epoch: 6 step: 720, loss is 0.00017759617185220122\n",
      "epoch: 6 step: 721, loss is 0.0008354315650649369\n",
      "epoch: 6 step: 722, loss is 7.825648208381608e-05\n",
      "epoch: 6 step: 723, loss is 0.002503134310245514\n",
      "epoch: 6 step: 724, loss is 0.04516379162669182\n",
      "epoch: 6 step: 725, loss is 0.40019795298576355\n",
      "epoch: 6 step: 726, loss is 0.003464112291112542\n",
      "epoch: 6 step: 727, loss is 0.013370051048696041\n",
      "epoch: 6 step: 728, loss is 0.014091882854700089\n",
      "epoch: 6 step: 729, loss is 0.0012983152410015464\n",
      "epoch: 6 step: 730, loss is 0.0003820916172116995\n",
      "epoch: 6 step: 731, loss is 0.01589784398674965\n",
      "epoch: 6 step: 732, loss is 0.007578220218420029\n",
      "epoch: 6 step: 733, loss is 0.0011208016658201814\n",
      "epoch: 6 step: 734, loss is 0.0038903867825865746\n",
      "epoch: 6 step: 735, loss is 0.001496849930845201\n",
      "epoch: 6 step: 736, loss is 0.041375357657670975\n",
      "epoch: 6 step: 737, loss is 0.06336309760808945\n",
      "epoch: 6 step: 738, loss is 0.001513440627604723\n",
      "epoch: 6 step: 739, loss is 0.0007887963438406587\n",
      "epoch: 6 step: 740, loss is 0.00030240340856835246\n",
      "epoch: 6 step: 741, loss is 0.026292793452739716\n",
      "epoch: 6 step: 742, loss is 0.004450435750186443\n",
      "epoch: 6 step: 743, loss is 0.001472229603677988\n",
      "epoch: 6 step: 744, loss is 0.010218583047389984\n",
      "epoch: 6 step: 745, loss is 0.07516265660524368\n",
      "epoch: 6 step: 746, loss is 0.0007874264265410602\n",
      "epoch: 6 step: 747, loss is 0.0020304671488702297\n",
      "epoch: 6 step: 748, loss is 0.00931492354720831\n",
      "epoch: 6 step: 749, loss is 0.005949737504124641\n",
      "epoch: 6 step: 750, loss is 0.03703375905752182\n",
      "epoch: 6 step: 751, loss is 0.00439020711928606\n",
      "epoch: 6 step: 752, loss is 0.0005032135522924364\n",
      "epoch: 6 step: 753, loss is 0.0010057719191536307\n",
      "epoch: 6 step: 754, loss is 0.007998363114893436\n",
      "epoch: 6 step: 755, loss is 0.01039833016693592\n",
      "epoch: 6 step: 756, loss is 0.0020253616385161877\n",
      "epoch: 6 step: 757, loss is 0.0005583399324677885\n",
      "epoch: 6 step: 758, loss is 0.036910492926836014\n",
      "epoch: 6 step: 759, loss is 0.22317524254322052\n",
      "epoch: 6 step: 760, loss is 0.0035594766959547997\n",
      "epoch: 6 step: 761, loss is 0.00047013425501063466\n",
      "epoch: 6 step: 762, loss is 0.010597503744065762\n",
      "epoch: 6 step: 763, loss is 0.006825464311987162\n",
      "epoch: 6 step: 764, loss is 0.0014998393598943949\n",
      "epoch: 6 step: 765, loss is 0.0032842974178493023\n",
      "epoch: 6 step: 766, loss is 0.0005214035627432168\n",
      "epoch: 6 step: 767, loss is 0.02185872010886669\n",
      "epoch: 6 step: 768, loss is 0.03793870657682419\n",
      "epoch: 6 step: 769, loss is 0.27197936177253723\n",
      "epoch: 6 step: 770, loss is 0.0166570246219635\n",
      "epoch: 6 step: 771, loss is 0.01164000853896141\n",
      "epoch: 6 step: 772, loss is 0.0012321181129664183\n",
      "epoch: 6 step: 773, loss is 0.0049730646423995495\n",
      "epoch: 6 step: 774, loss is 0.06897227466106415\n",
      "epoch: 6 step: 775, loss is 0.02549847774207592\n",
      "epoch: 6 step: 776, loss is 0.00037566994433291256\n",
      "epoch: 6 step: 777, loss is 0.00410669669508934\n",
      "epoch: 6 step: 778, loss is 0.03708348050713539\n",
      "epoch: 6 step: 779, loss is 0.0010541590163484216\n",
      "epoch: 6 step: 780, loss is 0.03327823802828789\n",
      "epoch: 6 step: 781, loss is 0.010033616796135902\n",
      "epoch: 6 step: 782, loss is 0.010214122012257576\n",
      "epoch: 6 step: 783, loss is 0.003177959006279707\n",
      "epoch: 6 step: 784, loss is 0.0570344477891922\n",
      "epoch: 6 step: 785, loss is 0.001572616514749825\n",
      "epoch: 6 step: 786, loss is 0.0006024034228175879\n",
      "epoch: 6 step: 787, loss is 0.012516815215349197\n",
      "epoch: 6 step: 788, loss is 0.0019350220682099462\n",
      "epoch: 6 step: 789, loss is 0.0013930218992754817\n",
      "epoch: 6 step: 790, loss is 0.00506730517372489\n",
      "epoch: 6 step: 791, loss is 0.0005031166365370154\n",
      "epoch: 6 step: 792, loss is 0.0006383377476595342\n",
      "epoch: 6 step: 793, loss is 0.009502094238996506\n",
      "epoch: 6 step: 794, loss is 0.0008791260770522058\n",
      "epoch: 6 step: 795, loss is 0.00745100062340498\n",
      "epoch: 6 step: 796, loss is 0.00025537313194945455\n",
      "epoch: 6 step: 797, loss is 0.0003712139732670039\n",
      "epoch: 6 step: 798, loss is 0.0030869077891111374\n",
      "epoch: 6 step: 799, loss is 0.0014794799499213696\n",
      "epoch: 6 step: 800, loss is 0.010570572689175606\n",
      "epoch: 6 step: 801, loss is 0.00720372935757041\n",
      "epoch: 6 step: 802, loss is 0.06694357097148895\n",
      "epoch: 6 step: 803, loss is 0.44570890069007874\n",
      "epoch: 6 step: 804, loss is 0.0024750407319515944\n",
      "epoch: 6 step: 805, loss is 0.031021801754832268\n",
      "epoch: 6 step: 806, loss is 0.0012498765718191862\n",
      "epoch: 6 step: 807, loss is 0.0006562192575074732\n",
      "epoch: 6 step: 808, loss is 0.0011622215388342738\n",
      "epoch: 6 step: 809, loss is 0.002956966869533062\n",
      "epoch: 6 step: 810, loss is 0.006182082928717136\n",
      "epoch: 6 step: 811, loss is 0.014822309836745262\n",
      "epoch: 6 step: 812, loss is 0.0018222688231617212\n",
      "epoch: 6 step: 813, loss is 0.0004274885868653655\n",
      "epoch: 6 step: 814, loss is 0.01162591390311718\n",
      "epoch: 6 step: 815, loss is 0.002533451421186328\n",
      "epoch: 6 step: 816, loss is 0.0025858355220407248\n",
      "epoch: 6 step: 817, loss is 0.002578197279945016\n",
      "epoch: 6 step: 818, loss is 0.004531290847808123\n",
      "epoch: 6 step: 819, loss is 0.004079283215105534\n",
      "epoch: 6 step: 820, loss is 0.04717076197266579\n",
      "epoch: 6 step: 821, loss is 0.020847775042057037\n",
      "epoch: 6 step: 822, loss is 0.004582244902849197\n",
      "epoch: 6 step: 823, loss is 0.017893031239509583\n",
      "epoch: 6 step: 824, loss is 0.01530222874134779\n",
      "epoch: 6 step: 825, loss is 0.00374008622020483\n",
      "epoch: 6 step: 826, loss is 0.013838628306984901\n",
      "epoch: 6 step: 827, loss is 0.0010561863891780376\n",
      "epoch: 6 step: 828, loss is 6.762271368643269e-05\n",
      "epoch: 6 step: 829, loss is 0.006933866534382105\n",
      "epoch: 6 step: 830, loss is 0.0033869044855237007\n",
      "epoch: 6 step: 831, loss is 0.016337187960743904\n",
      "epoch: 6 step: 832, loss is 0.03960609436035156\n",
      "epoch: 6 step: 833, loss is 0.002726328559219837\n",
      "epoch: 6 step: 834, loss is 0.002288408810272813\n",
      "epoch: 6 step: 835, loss is 0.00811793189495802\n",
      "epoch: 6 step: 836, loss is 0.0012899659341201186\n",
      "epoch: 6 step: 837, loss is 0.007570725865662098\n",
      "epoch: 6 step: 838, loss is 0.006318238563835621\n",
      "epoch: 6 step: 839, loss is 0.0036040721461176872\n",
      "epoch: 6 step: 840, loss is 0.002426374005153775\n",
      "epoch: 6 step: 841, loss is 0.03029894083738327\n",
      "epoch: 6 step: 842, loss is 0.027301065623760223\n",
      "epoch: 6 step: 843, loss is 0.02325853519141674\n",
      "epoch: 6 step: 844, loss is 0.05956319347023964\n",
      "epoch: 6 step: 845, loss is 0.0005417488282546401\n",
      "epoch: 6 step: 846, loss is 0.00393142132088542\n",
      "epoch: 6 step: 847, loss is 0.0140434131026268\n",
      "epoch: 6 step: 848, loss is 0.005547364242374897\n",
      "epoch: 6 step: 849, loss is 0.0040703644044697285\n",
      "epoch: 6 step: 850, loss is 0.0007922637742012739\n",
      "epoch: 6 step: 851, loss is 0.0008415925549343228\n",
      "epoch: 6 step: 852, loss is 0.0051561277359724045\n",
      "epoch: 6 step: 853, loss is 0.0001749091170495376\n",
      "epoch: 6 step: 854, loss is 0.0002096132084261626\n",
      "epoch: 6 step: 855, loss is 0.05882818251848221\n",
      "epoch: 6 step: 856, loss is 0.0006090524839237332\n",
      "epoch: 6 step: 857, loss is 0.007386401295661926\n",
      "epoch: 6 step: 858, loss is 0.00015363720012828708\n",
      "epoch: 6 step: 859, loss is 0.010718237608671188\n",
      "epoch: 6 step: 860, loss is 0.017463214695453644\n",
      "epoch: 6 step: 861, loss is 0.011991245672106743\n",
      "epoch: 6 step: 862, loss is 0.016344331204891205\n",
      "epoch: 6 step: 863, loss is 0.013260574080049992\n",
      "epoch: 6 step: 864, loss is 0.00012205477833049372\n",
      "epoch: 6 step: 865, loss is 0.1369345337152481\n",
      "epoch: 6 step: 866, loss is 0.0010047450195997953\n",
      "epoch: 6 step: 867, loss is 0.002038936596363783\n",
      "epoch: 6 step: 868, loss is 3.1839197617955506e-05\n",
      "epoch: 6 step: 869, loss is 0.10221010446548462\n",
      "epoch: 6 step: 870, loss is 0.00019155209884047508\n",
      "epoch: 6 step: 871, loss is 0.0100674107670784\n",
      "epoch: 6 step: 872, loss is 0.00013694586232304573\n",
      "epoch: 6 step: 873, loss is 0.00728818541392684\n",
      "epoch: 6 step: 874, loss is 0.20711399614810944\n",
      "epoch: 6 step: 875, loss is 0.016070770099759102\n",
      "epoch: 6 step: 876, loss is 0.013593097217381\n",
      "epoch: 6 step: 877, loss is 0.0066191209480166435\n",
      "epoch: 6 step: 878, loss is 0.00641863327473402\n",
      "epoch: 6 step: 879, loss is 0.0009450143552385271\n",
      "epoch: 6 step: 880, loss is 0.002000256907194853\n",
      "epoch: 6 step: 881, loss is 0.0003967188822571188\n",
      "epoch: 6 step: 882, loss is 0.00763465603813529\n",
      "epoch: 6 step: 883, loss is 0.04480141028761864\n",
      "epoch: 6 step: 884, loss is 0.0009753523045219481\n",
      "epoch: 6 step: 885, loss is 0.021132580935955048\n",
      "epoch: 6 step: 886, loss is 0.001376492902636528\n",
      "epoch: 6 step: 887, loss is 0.0010823826305568218\n",
      "epoch: 6 step: 888, loss is 0.002326617483049631\n",
      "epoch: 6 step: 889, loss is 0.0032515714410692453\n",
      "epoch: 6 step: 890, loss is 0.09156664460897446\n",
      "epoch: 6 step: 891, loss is 0.09914806485176086\n",
      "epoch: 6 step: 892, loss is 0.0005568211781792343\n",
      "epoch: 6 step: 893, loss is 2.8001806640531868e-05\n",
      "epoch: 6 step: 894, loss is 0.008819416165351868\n",
      "epoch: 6 step: 895, loss is 0.0015781413530930877\n",
      "epoch: 6 step: 896, loss is 0.00031165420659817755\n",
      "epoch: 6 step: 897, loss is 3.410954013816081e-05\n",
      "epoch: 6 step: 898, loss is 0.12670615315437317\n",
      "epoch: 6 step: 899, loss is 0.015058939345180988\n",
      "epoch: 6 step: 900, loss is 0.0006646157125942409\n",
      "epoch: 6 step: 901, loss is 0.0009779612300917506\n",
      "epoch: 6 step: 902, loss is 0.00041812704876065254\n",
      "epoch: 6 step: 903, loss is 0.00013072078581899405\n",
      "epoch: 6 step: 904, loss is 0.00021489296341314912\n",
      "epoch: 6 step: 905, loss is 0.0005582910380326211\n",
      "epoch: 6 step: 906, loss is 0.0001784728083293885\n",
      "epoch: 6 step: 907, loss is 0.01131628081202507\n",
      "epoch: 6 step: 908, loss is 0.0009423196315765381\n",
      "epoch: 6 step: 909, loss is 0.0006584971561096609\n",
      "epoch: 6 step: 910, loss is 0.012111921794712543\n",
      "epoch: 6 step: 911, loss is 0.000743998505640775\n",
      "epoch: 6 step: 912, loss is 0.008963792584836483\n",
      "epoch: 6 step: 913, loss is 0.0017092539928853512\n",
      "epoch: 6 step: 914, loss is 0.00016925069212447852\n",
      "epoch: 6 step: 915, loss is 0.06938391923904419\n",
      "epoch: 6 step: 916, loss is 0.005948802921921015\n",
      "epoch: 6 step: 917, loss is 0.001802978222258389\n",
      "epoch: 6 step: 918, loss is 0.00242292950861156\n",
      "epoch: 6 step: 919, loss is 0.0013490816345438361\n",
      "epoch: 6 step: 920, loss is 0.0031376036349684\n",
      "epoch: 6 step: 921, loss is 0.000288988376269117\n",
      "epoch: 6 step: 922, loss is 0.037170812487602234\n",
      "epoch: 6 step: 923, loss is 0.05165348947048187\n",
      "epoch: 6 step: 924, loss is 0.0012366622686386108\n",
      "epoch: 6 step: 925, loss is 0.021133601665496826\n",
      "epoch: 6 step: 926, loss is 0.08762980997562408\n",
      "epoch: 6 step: 927, loss is 0.0013685397570952773\n",
      "epoch: 6 step: 928, loss is 0.0002989544009324163\n",
      "epoch: 6 step: 929, loss is 0.05189777910709381\n",
      "epoch: 6 step: 930, loss is 4.8121448344318196e-05\n",
      "epoch: 6 step: 931, loss is 0.0494498573243618\n",
      "epoch: 6 step: 932, loss is 0.01001744158565998\n",
      "epoch: 6 step: 933, loss is 0.0008880600216798484\n",
      "epoch: 6 step: 934, loss is 0.06918174028396606\n",
      "epoch: 6 step: 935, loss is 0.00012503295147325844\n",
      "epoch: 6 step: 936, loss is 0.18531620502471924\n",
      "epoch: 6 step: 937, loss is 0.00018916474073193967\n",
      "epoch: 6 step: 938, loss is 0.000769505393691361\n",
      "epoch: 6 step: 939, loss is 8.247123332694173e-05\n",
      "epoch: 6 step: 940, loss is 0.00014752670540474355\n",
      "epoch: 6 step: 941, loss is 3.608572296798229e-05\n",
      "epoch: 6 step: 942, loss is 0.00016449550457764417\n",
      "epoch: 6 step: 943, loss is 0.10937295854091644\n",
      "epoch: 6 step: 944, loss is 0.00038393281283788383\n",
      "epoch: 6 step: 945, loss is 0.03734201937913895\n",
      "epoch: 6 step: 946, loss is 0.0010124985128641129\n",
      "epoch: 6 step: 947, loss is 0.03001115471124649\n",
      "epoch: 6 step: 948, loss is 0.006059988867491484\n",
      "epoch: 6 step: 949, loss is 0.0006263292743824422\n",
      "epoch: 6 step: 950, loss is 0.14849641919136047\n",
      "epoch: 6 step: 951, loss is 0.005323488265275955\n",
      "epoch: 6 step: 952, loss is 7.749344513285905e-05\n",
      "epoch: 6 step: 953, loss is 0.006231367588043213\n",
      "epoch: 6 step: 954, loss is 0.14985649287700653\n",
      "epoch: 6 step: 955, loss is 0.001590645289979875\n",
      "epoch: 6 step: 956, loss is 0.0023591506760567427\n",
      "epoch: 6 step: 957, loss is 0.01975061558187008\n",
      "epoch: 6 step: 958, loss is 0.008660162799060345\n",
      "epoch: 6 step: 959, loss is 0.005070640705525875\n",
      "epoch: 6 step: 960, loss is 0.17046675086021423\n",
      "epoch: 6 step: 961, loss is 0.0018252651207149029\n",
      "epoch: 6 step: 962, loss is 0.000935070333071053\n",
      "epoch: 6 step: 963, loss is 0.03283181041479111\n",
      "epoch: 6 step: 964, loss is 0.02286176197230816\n",
      "epoch: 6 step: 965, loss is 0.039396122097969055\n",
      "epoch: 6 step: 966, loss is 0.0169892068952322\n",
      "epoch: 6 step: 967, loss is 0.0010769149521365762\n",
      "epoch: 6 step: 968, loss is 0.009189519099891186\n",
      "epoch: 6 step: 969, loss is 0.061431560665369034\n",
      "epoch: 6 step: 970, loss is 0.06152215227484703\n",
      "epoch: 6 step: 971, loss is 0.06574805825948715\n",
      "epoch: 6 step: 972, loss is 0.00018628135148901492\n",
      "epoch: 6 step: 973, loss is 0.0008960111299529672\n",
      "epoch: 6 step: 974, loss is 0.09361499547958374\n",
      "epoch: 6 step: 975, loss is 0.022529104724526405\n",
      "epoch: 6 step: 976, loss is 0.11078745871782303\n",
      "epoch: 6 step: 977, loss is 0.007709604687988758\n",
      "epoch: 6 step: 978, loss is 0.0002664078201632947\n",
      "epoch: 6 step: 979, loss is 0.0008708563982509077\n",
      "epoch: 6 step: 980, loss is 0.012202843092381954\n",
      "epoch: 6 step: 981, loss is 0.011221904307603836\n",
      "epoch: 6 step: 982, loss is 0.029826128855347633\n",
      "epoch: 6 step: 983, loss is 0.004546100739389658\n",
      "epoch: 6 step: 984, loss is 0.00024022752768360078\n",
      "epoch: 6 step: 985, loss is 0.005326665937900543\n",
      "epoch: 6 step: 986, loss is 0.18141739070415497\n",
      "epoch: 6 step: 987, loss is 0.2825327217578888\n",
      "epoch: 6 step: 988, loss is 0.003074903506785631\n",
      "epoch: 6 step: 989, loss is 0.03747359290719032\n",
      "epoch: 6 step: 990, loss is 0.0018134170677512884\n",
      "epoch: 6 step: 991, loss is 0.0008583654998801649\n",
      "epoch: 6 step: 992, loss is 0.0015352959744632244\n",
      "epoch: 6 step: 993, loss is 0.022304629907011986\n",
      "epoch: 6 step: 994, loss is 0.008683277294039726\n",
      "epoch: 6 step: 995, loss is 6.029922951711342e-05\n",
      "epoch: 6 step: 996, loss is 0.02187807485461235\n",
      "epoch: 6 step: 997, loss is 0.013002810999751091\n",
      "epoch: 6 step: 998, loss is 0.002827798016369343\n",
      "epoch: 6 step: 999, loss is 0.06379514932632446\n",
      "epoch: 6 step: 1000, loss is 0.0003668926074169576\n",
      "epoch: 6 step: 1001, loss is 0.00017338694306090474\n",
      "epoch: 6 step: 1002, loss is 0.0011157632106915116\n",
      "epoch: 6 step: 1003, loss is 0.0069593507796525955\n",
      "epoch: 6 step: 1004, loss is 0.033130958676338196\n",
      "epoch: 6 step: 1005, loss is 0.12758558988571167\n",
      "epoch: 6 step: 1006, loss is 0.00153607118409127\n",
      "epoch: 6 step: 1007, loss is 0.06242083013057709\n",
      "epoch: 6 step: 1008, loss is 0.003934840206056833\n",
      "epoch: 6 step: 1009, loss is 7.300160359591246e-05\n",
      "epoch: 6 step: 1010, loss is 0.006198178976774216\n",
      "epoch: 6 step: 1011, loss is 0.00011593521048780531\n",
      "epoch: 6 step: 1012, loss is 6.942018080735579e-05\n",
      "epoch: 6 step: 1013, loss is 0.004678983706980944\n",
      "epoch: 6 step: 1014, loss is 0.005489921197295189\n",
      "epoch: 6 step: 1015, loss is 1.8663193259271793e-05\n",
      "epoch: 6 step: 1016, loss is 0.003957933746278286\n",
      "epoch: 6 step: 1017, loss is 0.006196187809109688\n",
      "epoch: 6 step: 1018, loss is 0.0029160315170884132\n",
      "epoch: 6 step: 1019, loss is 0.21723036468029022\n",
      "epoch: 6 step: 1020, loss is 0.014151387847959995\n",
      "epoch: 6 step: 1021, loss is 8.821556548355147e-05\n",
      "epoch: 6 step: 1022, loss is 0.008140002377331257\n",
      "epoch: 6 step: 1023, loss is 0.002196362242102623\n",
      "epoch: 6 step: 1024, loss is 0.00024625161313451827\n",
      "epoch: 6 step: 1025, loss is 0.06778696179389954\n",
      "epoch: 6 step: 1026, loss is 0.00026767957024276257\n",
      "epoch: 6 step: 1027, loss is 0.07113441079854965\n",
      "epoch: 6 step: 1028, loss is 0.009826011024415493\n",
      "epoch: 6 step: 1029, loss is 9.016259718919173e-05\n",
      "epoch: 6 step: 1030, loss is 0.007015536539256573\n",
      "epoch: 6 step: 1031, loss is 0.004183328244835138\n",
      "epoch: 6 step: 1032, loss is 0.0015723461983725429\n",
      "epoch: 6 step: 1033, loss is 0.08259713649749756\n",
      "epoch: 6 step: 1034, loss is 0.001539145247079432\n",
      "epoch: 6 step: 1035, loss is 0.00021663914958480746\n",
      "epoch: 6 step: 1036, loss is 0.10148429125547409\n",
      "epoch: 6 step: 1037, loss is 0.0026544902939349413\n",
      "epoch: 6 step: 1038, loss is 0.0007284440798684955\n",
      "epoch: 6 step: 1039, loss is 0.0063799903728067875\n",
      "epoch: 6 step: 1040, loss is 0.00032318817102350295\n",
      "epoch: 6 step: 1041, loss is 0.0015778692904859781\n",
      "epoch: 6 step: 1042, loss is 0.0034350608475506306\n",
      "epoch: 6 step: 1043, loss is 0.011017439886927605\n",
      "epoch: 6 step: 1044, loss is 0.026958955451846123\n",
      "epoch: 6 step: 1045, loss is 0.004455019254237413\n",
      "epoch: 6 step: 1046, loss is 0.0006920155137777328\n",
      "epoch: 6 step: 1047, loss is 0.032212018966674805\n",
      "epoch: 6 step: 1048, loss is 0.008884284645318985\n",
      "epoch: 6 step: 1049, loss is 0.00902886874973774\n",
      "epoch: 6 step: 1050, loss is 0.00790571141988039\n",
      "epoch: 6 step: 1051, loss is 0.01754804141819477\n",
      "epoch: 6 step: 1052, loss is 0.012953522615134716\n",
      "epoch: 6 step: 1053, loss is 0.0016610744642093778\n",
      "epoch: 6 step: 1054, loss is 0.027992378920316696\n",
      "epoch: 6 step: 1055, loss is 0.0006740703829564154\n",
      "epoch: 6 step: 1056, loss is 0.004166588187217712\n",
      "epoch: 6 step: 1057, loss is 0.018473099917173386\n",
      "epoch: 6 step: 1058, loss is 0.06521717458963394\n",
      "epoch: 6 step: 1059, loss is 0.05117690935730934\n",
      "epoch: 6 step: 1060, loss is 0.0035431915894150734\n",
      "epoch: 6 step: 1061, loss is 0.010786186903715134\n",
      "epoch: 6 step: 1062, loss is 0.005106827709823847\n",
      "epoch: 6 step: 1063, loss is 0.013236245140433311\n",
      "epoch: 6 step: 1064, loss is 0.0005457202787511051\n",
      "epoch: 6 step: 1065, loss is 0.004222549963742495\n",
      "epoch: 6 step: 1066, loss is 0.02507796883583069\n",
      "epoch: 6 step: 1067, loss is 0.028121698647737503\n",
      "epoch: 6 step: 1068, loss is 0.015138376504182816\n",
      "epoch: 6 step: 1069, loss is 1.4024926713318564e-05\n",
      "epoch: 6 step: 1070, loss is 0.0037519698962569237\n",
      "epoch: 6 step: 1071, loss is 0.0014033580664545298\n",
      "epoch: 6 step: 1072, loss is 0.0017793390434235334\n",
      "epoch: 6 step: 1073, loss is 0.03583343327045441\n",
      "epoch: 6 step: 1074, loss is 0.0011155217653140426\n",
      "epoch: 6 step: 1075, loss is 0.06828070431947708\n",
      "epoch: 6 step: 1076, loss is 0.0002641056780703366\n",
      "epoch: 6 step: 1077, loss is 0.00729028694331646\n",
      "epoch: 6 step: 1078, loss is 0.0015410053310915828\n",
      "epoch: 6 step: 1079, loss is 0.01664569228887558\n",
      "epoch: 6 step: 1080, loss is 0.0021106628701090813\n",
      "epoch: 6 step: 1081, loss is 6.096639845054597e-05\n",
      "epoch: 6 step: 1082, loss is 0.000993038760498166\n",
      "epoch: 6 step: 1083, loss is 0.0005903144483454525\n",
      "epoch: 6 step: 1084, loss is 0.011206058785319328\n",
      "epoch: 6 step: 1085, loss is 0.0006127402884885669\n",
      "epoch: 6 step: 1086, loss is 0.00025646729045547545\n",
      "epoch: 6 step: 1087, loss is 0.057107482105493546\n",
      "epoch: 6 step: 1088, loss is 0.08320349454879761\n",
      "epoch: 6 step: 1089, loss is 0.009987294673919678\n",
      "epoch: 6 step: 1090, loss is 0.07764085382223129\n",
      "epoch: 6 step: 1091, loss is 0.0029461951926350594\n",
      "epoch: 6 step: 1092, loss is 0.028614744544029236\n",
      "epoch: 6 step: 1093, loss is 6.535238935612142e-05\n",
      "epoch: 6 step: 1094, loss is 0.003936618100851774\n",
      "epoch: 6 step: 1095, loss is 0.003944947384297848\n",
      "epoch: 6 step: 1096, loss is 0.0006090272800065577\n",
      "epoch: 6 step: 1097, loss is 5.591624358203262e-05\n",
      "epoch: 6 step: 1098, loss is 0.001841468852944672\n",
      "epoch: 6 step: 1099, loss is 0.018570778891444206\n",
      "epoch: 6 step: 1100, loss is 0.0025187963619828224\n",
      "epoch: 6 step: 1101, loss is 0.01648695394396782\n",
      "epoch: 6 step: 1102, loss is 0.015069805085659027\n",
      "epoch: 6 step: 1103, loss is 0.0005369947175495327\n",
      "epoch: 6 step: 1104, loss is 0.04489331692457199\n",
      "epoch: 6 step: 1105, loss is 6.51172740617767e-05\n",
      "epoch: 6 step: 1106, loss is 0.02908904105424881\n",
      "epoch: 6 step: 1107, loss is 0.03987354412674904\n",
      "epoch: 6 step: 1108, loss is 0.03451622277498245\n",
      "epoch: 6 step: 1109, loss is 0.0032263605389744043\n",
      "epoch: 6 step: 1110, loss is 0.006909143179655075\n",
      "epoch: 6 step: 1111, loss is 0.0006607283721677959\n",
      "epoch: 6 step: 1112, loss is 0.0015048632631078362\n",
      "epoch: 6 step: 1113, loss is 0.002887131180614233\n",
      "epoch: 6 step: 1114, loss is 0.0001359331072308123\n",
      "epoch: 6 step: 1115, loss is 0.05077676102519035\n",
      "epoch: 6 step: 1116, loss is 0.002070438815280795\n",
      "epoch: 6 step: 1117, loss is 0.001516682910732925\n",
      "epoch: 6 step: 1118, loss is 0.003437296487390995\n",
      "epoch: 6 step: 1119, loss is 0.03808725252747536\n",
      "epoch: 6 step: 1120, loss is 0.002744788071140647\n",
      "epoch: 6 step: 1121, loss is 0.0003511893155518919\n",
      "epoch: 6 step: 1122, loss is 0.00954666268080473\n",
      "epoch: 6 step: 1123, loss is 0.0026196050457656384\n",
      "epoch: 6 step: 1124, loss is 0.0423009991645813\n",
      "epoch: 6 step: 1125, loss is 0.07507950812578201\n",
      "epoch: 6 step: 1126, loss is 0.004019697196781635\n",
      "epoch: 6 step: 1127, loss is 0.00136475614272058\n",
      "epoch: 6 step: 1128, loss is 0.06329131126403809\n",
      "epoch: 6 step: 1129, loss is 0.040989719331264496\n",
      "epoch: 6 step: 1130, loss is 0.0005400420050136745\n",
      "epoch: 6 step: 1131, loss is 0.000820225803181529\n",
      "epoch: 6 step: 1132, loss is 9.15329874260351e-05\n",
      "epoch: 6 step: 1133, loss is 0.0010664232540875673\n",
      "epoch: 6 step: 1134, loss is 0.0951109379529953\n",
      "epoch: 6 step: 1135, loss is 0.0011295709991827607\n",
      "epoch: 6 step: 1136, loss is 0.11875486373901367\n",
      "epoch: 6 step: 1137, loss is 0.0023682157043367624\n",
      "epoch: 6 step: 1138, loss is 0.0020213224925100803\n",
      "epoch: 6 step: 1139, loss is 0.14949820935726166\n",
      "epoch: 6 step: 1140, loss is 0.0015494455583393574\n",
      "epoch: 6 step: 1141, loss is 0.0013386389473453164\n",
      "epoch: 6 step: 1142, loss is 0.14055122435092926\n",
      "epoch: 6 step: 1143, loss is 0.16744156181812286\n",
      "epoch: 6 step: 1144, loss is 0.14257526397705078\n",
      "epoch: 6 step: 1145, loss is 0.049067042768001556\n",
      "epoch: 6 step: 1146, loss is 0.0766955241560936\n",
      "epoch: 6 step: 1147, loss is 0.003645126009359956\n",
      "epoch: 6 step: 1148, loss is 0.0014947740128263831\n",
      "epoch: 6 step: 1149, loss is 0.0005280218902043998\n",
      "epoch: 6 step: 1150, loss is 0.0005769517156295478\n",
      "epoch: 6 step: 1151, loss is 0.014759228564798832\n",
      "epoch: 6 step: 1152, loss is 0.11762816458940506\n",
      "epoch: 6 step: 1153, loss is 0.030244894325733185\n",
      "epoch: 6 step: 1154, loss is 0.08676233887672424\n",
      "epoch: 6 step: 1155, loss is 0.10841166973114014\n",
      "epoch: 6 step: 1156, loss is 0.011918592266738415\n",
      "epoch: 6 step: 1157, loss is 0.09416341781616211\n",
      "epoch: 6 step: 1158, loss is 0.0008808624697849154\n",
      "epoch: 6 step: 1159, loss is 0.012513973750174046\n",
      "epoch: 6 step: 1160, loss is 0.01933315396308899\n",
      "epoch: 6 step: 1161, loss is 7.389005622826517e-05\n",
      "epoch: 6 step: 1162, loss is 8.956234523793682e-05\n",
      "epoch: 6 step: 1163, loss is 0.0025908455718308687\n",
      "epoch: 6 step: 1164, loss is 0.07318387925624847\n",
      "epoch: 6 step: 1165, loss is 0.0018293001921847463\n",
      "epoch: 6 step: 1166, loss is 0.07236677408218384\n",
      "epoch: 6 step: 1167, loss is 0.021563446149230003\n",
      "epoch: 6 step: 1168, loss is 0.000997508643195033\n",
      "epoch: 6 step: 1169, loss is 0.08309923857450485\n",
      "epoch: 6 step: 1170, loss is 0.0012797587551176548\n",
      "epoch: 6 step: 1171, loss is 0.0016628147568553686\n",
      "epoch: 6 step: 1172, loss is 0.001146336318925023\n",
      "epoch: 6 step: 1173, loss is 0.0005868959124200046\n",
      "epoch: 6 step: 1174, loss is 0.0018345196731388569\n",
      "epoch: 6 step: 1175, loss is 0.008712682873010635\n",
      "epoch: 6 step: 1176, loss is 0.0003858934505842626\n",
      "epoch: 6 step: 1177, loss is 0.0010133010800927877\n",
      "epoch: 6 step: 1178, loss is 0.1600940078496933\n",
      "epoch: 6 step: 1179, loss is 0.09350495040416718\n",
      "epoch: 6 step: 1180, loss is 0.008633543737232685\n",
      "epoch: 6 step: 1181, loss is 0.14927241206169128\n",
      "epoch: 6 step: 1182, loss is 0.005231759510934353\n",
      "epoch: 6 step: 1183, loss is 0.04588906094431877\n",
      "epoch: 6 step: 1184, loss is 0.10931000858545303\n",
      "epoch: 6 step: 1185, loss is 0.11012301594018936\n",
      "epoch: 6 step: 1186, loss is 0.1403939425945282\n",
      "epoch: 6 step: 1187, loss is 0.018865667283535004\n",
      "epoch: 6 step: 1188, loss is 0.0007710066856816411\n",
      "epoch: 6 step: 1189, loss is 0.008995560929179192\n",
      "epoch: 6 step: 1190, loss is 0.040415357798337936\n",
      "epoch: 6 step: 1191, loss is 0.03202180936932564\n",
      "epoch: 6 step: 1192, loss is 0.017488403245806694\n",
      "epoch: 6 step: 1193, loss is 0.002810684498399496\n",
      "epoch: 6 step: 1194, loss is 0.019693909212946892\n",
      "epoch: 6 step: 1195, loss is 0.003489461960271001\n",
      "epoch: 6 step: 1196, loss is 0.00014818177442066371\n",
      "epoch: 6 step: 1197, loss is 0.013669254258275032\n",
      "epoch: 6 step: 1198, loss is 0.037851303815841675\n",
      "epoch: 6 step: 1199, loss is 0.00011060902033932507\n",
      "epoch: 6 step: 1200, loss is 0.00028038755408488214\n",
      "epoch: 6 step: 1201, loss is 0.0007169835735112429\n",
      "epoch: 6 step: 1202, loss is 0.00933876819908619\n",
      "epoch: 6 step: 1203, loss is 0.06664767861366272\n",
      "epoch: 6 step: 1204, loss is 0.03153733164072037\n",
      "epoch: 6 step: 1205, loss is 0.023783011361956596\n",
      "epoch: 6 step: 1206, loss is 0.02319304458796978\n",
      "epoch: 6 step: 1207, loss is 0.004083266016095877\n",
      "epoch: 6 step: 1208, loss is 0.11363773047924042\n",
      "epoch: 6 step: 1209, loss is 0.0003463448374532163\n",
      "epoch: 6 step: 1210, loss is 0.0020533758215606213\n",
      "epoch: 6 step: 1211, loss is 0.021786866709589958\n",
      "epoch: 6 step: 1212, loss is 0.005414715502411127\n",
      "epoch: 6 step: 1213, loss is 0.0015132466796785593\n",
      "epoch: 6 step: 1214, loss is 0.13150708377361298\n",
      "epoch: 6 step: 1215, loss is 0.00012430358037818223\n",
      "epoch: 6 step: 1216, loss is 0.00021615622972603887\n",
      "epoch: 6 step: 1217, loss is 0.00825590081512928\n",
      "epoch: 6 step: 1218, loss is 0.022466441616415977\n",
      "epoch: 6 step: 1219, loss is 0.0022134881000965834\n",
      "epoch: 6 step: 1220, loss is 0.00041019474156200886\n",
      "epoch: 6 step: 1221, loss is 0.0020007637795060873\n",
      "epoch: 6 step: 1222, loss is 0.32207611203193665\n",
      "epoch: 6 step: 1223, loss is 0.03731491416692734\n",
      "epoch: 6 step: 1224, loss is 0.09438621997833252\n",
      "epoch: 6 step: 1225, loss is 0.0002840571978595108\n",
      "epoch: 6 step: 1226, loss is 0.024829674512147903\n",
      "epoch: 6 step: 1227, loss is 0.00760566396638751\n",
      "epoch: 6 step: 1228, loss is 0.001992800273001194\n",
      "epoch: 6 step: 1229, loss is 0.03200433403253555\n",
      "epoch: 6 step: 1230, loss is 0.0003291593457106501\n",
      "epoch: 6 step: 1231, loss is 0.003356507746502757\n",
      "epoch: 6 step: 1232, loss is 0.0561063215136528\n",
      "epoch: 6 step: 1233, loss is 0.010514373891055584\n",
      "epoch: 6 step: 1234, loss is 0.0961603969335556\n",
      "epoch: 6 step: 1235, loss is 0.0038081645034253597\n",
      "epoch: 6 step: 1236, loss is 0.04834694787859917\n",
      "epoch: 6 step: 1237, loss is 0.0023662177845835686\n",
      "epoch: 6 step: 1238, loss is 0.0001386874937452376\n",
      "epoch: 6 step: 1239, loss is 0.0002518255787435919\n",
      "epoch: 6 step: 1240, loss is 0.001316715031862259\n",
      "epoch: 6 step: 1241, loss is 0.008548672311007977\n",
      "epoch: 6 step: 1242, loss is 0.001336961635388434\n",
      "epoch: 6 step: 1243, loss is 0.00037620935472659767\n",
      "epoch: 6 step: 1244, loss is 0.0018344606505706906\n",
      "epoch: 6 step: 1245, loss is 0.0004209429316688329\n",
      "epoch: 6 step: 1246, loss is 0.0781891867518425\n",
      "epoch: 6 step: 1247, loss is 0.015398655086755753\n",
      "epoch: 6 step: 1248, loss is 0.0030890433117747307\n",
      "epoch: 6 step: 1249, loss is 0.0030883029103279114\n",
      "epoch: 6 step: 1250, loss is 7.755206752335653e-05\n",
      "epoch: 6 step: 1251, loss is 0.004340935032814741\n",
      "epoch: 6 step: 1252, loss is 0.07067351788282394\n",
      "epoch: 6 step: 1253, loss is 0.003076188499107957\n",
      "epoch: 6 step: 1254, loss is 0.07669856399297714\n",
      "epoch: 6 step: 1255, loss is 0.005293880123645067\n",
      "epoch: 6 step: 1256, loss is 0.07257666438817978\n",
      "epoch: 6 step: 1257, loss is 0.0019922293722629547\n",
      "epoch: 6 step: 1258, loss is 0.0007602202822454274\n",
      "epoch: 6 step: 1259, loss is 0.018742702901363373\n",
      "epoch: 6 step: 1260, loss is 0.000946945627219975\n",
      "epoch: 6 step: 1261, loss is 0.0007632850320078433\n",
      "epoch: 6 step: 1262, loss is 0.0034056752920150757\n",
      "epoch: 6 step: 1263, loss is 0.008061553351581097\n",
      "epoch: 6 step: 1264, loss is 0.0020791669376194477\n",
      "epoch: 6 step: 1265, loss is 0.011475066654384136\n",
      "epoch: 6 step: 1266, loss is 0.24660055339336395\n",
      "epoch: 6 step: 1267, loss is 0.056793130934238434\n",
      "epoch: 6 step: 1268, loss is 0.003247914370149374\n",
      "epoch: 6 step: 1269, loss is 0.02318638563156128\n",
      "epoch: 6 step: 1270, loss is 0.01809561438858509\n",
      "epoch: 6 step: 1271, loss is 0.01300507690757513\n",
      "epoch: 6 step: 1272, loss is 0.00574882747605443\n",
      "epoch: 6 step: 1273, loss is 0.05367158353328705\n",
      "epoch: 6 step: 1274, loss is 0.008197181858122349\n",
      "epoch: 6 step: 1275, loss is 0.0009335442446172237\n",
      "epoch: 6 step: 1276, loss is 0.003818446071818471\n",
      "epoch: 6 step: 1277, loss is 0.004204051569104195\n",
      "epoch: 6 step: 1278, loss is 0.05505615845322609\n",
      "epoch: 6 step: 1279, loss is 0.004727862309664488\n",
      "epoch: 6 step: 1280, loss is 0.07548841089010239\n",
      "epoch: 6 step: 1281, loss is 0.011386074125766754\n",
      "epoch: 6 step: 1282, loss is 0.00026130551123060286\n",
      "epoch: 6 step: 1283, loss is 0.0021059755235910416\n",
      "epoch: 6 step: 1284, loss is 0.058738697320222855\n",
      "epoch: 6 step: 1285, loss is 0.00013699628470931202\n",
      "epoch: 6 step: 1286, loss is 0.032423656433820724\n",
      "epoch: 6 step: 1287, loss is 0.00040078503661789\n",
      "epoch: 6 step: 1288, loss is 0.009106804616749287\n",
      "epoch: 6 step: 1289, loss is 0.09571380913257599\n",
      "epoch: 6 step: 1290, loss is 0.0009884032187983394\n",
      "epoch: 6 step: 1291, loss is 0.3162389099597931\n",
      "epoch: 6 step: 1292, loss is 0.009984026663005352\n",
      "epoch: 6 step: 1293, loss is 0.08161047101020813\n",
      "epoch: 6 step: 1294, loss is 0.0013847977388650179\n",
      "epoch: 6 step: 1295, loss is 0.003908341284841299\n",
      "epoch: 6 step: 1296, loss is 0.034861963242292404\n",
      "epoch: 6 step: 1297, loss is 0.004330059979110956\n",
      "epoch: 6 step: 1298, loss is 0.002502467017620802\n",
      "epoch: 6 step: 1299, loss is 0.0044103870168328285\n",
      "epoch: 6 step: 1300, loss is 0.007108983118087053\n",
      "epoch: 6 step: 1301, loss is 0.13346415758132935\n",
      "epoch: 6 step: 1302, loss is 0.07120711356401443\n",
      "epoch: 6 step: 1303, loss is 0.000472891959361732\n",
      "epoch: 6 step: 1304, loss is 0.03628111258149147\n",
      "epoch: 6 step: 1305, loss is 0.015013831667602062\n",
      "epoch: 6 step: 1306, loss is 0.0016508419066667557\n",
      "epoch: 6 step: 1307, loss is 0.008672046475112438\n",
      "epoch: 6 step: 1308, loss is 0.02164572663605213\n",
      "epoch: 6 step: 1309, loss is 0.007403823547065258\n",
      "epoch: 6 step: 1310, loss is 0.02093903347849846\n",
      "epoch: 6 step: 1311, loss is 0.1785801351070404\n",
      "epoch: 6 step: 1312, loss is 0.1038866639137268\n",
      "epoch: 6 step: 1313, loss is 0.005902757402509451\n",
      "epoch: 6 step: 1314, loss is 0.03245807811617851\n",
      "epoch: 6 step: 1315, loss is 0.006495850160717964\n",
      "epoch: 6 step: 1316, loss is 0.017083704471588135\n",
      "epoch: 6 step: 1317, loss is 0.0022621778771281242\n",
      "epoch: 6 step: 1318, loss is 0.030030539259314537\n",
      "epoch: 6 step: 1319, loss is 0.1185821071267128\n",
      "epoch: 6 step: 1320, loss is 0.011676749214529991\n",
      "epoch: 6 step: 1321, loss is 0.1095706894993782\n",
      "epoch: 6 step: 1322, loss is 0.0010676884558051825\n",
      "epoch: 6 step: 1323, loss is 0.00046485260827466846\n",
      "epoch: 6 step: 1324, loss is 0.0005672798724845052\n",
      "epoch: 6 step: 1325, loss is 0.0017195454565808177\n",
      "epoch: 6 step: 1326, loss is 0.002714938949793577\n",
      "epoch: 6 step: 1327, loss is 0.001099359360523522\n",
      "epoch: 6 step: 1328, loss is 0.038697127252817154\n",
      "epoch: 6 step: 1329, loss is 0.002646917477250099\n",
      "epoch: 6 step: 1330, loss is 0.03499465063214302\n",
      "epoch: 6 step: 1331, loss is 0.0012465687468647957\n",
      "epoch: 6 step: 1332, loss is 0.022930577397346497\n",
      "epoch: 6 step: 1333, loss is 0.005868513602763414\n",
      "epoch: 6 step: 1334, loss is 0.0010413939598947763\n",
      "epoch: 6 step: 1335, loss is 0.007720455527305603\n",
      "epoch: 6 step: 1336, loss is 0.017114097252488136\n",
      "epoch: 6 step: 1337, loss is 0.011301922611892223\n",
      "epoch: 6 step: 1338, loss is 0.005951747298240662\n",
      "epoch: 6 step: 1339, loss is 0.0009292328031733632\n",
      "epoch: 6 step: 1340, loss is 0.0289930272847414\n",
      "epoch: 6 step: 1341, loss is 0.08205988258123398\n",
      "epoch: 6 step: 1342, loss is 0.21988177299499512\n",
      "epoch: 6 step: 1343, loss is 0.0009162121568806469\n",
      "epoch: 6 step: 1344, loss is 0.0003967730444855988\n",
      "epoch: 6 step: 1345, loss is 0.1480734795331955\n",
      "epoch: 6 step: 1346, loss is 0.0017108928877860308\n",
      "epoch: 6 step: 1347, loss is 0.013882980681955814\n",
      "epoch: 6 step: 1348, loss is 0.03227115422487259\n",
      "epoch: 6 step: 1349, loss is 0.034676652401685715\n",
      "epoch: 6 step: 1350, loss is 0.0061131427064538\n",
      "epoch: 6 step: 1351, loss is 0.0003830350178759545\n",
      "epoch: 6 step: 1352, loss is 0.011661294847726822\n",
      "epoch: 6 step: 1353, loss is 0.03295193985104561\n",
      "epoch: 6 step: 1354, loss is 0.009761316701769829\n",
      "epoch: 6 step: 1355, loss is 0.004223629832267761\n",
      "epoch: 6 step: 1356, loss is 0.008867338299751282\n",
      "epoch: 6 step: 1357, loss is 0.03637935221195221\n",
      "epoch: 6 step: 1358, loss is 0.009117170237004757\n",
      "epoch: 6 step: 1359, loss is 0.0024678311310708523\n",
      "epoch: 6 step: 1360, loss is 0.0018392157508060336\n",
      "epoch: 6 step: 1361, loss is 0.00303779193200171\n",
      "epoch: 6 step: 1362, loss is 0.00966221746057272\n",
      "epoch: 6 step: 1363, loss is 0.00013411659165285528\n",
      "epoch: 6 step: 1364, loss is 0.009598703123629093\n",
      "epoch: 6 step: 1365, loss is 0.03648034483194351\n",
      "epoch: 6 step: 1366, loss is 0.11988642811775208\n",
      "epoch: 6 step: 1367, loss is 0.02115689218044281\n",
      "epoch: 6 step: 1368, loss is 0.07341211289167404\n",
      "epoch: 6 step: 1369, loss is 0.009566345252096653\n",
      "epoch: 6 step: 1370, loss is 0.023622339591383934\n",
      "epoch: 6 step: 1371, loss is 0.003180905245244503\n",
      "epoch: 6 step: 1372, loss is 0.003817846067249775\n",
      "epoch: 6 step: 1373, loss is 0.0021862240973860025\n",
      "epoch: 6 step: 1374, loss is 0.013432305306196213\n",
      "epoch: 6 step: 1375, loss is 0.0964178591966629\n",
      "epoch: 6 step: 1376, loss is 0.0016739508137106895\n",
      "epoch: 6 step: 1377, loss is 0.003899573115631938\n",
      "epoch: 6 step: 1378, loss is 0.019449420273303986\n",
      "epoch: 6 step: 1379, loss is 0.013143818825483322\n",
      "epoch: 6 step: 1380, loss is 0.0038374834693968296\n",
      "epoch: 6 step: 1381, loss is 0.06734378635883331\n",
      "epoch: 6 step: 1382, loss is 0.09861992299556732\n",
      "epoch: 6 step: 1383, loss is 0.02323072776198387\n",
      "epoch: 6 step: 1384, loss is 0.0008011358440853655\n",
      "epoch: 6 step: 1385, loss is 6.835709791630507e-05\n",
      "epoch: 6 step: 1386, loss is 0.0007605039863847196\n",
      "epoch: 6 step: 1387, loss is 0.00046445662155747414\n",
      "epoch: 6 step: 1388, loss is 0.0014112511416897178\n",
      "epoch: 6 step: 1389, loss is 0.1175668016076088\n",
      "epoch: 6 step: 1390, loss is 0.0004270668141543865\n",
      "epoch: 6 step: 1391, loss is 0.022119488567113876\n",
      "epoch: 6 step: 1392, loss is 0.0018562674522399902\n",
      "epoch: 6 step: 1393, loss is 0.06394438445568085\n",
      "epoch: 6 step: 1394, loss is 0.0045881071127951145\n",
      "epoch: 6 step: 1395, loss is 0.010951376520097256\n",
      "epoch: 6 step: 1396, loss is 0.003500985447317362\n",
      "epoch: 6 step: 1397, loss is 0.002989772940054536\n",
      "epoch: 6 step: 1398, loss is 0.00027945596957579255\n",
      "epoch: 6 step: 1399, loss is 0.007374678738415241\n",
      "epoch: 6 step: 1400, loss is 0.011277368292212486\n",
      "epoch: 6 step: 1401, loss is 0.0016272100619971752\n",
      "epoch: 6 step: 1402, loss is 0.00036421892582438886\n",
      "epoch: 6 step: 1403, loss is 0.01575770601630211\n",
      "epoch: 6 step: 1404, loss is 0.0015498128486797214\n",
      "epoch: 6 step: 1405, loss is 0.0007001223275437951\n",
      "epoch: 6 step: 1406, loss is 0.009440007619559765\n",
      "epoch: 6 step: 1407, loss is 0.0012076158309355378\n",
      "epoch: 6 step: 1408, loss is 0.0017684190534055233\n",
      "epoch: 6 step: 1409, loss is 0.011060395278036594\n",
      "epoch: 6 step: 1410, loss is 0.008991698734462261\n",
      "epoch: 6 step: 1411, loss is 0.00021736405324190855\n",
      "epoch: 6 step: 1412, loss is 0.0006535335560329258\n",
      "epoch: 6 step: 1413, loss is 0.004186311736702919\n",
      "epoch: 6 step: 1414, loss is 0.0025557568296790123\n",
      "epoch: 6 step: 1415, loss is 0.0009679111535660923\n",
      "epoch: 6 step: 1416, loss is 0.0022018542513251305\n",
      "epoch: 6 step: 1417, loss is 0.005019958596676588\n",
      "epoch: 6 step: 1418, loss is 0.0027794179040938616\n",
      "epoch: 6 step: 1419, loss is 0.018585437908768654\n",
      "epoch: 6 step: 1420, loss is 0.0027364122215658426\n",
      "epoch: 6 step: 1421, loss is 0.023733966052532196\n",
      "epoch: 6 step: 1422, loss is 0.0026041388045996428\n",
      "epoch: 6 step: 1423, loss is 0.015635592862963676\n",
      "epoch: 6 step: 1424, loss is 0.00042665473301894963\n",
      "epoch: 6 step: 1425, loss is 0.0007209279574453831\n",
      "epoch: 6 step: 1426, loss is 0.09257102012634277\n",
      "epoch: 6 step: 1427, loss is 0.0034597828052937984\n",
      "epoch: 6 step: 1428, loss is 0.043615520000457764\n",
      "epoch: 6 step: 1429, loss is 0.0004425086663104594\n",
      "epoch: 6 step: 1430, loss is 0.19645223021507263\n",
      "epoch: 6 step: 1431, loss is 0.08494548499584198\n",
      "epoch: 6 step: 1432, loss is 0.01204629335552454\n",
      "epoch: 6 step: 1433, loss is 0.07431423664093018\n",
      "epoch: 6 step: 1434, loss is 0.006878690328449011\n",
      "epoch: 6 step: 1435, loss is 0.0075639090500772\n",
      "epoch: 6 step: 1436, loss is 0.0013689928455278277\n",
      "epoch: 6 step: 1437, loss is 0.00031039980240166187\n",
      "epoch: 6 step: 1438, loss is 0.0008383924723602831\n",
      "epoch: 6 step: 1439, loss is 0.00026678090216591954\n",
      "epoch: 6 step: 1440, loss is 0.0038167370948940516\n",
      "epoch: 6 step: 1441, loss is 0.1452791839838028\n",
      "epoch: 6 step: 1442, loss is 0.00996747799217701\n",
      "epoch: 6 step: 1443, loss is 0.0005726534291170537\n",
      "epoch: 6 step: 1444, loss is 0.1534826010465622\n",
      "epoch: 6 step: 1445, loss is 0.00018113967962563038\n",
      "epoch: 6 step: 1446, loss is 0.0017170100472867489\n",
      "epoch: 6 step: 1447, loss is 0.002154379617422819\n",
      "epoch: 6 step: 1448, loss is 0.04418502375483513\n",
      "epoch: 6 step: 1449, loss is 0.1239963173866272\n",
      "epoch: 6 step: 1450, loss is 0.019156426191329956\n",
      "epoch: 6 step: 1451, loss is 0.0010374748380854726\n",
      "epoch: 6 step: 1452, loss is 0.017990760505199432\n",
      "epoch: 6 step: 1453, loss is 0.09862111508846283\n",
      "epoch: 6 step: 1454, loss is 0.11380906403064728\n",
      "epoch: 6 step: 1455, loss is 0.0007016704767011106\n",
      "epoch: 6 step: 1456, loss is 0.003113271202892065\n",
      "epoch: 6 step: 1457, loss is 0.15641333162784576\n",
      "epoch: 6 step: 1458, loss is 0.01038401946425438\n",
      "epoch: 6 step: 1459, loss is 0.0005198990693315864\n",
      "epoch: 6 step: 1460, loss is 0.0009870147332549095\n",
      "epoch: 6 step: 1461, loss is 0.003080293070524931\n",
      "epoch: 6 step: 1462, loss is 0.0007793313125148416\n",
      "epoch: 6 step: 1463, loss is 0.0022855401039123535\n",
      "epoch: 6 step: 1464, loss is 0.005933677777647972\n",
      "epoch: 6 step: 1465, loss is 0.001892279600724578\n",
      "epoch: 6 step: 1466, loss is 0.0005691263941116631\n",
      "epoch: 6 step: 1467, loss is 0.11341860890388489\n",
      "epoch: 6 step: 1468, loss is 0.0010944855166599154\n",
      "epoch: 6 step: 1469, loss is 0.015396876260638237\n",
      "epoch: 6 step: 1470, loss is 0.0003151940181851387\n",
      "epoch: 6 step: 1471, loss is 0.04577726498246193\n",
      "epoch: 6 step: 1472, loss is 0.09763635694980621\n",
      "epoch: 6 step: 1473, loss is 0.023144999518990517\n",
      "epoch: 6 step: 1474, loss is 0.0005475509096868336\n",
      "epoch: 6 step: 1475, loss is 0.028174860402941704\n",
      "epoch: 6 step: 1476, loss is 0.04682920128107071\n",
      "epoch: 6 step: 1477, loss is 0.0018452891381457448\n",
      "epoch: 6 step: 1478, loss is 0.006988403387367725\n",
      "epoch: 6 step: 1479, loss is 0.03562718629837036\n",
      "epoch: 6 step: 1480, loss is 0.0066729276441037655\n",
      "epoch: 6 step: 1481, loss is 0.04282144084572792\n",
      "epoch: 6 step: 1482, loss is 0.0004384879139252007\n",
      "epoch: 6 step: 1483, loss is 0.0004525072581600398\n",
      "epoch: 6 step: 1484, loss is 0.0980931967496872\n",
      "epoch: 6 step: 1485, loss is 0.0003101873444393277\n",
      "epoch: 6 step: 1486, loss is 0.004525331314653158\n",
      "epoch: 6 step: 1487, loss is 0.005612005479633808\n",
      "epoch: 6 step: 1488, loss is 0.0027430704794824123\n",
      "epoch: 6 step: 1489, loss is 0.02542925998568535\n",
      "epoch: 6 step: 1490, loss is 0.0007779487641528249\n",
      "epoch: 6 step: 1491, loss is 0.00120361999142915\n",
      "epoch: 6 step: 1492, loss is 0.0528414361178875\n",
      "epoch: 6 step: 1493, loss is 0.0008798646740615368\n",
      "epoch: 6 step: 1494, loss is 0.006310687866061926\n",
      "epoch: 6 step: 1495, loss is 0.0005523486179299653\n",
      "epoch: 6 step: 1496, loss is 0.022935520857572556\n",
      "epoch: 6 step: 1497, loss is 0.041991062462329865\n",
      "epoch: 6 step: 1498, loss is 0.0038566812872886658\n",
      "epoch: 6 step: 1499, loss is 0.001860986347310245\n",
      "epoch: 6 step: 1500, loss is 0.08748681098222733\n",
      "epoch: 6 step: 1501, loss is 0.0007336019189096987\n",
      "epoch: 6 step: 1502, loss is 0.03237764537334442\n",
      "epoch: 6 step: 1503, loss is 0.009585082530975342\n",
      "epoch: 6 step: 1504, loss is 0.002977571915835142\n",
      "epoch: 6 step: 1505, loss is 0.21798959374427795\n",
      "epoch: 6 step: 1506, loss is 0.0016723619773983955\n",
      "epoch: 6 step: 1507, loss is 0.04208793863654137\n",
      "epoch: 6 step: 1508, loss is 0.000484014512039721\n",
      "epoch: 6 step: 1509, loss is 0.00027030741330236197\n",
      "epoch: 6 step: 1510, loss is 0.010300801135599613\n",
      "epoch: 6 step: 1511, loss is 0.05847208574414253\n",
      "epoch: 6 step: 1512, loss is 0.0004723592137452215\n",
      "epoch: 6 step: 1513, loss is 0.003004545345902443\n",
      "epoch: 6 step: 1514, loss is 0.14027903974056244\n",
      "epoch: 6 step: 1515, loss is 0.0019709609914571047\n",
      "epoch: 6 step: 1516, loss is 0.0008125999593175948\n",
      "epoch: 6 step: 1517, loss is 0.04638083279132843\n",
      "epoch: 6 step: 1518, loss is 0.006247921846807003\n",
      "epoch: 6 step: 1519, loss is 0.03324561566114426\n",
      "epoch: 6 step: 1520, loss is 0.15731129050254822\n",
      "epoch: 6 step: 1521, loss is 0.0001761678431648761\n",
      "epoch: 6 step: 1522, loss is 0.0020347991958260536\n",
      "epoch: 6 step: 1523, loss is 0.006428682245314121\n",
      "epoch: 6 step: 1524, loss is 0.0003575576702132821\n",
      "epoch: 6 step: 1525, loss is 0.0066736782900989056\n",
      "epoch: 6 step: 1526, loss is 0.03676965832710266\n",
      "epoch: 6 step: 1527, loss is 0.07311402261257172\n",
      "epoch: 6 step: 1528, loss is 0.006094381678849459\n",
      "epoch: 6 step: 1529, loss is 0.00016496900934726\n",
      "epoch: 6 step: 1530, loss is 0.00014342862414196134\n",
      "epoch: 6 step: 1531, loss is 0.018908318132162094\n",
      "epoch: 6 step: 1532, loss is 0.013069039210677147\n",
      "epoch: 6 step: 1533, loss is 0.04707981273531914\n",
      "epoch: 6 step: 1534, loss is 0.0005000229575671256\n",
      "epoch: 6 step: 1535, loss is 0.0011176643893122673\n",
      "epoch: 6 step: 1536, loss is 0.0755385011434555\n",
      "epoch: 6 step: 1537, loss is 0.0010362123139202595\n",
      "epoch: 6 step: 1538, loss is 0.004050060641020536\n",
      "epoch: 6 step: 1539, loss is 0.005730999633669853\n",
      "epoch: 6 step: 1540, loss is 0.002970084547996521\n",
      "epoch: 6 step: 1541, loss is 0.02315392531454563\n",
      "epoch: 6 step: 1542, loss is 0.002492366125807166\n",
      "epoch: 6 step: 1543, loss is 0.01159234531223774\n",
      "epoch: 6 step: 1544, loss is 0.005287171341478825\n",
      "epoch: 6 step: 1545, loss is 0.00010015122825279832\n",
      "epoch: 6 step: 1546, loss is 0.008342832326889038\n",
      "epoch: 6 step: 1547, loss is 0.002108887769281864\n",
      "epoch: 6 step: 1548, loss is 0.00872455257922411\n",
      "epoch: 6 step: 1549, loss is 0.006916005630046129\n",
      "epoch: 6 step: 1550, loss is 0.0013961255317553878\n",
      "epoch: 6 step: 1551, loss is 0.0001448659022571519\n",
      "epoch: 6 step: 1552, loss is 0.0031177836935967207\n",
      "epoch: 6 step: 1553, loss is 0.00013368889631237835\n",
      "epoch: 6 step: 1554, loss is 0.011296885088086128\n",
      "epoch: 6 step: 1555, loss is 0.034217823296785355\n",
      "epoch: 6 step: 1556, loss is 0.09904683381319046\n",
      "epoch: 6 step: 1557, loss is 0.07406390458345413\n",
      "epoch: 6 step: 1558, loss is 0.01693780906498432\n",
      "epoch: 6 step: 1559, loss is 4.397904922370799e-05\n",
      "epoch: 6 step: 1560, loss is 0.005355349741876125\n",
      "epoch: 6 step: 1561, loss is 0.10482816398143768\n",
      "epoch: 6 step: 1562, loss is 0.0002790098951663822\n",
      "epoch: 6 step: 1563, loss is 0.017383955419063568\n",
      "epoch: 6 step: 1564, loss is 0.007546745706349611\n",
      "epoch: 6 step: 1565, loss is 0.004014057572931051\n",
      "epoch: 6 step: 1566, loss is 0.0013285495806485415\n",
      "epoch: 6 step: 1567, loss is 0.05354788154363632\n",
      "epoch: 6 step: 1568, loss is 0.037512555718421936\n",
      "epoch: 6 step: 1569, loss is 0.0005351987201720476\n",
      "epoch: 6 step: 1570, loss is 0.0003107846714556217\n",
      "epoch: 6 step: 1571, loss is 0.006874032784253359\n",
      "epoch: 6 step: 1572, loss is 0.0007426731754094362\n",
      "epoch: 6 step: 1573, loss is 0.23418816924095154\n",
      "epoch: 6 step: 1574, loss is 0.0052385288290679455\n",
      "epoch: 6 step: 1575, loss is 0.451506108045578\n",
      "epoch: 6 step: 1576, loss is 0.039349786937236786\n",
      "epoch: 6 step: 1577, loss is 0.10413307696580887\n",
      "epoch: 6 step: 1578, loss is 0.005212849471718073\n",
      "epoch: 6 step: 1579, loss is 0.004087049979716539\n",
      "epoch: 6 step: 1580, loss is 0.0005819502403028309\n",
      "epoch: 6 step: 1581, loss is 0.012860411778092384\n",
      "epoch: 6 step: 1582, loss is 0.0005807355628348887\n",
      "epoch: 6 step: 1583, loss is 0.015065991319715977\n",
      "epoch: 6 step: 1584, loss is 0.04202358052134514\n",
      "epoch: 6 step: 1585, loss is 0.002367192879319191\n",
      "epoch: 6 step: 1586, loss is 0.005662041716277599\n",
      "epoch: 6 step: 1587, loss is 0.008044449612498283\n",
      "epoch: 6 step: 1588, loss is 0.049958743155002594\n",
      "epoch: 6 step: 1589, loss is 0.0010530881118029356\n",
      "epoch: 6 step: 1590, loss is 0.2268061488866806\n",
      "epoch: 6 step: 1591, loss is 0.02896270900964737\n",
      "epoch: 6 step: 1592, loss is 0.008695566095411777\n",
      "epoch: 6 step: 1593, loss is 0.019350210204720497\n",
      "epoch: 6 step: 1594, loss is 0.12843015789985657\n",
      "epoch: 6 step: 1595, loss is 0.0006878253188915551\n",
      "epoch: 6 step: 1596, loss is 0.03455082327127457\n",
      "epoch: 6 step: 1597, loss is 0.0019627290312200785\n",
      "epoch: 6 step: 1598, loss is 0.00768249249085784\n",
      "epoch: 6 step: 1599, loss is 0.04774271324276924\n",
      "epoch: 6 step: 1600, loss is 0.0011134120868518949\n",
      "epoch: 6 step: 1601, loss is 0.012176085263490677\n",
      "epoch: 6 step: 1602, loss is 0.038909412920475006\n",
      "epoch: 6 step: 1603, loss is 0.021069243550300598\n",
      "epoch: 6 step: 1604, loss is 0.02378782071173191\n",
      "epoch: 6 step: 1605, loss is 0.011724811047315598\n",
      "epoch: 6 step: 1606, loss is 0.009848653338849545\n",
      "epoch: 6 step: 1607, loss is 0.0025440643075853586\n",
      "epoch: 6 step: 1608, loss is 0.0007230453775264323\n",
      "epoch: 6 step: 1609, loss is 0.0031169799622148275\n",
      "epoch: 6 step: 1610, loss is 0.03145986422896385\n",
      "epoch: 6 step: 1611, loss is 0.09359648078680038\n",
      "epoch: 6 step: 1612, loss is 0.0012356530642136931\n",
      "epoch: 6 step: 1613, loss is 0.01163560338318348\n",
      "epoch: 6 step: 1614, loss is 0.00012923299800604582\n",
      "epoch: 6 step: 1615, loss is 0.009169796481728554\n",
      "epoch: 6 step: 1616, loss is 0.0004218083340674639\n",
      "epoch: 6 step: 1617, loss is 0.0009124643984250724\n",
      "epoch: 6 step: 1618, loss is 0.0021196547895669937\n",
      "epoch: 6 step: 1619, loss is 0.011364655569195747\n",
      "epoch: 6 step: 1620, loss is 0.0007207014714367688\n",
      "epoch: 6 step: 1621, loss is 0.057258300483226776\n",
      "epoch: 6 step: 1622, loss is 0.0008775605238042772\n",
      "epoch: 6 step: 1623, loss is 0.007048350293189287\n",
      "epoch: 6 step: 1624, loss is 0.09711463749408722\n",
      "epoch: 6 step: 1625, loss is 0.06038402393460274\n",
      "epoch: 6 step: 1626, loss is 0.0021587738301604986\n",
      "epoch: 6 step: 1627, loss is 0.03322703018784523\n",
      "epoch: 6 step: 1628, loss is 0.00019518363114912063\n",
      "epoch: 6 step: 1629, loss is 0.023870768025517464\n",
      "epoch: 6 step: 1630, loss is 0.0003438805288169533\n",
      "epoch: 6 step: 1631, loss is 0.12593556940555573\n",
      "epoch: 6 step: 1632, loss is 0.020687442272901535\n",
      "epoch: 6 step: 1633, loss is 0.16172632575035095\n",
      "epoch: 6 step: 1634, loss is 0.0022216883953660727\n",
      "epoch: 6 step: 1635, loss is 0.0021717355120927095\n",
      "epoch: 6 step: 1636, loss is 0.05932508036494255\n",
      "epoch: 6 step: 1637, loss is 0.0045815203338861465\n",
      "epoch: 6 step: 1638, loss is 0.001030850107781589\n",
      "epoch: 6 step: 1639, loss is 0.13336877524852753\n",
      "epoch: 6 step: 1640, loss is 0.0005103322910144925\n",
      "epoch: 6 step: 1641, loss is 0.007760413456708193\n",
      "epoch: 6 step: 1642, loss is 0.007778352126479149\n",
      "epoch: 6 step: 1643, loss is 0.001619114656932652\n",
      "epoch: 6 step: 1644, loss is 0.0005400415975600481\n",
      "epoch: 6 step: 1645, loss is 0.00153769098687917\n",
      "epoch: 6 step: 1646, loss is 0.171775221824646\n",
      "epoch: 6 step: 1647, loss is 0.00832399446517229\n",
      "epoch: 6 step: 1648, loss is 0.3148757517337799\n",
      "epoch: 6 step: 1649, loss is 0.0011408890131860971\n",
      "epoch: 6 step: 1650, loss is 0.013377157971262932\n",
      "epoch: 6 step: 1651, loss is 0.00167456257622689\n",
      "epoch: 6 step: 1652, loss is 0.023237917572259903\n",
      "epoch: 6 step: 1653, loss is 0.017852630466222763\n",
      "epoch: 6 step: 1654, loss is 0.0012965593487024307\n",
      "epoch: 6 step: 1655, loss is 0.0010622615227475762\n",
      "epoch: 6 step: 1656, loss is 0.0001411578559782356\n",
      "epoch: 6 step: 1657, loss is 0.0013906856765970588\n",
      "epoch: 6 step: 1658, loss is 0.02336929365992546\n",
      "epoch: 6 step: 1659, loss is 0.013530966825783253\n",
      "epoch: 6 step: 1660, loss is 0.0020810915157198906\n",
      "epoch: 6 step: 1661, loss is 0.08770909905433655\n",
      "epoch: 6 step: 1662, loss is 0.0008348787669092417\n",
      "epoch: 6 step: 1663, loss is 0.0050604441203176975\n",
      "epoch: 6 step: 1664, loss is 0.0010265358723700047\n",
      "epoch: 6 step: 1665, loss is 0.08328758180141449\n",
      "epoch: 6 step: 1666, loss is 0.0009169045370072126\n",
      "epoch: 6 step: 1667, loss is 0.0010190283646807075\n",
      "epoch: 6 step: 1668, loss is 0.16584736108779907\n",
      "epoch: 6 step: 1669, loss is 0.020875640213489532\n",
      "epoch: 6 step: 1670, loss is 0.016522470861673355\n",
      "epoch: 6 step: 1671, loss is 0.061923399567604065\n",
      "epoch: 6 step: 1672, loss is 0.0016011238330975175\n",
      "epoch: 6 step: 1673, loss is 0.08127874881029129\n",
      "epoch: 6 step: 1674, loss is 0.002512293867766857\n",
      "epoch: 6 step: 1675, loss is 0.0005761406500823796\n",
      "epoch: 6 step: 1676, loss is 0.11659754067659378\n",
      "epoch: 6 step: 1677, loss is 0.14570128917694092\n",
      "epoch: 6 step: 1678, loss is 0.06145230680704117\n",
      "epoch: 6 step: 1679, loss is 0.011667652986943722\n",
      "epoch: 6 step: 1680, loss is 0.053228676319122314\n",
      "epoch: 6 step: 1681, loss is 0.007419608999043703\n",
      "epoch: 6 step: 1682, loss is 0.005368503276258707\n",
      "epoch: 6 step: 1683, loss is 0.035716816782951355\n",
      "epoch: 6 step: 1684, loss is 0.006630418356508017\n",
      "epoch: 6 step: 1685, loss is 0.0016908799298107624\n",
      "epoch: 6 step: 1686, loss is 0.05475331097841263\n",
      "epoch: 6 step: 1687, loss is 0.000583382323384285\n",
      "epoch: 6 step: 1688, loss is 0.011110336519777775\n",
      "epoch: 6 step: 1689, loss is 0.012396401725709438\n",
      "epoch: 6 step: 1690, loss is 0.028815025463700294\n",
      "epoch: 6 step: 1691, loss is 0.001096209860406816\n",
      "epoch: 6 step: 1692, loss is 0.0026147349271923304\n",
      "epoch: 6 step: 1693, loss is 0.1093892976641655\n",
      "epoch: 6 step: 1694, loss is 0.06705982238054276\n",
      "epoch: 6 step: 1695, loss is 0.00017183332238346338\n",
      "epoch: 6 step: 1696, loss is 0.02085975743830204\n",
      "epoch: 6 step: 1697, loss is 0.02092551812529564\n",
      "epoch: 6 step: 1698, loss is 0.03853739798069\n",
      "epoch: 6 step: 1699, loss is 0.03107689507305622\n",
      "epoch: 6 step: 1700, loss is 0.007695227395743132\n",
      "epoch: 6 step: 1701, loss is 0.003038870869204402\n",
      "epoch: 6 step: 1702, loss is 0.019236423075199127\n",
      "epoch: 6 step: 1703, loss is 0.136482834815979\n",
      "epoch: 6 step: 1704, loss is 0.002805073047056794\n",
      "epoch: 6 step: 1705, loss is 0.08421844989061356\n",
      "epoch: 6 step: 1706, loss is 0.0008557790424674749\n",
      "epoch: 6 step: 1707, loss is 0.07978697121143341\n",
      "epoch: 6 step: 1708, loss is 0.0020442125387489796\n",
      "epoch: 6 step: 1709, loss is 0.0010216854279860854\n",
      "epoch: 6 step: 1710, loss is 0.061280447989702225\n",
      "epoch: 6 step: 1711, loss is 0.04303883761167526\n",
      "epoch: 6 step: 1712, loss is 0.004046939779073\n",
      "epoch: 6 step: 1713, loss is 0.13913734257221222\n",
      "epoch: 6 step: 1714, loss is 0.006140062585473061\n",
      "epoch: 6 step: 1715, loss is 0.017894800752401352\n",
      "epoch: 6 step: 1716, loss is 0.02017083391547203\n",
      "epoch: 6 step: 1717, loss is 0.024795373901724815\n",
      "epoch: 6 step: 1718, loss is 0.026743082329630852\n",
      "epoch: 6 step: 1719, loss is 0.0013253206852823496\n",
      "epoch: 6 step: 1720, loss is 0.0123656140640378\n",
      "epoch: 6 step: 1721, loss is 0.036353763192892075\n",
      "epoch: 6 step: 1722, loss is 0.003536923322826624\n",
      "epoch: 6 step: 1723, loss is 0.0007866635569371283\n",
      "epoch: 6 step: 1724, loss is 0.007905201055109501\n",
      "epoch: 6 step: 1725, loss is 0.018581867218017578\n",
      "epoch: 6 step: 1726, loss is 0.0017134665977209806\n",
      "epoch: 6 step: 1727, loss is 0.16149795055389404\n",
      "epoch: 6 step: 1728, loss is 0.08419150114059448\n",
      "epoch: 6 step: 1729, loss is 0.0008538566762581468\n",
      "epoch: 6 step: 1730, loss is 0.006985954474657774\n",
      "epoch: 6 step: 1731, loss is 0.1121123731136322\n",
      "epoch: 6 step: 1732, loss is 0.04647096246480942\n",
      "epoch: 6 step: 1733, loss is 0.0013679510448127985\n",
      "epoch: 6 step: 1734, loss is 0.0956743136048317\n",
      "epoch: 6 step: 1735, loss is 0.03889292851090431\n",
      "epoch: 6 step: 1736, loss is 0.004660151433199644\n",
      "epoch: 6 step: 1737, loss is 0.21651093661785126\n",
      "epoch: 6 step: 1738, loss is 0.0019832225516438484\n",
      "epoch: 6 step: 1739, loss is 0.004375241696834564\n",
      "epoch: 6 step: 1740, loss is 0.0002812344173435122\n",
      "epoch: 6 step: 1741, loss is 0.030971530824899673\n",
      "epoch: 6 step: 1742, loss is 0.026121458038687706\n",
      "epoch: 6 step: 1743, loss is 0.00042780573130585253\n",
      "epoch: 6 step: 1744, loss is 0.002208166755735874\n",
      "epoch: 6 step: 1745, loss is 0.0022640819661319256\n",
      "epoch: 6 step: 1746, loss is 0.005464330781251192\n",
      "epoch: 6 step: 1747, loss is 0.060022175312042236\n",
      "epoch: 6 step: 1748, loss is 0.011260799132287502\n",
      "epoch: 6 step: 1749, loss is 0.00027976417914032936\n",
      "epoch: 6 step: 1750, loss is 0.0010021477937698364\n",
      "epoch: 6 step: 1751, loss is 0.04693407192826271\n",
      "epoch: 6 step: 1752, loss is 0.000415031478041783\n",
      "epoch: 6 step: 1753, loss is 0.11733115464448929\n",
      "epoch: 6 step: 1754, loss is 0.0035328660160303116\n",
      "epoch: 6 step: 1755, loss is 0.002864838344976306\n",
      "epoch: 6 step: 1756, loss is 0.026476161554455757\n",
      "epoch: 6 step: 1757, loss is 0.011353988200426102\n",
      "epoch: 6 step: 1758, loss is 0.000938152486924082\n",
      "epoch: 6 step: 1759, loss is 0.0004637632519006729\n",
      "epoch: 6 step: 1760, loss is 0.02859783172607422\n",
      "epoch: 6 step: 1761, loss is 0.0009051039814949036\n",
      "epoch: 6 step: 1762, loss is 0.01071029994636774\n",
      "epoch: 6 step: 1763, loss is 0.003026269841939211\n",
      "epoch: 6 step: 1764, loss is 0.0015064394101500511\n",
      "epoch: 6 step: 1765, loss is 0.008454691618680954\n",
      "epoch: 6 step: 1766, loss is 0.04892642796039581\n",
      "epoch: 6 step: 1767, loss is 0.004849912133067846\n",
      "epoch: 6 step: 1768, loss is 0.010606901720166206\n",
      "epoch: 6 step: 1769, loss is 0.034903187304735184\n",
      "epoch: 6 step: 1770, loss is 0.009721028618514538\n",
      "epoch: 6 step: 1771, loss is 0.0007937984191812575\n",
      "epoch: 6 step: 1772, loss is 0.0042325276881456375\n",
      "epoch: 6 step: 1773, loss is 0.0011287236120551825\n",
      "epoch: 6 step: 1774, loss is 0.011337479576468468\n",
      "epoch: 6 step: 1775, loss is 0.08716071397066116\n",
      "epoch: 6 step: 1776, loss is 0.008572726510465145\n",
      "epoch: 6 step: 1777, loss is 0.001991475699469447\n",
      "epoch: 6 step: 1778, loss is 0.045237693935632706\n",
      "epoch: 6 step: 1779, loss is 0.0021645899396389723\n",
      "epoch: 6 step: 1780, loss is 0.014345568604767323\n",
      "epoch: 6 step: 1781, loss is 0.003733559511601925\n",
      "epoch: 6 step: 1782, loss is 0.0015062398742884398\n",
      "epoch: 6 step: 1783, loss is 0.00018023679149337113\n",
      "epoch: 6 step: 1784, loss is 0.003390832571312785\n",
      "epoch: 6 step: 1785, loss is 0.0006181179778650403\n",
      "epoch: 6 step: 1786, loss is 0.01995706930756569\n",
      "epoch: 6 step: 1787, loss is 0.09317323565483093\n",
      "epoch: 6 step: 1788, loss is 0.0009219423518516123\n",
      "epoch: 6 step: 1789, loss is 0.05416285619139671\n",
      "epoch: 6 step: 1790, loss is 0.0533597469329834\n",
      "epoch: 6 step: 1791, loss is 0.00026868851273320615\n",
      "epoch: 6 step: 1792, loss is 0.01680082082748413\n",
      "epoch: 6 step: 1793, loss is 0.018028665333986282\n",
      "epoch: 6 step: 1794, loss is 0.00040485081262886524\n",
      "epoch: 6 step: 1795, loss is 0.00016035638691391796\n",
      "epoch: 6 step: 1796, loss is 0.005428722593933344\n",
      "epoch: 6 step: 1797, loss is 0.0037410419899970293\n",
      "epoch: 6 step: 1798, loss is 0.010845302604138851\n",
      "epoch: 6 step: 1799, loss is 0.0014027567813172936\n",
      "epoch: 6 step: 1800, loss is 0.11784466356039047\n",
      "epoch: 6 step: 1801, loss is 0.0045833103358745575\n",
      "epoch: 6 step: 1802, loss is 0.0008465718710795045\n",
      "epoch: 6 step: 1803, loss is 0.005821271799504757\n",
      "epoch: 6 step: 1804, loss is 0.0692746564745903\n",
      "epoch: 6 step: 1805, loss is 0.00836784578859806\n",
      "epoch: 6 step: 1806, loss is 0.08818075060844421\n",
      "epoch: 6 step: 1807, loss is 0.00033275375608354807\n",
      "epoch: 6 step: 1808, loss is 0.003766206558793783\n",
      "epoch: 6 step: 1809, loss is 0.05926539748907089\n",
      "epoch: 6 step: 1810, loss is 0.0018312059110030532\n",
      "epoch: 6 step: 1811, loss is 0.001058212947100401\n",
      "epoch: 6 step: 1812, loss is 0.030452663078904152\n",
      "epoch: 6 step: 1813, loss is 0.01640392653644085\n",
      "epoch: 6 step: 1814, loss is 0.08063717186450958\n",
      "epoch: 6 step: 1815, loss is 0.0018908611964434385\n",
      "epoch: 6 step: 1816, loss is 0.0318254716694355\n",
      "epoch: 6 step: 1817, loss is 0.02809733711183071\n",
      "epoch: 6 step: 1818, loss is 0.002688546199351549\n",
      "epoch: 6 step: 1819, loss is 0.06315702199935913\n",
      "epoch: 6 step: 1820, loss is 0.0016498765908181667\n",
      "epoch: 6 step: 1821, loss is 0.012946243397891521\n",
      "epoch: 6 step: 1822, loss is 0.00016969512216746807\n",
      "epoch: 6 step: 1823, loss is 0.005741561762988567\n",
      "epoch: 6 step: 1824, loss is 0.00538936723023653\n",
      "epoch: 6 step: 1825, loss is 0.006705508101731539\n",
      "epoch: 6 step: 1826, loss is 0.020318878814578056\n",
      "epoch: 6 step: 1827, loss is 0.148345485329628\n",
      "epoch: 6 step: 1828, loss is 0.0022158226929605007\n",
      "epoch: 6 step: 1829, loss is 0.008651363663375378\n",
      "epoch: 6 step: 1830, loss is 0.15182390809059143\n",
      "epoch: 6 step: 1831, loss is 0.001277800416573882\n",
      "epoch: 6 step: 1832, loss is 0.0029233049135655165\n",
      "epoch: 6 step: 1833, loss is 0.0025384684558957815\n",
      "epoch: 6 step: 1834, loss is 0.006037727929651737\n",
      "epoch: 6 step: 1835, loss is 0.005254372488707304\n",
      "epoch: 6 step: 1836, loss is 0.0041994014754891396\n",
      "epoch: 6 step: 1837, loss is 0.010523379780352116\n",
      "epoch: 6 step: 1838, loss is 0.001344021293334663\n",
      "epoch: 6 step: 1839, loss is 0.0023238742724061012\n",
      "epoch: 6 step: 1840, loss is 0.01836957409977913\n",
      "epoch: 6 step: 1841, loss is 0.0013249395415186882\n",
      "epoch: 6 step: 1842, loss is 0.0018784430576488376\n",
      "epoch: 6 step: 1843, loss is 0.0002034048520727083\n",
      "epoch: 6 step: 1844, loss is 0.04906567186117172\n",
      "epoch: 6 step: 1845, loss is 0.0005643117474392056\n",
      "epoch: 6 step: 1846, loss is 0.009096261113882065\n",
      "epoch: 6 step: 1847, loss is 0.006892526522278786\n",
      "epoch: 6 step: 1848, loss is 0.037453003227710724\n",
      "epoch: 6 step: 1849, loss is 0.16577906906604767\n",
      "epoch: 6 step: 1850, loss is 0.006109852343797684\n",
      "epoch: 6 step: 1851, loss is 0.007487293798476458\n",
      "epoch: 6 step: 1852, loss is 0.006058874074369669\n",
      "epoch: 6 step: 1853, loss is 0.03287748992443085\n",
      "epoch: 6 step: 1854, loss is 0.016894042491912842\n",
      "epoch: 6 step: 1855, loss is 0.0010539729846641421\n",
      "epoch: 6 step: 1856, loss is 0.0008356653270311654\n",
      "epoch: 6 step: 1857, loss is 0.0010533890454098582\n",
      "epoch: 6 step: 1858, loss is 0.0004448062973096967\n",
      "epoch: 6 step: 1859, loss is 0.0034181426744908094\n",
      "epoch: 6 step: 1860, loss is 0.001248873071745038\n",
      "epoch: 6 step: 1861, loss is 0.05116187036037445\n",
      "epoch: 6 step: 1862, loss is 0.004409357439726591\n",
      "epoch: 6 step: 1863, loss is 0.025430778041481972\n",
      "epoch: 6 step: 1864, loss is 0.0950322300195694\n",
      "epoch: 6 step: 1865, loss is 0.0013235702645033598\n",
      "epoch: 6 step: 1866, loss is 0.0011344990925863385\n",
      "epoch: 6 step: 1867, loss is 0.003723268862813711\n",
      "epoch: 6 step: 1868, loss is 0.02076220139861107\n",
      "epoch: 6 step: 1869, loss is 0.003927767276763916\n",
      "epoch: 6 step: 1870, loss is 0.0005356594920158386\n",
      "epoch: 6 step: 1871, loss is 0.038888975977897644\n",
      "epoch: 6 step: 1872, loss is 0.005495020188391209\n",
      "epoch: 6 step: 1873, loss is 0.06993994116783142\n",
      "epoch: 6 step: 1874, loss is 0.0007594836060889065\n",
      "epoch: 6 step: 1875, loss is 0.0012023687595501542\n",
      "epoch: 7 step: 1, loss is 0.0004084658867213875\n",
      "epoch: 7 step: 2, loss is 0.09303925186395645\n",
      "epoch: 7 step: 3, loss is 0.0017170784994959831\n",
      "epoch: 7 step: 4, loss is 0.0006494109402410686\n",
      "epoch: 7 step: 5, loss is 0.019293727353215218\n",
      "epoch: 7 step: 6, loss is 0.0008113685762509704\n",
      "epoch: 7 step: 7, loss is 0.03657344728708267\n",
      "epoch: 7 step: 8, loss is 0.03737938776612282\n",
      "epoch: 7 step: 9, loss is 0.0015344920102506876\n",
      "epoch: 7 step: 10, loss is 0.009334740228950977\n",
      "epoch: 7 step: 11, loss is 0.00037364120362326503\n",
      "epoch: 7 step: 12, loss is 0.0017580899875611067\n",
      "epoch: 7 step: 13, loss is 0.01869616098701954\n",
      "epoch: 7 step: 14, loss is 0.0401783250272274\n",
      "epoch: 7 step: 15, loss is 0.0022257643286138773\n",
      "epoch: 7 step: 16, loss is 0.001927371951751411\n",
      "epoch: 7 step: 17, loss is 0.0002670880639925599\n",
      "epoch: 7 step: 18, loss is 0.0036602008622139692\n",
      "epoch: 7 step: 19, loss is 0.008970999158918858\n",
      "epoch: 7 step: 20, loss is 0.0054275537841022015\n",
      "epoch: 7 step: 21, loss is 0.004215221386402845\n",
      "epoch: 7 step: 22, loss is 0.007759372238069773\n",
      "epoch: 7 step: 23, loss is 0.0041556330397725105\n",
      "epoch: 7 step: 24, loss is 0.006802810356020927\n",
      "epoch: 7 step: 25, loss is 0.05363711714744568\n",
      "epoch: 7 step: 26, loss is 0.011019465513527393\n",
      "epoch: 7 step: 27, loss is 0.08446750789880753\n",
      "epoch: 7 step: 28, loss is 0.001964908093214035\n",
      "epoch: 7 step: 29, loss is 0.0007561689708381891\n",
      "epoch: 7 step: 30, loss is 0.0029395439196377993\n",
      "epoch: 7 step: 31, loss is 0.003994871396571398\n",
      "epoch: 7 step: 32, loss is 0.0016688312171027064\n",
      "epoch: 7 step: 33, loss is 0.0073432172648608685\n",
      "epoch: 7 step: 34, loss is 0.0006173907313495874\n",
      "epoch: 7 step: 35, loss is 0.003817838616669178\n",
      "epoch: 7 step: 36, loss is 0.003365076147019863\n",
      "epoch: 7 step: 37, loss is 0.2517622113227844\n",
      "epoch: 7 step: 38, loss is 0.00566417071968317\n",
      "epoch: 7 step: 39, loss is 0.060430798679590225\n",
      "epoch: 7 step: 40, loss is 0.003052148036658764\n",
      "epoch: 7 step: 41, loss is 0.0019232920603826642\n",
      "epoch: 7 step: 42, loss is 0.0003512182738631964\n",
      "epoch: 7 step: 43, loss is 0.0311402790248394\n",
      "epoch: 7 step: 44, loss is 0.0008550732163712382\n",
      "epoch: 7 step: 45, loss is 0.0017291806871071458\n",
      "epoch: 7 step: 46, loss is 0.0038227420300245285\n",
      "epoch: 7 step: 47, loss is 0.06625734269618988\n",
      "epoch: 7 step: 48, loss is 0.017394455149769783\n",
      "epoch: 7 step: 49, loss is 0.0021401105914264917\n",
      "epoch: 7 step: 50, loss is 0.006208664271980524\n",
      "epoch: 7 step: 51, loss is 0.00019037642050534487\n",
      "epoch: 7 step: 52, loss is 0.0008228727383539081\n",
      "epoch: 7 step: 53, loss is 0.0015289663570001721\n",
      "epoch: 7 step: 54, loss is 0.001224425621330738\n",
      "epoch: 7 step: 55, loss is 0.08290211856365204\n",
      "epoch: 7 step: 56, loss is 0.0034121135249733925\n",
      "epoch: 7 step: 57, loss is 0.0017194265965372324\n",
      "epoch: 7 step: 58, loss is 0.0009199132909998298\n",
      "epoch: 7 step: 59, loss is 0.05174559727311134\n",
      "epoch: 7 step: 60, loss is 0.001506479107774794\n",
      "epoch: 7 step: 61, loss is 0.006569637451320887\n",
      "epoch: 7 step: 62, loss is 0.03270583972334862\n",
      "epoch: 7 step: 63, loss is 0.0019791768863797188\n",
      "epoch: 7 step: 64, loss is 0.00045415633940137923\n",
      "epoch: 7 step: 65, loss is 0.025038111954927444\n",
      "epoch: 7 step: 66, loss is 0.004359359387308359\n",
      "epoch: 7 step: 67, loss is 0.0098947212100029\n",
      "epoch: 7 step: 68, loss is 0.000490937614813447\n",
      "epoch: 7 step: 69, loss is 0.0016368876677006483\n",
      "epoch: 7 step: 70, loss is 0.0003628243284765631\n",
      "epoch: 7 step: 71, loss is 0.017314214259386063\n",
      "epoch: 7 step: 72, loss is 0.004280645865947008\n",
      "epoch: 7 step: 73, loss is 0.0037111968267709017\n",
      "epoch: 7 step: 74, loss is 0.01572023145854473\n",
      "epoch: 7 step: 75, loss is 0.0003730505413841456\n",
      "epoch: 7 step: 76, loss is 0.011637195013463497\n",
      "epoch: 7 step: 77, loss is 0.0006015513208694756\n",
      "epoch: 7 step: 78, loss is 0.0036031499039381742\n",
      "epoch: 7 step: 79, loss is 0.0002072616043733433\n",
      "epoch: 7 step: 80, loss is 0.0006103086052462459\n",
      "epoch: 7 step: 81, loss is 0.00018164896755479276\n",
      "epoch: 7 step: 82, loss is 0.0007138638757169247\n",
      "epoch: 7 step: 83, loss is 0.008828628808259964\n",
      "epoch: 7 step: 84, loss is 0.0006309504387900233\n",
      "epoch: 7 step: 85, loss is 0.050963394343853\n",
      "epoch: 7 step: 86, loss is 0.001211912021972239\n",
      "epoch: 7 step: 87, loss is 0.0001485696848249063\n",
      "epoch: 7 step: 88, loss is 0.007153618149459362\n",
      "epoch: 7 step: 89, loss is 0.012362365610897541\n",
      "epoch: 7 step: 90, loss is 0.005904656834900379\n",
      "epoch: 7 step: 91, loss is 0.001119447173550725\n",
      "epoch: 7 step: 92, loss is 0.0029480892699211836\n",
      "epoch: 7 step: 93, loss is 0.004950832575559616\n",
      "epoch: 7 step: 94, loss is 0.0005713356658816338\n",
      "epoch: 7 step: 95, loss is 6.766113801859319e-05\n",
      "epoch: 7 step: 96, loss is 0.07734690606594086\n",
      "epoch: 7 step: 97, loss is 0.0012105860514566302\n",
      "epoch: 7 step: 98, loss is 0.002185761695727706\n",
      "epoch: 7 step: 99, loss is 0.016496434807777405\n",
      "epoch: 7 step: 100, loss is 0.04391130059957504\n",
      "epoch: 7 step: 101, loss is 0.0057595581747591496\n",
      "epoch: 7 step: 102, loss is 0.10512856394052505\n",
      "epoch: 7 step: 103, loss is 4.259937850292772e-05\n",
      "epoch: 7 step: 104, loss is 0.00012603179493453354\n",
      "epoch: 7 step: 105, loss is 0.00020778816542588174\n",
      "epoch: 7 step: 106, loss is 0.006954893935471773\n",
      "epoch: 7 step: 107, loss is 0.007958429865539074\n",
      "epoch: 7 step: 108, loss is 0.018712637946009636\n",
      "epoch: 7 step: 109, loss is 0.008276714943349361\n",
      "epoch: 7 step: 110, loss is 0.08015671372413635\n",
      "epoch: 7 step: 111, loss is 0.00030041325953789055\n",
      "epoch: 7 step: 112, loss is 0.0003299871750641614\n",
      "epoch: 7 step: 113, loss is 0.0073715741746127605\n",
      "epoch: 7 step: 114, loss is 0.01023364532738924\n",
      "epoch: 7 step: 115, loss is 0.0001782437029760331\n",
      "epoch: 7 step: 116, loss is 0.19562534987926483\n",
      "epoch: 7 step: 117, loss is 0.0011165649630129337\n",
      "epoch: 7 step: 118, loss is 0.028554532676935196\n",
      "epoch: 7 step: 119, loss is 0.01457587257027626\n",
      "epoch: 7 step: 120, loss is 0.00031702444539405406\n",
      "epoch: 7 step: 121, loss is 2.34518065553857e-05\n",
      "epoch: 7 step: 122, loss is 0.0015471335500478745\n",
      "epoch: 7 step: 123, loss is 0.030709682032465935\n",
      "epoch: 7 step: 124, loss is 4.7498215280938894e-05\n",
      "epoch: 7 step: 125, loss is 0.00628642039373517\n",
      "epoch: 7 step: 126, loss is 0.0024337577633559704\n",
      "epoch: 7 step: 127, loss is 0.019878800958395004\n",
      "epoch: 7 step: 128, loss is 0.0004565600829664618\n",
      "epoch: 7 step: 129, loss is 0.00029851729050278664\n",
      "epoch: 7 step: 130, loss is 0.013247095048427582\n",
      "epoch: 7 step: 131, loss is 0.049078285694122314\n",
      "epoch: 7 step: 132, loss is 0.040192510932683945\n",
      "epoch: 7 step: 133, loss is 0.004974226001650095\n",
      "epoch: 7 step: 134, loss is 0.00021977120195515454\n",
      "epoch: 7 step: 135, loss is 0.0004883043002337217\n",
      "epoch: 7 step: 136, loss is 0.012109014205634594\n",
      "epoch: 7 step: 137, loss is 0.020009450614452362\n",
      "epoch: 7 step: 138, loss is 0.003390099387615919\n",
      "epoch: 7 step: 139, loss is 0.00034325511660426855\n",
      "epoch: 7 step: 140, loss is 0.02674560435116291\n",
      "epoch: 7 step: 141, loss is 4.295967664802447e-05\n",
      "epoch: 7 step: 142, loss is 0.00749422051012516\n",
      "epoch: 7 step: 143, loss is 0.011795911937952042\n",
      "epoch: 7 step: 144, loss is 0.00014285409997683018\n",
      "epoch: 7 step: 145, loss is 0.017240429297089577\n",
      "epoch: 7 step: 146, loss is 0.0017605186440050602\n",
      "epoch: 7 step: 147, loss is 0.003295706817880273\n",
      "epoch: 7 step: 148, loss is 0.04503330588340759\n",
      "epoch: 7 step: 149, loss is 0.0013591893948614597\n",
      "epoch: 7 step: 150, loss is 0.0003258184005971998\n",
      "epoch: 7 step: 151, loss is 0.0020803953520953655\n",
      "epoch: 7 step: 152, loss is 0.018823817372322083\n",
      "epoch: 7 step: 153, loss is 0.005963293835520744\n",
      "epoch: 7 step: 154, loss is 0.0021409792825579643\n",
      "epoch: 7 step: 155, loss is 0.007092995103448629\n",
      "epoch: 7 step: 156, loss is 0.00022452379926107824\n",
      "epoch: 7 step: 157, loss is 0.0006141277845017612\n",
      "epoch: 7 step: 158, loss is 0.05842462554574013\n",
      "epoch: 7 step: 159, loss is 0.005921126808971167\n",
      "epoch: 7 step: 160, loss is 0.2741214632987976\n",
      "epoch: 7 step: 161, loss is 0.00014163223386276513\n",
      "epoch: 7 step: 162, loss is 0.02479531615972519\n",
      "epoch: 7 step: 163, loss is 0.005741130094975233\n",
      "epoch: 7 step: 164, loss is 0.0008447347208857536\n",
      "epoch: 7 step: 165, loss is 0.006425465922802687\n",
      "epoch: 7 step: 166, loss is 0.00014568568440154195\n",
      "epoch: 7 step: 167, loss is 0.00040589901618659496\n",
      "epoch: 7 step: 168, loss is 0.00015944705228321254\n",
      "epoch: 7 step: 169, loss is 0.0001159629100584425\n",
      "epoch: 7 step: 170, loss is 0.0003395846579223871\n",
      "epoch: 7 step: 171, loss is 0.0005155689432285726\n",
      "epoch: 7 step: 172, loss is 0.0013189674355089664\n",
      "epoch: 7 step: 173, loss is 0.02771216630935669\n",
      "epoch: 7 step: 174, loss is 0.006158754695206881\n",
      "epoch: 7 step: 175, loss is 0.003068111836910248\n",
      "epoch: 7 step: 176, loss is 0.007454257924109697\n",
      "epoch: 7 step: 177, loss is 0.0008771372376941144\n",
      "epoch: 7 step: 178, loss is 0.0008648245711810887\n",
      "epoch: 7 step: 179, loss is 0.019747719168663025\n",
      "epoch: 7 step: 180, loss is 0.0007305911276489496\n",
      "epoch: 7 step: 181, loss is 0.002177846385166049\n",
      "epoch: 7 step: 182, loss is 0.00421528285369277\n",
      "epoch: 7 step: 183, loss is 0.00011942724813707173\n",
      "epoch: 7 step: 184, loss is 0.08878438174724579\n",
      "epoch: 7 step: 185, loss is 0.005979734938591719\n",
      "epoch: 7 step: 186, loss is 0.020092321559786797\n",
      "epoch: 7 step: 187, loss is 0.000761013594456017\n",
      "epoch: 7 step: 188, loss is 0.00361691159196198\n",
      "epoch: 7 step: 189, loss is 0.005204215180128813\n",
      "epoch: 7 step: 190, loss is 2.6349167455919087e-05\n",
      "epoch: 7 step: 191, loss is 0.011416882276535034\n",
      "epoch: 7 step: 192, loss is 0.0015752993058413267\n",
      "epoch: 7 step: 193, loss is 1.2508337022154592e-05\n",
      "epoch: 7 step: 194, loss is 0.0006124136853031814\n",
      "epoch: 7 step: 195, loss is 0.00025698868557810783\n",
      "epoch: 7 step: 196, loss is 0.0019336763070896268\n",
      "epoch: 7 step: 197, loss is 0.11657359451055527\n",
      "epoch: 7 step: 198, loss is 0.004838995169848204\n",
      "epoch: 7 step: 199, loss is 5.706921365344897e-05\n",
      "epoch: 7 step: 200, loss is 0.001125610782764852\n",
      "epoch: 7 step: 201, loss is 0.01791447587311268\n",
      "epoch: 7 step: 202, loss is 0.0001949627185240388\n",
      "epoch: 7 step: 203, loss is 0.00010675792145775631\n",
      "epoch: 7 step: 204, loss is 7.908695260994136e-05\n",
      "epoch: 7 step: 205, loss is 0.0019673146307468414\n",
      "epoch: 7 step: 206, loss is 0.00050928391283378\n",
      "epoch: 7 step: 207, loss is 0.0035073296166956425\n",
      "epoch: 7 step: 208, loss is 0.00042666701483540237\n",
      "epoch: 7 step: 209, loss is 0.017935721203684807\n",
      "epoch: 7 step: 210, loss is 0.00011277854355284944\n",
      "epoch: 7 step: 211, loss is 0.0008507099701091647\n",
      "epoch: 7 step: 212, loss is 0.002575628226622939\n",
      "epoch: 7 step: 213, loss is 0.0009391062194481492\n",
      "epoch: 7 step: 214, loss is 0.0029930011369287968\n",
      "epoch: 7 step: 215, loss is 0.0006875901017338037\n",
      "epoch: 7 step: 216, loss is 0.08372636884450912\n",
      "epoch: 7 step: 217, loss is 0.00022900456679053605\n",
      "epoch: 7 step: 218, loss is 0.0010009304387494922\n",
      "epoch: 7 step: 219, loss is 0.000971077592112124\n",
      "epoch: 7 step: 220, loss is 6.620636122534052e-05\n",
      "epoch: 7 step: 221, loss is 0.006125756539404392\n",
      "epoch: 7 step: 222, loss is 0.0002605436893645674\n",
      "epoch: 7 step: 223, loss is 0.008972110226750374\n",
      "epoch: 7 step: 224, loss is 0.0003545978106558323\n",
      "epoch: 7 step: 225, loss is 0.0002948532928712666\n",
      "epoch: 7 step: 226, loss is 0.005924084689468145\n",
      "epoch: 7 step: 227, loss is 8.549718768335879e-05\n",
      "epoch: 7 step: 228, loss is 0.004156327340751886\n",
      "epoch: 7 step: 229, loss is 0.003969977144151926\n",
      "epoch: 7 step: 230, loss is 0.04187732934951782\n",
      "epoch: 7 step: 231, loss is 0.001152018434368074\n",
      "epoch: 7 step: 232, loss is 0.001653944025747478\n",
      "epoch: 7 step: 233, loss is 0.00018422884750179946\n",
      "epoch: 7 step: 234, loss is 0.003420684952288866\n",
      "epoch: 7 step: 235, loss is 0.000503914023283869\n",
      "epoch: 7 step: 236, loss is 5.123124356032349e-05\n",
      "epoch: 7 step: 237, loss is 0.003711543744429946\n",
      "epoch: 7 step: 238, loss is 0.006236093584448099\n",
      "epoch: 7 step: 239, loss is 0.00549831660464406\n",
      "epoch: 7 step: 240, loss is 0.00027595035498961806\n",
      "epoch: 7 step: 241, loss is 0.0002864687703549862\n",
      "epoch: 7 step: 242, loss is 0.013737727887928486\n",
      "epoch: 7 step: 243, loss is 0.00031110781128518283\n",
      "epoch: 7 step: 244, loss is 0.0035898168571293354\n",
      "epoch: 7 step: 245, loss is 0.0004695203679148108\n",
      "epoch: 7 step: 246, loss is 0.0061504896730184555\n",
      "epoch: 7 step: 247, loss is 7.840539910830557e-05\n",
      "epoch: 7 step: 248, loss is 0.05668390914797783\n",
      "epoch: 7 step: 249, loss is 0.00011461970279924572\n",
      "epoch: 7 step: 250, loss is 0.0001055771645042114\n",
      "epoch: 7 step: 251, loss is 0.005452924873679876\n",
      "epoch: 7 step: 252, loss is 3.5667770134750754e-05\n",
      "epoch: 7 step: 253, loss is 0.0007756329723633826\n",
      "epoch: 7 step: 254, loss is 0.0008860485977493227\n",
      "epoch: 7 step: 255, loss is 0.0010933342855423689\n",
      "epoch: 7 step: 256, loss is 1.3699836927116849e-05\n",
      "epoch: 7 step: 257, loss is 0.0036710496060550213\n",
      "epoch: 7 step: 258, loss is 0.0016919173067435622\n",
      "epoch: 7 step: 259, loss is 0.00010905430826824158\n",
      "epoch: 7 step: 260, loss is 0.043294232338666916\n",
      "epoch: 7 step: 261, loss is 0.00011699521564878523\n",
      "epoch: 7 step: 262, loss is 0.01433008722960949\n",
      "epoch: 7 step: 263, loss is 0.000776061147917062\n",
      "epoch: 7 step: 264, loss is 0.09865166991949081\n",
      "epoch: 7 step: 265, loss is 0.0048456210643053055\n",
      "epoch: 7 step: 266, loss is 0.0017786009702831507\n",
      "epoch: 7 step: 267, loss is 0.0005951881757937372\n",
      "epoch: 7 step: 268, loss is 0.10595133900642395\n",
      "epoch: 7 step: 269, loss is 0.0008644168265163898\n",
      "epoch: 7 step: 270, loss is 0.15967680513858795\n",
      "epoch: 7 step: 271, loss is 2.7764377591665834e-05\n",
      "epoch: 7 step: 272, loss is 0.00098058779258281\n",
      "epoch: 7 step: 273, loss is 0.00011885374260600656\n",
      "epoch: 7 step: 274, loss is 0.0002384675754001364\n",
      "epoch: 7 step: 275, loss is 0.021730899810791016\n",
      "epoch: 7 step: 276, loss is 0.00178583525121212\n",
      "epoch: 7 step: 277, loss is 0.0023979833349585533\n",
      "epoch: 7 step: 278, loss is 0.19558638334274292\n",
      "epoch: 7 step: 279, loss is 0.001347975805401802\n",
      "epoch: 7 step: 280, loss is 0.0013370377710089087\n",
      "epoch: 7 step: 281, loss is 0.03721429407596588\n",
      "epoch: 7 step: 282, loss is 0.010269525460898876\n",
      "epoch: 7 step: 283, loss is 0.15977825224399567\n",
      "epoch: 7 step: 284, loss is 0.07451159507036209\n",
      "epoch: 7 step: 285, loss is 0.001893069944344461\n",
      "epoch: 7 step: 286, loss is 0.0006134325521998107\n",
      "epoch: 7 step: 287, loss is 0.04484838247299194\n",
      "epoch: 7 step: 288, loss is 0.006924726068973541\n",
      "epoch: 7 step: 289, loss is 0.01742914505302906\n",
      "epoch: 7 step: 290, loss is 0.00020563381258398294\n",
      "epoch: 7 step: 291, loss is 0.009792917408049107\n",
      "epoch: 7 step: 292, loss is 0.05914201959967613\n",
      "epoch: 7 step: 293, loss is 0.00017348106484860182\n",
      "epoch: 7 step: 294, loss is 0.007591628469526768\n",
      "epoch: 7 step: 295, loss is 0.08544525504112244\n",
      "epoch: 7 step: 296, loss is 0.007086361758410931\n",
      "epoch: 7 step: 297, loss is 0.053950902074575424\n",
      "epoch: 7 step: 298, loss is 0.00035809134715236723\n",
      "epoch: 7 step: 299, loss is 0.027722949162125587\n",
      "epoch: 7 step: 300, loss is 0.03230606019496918\n",
      "epoch: 7 step: 301, loss is 0.0018551986431702971\n",
      "epoch: 7 step: 302, loss is 0.0934511125087738\n",
      "epoch: 7 step: 303, loss is 0.0023314659483730793\n",
      "epoch: 7 step: 304, loss is 0.028945544734597206\n",
      "epoch: 7 step: 305, loss is 0.008336772210896015\n",
      "epoch: 7 step: 306, loss is 0.0016463977517560124\n",
      "epoch: 7 step: 307, loss is 0.10452069342136383\n",
      "epoch: 7 step: 308, loss is 0.05015288293361664\n",
      "epoch: 7 step: 309, loss is 0.0059005022048950195\n",
      "epoch: 7 step: 310, loss is 0.003373167710378766\n",
      "epoch: 7 step: 311, loss is 0.008657767437398434\n",
      "epoch: 7 step: 312, loss is 0.00031675840727984905\n",
      "epoch: 7 step: 313, loss is 0.00027833873173221946\n",
      "epoch: 7 step: 314, loss is 0.0007497853366658092\n",
      "epoch: 7 step: 315, loss is 0.005636984948068857\n",
      "epoch: 7 step: 316, loss is 0.0013753219973295927\n",
      "epoch: 7 step: 317, loss is 0.02476426586508751\n",
      "epoch: 7 step: 318, loss is 0.0014414219185709953\n",
      "epoch: 7 step: 319, loss is 0.003104348434135318\n",
      "epoch: 7 step: 320, loss is 0.001123013673350215\n",
      "epoch: 7 step: 321, loss is 0.06708535552024841\n",
      "epoch: 7 step: 322, loss is 0.0008383166859857738\n",
      "epoch: 7 step: 323, loss is 0.0014901828253641725\n",
      "epoch: 7 step: 324, loss is 0.005617559887468815\n",
      "epoch: 7 step: 325, loss is 0.0013915251474827528\n",
      "epoch: 7 step: 326, loss is 0.013103283010423183\n",
      "epoch: 7 step: 327, loss is 0.004142757039517164\n",
      "epoch: 7 step: 328, loss is 0.14260734617710114\n",
      "epoch: 7 step: 329, loss is 0.032874323427677155\n",
      "epoch: 7 step: 330, loss is 0.00394098274409771\n",
      "epoch: 7 step: 331, loss is 3.360356640769169e-05\n",
      "epoch: 7 step: 332, loss is 0.0009306669817306101\n",
      "epoch: 7 step: 333, loss is 0.001961529953405261\n",
      "epoch: 7 step: 334, loss is 0.0023379919584840536\n",
      "epoch: 7 step: 335, loss is 0.006051213480532169\n",
      "epoch: 7 step: 336, loss is 0.05996337905526161\n",
      "epoch: 7 step: 337, loss is 3.127963645965792e-05\n",
      "epoch: 7 step: 338, loss is 0.0059706601314246655\n",
      "epoch: 7 step: 339, loss is 0.0054775686003267765\n",
      "epoch: 7 step: 340, loss is 0.0006331565673463047\n",
      "epoch: 7 step: 341, loss is 0.02336507849395275\n",
      "epoch: 7 step: 342, loss is 0.000458872556919232\n",
      "epoch: 7 step: 343, loss is 0.00019027042435482144\n",
      "epoch: 7 step: 344, loss is 0.0024613013956695795\n",
      "epoch: 7 step: 345, loss is 0.002554666716605425\n",
      "epoch: 7 step: 346, loss is 0.0030827794689685106\n",
      "epoch: 7 step: 347, loss is 0.00878873746842146\n",
      "epoch: 7 step: 348, loss is 0.02455703541636467\n",
      "epoch: 7 step: 349, loss is 0.04020293429493904\n",
      "epoch: 7 step: 350, loss is 0.00013662483252119273\n",
      "epoch: 7 step: 351, loss is 0.0010287669720128179\n",
      "epoch: 7 step: 352, loss is 0.001624105148948729\n",
      "epoch: 7 step: 353, loss is 0.000545261602383107\n",
      "epoch: 7 step: 354, loss is 0.010059339925646782\n",
      "epoch: 7 step: 355, loss is 0.003549820277839899\n",
      "epoch: 7 step: 356, loss is 0.03753234073519707\n",
      "epoch: 7 step: 357, loss is 0.0447937473654747\n",
      "epoch: 7 step: 358, loss is 0.00014594418462365866\n",
      "epoch: 7 step: 359, loss is 0.00012646573304664344\n",
      "epoch: 7 step: 360, loss is 0.0001986333663808182\n",
      "epoch: 7 step: 361, loss is 0.11356277763843536\n",
      "epoch: 7 step: 362, loss is 0.012773081660270691\n",
      "epoch: 7 step: 363, loss is 0.0025827728677541018\n",
      "epoch: 7 step: 364, loss is 0.0008392492891289294\n",
      "epoch: 7 step: 365, loss is 0.015613614581525326\n",
      "epoch: 7 step: 366, loss is 0.006080822087824345\n",
      "epoch: 7 step: 367, loss is 0.00014907568402122706\n",
      "epoch: 7 step: 368, loss is 1.3728288649872411e-05\n",
      "epoch: 7 step: 369, loss is 0.009089116007089615\n",
      "epoch: 7 step: 370, loss is 0.1352550983428955\n",
      "epoch: 7 step: 371, loss is 1.3276434401632287e-05\n",
      "epoch: 7 step: 372, loss is 0.055065374821424484\n",
      "epoch: 7 step: 373, loss is 0.0008446690044365823\n",
      "epoch: 7 step: 374, loss is 0.06143234297633171\n",
      "epoch: 7 step: 375, loss is 0.004037493374198675\n",
      "epoch: 7 step: 376, loss is 0.013068352825939655\n",
      "epoch: 7 step: 377, loss is 5.729774784413166e-06\n",
      "epoch: 7 step: 378, loss is 0.000455905101262033\n",
      "epoch: 7 step: 379, loss is 0.0023639188148081303\n",
      "epoch: 7 step: 380, loss is 2.2264322979026474e-05\n",
      "epoch: 7 step: 381, loss is 1.687988333287649e-05\n",
      "epoch: 7 step: 382, loss is 0.004192758351564407\n",
      "epoch: 7 step: 383, loss is 0.0012489273212850094\n",
      "epoch: 7 step: 384, loss is 0.004216588567942381\n",
      "epoch: 7 step: 385, loss is 0.047159891575574875\n",
      "epoch: 7 step: 386, loss is 0.20174741744995117\n",
      "epoch: 7 step: 387, loss is 0.15115603804588318\n",
      "epoch: 7 step: 388, loss is 0.017546020448207855\n",
      "epoch: 7 step: 389, loss is 9.315784154750872e-06\n",
      "epoch: 7 step: 390, loss is 0.0035039528738707304\n",
      "epoch: 7 step: 391, loss is 0.02816072292625904\n",
      "epoch: 7 step: 392, loss is 0.00010874651343328878\n",
      "epoch: 7 step: 393, loss is 0.028221456333994865\n",
      "epoch: 7 step: 394, loss is 0.052417874336242676\n",
      "epoch: 7 step: 395, loss is 0.0012029347708448768\n",
      "epoch: 7 step: 396, loss is 0.17207647860050201\n",
      "epoch: 7 step: 397, loss is 0.016658766195178032\n",
      "epoch: 7 step: 398, loss is 0.009391524828970432\n",
      "epoch: 7 step: 399, loss is 0.004397896584123373\n",
      "epoch: 7 step: 400, loss is 0.005663744173943996\n",
      "epoch: 7 step: 401, loss is 0.001890492276288569\n",
      "epoch: 7 step: 402, loss is 0.019160397350788116\n",
      "epoch: 7 step: 403, loss is 0.0027194067370146513\n",
      "epoch: 7 step: 404, loss is 0.00046934265992604196\n",
      "epoch: 7 step: 405, loss is 0.03123151697218418\n",
      "epoch: 7 step: 406, loss is 0.032747913151979446\n",
      "epoch: 7 step: 407, loss is 0.014330987818539143\n",
      "epoch: 7 step: 408, loss is 0.001530674286186695\n",
      "epoch: 7 step: 409, loss is 0.09187080711126328\n",
      "epoch: 7 step: 410, loss is 0.06429778039455414\n",
      "epoch: 7 step: 411, loss is 0.00837258156388998\n",
      "epoch: 7 step: 412, loss is 0.029576396569609642\n",
      "epoch: 7 step: 413, loss is 0.029873130843043327\n",
      "epoch: 7 step: 414, loss is 0.001768653397448361\n",
      "epoch: 7 step: 415, loss is 0.0008732000133022666\n",
      "epoch: 7 step: 416, loss is 0.0012603249633684754\n",
      "epoch: 7 step: 417, loss is 0.0040260436944663525\n",
      "epoch: 7 step: 418, loss is 0.07033027708530426\n",
      "epoch: 7 step: 419, loss is 0.11071475595235825\n",
      "epoch: 7 step: 420, loss is 0.01102351862937212\n",
      "epoch: 7 step: 421, loss is 7.563180406577885e-05\n",
      "epoch: 7 step: 422, loss is 0.011959037743508816\n",
      "epoch: 7 step: 423, loss is 8.681401232024655e-05\n",
      "epoch: 7 step: 424, loss is 0.001437711762264371\n",
      "epoch: 7 step: 425, loss is 0.1850729137659073\n",
      "epoch: 7 step: 426, loss is 0.0018978298176079988\n",
      "epoch: 7 step: 427, loss is 0.008866103366017342\n",
      "epoch: 7 step: 428, loss is 0.0026510925963521004\n",
      "epoch: 7 step: 429, loss is 0.001679967506788671\n",
      "epoch: 7 step: 430, loss is 0.005446536000818014\n",
      "epoch: 7 step: 431, loss is 0.08877391368150711\n",
      "epoch: 7 step: 432, loss is 0.0004129019216634333\n",
      "epoch: 7 step: 433, loss is 0.0016182385152205825\n",
      "epoch: 7 step: 434, loss is 0.0020113226491957903\n",
      "epoch: 7 step: 435, loss is 0.000803723931312561\n",
      "epoch: 7 step: 436, loss is 5.325684469426051e-05\n",
      "epoch: 7 step: 437, loss is 0.005841628648340702\n",
      "epoch: 7 step: 438, loss is 0.029229408130049706\n",
      "epoch: 7 step: 439, loss is 0.03332939371466637\n",
      "epoch: 7 step: 440, loss is 0.004885252099484205\n",
      "epoch: 7 step: 441, loss is 1.905648605315946e-05\n",
      "epoch: 7 step: 442, loss is 0.0034452734980732203\n",
      "epoch: 7 step: 443, loss is 0.04031965136528015\n",
      "epoch: 7 step: 444, loss is 0.011394960805773735\n",
      "epoch: 7 step: 445, loss is 0.00137958989944309\n",
      "epoch: 7 step: 446, loss is 0.00015027711924631149\n",
      "epoch: 7 step: 447, loss is 0.0021746198181062937\n",
      "epoch: 7 step: 448, loss is 0.0022374605759978294\n",
      "epoch: 7 step: 449, loss is 0.0002830802113749087\n",
      "epoch: 7 step: 450, loss is 0.003689700970426202\n",
      "epoch: 7 step: 451, loss is 0.00439482880756259\n",
      "epoch: 7 step: 452, loss is 0.014425713568925858\n",
      "epoch: 7 step: 453, loss is 0.003443444613367319\n",
      "epoch: 7 step: 454, loss is 0.008603391237556934\n",
      "epoch: 7 step: 455, loss is 0.0006448288913816214\n",
      "epoch: 7 step: 456, loss is 0.0001047038531396538\n",
      "epoch: 7 step: 457, loss is 0.049500271677970886\n",
      "epoch: 7 step: 458, loss is 0.0034555047750473022\n",
      "epoch: 7 step: 459, loss is 0.00692240335047245\n",
      "epoch: 7 step: 460, loss is 0.019122852012515068\n",
      "epoch: 7 step: 461, loss is 0.00026933709159493446\n",
      "epoch: 7 step: 462, loss is 0.00032378092873841524\n",
      "epoch: 7 step: 463, loss is 0.0011421921662986279\n",
      "epoch: 7 step: 464, loss is 0.0031672553159296513\n",
      "epoch: 7 step: 465, loss is 0.014220149256289005\n",
      "epoch: 7 step: 466, loss is 0.000131043532746844\n",
      "epoch: 7 step: 467, loss is 0.0015730398008599877\n",
      "epoch: 7 step: 468, loss is 0.00039132751408033073\n",
      "epoch: 7 step: 469, loss is 0.0006562580820173025\n",
      "epoch: 7 step: 470, loss is 0.03653336316347122\n",
      "epoch: 7 step: 471, loss is 0.003078134497627616\n",
      "epoch: 7 step: 472, loss is 0.002786509692668915\n",
      "epoch: 7 step: 473, loss is 0.0033120301086455584\n",
      "epoch: 7 step: 474, loss is 0.0003657328197732568\n",
      "epoch: 7 step: 475, loss is 0.0001342837349511683\n",
      "epoch: 7 step: 476, loss is 0.0003901015443261713\n",
      "epoch: 7 step: 477, loss is 0.00010172815382247791\n",
      "epoch: 7 step: 478, loss is 0.000499113870318979\n",
      "epoch: 7 step: 479, loss is 0.01614193804562092\n",
      "epoch: 7 step: 480, loss is 0.038027066737413406\n",
      "epoch: 7 step: 481, loss is 0.00020167705952189863\n",
      "epoch: 7 step: 482, loss is 0.004305826500058174\n",
      "epoch: 7 step: 483, loss is 1.2722496649075765e-05\n",
      "epoch: 7 step: 484, loss is 0.2131967842578888\n",
      "epoch: 7 step: 485, loss is 7.235688099171966e-05\n",
      "epoch: 7 step: 486, loss is 0.022671200335025787\n",
      "epoch: 7 step: 487, loss is 0.0006704161060042679\n",
      "epoch: 7 step: 488, loss is 0.015257815830409527\n",
      "epoch: 7 step: 489, loss is 7.095854380168021e-05\n",
      "epoch: 7 step: 490, loss is 0.004692988935858011\n",
      "epoch: 7 step: 491, loss is 0.02025137096643448\n",
      "epoch: 7 step: 492, loss is 0.016723332926630974\n",
      "epoch: 7 step: 493, loss is 0.0008657650905661285\n",
      "epoch: 7 step: 494, loss is 0.01411493681371212\n",
      "epoch: 7 step: 495, loss is 7.944663229864091e-05\n",
      "epoch: 7 step: 496, loss is 0.01114855520427227\n",
      "epoch: 7 step: 497, loss is 0.0005220759776420891\n",
      "epoch: 7 step: 498, loss is 3.717227082233876e-05\n",
      "epoch: 7 step: 499, loss is 0.02782735787332058\n",
      "epoch: 7 step: 500, loss is 0.0022377693094313145\n",
      "epoch: 7 step: 501, loss is 0.00023966313165146858\n",
      "epoch: 7 step: 502, loss is 0.011845756322145462\n",
      "epoch: 7 step: 503, loss is 0.0001107237912947312\n",
      "epoch: 7 step: 504, loss is 0.00461897486820817\n",
      "epoch: 7 step: 505, loss is 0.0023820768110454082\n",
      "epoch: 7 step: 506, loss is 0.031060729175806046\n",
      "epoch: 7 step: 507, loss is 0.00031896104337647557\n",
      "epoch: 7 step: 508, loss is 0.021601488813757896\n",
      "epoch: 7 step: 509, loss is 0.0022809982765465975\n",
      "epoch: 7 step: 510, loss is 0.008939748629927635\n",
      "epoch: 7 step: 511, loss is 0.044542185962200165\n",
      "epoch: 7 step: 512, loss is 0.00395531952381134\n",
      "epoch: 7 step: 513, loss is 0.001732855336740613\n",
      "epoch: 7 step: 514, loss is 0.00590326776728034\n",
      "epoch: 7 step: 515, loss is 0.012439597398042679\n",
      "epoch: 7 step: 516, loss is 0.0016179161611944437\n",
      "epoch: 7 step: 517, loss is 0.029155999422073364\n",
      "epoch: 7 step: 518, loss is 0.0005021634860895574\n",
      "epoch: 7 step: 519, loss is 0.001965045928955078\n",
      "epoch: 7 step: 520, loss is 3.342419950058684e-05\n",
      "epoch: 7 step: 521, loss is 0.006014416925609112\n",
      "epoch: 7 step: 522, loss is 0.001717913430184126\n",
      "epoch: 7 step: 523, loss is 0.042691782116889954\n",
      "epoch: 7 step: 524, loss is 0.0003485928173176944\n",
      "epoch: 7 step: 525, loss is 0.003837652737274766\n",
      "epoch: 7 step: 526, loss is 0.003544705221429467\n",
      "epoch: 7 step: 527, loss is 0.00014123989967629313\n",
      "epoch: 7 step: 528, loss is 0.00024917576229199767\n",
      "epoch: 7 step: 529, loss is 0.006781259085983038\n",
      "epoch: 7 step: 530, loss is 0.006932123564183712\n",
      "epoch: 7 step: 531, loss is 0.0012347662122920156\n",
      "epoch: 7 step: 532, loss is 0.00023929275630507618\n",
      "epoch: 7 step: 533, loss is 0.0026729993987828493\n",
      "epoch: 7 step: 534, loss is 0.28746187686920166\n",
      "epoch: 7 step: 535, loss is 6.919229053892195e-05\n",
      "epoch: 7 step: 536, loss is 0.07689852267503738\n",
      "epoch: 7 step: 537, loss is 0.009175648912787437\n",
      "epoch: 7 step: 538, loss is 0.00034609579597599804\n",
      "epoch: 7 step: 539, loss is 0.0010140594094991684\n",
      "epoch: 7 step: 540, loss is 0.027773091569542885\n",
      "epoch: 7 step: 541, loss is 0.00039347639540210366\n",
      "epoch: 7 step: 542, loss is 0.014659298583865166\n",
      "epoch: 7 step: 543, loss is 0.03162012994289398\n",
      "epoch: 7 step: 544, loss is 0.025430599227547646\n",
      "epoch: 7 step: 545, loss is 0.004185569006949663\n",
      "epoch: 7 step: 546, loss is 0.002316439989954233\n",
      "epoch: 7 step: 547, loss is 0.0004438504984136671\n",
      "epoch: 7 step: 548, loss is 0.001774885575287044\n",
      "epoch: 7 step: 549, loss is 0.06506460905075073\n",
      "epoch: 7 step: 550, loss is 0.0075875213369727135\n",
      "epoch: 7 step: 551, loss is 0.0017267916118726134\n",
      "epoch: 7 step: 552, loss is 0.01296928245574236\n",
      "epoch: 7 step: 553, loss is 0.015270971693098545\n",
      "epoch: 7 step: 554, loss is 0.0026276439893990755\n",
      "epoch: 7 step: 555, loss is 0.00012369871546979994\n",
      "epoch: 7 step: 556, loss is 0.000591327203437686\n",
      "epoch: 7 step: 557, loss is 0.008072092197835445\n",
      "epoch: 7 step: 558, loss is 0.01580302231013775\n",
      "epoch: 7 step: 559, loss is 0.08581329882144928\n",
      "epoch: 7 step: 560, loss is 0.0056753153912723064\n",
      "epoch: 7 step: 561, loss is 0.15475748479366302\n",
      "epoch: 7 step: 562, loss is 0.0008163726306520402\n",
      "epoch: 7 step: 563, loss is 0.0004787414218299091\n",
      "epoch: 7 step: 564, loss is 0.0009368281462229788\n",
      "epoch: 7 step: 565, loss is 0.00010170370660489425\n",
      "epoch: 7 step: 566, loss is 0.0004675963136833161\n",
      "epoch: 7 step: 567, loss is 0.0542253814637661\n",
      "epoch: 7 step: 568, loss is 0.0059915934689342976\n",
      "epoch: 7 step: 569, loss is 0.008783482015132904\n",
      "epoch: 7 step: 570, loss is 6.176299211801961e-05\n",
      "epoch: 7 step: 571, loss is 0.08386802673339844\n",
      "epoch: 7 step: 572, loss is 0.0025442508049309254\n",
      "epoch: 7 step: 573, loss is 0.0002888112503569573\n",
      "epoch: 7 step: 574, loss is 0.0007655624649487436\n",
      "epoch: 7 step: 575, loss is 0.00010999327059835196\n",
      "epoch: 7 step: 576, loss is 4.096011252840981e-05\n",
      "epoch: 7 step: 577, loss is 0.008832421153783798\n",
      "epoch: 7 step: 578, loss is 0.004081385675817728\n",
      "epoch: 7 step: 579, loss is 0.0005647460930049419\n",
      "epoch: 7 step: 580, loss is 0.000324432912748307\n",
      "epoch: 7 step: 581, loss is 0.0008609591750428081\n",
      "epoch: 7 step: 582, loss is 0.16463874280452728\n",
      "epoch: 7 step: 583, loss is 0.01609605737030506\n",
      "epoch: 7 step: 584, loss is 0.024044344201683998\n",
      "epoch: 7 step: 585, loss is 0.17008279263973236\n",
      "epoch: 7 step: 586, loss is 0.00047527841525152326\n",
      "epoch: 7 step: 587, loss is 0.00032668773201294243\n",
      "epoch: 7 step: 588, loss is 0.008229491300880909\n",
      "epoch: 7 step: 589, loss is 0.06719010323286057\n",
      "epoch: 7 step: 590, loss is 0.012021362781524658\n",
      "epoch: 7 step: 591, loss is 0.0283155869692564\n",
      "epoch: 7 step: 592, loss is 0.00016346490883734077\n",
      "epoch: 7 step: 593, loss is 0.0007139384397305548\n",
      "epoch: 7 step: 594, loss is 0.0002507633762434125\n",
      "epoch: 7 step: 595, loss is 0.00046451916568912566\n",
      "epoch: 7 step: 596, loss is 0.08067638427019119\n",
      "epoch: 7 step: 597, loss is 0.0004519475915003568\n",
      "epoch: 7 step: 598, loss is 0.000277446408290416\n",
      "epoch: 7 step: 599, loss is 0.0012148178648203611\n",
      "epoch: 7 step: 600, loss is 0.0011666585924103856\n",
      "epoch: 7 step: 601, loss is 0.0010818263981491327\n",
      "epoch: 7 step: 602, loss is 0.001097687752917409\n",
      "epoch: 7 step: 603, loss is 0.004630214534699917\n",
      "epoch: 7 step: 604, loss is 0.0003849460044875741\n",
      "epoch: 7 step: 605, loss is 0.0012306644348427653\n",
      "epoch: 7 step: 606, loss is 2.423272599116899e-05\n",
      "epoch: 7 step: 607, loss is 1.1767109754146077e-05\n",
      "epoch: 7 step: 608, loss is 0.000247626390773803\n",
      "epoch: 7 step: 609, loss is 0.008389011956751347\n",
      "epoch: 7 step: 610, loss is 0.022338125854730606\n",
      "epoch: 7 step: 611, loss is 0.00013638046220876276\n",
      "epoch: 7 step: 612, loss is 0.0012654750607907772\n",
      "epoch: 7 step: 613, loss is 0.019122429192066193\n",
      "epoch: 7 step: 614, loss is 0.00144580181222409\n",
      "epoch: 7 step: 615, loss is 0.0009048486244864762\n",
      "epoch: 7 step: 616, loss is 0.016242213547229767\n",
      "epoch: 7 step: 617, loss is 0.026209190487861633\n",
      "epoch: 7 step: 618, loss is 0.009060821495950222\n",
      "epoch: 7 step: 619, loss is 0.2462279349565506\n",
      "epoch: 7 step: 620, loss is 0.12983325123786926\n",
      "epoch: 7 step: 621, loss is 0.10155509412288666\n",
      "epoch: 7 step: 622, loss is 0.002876811660826206\n",
      "epoch: 7 step: 623, loss is 0.0009287180146202445\n",
      "epoch: 7 step: 624, loss is 0.001564465113915503\n",
      "epoch: 7 step: 625, loss is 4.714785609394312e-05\n",
      "epoch: 7 step: 626, loss is 6.669622962363064e-05\n",
      "epoch: 7 step: 627, loss is 0.00014239006850402802\n",
      "epoch: 7 step: 628, loss is 0.06743975728750229\n",
      "epoch: 7 step: 629, loss is 0.016584699973464012\n",
      "epoch: 7 step: 630, loss is 0.02588232234120369\n",
      "epoch: 7 step: 631, loss is 0.10295173525810242\n",
      "epoch: 7 step: 632, loss is 0.0020395915489643812\n",
      "epoch: 7 step: 633, loss is 0.0021063382737338543\n",
      "epoch: 7 step: 634, loss is 0.001575691276229918\n",
      "epoch: 7 step: 635, loss is 0.0008113023941405118\n",
      "epoch: 7 step: 636, loss is 0.0001184551147161983\n",
      "epoch: 7 step: 637, loss is 0.29858630895614624\n",
      "epoch: 7 step: 638, loss is 0.008262591436505318\n",
      "epoch: 7 step: 639, loss is 0.001265556551516056\n",
      "epoch: 7 step: 640, loss is 0.0006875547696836293\n",
      "epoch: 7 step: 641, loss is 0.04484424740076065\n",
      "epoch: 7 step: 642, loss is 0.00019618876103777438\n",
      "epoch: 7 step: 643, loss is 0.028660975396633148\n",
      "epoch: 7 step: 644, loss is 0.006711076013743877\n",
      "epoch: 7 step: 645, loss is 0.007179176434874535\n",
      "epoch: 7 step: 646, loss is 0.016811620444059372\n",
      "epoch: 7 step: 647, loss is 0.00391273433342576\n",
      "epoch: 7 step: 648, loss is 0.2212168276309967\n",
      "epoch: 7 step: 649, loss is 0.05239638313651085\n",
      "epoch: 7 step: 650, loss is 0.00034872215474024415\n",
      "epoch: 7 step: 651, loss is 0.0008596163825131953\n",
      "epoch: 7 step: 652, loss is 0.04680320620536804\n",
      "epoch: 7 step: 653, loss is 0.0032531199976801872\n",
      "epoch: 7 step: 654, loss is 0.00016012364358175546\n",
      "epoch: 7 step: 655, loss is 0.05897177383303642\n",
      "epoch: 7 step: 656, loss is 0.0010215082438662648\n",
      "epoch: 7 step: 657, loss is 0.0010945502435788512\n",
      "epoch: 7 step: 658, loss is 0.0009459649445489049\n",
      "epoch: 7 step: 659, loss is 0.0006902878521941602\n",
      "epoch: 7 step: 660, loss is 0.023517630994319916\n",
      "epoch: 7 step: 661, loss is 0.06476177275180817\n",
      "epoch: 7 step: 662, loss is 0.002911926247179508\n",
      "epoch: 7 step: 663, loss is 0.0011929457541555166\n",
      "epoch: 7 step: 664, loss is 0.002723335986956954\n",
      "epoch: 7 step: 665, loss is 0.0190636795014143\n",
      "epoch: 7 step: 666, loss is 0.0015568241942673922\n",
      "epoch: 7 step: 667, loss is 0.002857003826647997\n",
      "epoch: 7 step: 668, loss is 0.00045417636283673346\n",
      "epoch: 7 step: 669, loss is 0.004388849716633558\n",
      "epoch: 7 step: 670, loss is 0.014969871379435062\n",
      "epoch: 7 step: 671, loss is 0.0010908679105341434\n",
      "epoch: 7 step: 672, loss is 0.03438332676887512\n",
      "epoch: 7 step: 673, loss is 0.056278422474861145\n",
      "epoch: 7 step: 674, loss is 0.0022866029758006334\n",
      "epoch: 7 step: 675, loss is 0.00022332821390591562\n",
      "epoch: 7 step: 676, loss is 0.026337096467614174\n",
      "epoch: 7 step: 677, loss is 0.0031739231199026108\n",
      "epoch: 7 step: 678, loss is 0.01116225402802229\n",
      "epoch: 7 step: 679, loss is 0.03341634199023247\n",
      "epoch: 7 step: 680, loss is 0.0012266593985259533\n",
      "epoch: 7 step: 681, loss is 0.04063282907009125\n",
      "epoch: 7 step: 682, loss is 0.005990064237266779\n",
      "epoch: 7 step: 683, loss is 0.0016064918600022793\n",
      "epoch: 7 step: 684, loss is 0.012757048942148685\n",
      "epoch: 7 step: 685, loss is 0.0938999354839325\n",
      "epoch: 7 step: 686, loss is 0.008066239766776562\n",
      "epoch: 7 step: 687, loss is 0.00010622654372127727\n",
      "epoch: 7 step: 688, loss is 0.0036928586196154356\n",
      "epoch: 7 step: 689, loss is 0.04260704666376114\n",
      "epoch: 7 step: 690, loss is 0.0021566867362707853\n",
      "epoch: 7 step: 691, loss is 0.002056826837360859\n",
      "epoch: 7 step: 692, loss is 0.009934800677001476\n",
      "epoch: 7 step: 693, loss is 0.011514863930642605\n",
      "epoch: 7 step: 694, loss is 0.002020512940362096\n",
      "epoch: 7 step: 695, loss is 0.017636513337492943\n",
      "epoch: 7 step: 696, loss is 0.0033174974378198385\n",
      "epoch: 7 step: 697, loss is 0.0013300017453730106\n",
      "epoch: 7 step: 698, loss is 0.0004477212205529213\n",
      "epoch: 7 step: 699, loss is 0.018460813909769058\n",
      "epoch: 7 step: 700, loss is 0.0003598587936721742\n",
      "epoch: 7 step: 701, loss is 0.0014923588605597615\n",
      "epoch: 7 step: 702, loss is 0.0005304868100211024\n",
      "epoch: 7 step: 703, loss is 0.15756066143512726\n",
      "epoch: 7 step: 704, loss is 0.00577280018478632\n",
      "epoch: 7 step: 705, loss is 0.036380235105752945\n",
      "epoch: 7 step: 706, loss is 4.2198222217848524e-05\n",
      "epoch: 7 step: 707, loss is 0.08552797138690948\n",
      "epoch: 7 step: 708, loss is 0.028547659516334534\n",
      "epoch: 7 step: 709, loss is 0.0016243058489635587\n",
      "epoch: 7 step: 710, loss is 0.008223200216889381\n",
      "epoch: 7 step: 711, loss is 0.01530008390545845\n",
      "epoch: 7 step: 712, loss is 0.00547238951548934\n",
      "epoch: 7 step: 713, loss is 0.00013516969920601696\n",
      "epoch: 7 step: 714, loss is 0.07887183874845505\n",
      "epoch: 7 step: 715, loss is 0.0011383957462385297\n",
      "epoch: 7 step: 716, loss is 0.00434423703700304\n",
      "epoch: 7 step: 717, loss is 0.002997933654114604\n",
      "epoch: 7 step: 718, loss is 0.002960437210276723\n",
      "epoch: 7 step: 719, loss is 0.0061393799260258675\n",
      "epoch: 7 step: 720, loss is 0.000895890814717859\n",
      "epoch: 7 step: 721, loss is 0.00303802452981472\n",
      "epoch: 7 step: 722, loss is 0.002404896542429924\n",
      "epoch: 7 step: 723, loss is 0.003228656481951475\n",
      "epoch: 7 step: 724, loss is 0.0010640304535627365\n",
      "epoch: 7 step: 725, loss is 0.0021081676241010427\n",
      "epoch: 7 step: 726, loss is 0.0009686843841336668\n",
      "epoch: 7 step: 727, loss is 0.0007866551750339568\n",
      "epoch: 7 step: 728, loss is 0.0198349766433239\n",
      "epoch: 7 step: 729, loss is 0.11328771710395813\n",
      "epoch: 7 step: 730, loss is 0.00271499902009964\n",
      "epoch: 7 step: 731, loss is 0.007462852168828249\n",
      "epoch: 7 step: 732, loss is 0.24614301323890686\n",
      "epoch: 7 step: 733, loss is 0.0006060768500901759\n",
      "epoch: 7 step: 734, loss is 0.015048547647893429\n",
      "epoch: 7 step: 735, loss is 0.04498819261789322\n",
      "epoch: 7 step: 736, loss is 0.0033131332602351904\n",
      "epoch: 7 step: 737, loss is 0.00830095075070858\n",
      "epoch: 7 step: 738, loss is 0.00024507136549800634\n",
      "epoch: 7 step: 739, loss is 0.015907803550362587\n",
      "epoch: 7 step: 740, loss is 0.0006561222253367305\n",
      "epoch: 7 step: 741, loss is 7.674357766518369e-05\n",
      "epoch: 7 step: 742, loss is 0.06741967797279358\n",
      "epoch: 7 step: 743, loss is 0.18134166300296783\n",
      "epoch: 7 step: 744, loss is 0.09028005599975586\n",
      "epoch: 7 step: 745, loss is 0.012875650078058243\n",
      "epoch: 7 step: 746, loss is 0.2964836359024048\n",
      "epoch: 7 step: 747, loss is 0.023545807227492332\n",
      "epoch: 7 step: 748, loss is 0.4157111644744873\n",
      "epoch: 7 step: 749, loss is 0.000290762196527794\n",
      "epoch: 7 step: 750, loss is 0.0021800242830067873\n",
      "epoch: 7 step: 751, loss is 0.03590124100446701\n",
      "epoch: 7 step: 752, loss is 0.0798618420958519\n",
      "epoch: 7 step: 753, loss is 0.026373527944087982\n",
      "epoch: 7 step: 754, loss is 0.0003342381096445024\n",
      "epoch: 7 step: 755, loss is 0.17096127569675446\n",
      "epoch: 7 step: 756, loss is 0.0010193956550210714\n",
      "epoch: 7 step: 757, loss is 0.024167431518435478\n",
      "epoch: 7 step: 758, loss is 0.0031984718516469\n",
      "epoch: 7 step: 759, loss is 0.0341070257127285\n",
      "epoch: 7 step: 760, loss is 0.03953150287270546\n",
      "epoch: 7 step: 761, loss is 0.0017825399991124868\n",
      "epoch: 7 step: 762, loss is 0.06435378640890121\n",
      "epoch: 7 step: 763, loss is 0.0802120789885521\n",
      "epoch: 7 step: 764, loss is 0.011372646316885948\n",
      "epoch: 7 step: 765, loss is 0.006009203847497702\n",
      "epoch: 7 step: 766, loss is 0.006873897276818752\n",
      "epoch: 7 step: 767, loss is 0.006642933003604412\n",
      "epoch: 7 step: 768, loss is 0.0003638999769464135\n",
      "epoch: 7 step: 769, loss is 0.05422906577587128\n",
      "epoch: 7 step: 770, loss is 0.011553286574780941\n",
      "epoch: 7 step: 771, loss is 0.059816643595695496\n",
      "epoch: 7 step: 772, loss is 0.02775021456182003\n",
      "epoch: 7 step: 773, loss is 0.0022918805480003357\n",
      "epoch: 7 step: 774, loss is 0.070777527987957\n",
      "epoch: 7 step: 775, loss is 0.0024728248827159405\n",
      "epoch: 7 step: 776, loss is 0.006369461305439472\n",
      "epoch: 7 step: 777, loss is 0.012042456306517124\n",
      "epoch: 7 step: 778, loss is 0.00355353276245296\n",
      "epoch: 7 step: 779, loss is 0.051484670490026474\n",
      "epoch: 7 step: 780, loss is 0.012160470709204674\n",
      "epoch: 7 step: 781, loss is 0.0010928609408438206\n",
      "epoch: 7 step: 782, loss is 0.008352070115506649\n",
      "epoch: 7 step: 783, loss is 0.0016500839265063405\n",
      "epoch: 7 step: 784, loss is 0.0006475064437836409\n",
      "epoch: 7 step: 785, loss is 0.014288429170846939\n",
      "epoch: 7 step: 786, loss is 0.002374124014750123\n",
      "epoch: 7 step: 787, loss is 0.0011895785573869944\n",
      "epoch: 7 step: 788, loss is 0.0016367373755201697\n",
      "epoch: 7 step: 789, loss is 0.0007166647701524198\n",
      "epoch: 7 step: 790, loss is 0.001789723988622427\n",
      "epoch: 7 step: 791, loss is 0.00419279932975769\n",
      "epoch: 7 step: 792, loss is 0.017234060913324356\n",
      "epoch: 7 step: 793, loss is 0.018821395933628082\n",
      "epoch: 7 step: 794, loss is 0.08268071711063385\n",
      "epoch: 7 step: 795, loss is 0.00026508141309022903\n",
      "epoch: 7 step: 796, loss is 0.00041272694943472743\n",
      "epoch: 7 step: 797, loss is 0.001979731721803546\n",
      "epoch: 7 step: 798, loss is 0.0029608129989355803\n",
      "epoch: 7 step: 799, loss is 0.0014904688578099012\n",
      "epoch: 7 step: 800, loss is 0.0006297955405898392\n",
      "epoch: 7 step: 801, loss is 0.001464730128645897\n",
      "epoch: 7 step: 802, loss is 0.11357765644788742\n",
      "epoch: 7 step: 803, loss is 0.03636467084288597\n",
      "epoch: 7 step: 804, loss is 0.01307962741702795\n",
      "epoch: 7 step: 805, loss is 0.006068164482712746\n",
      "epoch: 7 step: 806, loss is 0.04269016906619072\n",
      "epoch: 7 step: 807, loss is 0.04030361771583557\n",
      "epoch: 7 step: 808, loss is 0.0004940018989145756\n",
      "epoch: 7 step: 809, loss is 0.011792694218456745\n",
      "epoch: 7 step: 810, loss is 0.0801076889038086\n",
      "epoch: 7 step: 811, loss is 0.008800530806183815\n",
      "epoch: 7 step: 812, loss is 0.004380036145448685\n",
      "epoch: 7 step: 813, loss is 0.0021319626830518246\n",
      "epoch: 7 step: 814, loss is 0.0005188664654269814\n",
      "epoch: 7 step: 815, loss is 9.573849092703313e-05\n",
      "epoch: 7 step: 816, loss is 0.0024226163513958454\n",
      "epoch: 7 step: 817, loss is 0.0008335539023391902\n",
      "epoch: 7 step: 818, loss is 0.13154961168766022\n",
      "epoch: 7 step: 819, loss is 0.0006563309580087662\n",
      "epoch: 7 step: 820, loss is 0.000780800764914602\n",
      "epoch: 7 step: 821, loss is 0.0009180028573609889\n",
      "epoch: 7 step: 822, loss is 0.0009967220248654485\n",
      "epoch: 7 step: 823, loss is 0.0014574278611689806\n",
      "epoch: 7 step: 824, loss is 0.002738380338996649\n",
      "epoch: 7 step: 825, loss is 0.13702766597270966\n",
      "epoch: 7 step: 826, loss is 0.0003235928015783429\n",
      "epoch: 7 step: 827, loss is 0.013876230455935001\n",
      "epoch: 7 step: 828, loss is 0.0016167282592505217\n",
      "epoch: 7 step: 829, loss is 0.009451216086745262\n",
      "epoch: 7 step: 830, loss is 0.00039532638038508594\n",
      "epoch: 7 step: 831, loss is 0.0016862498596310616\n",
      "epoch: 7 step: 832, loss is 0.0007803735788911581\n",
      "epoch: 7 step: 833, loss is 0.00037854709080420434\n",
      "epoch: 7 step: 834, loss is 0.00028832783573307097\n",
      "epoch: 7 step: 835, loss is 0.0007190206670202315\n",
      "epoch: 7 step: 836, loss is 0.02537798136472702\n",
      "epoch: 7 step: 837, loss is 0.0023651039227843285\n",
      "epoch: 7 step: 838, loss is 0.0010041826171800494\n",
      "epoch: 7 step: 839, loss is 0.03566867858171463\n",
      "epoch: 7 step: 840, loss is 0.019130149856209755\n",
      "epoch: 7 step: 841, loss is 0.0035922855604439974\n",
      "epoch: 7 step: 842, loss is 0.015703026205301285\n",
      "epoch: 7 step: 843, loss is 0.002129544271156192\n",
      "epoch: 7 step: 844, loss is 0.06500105559825897\n",
      "epoch: 7 step: 845, loss is 0.0004627778544090688\n",
      "epoch: 7 step: 846, loss is 0.0024624010547995567\n",
      "epoch: 7 step: 847, loss is 0.0014480248792096972\n",
      "epoch: 7 step: 848, loss is 0.02787010744214058\n",
      "epoch: 7 step: 849, loss is 0.0003373970394022763\n",
      "epoch: 7 step: 850, loss is 0.05369007587432861\n",
      "epoch: 7 step: 851, loss is 0.012178490869700909\n",
      "epoch: 7 step: 852, loss is 0.00244090030901134\n",
      "epoch: 7 step: 853, loss is 0.0015794496284797788\n",
      "epoch: 7 step: 854, loss is 0.0046654390171170235\n",
      "epoch: 7 step: 855, loss is 0.006775910034775734\n",
      "epoch: 7 step: 856, loss is 0.004502398893237114\n",
      "epoch: 7 step: 857, loss is 0.0011268439702689648\n",
      "epoch: 7 step: 858, loss is 0.0028597621712833643\n",
      "epoch: 7 step: 859, loss is 0.00035126402508467436\n",
      "epoch: 7 step: 860, loss is 0.0005079658585600555\n",
      "epoch: 7 step: 861, loss is 0.0225642342120409\n",
      "epoch: 7 step: 862, loss is 0.0039084856398403645\n",
      "epoch: 7 step: 863, loss is 0.0023806386161595583\n",
      "epoch: 7 step: 864, loss is 0.022765347734093666\n",
      "epoch: 7 step: 865, loss is 0.0006576773012056947\n",
      "epoch: 7 step: 866, loss is 0.0020438835490494967\n",
      "epoch: 7 step: 867, loss is 0.06176290661096573\n",
      "epoch: 7 step: 868, loss is 0.06390257924795151\n",
      "epoch: 7 step: 869, loss is 0.00032074388582259417\n",
      "epoch: 7 step: 870, loss is 0.0008872207254171371\n",
      "epoch: 7 step: 871, loss is 0.0003193800803273916\n",
      "epoch: 7 step: 872, loss is 0.0006287511205300689\n",
      "epoch: 7 step: 873, loss is 0.001370150363072753\n",
      "epoch: 7 step: 874, loss is 0.00048469568719156086\n",
      "epoch: 7 step: 875, loss is 0.0005381918745115399\n",
      "epoch: 7 step: 876, loss is 0.0008897358202375472\n",
      "epoch: 7 step: 877, loss is 4.9327281885780394e-05\n",
      "epoch: 7 step: 878, loss is 0.00199142936617136\n",
      "epoch: 7 step: 879, loss is 0.0024255099706351757\n",
      "epoch: 7 step: 880, loss is 0.17564043402671814\n",
      "epoch: 7 step: 881, loss is 0.002469639526680112\n",
      "epoch: 7 step: 882, loss is 0.0018310683080926538\n",
      "epoch: 7 step: 883, loss is 0.004339056089520454\n",
      "epoch: 7 step: 884, loss is 0.0012354350183159113\n",
      "epoch: 7 step: 885, loss is 0.0022024381905794144\n",
      "epoch: 7 step: 886, loss is 0.001586851547472179\n",
      "epoch: 7 step: 887, loss is 0.04866530001163483\n",
      "epoch: 7 step: 888, loss is 0.02546602301299572\n",
      "epoch: 7 step: 889, loss is 4.29162937507499e-05\n",
      "epoch: 7 step: 890, loss is 0.0014066175790503621\n",
      "epoch: 7 step: 891, loss is 0.00011136211105622351\n",
      "epoch: 7 step: 892, loss is 0.00010831520921783522\n",
      "epoch: 7 step: 893, loss is 0.0003014080866705626\n",
      "epoch: 7 step: 894, loss is 0.004274016246199608\n",
      "epoch: 7 step: 895, loss is 6.18858975940384e-05\n",
      "epoch: 7 step: 896, loss is 0.000995898270048201\n",
      "epoch: 7 step: 897, loss is 0.03252461925148964\n",
      "epoch: 7 step: 898, loss is 0.012221695855259895\n",
      "epoch: 7 step: 899, loss is 0.03539684787392616\n",
      "epoch: 7 step: 900, loss is 0.06012287363409996\n",
      "epoch: 7 step: 901, loss is 0.007896543480455875\n",
      "epoch: 7 step: 902, loss is 0.0001228911423822865\n",
      "epoch: 7 step: 903, loss is 0.007503536995500326\n",
      "epoch: 7 step: 904, loss is 0.045281123369932175\n",
      "epoch: 7 step: 905, loss is 0.015892960131168365\n",
      "epoch: 7 step: 906, loss is 0.024457193911075592\n",
      "epoch: 7 step: 907, loss is 0.009346548467874527\n",
      "epoch: 7 step: 908, loss is 0.016832871362566948\n",
      "epoch: 7 step: 909, loss is 0.00012139381578890607\n",
      "epoch: 7 step: 910, loss is 0.014870207756757736\n",
      "epoch: 7 step: 911, loss is 0.0007402022602036595\n",
      "epoch: 7 step: 912, loss is 0.0025611347518861294\n",
      "epoch: 7 step: 913, loss is 0.10340701043605804\n",
      "epoch: 7 step: 914, loss is 0.05739622935652733\n",
      "epoch: 7 step: 915, loss is 0.010745132341980934\n",
      "epoch: 7 step: 916, loss is 0.003371955594047904\n",
      "epoch: 7 step: 917, loss is 0.12240523099899292\n",
      "epoch: 7 step: 918, loss is 0.004945870954543352\n",
      "epoch: 7 step: 919, loss is 0.0004872654390055686\n",
      "epoch: 7 step: 920, loss is 0.0006419994169846177\n",
      "epoch: 7 step: 921, loss is 0.002278129570186138\n",
      "epoch: 7 step: 922, loss is 0.12074729800224304\n",
      "epoch: 7 step: 923, loss is 0.0033249801490455866\n",
      "epoch: 7 step: 924, loss is 0.00034737811074592173\n",
      "epoch: 7 step: 925, loss is 0.05697834864258766\n",
      "epoch: 7 step: 926, loss is 0.001908819773234427\n",
      "epoch: 7 step: 927, loss is 0.0037956037558615208\n",
      "epoch: 7 step: 928, loss is 0.0006211908766999841\n",
      "epoch: 7 step: 929, loss is 4.9698999646352604e-05\n",
      "epoch: 7 step: 930, loss is 0.06007423996925354\n",
      "epoch: 7 step: 931, loss is 0.0004032560100313276\n",
      "epoch: 7 step: 932, loss is 0.012515930458903313\n",
      "epoch: 7 step: 933, loss is 0.005931769032031298\n",
      "epoch: 7 step: 934, loss is 0.0030756068881601095\n",
      "epoch: 7 step: 935, loss is 0.0026181908324360847\n",
      "epoch: 7 step: 936, loss is 0.0032320646569132805\n",
      "epoch: 7 step: 937, loss is 0.0031204638071358204\n",
      "epoch: 7 step: 938, loss is 0.0032577808015048504\n",
      "epoch: 7 step: 939, loss is 0.0777050033211708\n",
      "epoch: 7 step: 940, loss is 0.0009243629174306989\n",
      "epoch: 7 step: 941, loss is 0.0005848623695783317\n",
      "epoch: 7 step: 942, loss is 0.013728523626923561\n",
      "epoch: 7 step: 943, loss is 0.002743092365562916\n",
      "epoch: 7 step: 944, loss is 0.00366879440844059\n",
      "epoch: 7 step: 945, loss is 0.004480302333831787\n",
      "epoch: 7 step: 946, loss is 0.12066807597875595\n",
      "epoch: 7 step: 947, loss is 0.0767771527171135\n",
      "epoch: 7 step: 948, loss is 0.0026738664600998163\n",
      "epoch: 7 step: 949, loss is 0.0013242950662970543\n",
      "epoch: 7 step: 950, loss is 0.001780421007424593\n",
      "epoch: 7 step: 951, loss is 0.0008668153895996511\n",
      "epoch: 7 step: 952, loss is 0.058047812432050705\n",
      "epoch: 7 step: 953, loss is 0.004127774387598038\n",
      "epoch: 7 step: 954, loss is 0.0019419181626290083\n",
      "epoch: 7 step: 955, loss is 0.07252316921949387\n",
      "epoch: 7 step: 956, loss is 0.005044127814471722\n",
      "epoch: 7 step: 957, loss is 0.002712972927838564\n",
      "epoch: 7 step: 958, loss is 0.018263723701238632\n",
      "epoch: 7 step: 959, loss is 0.021312598139047623\n",
      "epoch: 7 step: 960, loss is 0.013281753286719322\n",
      "epoch: 7 step: 961, loss is 0.0027932943776249886\n",
      "epoch: 7 step: 962, loss is 0.07788970321416855\n",
      "epoch: 7 step: 963, loss is 0.00045413055340759456\n",
      "epoch: 7 step: 964, loss is 0.008172091096639633\n",
      "epoch: 7 step: 965, loss is 0.0005766578251495957\n",
      "epoch: 7 step: 966, loss is 0.051337771117687225\n",
      "epoch: 7 step: 967, loss is 0.019679240882396698\n",
      "epoch: 7 step: 968, loss is 0.022552471607923508\n",
      "epoch: 7 step: 969, loss is 0.0011548151960596442\n",
      "epoch: 7 step: 970, loss is 0.001100583584047854\n",
      "epoch: 7 step: 971, loss is 0.019361643120646477\n",
      "epoch: 7 step: 972, loss is 0.015252780169248581\n",
      "epoch: 7 step: 973, loss is 0.0004007726674899459\n",
      "epoch: 7 step: 974, loss is 0.019981330260634422\n",
      "epoch: 7 step: 975, loss is 0.018473848700523376\n",
      "epoch: 7 step: 976, loss is 0.0015270852018147707\n",
      "epoch: 7 step: 977, loss is 0.056822460144758224\n",
      "epoch: 7 step: 978, loss is 0.0036630535032600164\n",
      "epoch: 7 step: 979, loss is 0.010730642825365067\n",
      "epoch: 7 step: 980, loss is 0.014660301618278027\n",
      "epoch: 7 step: 981, loss is 0.003246061038225889\n",
      "epoch: 7 step: 982, loss is 0.004185637924820185\n",
      "epoch: 7 step: 983, loss is 4.1169154428644106e-05\n",
      "epoch: 7 step: 984, loss is 0.035915639251470566\n",
      "epoch: 7 step: 985, loss is 0.003516331547871232\n",
      "epoch: 7 step: 986, loss is 0.00018757556972559541\n",
      "epoch: 7 step: 987, loss is 0.0009662142256274819\n",
      "epoch: 7 step: 988, loss is 0.0006964710773900151\n",
      "epoch: 7 step: 989, loss is 0.028873329982161522\n",
      "epoch: 7 step: 990, loss is 0.0023501759860664606\n",
      "epoch: 7 step: 991, loss is 0.0007172837504185736\n",
      "epoch: 7 step: 992, loss is 0.00320912036113441\n",
      "epoch: 7 step: 993, loss is 0.005354668945074081\n",
      "epoch: 7 step: 994, loss is 0.010572032071650028\n",
      "epoch: 7 step: 995, loss is 0.001551238470710814\n",
      "epoch: 7 step: 996, loss is 0.0003388127952348441\n",
      "epoch: 7 step: 997, loss is 0.11977913975715637\n",
      "epoch: 7 step: 998, loss is 0.003945854492485523\n",
      "epoch: 7 step: 999, loss is 0.03219803050160408\n",
      "epoch: 7 step: 1000, loss is 0.007035453338176012\n",
      "epoch: 7 step: 1001, loss is 0.002885169582441449\n",
      "epoch: 7 step: 1002, loss is 0.0037129761185497046\n",
      "epoch: 7 step: 1003, loss is 0.002727955812588334\n",
      "epoch: 7 step: 1004, loss is 0.001191998366266489\n",
      "epoch: 7 step: 1005, loss is 0.06246577203273773\n",
      "epoch: 7 step: 1006, loss is 0.01896488480269909\n",
      "epoch: 7 step: 1007, loss is 0.004361768718808889\n",
      "epoch: 7 step: 1008, loss is 0.00020112292259000242\n",
      "epoch: 7 step: 1009, loss is 0.00018328920123167336\n",
      "epoch: 7 step: 1010, loss is 0.0016701568383723497\n",
      "epoch: 7 step: 1011, loss is 0.00043310615001246333\n",
      "epoch: 7 step: 1012, loss is 0.00548823457211256\n",
      "epoch: 7 step: 1013, loss is 3.3520860597491264e-05\n",
      "epoch: 7 step: 1014, loss is 0.0007468783878721297\n",
      "epoch: 7 step: 1015, loss is 0.01213830430060625\n",
      "epoch: 7 step: 1016, loss is 0.025440126657485962\n",
      "epoch: 7 step: 1017, loss is 0.0008534360094927251\n",
      "epoch: 7 step: 1018, loss is 0.00038921565283089876\n",
      "epoch: 7 step: 1019, loss is 0.002315669786185026\n",
      "epoch: 7 step: 1020, loss is 0.010653180070221424\n",
      "epoch: 7 step: 1021, loss is 0.003047972684726119\n",
      "epoch: 7 step: 1022, loss is 0.00010792937246151268\n",
      "epoch: 7 step: 1023, loss is 0.03444619104266167\n",
      "epoch: 7 step: 1024, loss is 0.003699697321280837\n",
      "epoch: 7 step: 1025, loss is 0.010414041578769684\n",
      "epoch: 7 step: 1026, loss is 0.0015118090668693185\n",
      "epoch: 7 step: 1027, loss is 0.00976652279496193\n",
      "epoch: 7 step: 1028, loss is 0.0012921423185616732\n",
      "epoch: 7 step: 1029, loss is 0.00280755921266973\n",
      "epoch: 7 step: 1030, loss is 0.002619192935526371\n",
      "epoch: 7 step: 1031, loss is 0.00014584104064852\n",
      "epoch: 7 step: 1032, loss is 0.0013039764016866684\n",
      "epoch: 7 step: 1033, loss is 0.00038064055843278766\n",
      "epoch: 7 step: 1034, loss is 0.0012446073815226555\n",
      "epoch: 7 step: 1035, loss is 0.00013768319331575185\n",
      "epoch: 7 step: 1036, loss is 0.0014646839117631316\n",
      "epoch: 7 step: 1037, loss is 0.0021901491563767195\n",
      "epoch: 7 step: 1038, loss is 0.006083248648792505\n",
      "epoch: 7 step: 1039, loss is 0.012385704554617405\n",
      "epoch: 7 step: 1040, loss is 0.05329400300979614\n",
      "epoch: 7 step: 1041, loss is 0.00011192324018338695\n",
      "epoch: 7 step: 1042, loss is 0.08181167393922806\n",
      "epoch: 7 step: 1043, loss is 0.05039151757955551\n",
      "epoch: 7 step: 1044, loss is 0.03187777101993561\n",
      "epoch: 7 step: 1045, loss is 0.0036059445701539516\n",
      "epoch: 7 step: 1046, loss is 0.04971322417259216\n",
      "epoch: 7 step: 1047, loss is 9.10405651666224e-05\n",
      "epoch: 7 step: 1048, loss is 0.0033725278917700052\n",
      "epoch: 7 step: 1049, loss is 0.0011875121854245663\n",
      "epoch: 7 step: 1050, loss is 0.0003120306646451354\n",
      "epoch: 7 step: 1051, loss is 7.11743050487712e-05\n",
      "epoch: 7 step: 1052, loss is 0.08725493401288986\n",
      "epoch: 7 step: 1053, loss is 0.00041489829891361296\n",
      "epoch: 7 step: 1054, loss is 0.0015967980725690722\n",
      "epoch: 7 step: 1055, loss is 0.03076089732348919\n",
      "epoch: 7 step: 1056, loss is 0.0008480846881866455\n",
      "epoch: 7 step: 1057, loss is 0.0007774286204949021\n",
      "epoch: 7 step: 1058, loss is 0.00038603850407525897\n",
      "epoch: 7 step: 1059, loss is 0.005917305592447519\n",
      "epoch: 7 step: 1060, loss is 0.0007752080564387143\n",
      "epoch: 7 step: 1061, loss is 0.004623311571776867\n",
      "epoch: 7 step: 1062, loss is 0.0005297602619975805\n",
      "epoch: 7 step: 1063, loss is 0.0008019510423764586\n",
      "epoch: 7 step: 1064, loss is 0.09336408972740173\n",
      "epoch: 7 step: 1065, loss is 0.03974349424242973\n",
      "epoch: 7 step: 1066, loss is 0.0025628062430769205\n",
      "epoch: 7 step: 1067, loss is 0.00022368284408003092\n",
      "epoch: 7 step: 1068, loss is 0.05249318853020668\n",
      "epoch: 7 step: 1069, loss is 0.05112781748175621\n",
      "epoch: 7 step: 1070, loss is 0.04482901468873024\n",
      "epoch: 7 step: 1071, loss is 0.0014082336565479636\n",
      "epoch: 7 step: 1072, loss is 0.0005051394691690803\n",
      "epoch: 7 step: 1073, loss is 0.01196181122213602\n",
      "epoch: 7 step: 1074, loss is 0.04417558014392853\n",
      "epoch: 7 step: 1075, loss is 0.08301683515310287\n",
      "epoch: 7 step: 1076, loss is 0.002675974741578102\n",
      "epoch: 7 step: 1077, loss is 0.017104310914874077\n",
      "epoch: 7 step: 1078, loss is 0.007061793934553862\n",
      "epoch: 7 step: 1079, loss is 0.0008106611203402281\n",
      "epoch: 7 step: 1080, loss is 0.00042367386049591005\n",
      "epoch: 7 step: 1081, loss is 0.001966664334759116\n",
      "epoch: 7 step: 1082, loss is 0.0005393539904616773\n",
      "epoch: 7 step: 1083, loss is 0.0020074208732694387\n",
      "epoch: 7 step: 1084, loss is 0.010772813111543655\n",
      "epoch: 7 step: 1085, loss is 0.24644801020622253\n",
      "epoch: 7 step: 1086, loss is 0.0031045565847307444\n",
      "epoch: 7 step: 1087, loss is 0.0028607421554625034\n",
      "epoch: 7 step: 1088, loss is 0.02926127426326275\n",
      "epoch: 7 step: 1089, loss is 0.006811891682446003\n",
      "epoch: 7 step: 1090, loss is 0.1307694911956787\n",
      "epoch: 7 step: 1091, loss is 0.046617601066827774\n",
      "epoch: 7 step: 1092, loss is 0.051803890615701675\n",
      "epoch: 7 step: 1093, loss is 0.011618145741522312\n",
      "epoch: 7 step: 1094, loss is 0.0007973801111802459\n",
      "epoch: 7 step: 1095, loss is 0.027534054592251778\n",
      "epoch: 7 step: 1096, loss is 0.0024279409553855658\n",
      "epoch: 7 step: 1097, loss is 0.0025407858192920685\n",
      "epoch: 7 step: 1098, loss is 0.0540221631526947\n",
      "epoch: 7 step: 1099, loss is 0.013781191781163216\n",
      "epoch: 7 step: 1100, loss is 0.021891728043556213\n",
      "epoch: 7 step: 1101, loss is 0.0003803910221904516\n",
      "epoch: 7 step: 1102, loss is 0.0005083586438558996\n",
      "epoch: 7 step: 1103, loss is 0.001914914813823998\n",
      "epoch: 7 step: 1104, loss is 0.0011655978159978986\n",
      "epoch: 7 step: 1105, loss is 0.0008697858429513872\n",
      "epoch: 7 step: 1106, loss is 0.0021568401716649532\n",
      "epoch: 7 step: 1107, loss is 0.0042668734677135944\n",
      "epoch: 7 step: 1108, loss is 0.1098606139421463\n",
      "epoch: 7 step: 1109, loss is 0.001498937839642167\n",
      "epoch: 7 step: 1110, loss is 0.0038144756108522415\n",
      "epoch: 7 step: 1111, loss is 0.031935255974531174\n",
      "epoch: 7 step: 1112, loss is 0.0013062478974461555\n",
      "epoch: 7 step: 1113, loss is 0.026604654267430305\n",
      "epoch: 7 step: 1114, loss is 0.0011345779057592154\n",
      "epoch: 7 step: 1115, loss is 0.00915688369423151\n",
      "epoch: 7 step: 1116, loss is 0.0009958891896530986\n",
      "epoch: 7 step: 1117, loss is 0.00021959800506010652\n",
      "epoch: 7 step: 1118, loss is 0.0002962252765428275\n",
      "epoch: 7 step: 1119, loss is 0.0005133141530677676\n",
      "epoch: 7 step: 1120, loss is 0.001549605862237513\n",
      "epoch: 7 step: 1121, loss is 0.10279940813779831\n",
      "epoch: 7 step: 1122, loss is 0.00156778609380126\n",
      "epoch: 7 step: 1123, loss is 0.0016510109417140484\n",
      "epoch: 7 step: 1124, loss is 0.0005378869245760143\n",
      "epoch: 7 step: 1125, loss is 0.00031929524266161025\n",
      "epoch: 7 step: 1126, loss is 0.00039562935126014054\n",
      "epoch: 7 step: 1127, loss is 0.2948039174079895\n",
      "epoch: 7 step: 1128, loss is 0.000706263177562505\n",
      "epoch: 7 step: 1129, loss is 0.0018579914467409253\n",
      "epoch: 7 step: 1130, loss is 0.010745593346655369\n",
      "epoch: 7 step: 1131, loss is 0.03647463023662567\n",
      "epoch: 7 step: 1132, loss is 0.19294308125972748\n",
      "epoch: 7 step: 1133, loss is 0.0006956953438930213\n",
      "epoch: 7 step: 1134, loss is 0.0005791778676211834\n",
      "epoch: 7 step: 1135, loss is 0.005991332698613405\n",
      "epoch: 7 step: 1136, loss is 0.0010819538729265332\n",
      "epoch: 7 step: 1137, loss is 0.06385799497365952\n",
      "epoch: 7 step: 1138, loss is 0.013217531144618988\n",
      "epoch: 7 step: 1139, loss is 0.03127430006861687\n",
      "epoch: 7 step: 1140, loss is 0.004163840319961309\n",
      "epoch: 7 step: 1141, loss is 0.0004483702650759369\n",
      "epoch: 7 step: 1142, loss is 0.003101120237261057\n",
      "epoch: 7 step: 1143, loss is 0.00023203827731776983\n",
      "epoch: 7 step: 1144, loss is 0.0009233311284333467\n",
      "epoch: 7 step: 1145, loss is 0.014726391062140465\n",
      "epoch: 7 step: 1146, loss is 0.0028999822679907084\n",
      "epoch: 7 step: 1147, loss is 0.00041724895709194243\n",
      "epoch: 7 step: 1148, loss is 0.00041292692185379565\n",
      "epoch: 7 step: 1149, loss is 0.006968193221837282\n",
      "epoch: 7 step: 1150, loss is 0.0001206050073960796\n",
      "epoch: 7 step: 1151, loss is 0.002128944033756852\n",
      "epoch: 7 step: 1152, loss is 0.000410535343689844\n",
      "epoch: 7 step: 1153, loss is 0.00017380149802193046\n",
      "epoch: 7 step: 1154, loss is 0.004744555801153183\n",
      "epoch: 7 step: 1155, loss is 0.006834701169282198\n",
      "epoch: 7 step: 1156, loss is 0.0006075503188185394\n",
      "epoch: 7 step: 1157, loss is 0.0019770467188209295\n",
      "epoch: 7 step: 1158, loss is 0.00455646775662899\n",
      "epoch: 7 step: 1159, loss is 0.0011871487367898226\n",
      "epoch: 7 step: 1160, loss is 0.0016694486839696765\n",
      "epoch: 7 step: 1161, loss is 0.19150952994823456\n",
      "epoch: 7 step: 1162, loss is 0.008187375031411648\n",
      "epoch: 7 step: 1163, loss is 0.00043513011769391596\n",
      "epoch: 7 step: 1164, loss is 0.00021647002722602338\n",
      "epoch: 7 step: 1165, loss is 0.03000074066221714\n",
      "epoch: 7 step: 1166, loss is 0.006895534694194794\n",
      "epoch: 7 step: 1167, loss is 0.02483258582651615\n",
      "epoch: 7 step: 1168, loss is 0.030404502525925636\n",
      "epoch: 7 step: 1169, loss is 0.002445760415866971\n",
      "epoch: 7 step: 1170, loss is 0.004152143374085426\n",
      "epoch: 7 step: 1171, loss is 0.01769712194800377\n",
      "epoch: 7 step: 1172, loss is 0.00012426289322320372\n",
      "epoch: 7 step: 1173, loss is 0.0006616837927140296\n",
      "epoch: 7 step: 1174, loss is 0.014129194431006908\n",
      "epoch: 7 step: 1175, loss is 0.00032547200680710375\n",
      "epoch: 7 step: 1176, loss is 0.02930343896150589\n",
      "epoch: 7 step: 1177, loss is 0.0008031679317355156\n",
      "epoch: 7 step: 1178, loss is 0.01671840250492096\n",
      "epoch: 7 step: 1179, loss is 0.0053664748556911945\n",
      "epoch: 7 step: 1180, loss is 0.0032440414652228355\n",
      "epoch: 7 step: 1181, loss is 0.010252908803522587\n",
      "epoch: 7 step: 1182, loss is 0.006632960867136717\n",
      "epoch: 7 step: 1183, loss is 0.019362922757864\n",
      "epoch: 7 step: 1184, loss is 8.495010115439072e-05\n",
      "epoch: 7 step: 1185, loss is 0.02417067065834999\n",
      "epoch: 7 step: 1186, loss is 0.0010306735057383776\n",
      "epoch: 7 step: 1187, loss is 0.00020666231284849346\n",
      "epoch: 7 step: 1188, loss is 0.012703368440270424\n",
      "epoch: 7 step: 1189, loss is 0.0013555827317759395\n",
      "epoch: 7 step: 1190, loss is 0.006318308878690004\n",
      "epoch: 7 step: 1191, loss is 2.060051883745473e-05\n",
      "epoch: 7 step: 1192, loss is 0.0007595520582981408\n",
      "epoch: 7 step: 1193, loss is 0.001976597821339965\n",
      "epoch: 7 step: 1194, loss is 0.000618398014921695\n",
      "epoch: 7 step: 1195, loss is 0.0038694292306900024\n",
      "epoch: 7 step: 1196, loss is 0.016623016446828842\n",
      "epoch: 7 step: 1197, loss is 0.0011547281173989177\n",
      "epoch: 7 step: 1198, loss is 0.001693247933872044\n",
      "epoch: 7 step: 1199, loss is 0.003468457143753767\n",
      "epoch: 7 step: 1200, loss is 0.00012708942813333124\n",
      "epoch: 7 step: 1201, loss is 0.037004608660936356\n",
      "epoch: 7 step: 1202, loss is 0.008488927967846394\n",
      "epoch: 7 step: 1203, loss is 0.00026350715779699385\n",
      "epoch: 7 step: 1204, loss is 0.0005128915072418749\n",
      "epoch: 7 step: 1205, loss is 0.011977397836744785\n",
      "epoch: 7 step: 1206, loss is 0.03380120173096657\n",
      "epoch: 7 step: 1207, loss is 0.007158276624977589\n",
      "epoch: 7 step: 1208, loss is 0.012543048709630966\n",
      "epoch: 7 step: 1209, loss is 0.011627879925072193\n",
      "epoch: 7 step: 1210, loss is 0.0009880559518933296\n",
      "epoch: 7 step: 1211, loss is 2.7075955586042255e-05\n",
      "epoch: 7 step: 1212, loss is 0.001737685059197247\n",
      "epoch: 7 step: 1213, loss is 0.1607659012079239\n",
      "epoch: 7 step: 1214, loss is 0.004914028104394674\n",
      "epoch: 7 step: 1215, loss is 0.05997614562511444\n",
      "epoch: 7 step: 1216, loss is 0.0023764290381222963\n",
      "epoch: 7 step: 1217, loss is 0.0048211864195764065\n",
      "epoch: 7 step: 1218, loss is 0.02077479660511017\n",
      "epoch: 7 step: 1219, loss is 0.001290788990445435\n",
      "epoch: 7 step: 1220, loss is 0.004644491244107485\n",
      "epoch: 7 step: 1221, loss is 0.000712305074557662\n",
      "epoch: 7 step: 1222, loss is 0.007777380757033825\n",
      "epoch: 7 step: 1223, loss is 0.011405929923057556\n",
      "epoch: 7 step: 1224, loss is 0.0018423760775476694\n",
      "epoch: 7 step: 1225, loss is 0.00016822386533021927\n",
      "epoch: 7 step: 1226, loss is 0.026081010699272156\n",
      "epoch: 7 step: 1227, loss is 0.00021776242647320032\n",
      "epoch: 7 step: 1228, loss is 0.009169240482151508\n",
      "epoch: 7 step: 1229, loss is 0.003925967030227184\n",
      "epoch: 7 step: 1230, loss is 0.010008069686591625\n",
      "epoch: 7 step: 1231, loss is 0.00048352801240980625\n",
      "epoch: 7 step: 1232, loss is 0.0014406273839995265\n",
      "epoch: 7 step: 1233, loss is 0.010938327759504318\n",
      "epoch: 7 step: 1234, loss is 0.0005882424302399158\n",
      "epoch: 7 step: 1235, loss is 0.04490237310528755\n",
      "epoch: 7 step: 1236, loss is 0.02613251283764839\n",
      "epoch: 7 step: 1237, loss is 0.00015165293007157743\n",
      "epoch: 7 step: 1238, loss is 0.0004104928229935467\n",
      "epoch: 7 step: 1239, loss is 0.0018143834313377738\n",
      "epoch: 7 step: 1240, loss is 5.8457982959225774e-05\n",
      "epoch: 7 step: 1241, loss is 0.000294238212518394\n",
      "epoch: 7 step: 1242, loss is 8.8583241449669e-05\n",
      "epoch: 7 step: 1243, loss is 0.0068929968401789665\n",
      "epoch: 7 step: 1244, loss is 0.0018382470589131117\n",
      "epoch: 7 step: 1245, loss is 0.013475868850946426\n",
      "epoch: 7 step: 1246, loss is 0.002491619437932968\n",
      "epoch: 7 step: 1247, loss is 0.0001101680172723718\n",
      "epoch: 7 step: 1248, loss is 0.00017605855828151107\n",
      "epoch: 7 step: 1249, loss is 0.0009255284676328301\n",
      "epoch: 7 step: 1250, loss is 0.0049006156623363495\n",
      "epoch: 7 step: 1251, loss is 0.013500255532562733\n",
      "epoch: 7 step: 1252, loss is 0.0021045696921646595\n",
      "epoch: 7 step: 1253, loss is 0.02765333466231823\n",
      "epoch: 7 step: 1254, loss is 0.010255937464535236\n",
      "epoch: 7 step: 1255, loss is 0.007617054972797632\n",
      "epoch: 7 step: 1256, loss is 0.0011093930806964636\n",
      "epoch: 7 step: 1257, loss is 0.000939610181376338\n",
      "epoch: 7 step: 1258, loss is 0.07943665236234665\n",
      "epoch: 7 step: 1259, loss is 0.058701496571302414\n",
      "epoch: 7 step: 1260, loss is 0.010553602129220963\n",
      "epoch: 7 step: 1261, loss is 8.560340211261064e-05\n",
      "epoch: 7 step: 1262, loss is 0.012003299780189991\n",
      "epoch: 7 step: 1263, loss is 0.0006577009917236865\n",
      "epoch: 7 step: 1264, loss is 0.0006192091968841851\n",
      "epoch: 7 step: 1265, loss is 0.001102609559893608\n",
      "epoch: 7 step: 1266, loss is 0.0010270795319229364\n",
      "epoch: 7 step: 1267, loss is 0.0024223607033491135\n",
      "epoch: 7 step: 1268, loss is 0.0031119720079004765\n",
      "epoch: 7 step: 1269, loss is 0.02928522415459156\n",
      "epoch: 7 step: 1270, loss is 0.0024915379472076893\n",
      "epoch: 7 step: 1271, loss is 0.009281609207391739\n",
      "epoch: 7 step: 1272, loss is 0.0031306713353842497\n",
      "epoch: 7 step: 1273, loss is 0.00010444436338730156\n",
      "epoch: 7 step: 1274, loss is 0.00011327613174216822\n",
      "epoch: 7 step: 1275, loss is 0.0008064472931437194\n",
      "epoch: 7 step: 1276, loss is 0.08329348266124725\n",
      "epoch: 7 step: 1277, loss is 0.0015363050624728203\n",
      "epoch: 7 step: 1278, loss is 0.007001841440796852\n",
      "epoch: 7 step: 1279, loss is 0.03145497292280197\n",
      "epoch: 7 step: 1280, loss is 0.0017874984769150615\n",
      "epoch: 7 step: 1281, loss is 0.0015476481057703495\n",
      "epoch: 7 step: 1282, loss is 0.00028104634839110076\n",
      "epoch: 7 step: 1283, loss is 6.259892688831314e-05\n",
      "epoch: 7 step: 1284, loss is 7.283578452188522e-05\n",
      "epoch: 7 step: 1285, loss is 0.0010509216226637363\n",
      "epoch: 7 step: 1286, loss is 0.0007953515741974115\n",
      "epoch: 7 step: 1287, loss is 0.13392405211925507\n",
      "epoch: 7 step: 1288, loss is 0.0006869389326311648\n",
      "epoch: 7 step: 1289, loss is 0.04054727777838707\n",
      "epoch: 7 step: 1290, loss is 0.001206218614242971\n",
      "epoch: 7 step: 1291, loss is 0.001829746412113309\n",
      "epoch: 7 step: 1292, loss is 0.00036816438660025597\n",
      "epoch: 7 step: 1293, loss is 0.1391846090555191\n",
      "epoch: 7 step: 1294, loss is 0.0007800187449902296\n",
      "epoch: 7 step: 1295, loss is 0.03638529032468796\n",
      "epoch: 7 step: 1296, loss is 0.043103549629449844\n",
      "epoch: 7 step: 1297, loss is 0.0002107410691678524\n",
      "epoch: 7 step: 1298, loss is 3.094255589530803e-05\n",
      "epoch: 7 step: 1299, loss is 0.004188801161944866\n",
      "epoch: 7 step: 1300, loss is 0.0004384172789286822\n",
      "epoch: 7 step: 1301, loss is 0.01046354416757822\n",
      "epoch: 7 step: 1302, loss is 5.131863144924864e-05\n",
      "epoch: 7 step: 1303, loss is 0.0003122458001598716\n",
      "epoch: 7 step: 1304, loss is 0.002524404088035226\n",
      "epoch: 7 step: 1305, loss is 0.0024689820129424334\n",
      "epoch: 7 step: 1306, loss is 0.0016994113102555275\n",
      "epoch: 7 step: 1307, loss is 0.002122606383636594\n",
      "epoch: 7 step: 1308, loss is 0.0001466801477363333\n",
      "epoch: 7 step: 1309, loss is 0.0003119876200798899\n",
      "epoch: 7 step: 1310, loss is 0.0011818798957392573\n",
      "epoch: 7 step: 1311, loss is 0.0007447046227753162\n",
      "epoch: 7 step: 1312, loss is 0.024464303627610207\n",
      "epoch: 7 step: 1313, loss is 4.414534487295896e-05\n",
      "epoch: 7 step: 1314, loss is 0.0009750263998284936\n",
      "epoch: 7 step: 1315, loss is 0.0014619136927649379\n",
      "epoch: 7 step: 1316, loss is 0.038161501288414\n",
      "epoch: 7 step: 1317, loss is 6.402537110261619e-05\n",
      "epoch: 7 step: 1318, loss is 0.00019391377281863242\n",
      "epoch: 7 step: 1319, loss is 0.08861054480075836\n",
      "epoch: 7 step: 1320, loss is 0.004485854413360357\n",
      "epoch: 7 step: 1321, loss is 0.1702975481748581\n",
      "epoch: 7 step: 1322, loss is 0.009012608788907528\n",
      "epoch: 7 step: 1323, loss is 0.006059711333364248\n",
      "epoch: 7 step: 1324, loss is 0.006527658086270094\n",
      "epoch: 7 step: 1325, loss is 0.14640727639198303\n",
      "epoch: 7 step: 1326, loss is 0.00372643512673676\n",
      "epoch: 7 step: 1327, loss is 0.0030868789181113243\n",
      "epoch: 7 step: 1328, loss is 0.00159806152805686\n",
      "epoch: 7 step: 1329, loss is 0.016999252140522003\n",
      "epoch: 7 step: 1330, loss is 0.00038320536259561777\n",
      "epoch: 7 step: 1331, loss is 0.006776735186576843\n",
      "epoch: 7 step: 1332, loss is 0.0010806487407535315\n",
      "epoch: 7 step: 1333, loss is 0.00016872683772817254\n",
      "epoch: 7 step: 1334, loss is 0.005796195473521948\n",
      "epoch: 7 step: 1335, loss is 0.09994152933359146\n",
      "epoch: 7 step: 1336, loss is 0.011950487270951271\n",
      "epoch: 7 step: 1337, loss is 0.0014223660109564662\n",
      "epoch: 7 step: 1338, loss is 0.0014270013198256493\n",
      "epoch: 7 step: 1339, loss is 0.0063939704559743404\n",
      "epoch: 7 step: 1340, loss is 0.002968079410493374\n",
      "epoch: 7 step: 1341, loss is 0.0179971344769001\n",
      "epoch: 7 step: 1342, loss is 0.005576943047344685\n",
      "epoch: 7 step: 1343, loss is 0.062241699546575546\n",
      "epoch: 7 step: 1344, loss is 0.003537040436640382\n",
      "epoch: 7 step: 1345, loss is 0.0015415754169225693\n",
      "epoch: 7 step: 1346, loss is 0.0004751567030325532\n",
      "epoch: 7 step: 1347, loss is 0.04566527158021927\n",
      "epoch: 7 step: 1348, loss is 0.027994073927402496\n",
      "epoch: 7 step: 1349, loss is 0.00045050421613268554\n",
      "epoch: 7 step: 1350, loss is 0.3390483856201172\n",
      "epoch: 7 step: 1351, loss is 0.010191901586949825\n",
      "epoch: 7 step: 1352, loss is 0.024121198803186417\n",
      "epoch: 7 step: 1353, loss is 0.0037110657431185246\n",
      "epoch: 7 step: 1354, loss is 0.0002292565186508\n",
      "epoch: 7 step: 1355, loss is 0.001830928958952427\n",
      "epoch: 7 step: 1356, loss is 0.03094310313463211\n",
      "epoch: 7 step: 1357, loss is 0.0018165510846301913\n",
      "epoch: 7 step: 1358, loss is 0.009238073602318764\n",
      "epoch: 7 step: 1359, loss is 0.06867469102144241\n",
      "epoch: 7 step: 1360, loss is 0.020337339490652084\n",
      "epoch: 7 step: 1361, loss is 0.013140500523149967\n",
      "epoch: 7 step: 1362, loss is 0.002632353687658906\n",
      "epoch: 7 step: 1363, loss is 0.000433038774644956\n",
      "epoch: 7 step: 1364, loss is 0.002119014970958233\n",
      "epoch: 7 step: 1365, loss is 0.003139057196676731\n",
      "epoch: 7 step: 1366, loss is 0.0008653869153931737\n",
      "epoch: 7 step: 1367, loss is 0.004959454294294119\n",
      "epoch: 7 step: 1368, loss is 0.0029418030753731728\n",
      "epoch: 7 step: 1369, loss is 0.0005018070805817842\n",
      "epoch: 7 step: 1370, loss is 0.001599541399627924\n",
      "epoch: 7 step: 1371, loss is 0.0003735196078196168\n",
      "epoch: 7 step: 1372, loss is 0.0008413157775066793\n",
      "epoch: 7 step: 1373, loss is 0.006003732793033123\n",
      "epoch: 7 step: 1374, loss is 0.004141352139413357\n",
      "epoch: 7 step: 1375, loss is 0.03098726086318493\n",
      "epoch: 7 step: 1376, loss is 0.0007024586666375399\n",
      "epoch: 7 step: 1377, loss is 0.006688919384032488\n",
      "epoch: 7 step: 1378, loss is 0.0408187098801136\n",
      "epoch: 7 step: 1379, loss is 0.016912037506699562\n",
      "epoch: 7 step: 1380, loss is 0.001471004099585116\n",
      "epoch: 7 step: 1381, loss is 0.001927906763739884\n",
      "epoch: 7 step: 1382, loss is 0.015874316915869713\n",
      "epoch: 7 step: 1383, loss is 0.044034846127033234\n",
      "epoch: 7 step: 1384, loss is 1.56041332957102e-05\n",
      "epoch: 7 step: 1385, loss is 0.04096454009413719\n",
      "epoch: 7 step: 1386, loss is 0.0003008214698638767\n",
      "epoch: 7 step: 1387, loss is 0.0002390711015323177\n",
      "epoch: 7 step: 1388, loss is 0.0025708649773150682\n",
      "epoch: 7 step: 1389, loss is 0.000226591742830351\n",
      "epoch: 7 step: 1390, loss is 0.0031620741356164217\n",
      "epoch: 7 step: 1391, loss is 0.00022818856814410537\n",
      "epoch: 7 step: 1392, loss is 0.0037384587340056896\n",
      "epoch: 7 step: 1393, loss is 0.0028348995838314295\n",
      "epoch: 7 step: 1394, loss is 0.0012233786983415484\n",
      "epoch: 7 step: 1395, loss is 0.00021110710804350674\n",
      "epoch: 7 step: 1396, loss is 0.007038790266960859\n",
      "epoch: 7 step: 1397, loss is 0.0003912969841621816\n",
      "epoch: 7 step: 1398, loss is 0.0005511945346370339\n",
      "epoch: 7 step: 1399, loss is 0.0005874863127246499\n",
      "epoch: 7 step: 1400, loss is 0.03881184011697769\n",
      "epoch: 7 step: 1401, loss is 0.00157047132961452\n",
      "epoch: 7 step: 1402, loss is 0.0560227707028389\n",
      "epoch: 7 step: 1403, loss is 0.13996484875679016\n",
      "epoch: 7 step: 1404, loss is 0.0023376168683171272\n",
      "epoch: 7 step: 1405, loss is 3.589372499845922e-05\n",
      "epoch: 7 step: 1406, loss is 0.01287126075476408\n",
      "epoch: 7 step: 1407, loss is 0.03419181704521179\n",
      "epoch: 7 step: 1408, loss is 0.010147860273718834\n",
      "epoch: 7 step: 1409, loss is 0.018644947558641434\n",
      "epoch: 7 step: 1410, loss is 0.0001362409966532141\n",
      "epoch: 7 step: 1411, loss is 0.004638681188225746\n",
      "epoch: 7 step: 1412, loss is 0.015811827033758163\n",
      "epoch: 7 step: 1413, loss is 0.00047406487283296883\n",
      "epoch: 7 step: 1414, loss is 2.274088365084026e-05\n",
      "epoch: 7 step: 1415, loss is 0.007415593136101961\n",
      "epoch: 7 step: 1416, loss is 0.037884458899497986\n",
      "epoch: 7 step: 1417, loss is 0.0019904193468391895\n",
      "epoch: 7 step: 1418, loss is 0.006495171692222357\n",
      "epoch: 7 step: 1419, loss is 0.0183698907494545\n",
      "epoch: 7 step: 1420, loss is 0.0014320199843496084\n",
      "epoch: 7 step: 1421, loss is 0.01226785872131586\n",
      "epoch: 7 step: 1422, loss is 0.011204163543879986\n",
      "epoch: 7 step: 1423, loss is 1.3154450243746396e-05\n",
      "epoch: 7 step: 1424, loss is 0.008820540271699429\n",
      "epoch: 7 step: 1425, loss is 0.005460361950099468\n",
      "epoch: 7 step: 1426, loss is 0.008210493251681328\n",
      "epoch: 7 step: 1427, loss is 0.042011041194200516\n",
      "epoch: 7 step: 1428, loss is 0.08545717597007751\n",
      "epoch: 7 step: 1429, loss is 0.0010481809731572866\n",
      "epoch: 7 step: 1430, loss is 0.0009519106242805719\n",
      "epoch: 7 step: 1431, loss is 0.011076347902417183\n",
      "epoch: 7 step: 1432, loss is 0.031014246866106987\n",
      "epoch: 7 step: 1433, loss is 0.1018250435590744\n",
      "epoch: 7 step: 1434, loss is 0.027005590498447418\n",
      "epoch: 7 step: 1435, loss is 0.0006365413428284228\n",
      "epoch: 7 step: 1436, loss is 0.000582000007852912\n",
      "epoch: 7 step: 1437, loss is 0.003997793886810541\n",
      "epoch: 7 step: 1438, loss is 0.0008466102299280465\n",
      "epoch: 7 step: 1439, loss is 0.01902548409998417\n",
      "epoch: 7 step: 1440, loss is 0.03492613136768341\n",
      "epoch: 7 step: 1441, loss is 0.0016019659815356135\n",
      "epoch: 7 step: 1442, loss is 0.023226721212267876\n",
      "epoch: 7 step: 1443, loss is 0.06794741004705429\n",
      "epoch: 7 step: 1444, loss is 0.005870761349797249\n",
      "epoch: 7 step: 1445, loss is 0.061074502766132355\n",
      "epoch: 7 step: 1446, loss is 0.004799718037247658\n",
      "epoch: 7 step: 1447, loss is 0.0002845742565114051\n",
      "epoch: 7 step: 1448, loss is 0.012222329154610634\n",
      "epoch: 7 step: 1449, loss is 0.030969424173235893\n",
      "epoch: 7 step: 1450, loss is 0.0015548693481832743\n",
      "epoch: 7 step: 1451, loss is 0.007756779436022043\n",
      "epoch: 7 step: 1452, loss is 0.006944932974874973\n",
      "epoch: 7 step: 1453, loss is 0.013168364763259888\n",
      "epoch: 7 step: 1454, loss is 0.0002537246618885547\n",
      "epoch: 7 step: 1455, loss is 0.006446297280490398\n",
      "epoch: 7 step: 1456, loss is 0.022149577736854553\n",
      "epoch: 7 step: 1457, loss is 5.917609450989403e-05\n",
      "epoch: 7 step: 1458, loss is 0.06991537660360336\n",
      "epoch: 7 step: 1459, loss is 0.0005837999051436782\n",
      "epoch: 7 step: 1460, loss is 0.06909418106079102\n",
      "epoch: 7 step: 1461, loss is 0.21100138127803802\n",
      "epoch: 7 step: 1462, loss is 0.01228447537869215\n",
      "epoch: 7 step: 1463, loss is 0.0007719529676251113\n",
      "epoch: 7 step: 1464, loss is 0.00392336817458272\n",
      "epoch: 7 step: 1465, loss is 0.0007131511811167002\n",
      "epoch: 7 step: 1466, loss is 9.40203681238927e-05\n",
      "epoch: 7 step: 1467, loss is 0.0003070492239203304\n",
      "epoch: 7 step: 1468, loss is 0.04830632358789444\n",
      "epoch: 7 step: 1469, loss is 0.04812448471784592\n",
      "epoch: 7 step: 1470, loss is 0.006762620061635971\n",
      "epoch: 7 step: 1471, loss is 0.0002486014855094254\n",
      "epoch: 7 step: 1472, loss is 0.00754462368786335\n",
      "epoch: 7 step: 1473, loss is 0.0016461230115965009\n",
      "epoch: 7 step: 1474, loss is 0.0011991527862846851\n",
      "epoch: 7 step: 1475, loss is 0.00117145711556077\n",
      "epoch: 7 step: 1476, loss is 0.055617380887269974\n",
      "epoch: 7 step: 1477, loss is 4.1505722037982196e-05\n",
      "epoch: 7 step: 1478, loss is 0.004705359227955341\n",
      "epoch: 7 step: 1479, loss is 0.019420377910137177\n",
      "epoch: 7 step: 1480, loss is 0.010074635967612267\n",
      "epoch: 7 step: 1481, loss is 0.000541499990504235\n",
      "epoch: 7 step: 1482, loss is 0.0003242288366891444\n",
      "epoch: 7 step: 1483, loss is 0.1412520557641983\n",
      "epoch: 7 step: 1484, loss is 0.01804736629128456\n",
      "epoch: 7 step: 1485, loss is 0.00015870756760705262\n",
      "epoch: 7 step: 1486, loss is 0.09866927564144135\n",
      "epoch: 7 step: 1487, loss is 0.10555685311555862\n",
      "epoch: 7 step: 1488, loss is 0.006395361851900816\n",
      "epoch: 7 step: 1489, loss is 0.0035489131696522236\n",
      "epoch: 7 step: 1490, loss is 0.0008732319111004472\n",
      "epoch: 7 step: 1491, loss is 9.712921746540815e-05\n",
      "epoch: 7 step: 1492, loss is 2.232518272649031e-05\n",
      "epoch: 7 step: 1493, loss is 0.028122663497924805\n",
      "epoch: 7 step: 1494, loss is 0.0135435089468956\n",
      "epoch: 7 step: 1495, loss is 0.00015349671593867242\n",
      "epoch: 7 step: 1496, loss is 5.756135215051472e-05\n",
      "epoch: 7 step: 1497, loss is 0.0019066280219703913\n",
      "epoch: 7 step: 1498, loss is 0.0002768438425846398\n",
      "epoch: 7 step: 1499, loss is 0.3247699439525604\n",
      "epoch: 7 step: 1500, loss is 0.0018011137144640088\n",
      "epoch: 7 step: 1501, loss is 0.04297638684511185\n",
      "epoch: 7 step: 1502, loss is 0.0668911337852478\n",
      "epoch: 7 step: 1503, loss is 0.007807013113051653\n",
      "epoch: 7 step: 1504, loss is 0.011566336266696453\n",
      "epoch: 7 step: 1505, loss is 0.002366208005696535\n",
      "epoch: 7 step: 1506, loss is 0.20888350903987885\n",
      "epoch: 7 step: 1507, loss is 0.000283953471807763\n",
      "epoch: 7 step: 1508, loss is 0.0002945154265034944\n",
      "epoch: 7 step: 1509, loss is 0.00035792903508991003\n",
      "epoch: 7 step: 1510, loss is 0.0006579144974239171\n",
      "epoch: 7 step: 1511, loss is 0.04077591747045517\n",
      "epoch: 7 step: 1512, loss is 0.000786009943112731\n",
      "epoch: 7 step: 1513, loss is 0.02311757765710354\n",
      "epoch: 7 step: 1514, loss is 0.20648527145385742\n",
      "epoch: 7 step: 1515, loss is 0.013052674010396004\n",
      "epoch: 7 step: 1516, loss is 0.05803515762090683\n",
      "epoch: 7 step: 1517, loss is 0.006365482695400715\n",
      "epoch: 7 step: 1518, loss is 0.04099640995264053\n",
      "epoch: 7 step: 1519, loss is 0.0100579047575593\n",
      "epoch: 7 step: 1520, loss is 0.0005959667614661157\n",
      "epoch: 7 step: 1521, loss is 0.0193887110799551\n",
      "epoch: 7 step: 1522, loss is 0.05426190793514252\n",
      "epoch: 7 step: 1523, loss is 0.00020208806381560862\n",
      "epoch: 7 step: 1524, loss is 0.0018518849974498153\n",
      "epoch: 7 step: 1525, loss is 0.145297110080719\n",
      "epoch: 7 step: 1526, loss is 0.10787328332662582\n",
      "epoch: 7 step: 1527, loss is 0.10133180767297745\n",
      "epoch: 7 step: 1528, loss is 0.040991898626089096\n",
      "epoch: 7 step: 1529, loss is 0.005722088739275932\n",
      "epoch: 7 step: 1530, loss is 0.0047746263444423676\n",
      "epoch: 7 step: 1531, loss is 0.0567459911108017\n",
      "epoch: 7 step: 1532, loss is 0.12258943170309067\n",
      "epoch: 7 step: 1533, loss is 0.01574126072227955\n",
      "epoch: 7 step: 1534, loss is 0.2052025943994522\n",
      "epoch: 7 step: 1535, loss is 0.000561995548196137\n",
      "epoch: 7 step: 1536, loss is 0.051755715161561966\n",
      "epoch: 7 step: 1537, loss is 0.0018891706131398678\n",
      "epoch: 7 step: 1538, loss is 0.003096270142123103\n",
      "epoch: 7 step: 1539, loss is 0.01878548227250576\n",
      "epoch: 7 step: 1540, loss is 0.004683464765548706\n",
      "epoch: 7 step: 1541, loss is 0.0015001135179772973\n",
      "epoch: 7 step: 1542, loss is 0.024138230830430984\n",
      "epoch: 7 step: 1543, loss is 0.006537646986544132\n",
      "epoch: 7 step: 1544, loss is 0.0010668588802218437\n",
      "epoch: 7 step: 1545, loss is 0.005674269516021013\n",
      "epoch: 7 step: 1546, loss is 0.005485196132212877\n",
      "epoch: 7 step: 1547, loss is 0.014052432030439377\n",
      "epoch: 7 step: 1548, loss is 0.0004961224040016532\n",
      "epoch: 7 step: 1549, loss is 0.010173825547099113\n",
      "epoch: 7 step: 1550, loss is 0.001051545375958085\n",
      "epoch: 7 step: 1551, loss is 0.03499513119459152\n",
      "epoch: 7 step: 1552, loss is 0.003768602618947625\n",
      "epoch: 7 step: 1553, loss is 0.09419921040534973\n",
      "epoch: 7 step: 1554, loss is 0.00045658115413971245\n",
      "epoch: 7 step: 1555, loss is 0.11790022253990173\n",
      "epoch: 7 step: 1556, loss is 0.0010910911951214075\n",
      "epoch: 7 step: 1557, loss is 0.07004334032535553\n",
      "epoch: 7 step: 1558, loss is 0.016586527228355408\n",
      "epoch: 7 step: 1559, loss is 0.0007198692765086889\n",
      "epoch: 7 step: 1560, loss is 0.09901025891304016\n",
      "epoch: 7 step: 1561, loss is 0.0005386745324358344\n",
      "epoch: 7 step: 1562, loss is 0.0004142024554312229\n",
      "epoch: 7 step: 1563, loss is 0.06613017618656158\n",
      "epoch: 7 step: 1564, loss is 0.045402802526950836\n",
      "epoch: 7 step: 1565, loss is 0.004122793208807707\n",
      "epoch: 7 step: 1566, loss is 0.008382969535887241\n",
      "epoch: 7 step: 1567, loss is 0.0005258554010652006\n",
      "epoch: 7 step: 1568, loss is 0.028056221082806587\n",
      "epoch: 7 step: 1569, loss is 0.0009299393277615309\n",
      "epoch: 7 step: 1570, loss is 0.018908392637968063\n",
      "epoch: 7 step: 1571, loss is 0.014292346313595772\n",
      "epoch: 7 step: 1572, loss is 0.003971351310610771\n",
      "epoch: 7 step: 1573, loss is 0.0067654019221663475\n",
      "epoch: 7 step: 1574, loss is 0.003877499606460333\n",
      "epoch: 7 step: 1575, loss is 0.00018049523350782692\n",
      "epoch: 7 step: 1576, loss is 0.00451809773221612\n",
      "epoch: 7 step: 1577, loss is 0.0005021237884648144\n",
      "epoch: 7 step: 1578, loss is 0.0010915093589574099\n",
      "epoch: 7 step: 1579, loss is 0.0005975948297418654\n",
      "epoch: 7 step: 1580, loss is 0.019473467022180557\n",
      "epoch: 7 step: 1581, loss is 0.0008956133387982845\n",
      "epoch: 7 step: 1582, loss is 0.0069860173389315605\n",
      "epoch: 7 step: 1583, loss is 0.005648590158671141\n",
      "epoch: 7 step: 1584, loss is 0.005399229470640421\n",
      "epoch: 7 step: 1585, loss is 0.24182166159152985\n",
      "epoch: 7 step: 1586, loss is 0.1293763369321823\n",
      "epoch: 7 step: 1587, loss is 6.922247121110559e-05\n",
      "epoch: 7 step: 1588, loss is 0.015185932628810406\n",
      "epoch: 7 step: 1589, loss is 0.02328926883637905\n",
      "epoch: 7 step: 1590, loss is 0.0013752157101407647\n",
      "epoch: 7 step: 1591, loss is 0.040156908333301544\n",
      "epoch: 7 step: 1592, loss is 0.00023022577806841582\n",
      "epoch: 7 step: 1593, loss is 0.001172709045931697\n",
      "epoch: 7 step: 1594, loss is 0.0025362803135067225\n",
      "epoch: 7 step: 1595, loss is 0.00016196316573768854\n",
      "epoch: 7 step: 1596, loss is 0.0017500006360933185\n",
      "epoch: 7 step: 1597, loss is 0.03168247640132904\n",
      "epoch: 7 step: 1598, loss is 0.013243758119642735\n",
      "epoch: 7 step: 1599, loss is 0.0020542514976114035\n",
      "epoch: 7 step: 1600, loss is 0.0003662266244646162\n",
      "epoch: 7 step: 1601, loss is 0.00842707883566618\n",
      "epoch: 7 step: 1602, loss is 0.00573776988312602\n",
      "epoch: 7 step: 1603, loss is 0.00030420636176131666\n",
      "epoch: 7 step: 1604, loss is 0.004149253014475107\n",
      "epoch: 7 step: 1605, loss is 0.0003754696808755398\n",
      "epoch: 7 step: 1606, loss is 0.0013997697969898582\n",
      "epoch: 7 step: 1607, loss is 0.01528625562787056\n",
      "epoch: 7 step: 1608, loss is 0.00015586751396767795\n",
      "epoch: 7 step: 1609, loss is 0.000292791984975338\n",
      "epoch: 7 step: 1610, loss is 0.007355066016316414\n",
      "epoch: 7 step: 1611, loss is 0.011868893168866634\n",
      "epoch: 7 step: 1612, loss is 0.002991070970892906\n",
      "epoch: 7 step: 1613, loss is 0.00847602542489767\n",
      "epoch: 7 step: 1614, loss is 0.06117163226008415\n",
      "epoch: 7 step: 1615, loss is 0.00021212536375969648\n",
      "epoch: 7 step: 1616, loss is 0.0003941063769161701\n",
      "epoch: 7 step: 1617, loss is 0.18430541455745697\n",
      "epoch: 7 step: 1618, loss is 0.05594209209084511\n",
      "epoch: 7 step: 1619, loss is 0.011077514849603176\n",
      "epoch: 7 step: 1620, loss is 0.0003675772750284523\n",
      "epoch: 7 step: 1621, loss is 0.0005046811420470476\n",
      "epoch: 7 step: 1622, loss is 0.0005099847330711782\n",
      "epoch: 7 step: 1623, loss is 0.08672002702951431\n",
      "epoch: 7 step: 1624, loss is 0.003756282851099968\n",
      "epoch: 7 step: 1625, loss is 0.005230347625911236\n",
      "epoch: 7 step: 1626, loss is 0.07297651469707489\n",
      "epoch: 7 step: 1627, loss is 0.00018430502677801996\n",
      "epoch: 7 step: 1628, loss is 0.02267608419060707\n",
      "epoch: 7 step: 1629, loss is 0.0002266332448925823\n",
      "epoch: 7 step: 1630, loss is 0.009464246220886707\n",
      "epoch: 7 step: 1631, loss is 0.0023770995903760195\n",
      "epoch: 7 step: 1632, loss is 7.988120341906324e-05\n",
      "epoch: 7 step: 1633, loss is 0.1598651260137558\n",
      "epoch: 7 step: 1634, loss is 0.00012382835848256946\n",
      "epoch: 7 step: 1635, loss is 0.0005640343297272921\n",
      "epoch: 7 step: 1636, loss is 0.047493189573287964\n",
      "epoch: 7 step: 1637, loss is 0.0022689946927130222\n",
      "epoch: 7 step: 1638, loss is 0.012884119525551796\n",
      "epoch: 7 step: 1639, loss is 0.04770702123641968\n",
      "epoch: 7 step: 1640, loss is 0.005614466965198517\n",
      "epoch: 7 step: 1641, loss is 0.04233943298459053\n",
      "epoch: 7 step: 1642, loss is 0.011910072527825832\n",
      "epoch: 7 step: 1643, loss is 0.01183590292930603\n",
      "epoch: 7 step: 1644, loss is 0.012887455523014069\n",
      "epoch: 7 step: 1645, loss is 0.0007858691969886422\n",
      "epoch: 7 step: 1646, loss is 0.0016327349003404379\n",
      "epoch: 7 step: 1647, loss is 0.002530132420361042\n",
      "epoch: 7 step: 1648, loss is 0.022983312606811523\n",
      "epoch: 7 step: 1649, loss is 0.002099888864904642\n",
      "epoch: 7 step: 1650, loss is 0.012769095599651337\n",
      "epoch: 7 step: 1651, loss is 0.008565908297896385\n",
      "epoch: 7 step: 1652, loss is 0.13974332809448242\n",
      "epoch: 7 step: 1653, loss is 0.00045572477392852306\n",
      "epoch: 7 step: 1654, loss is 0.0007860877667553723\n",
      "epoch: 7 step: 1655, loss is 0.02877127192914486\n",
      "epoch: 7 step: 1656, loss is 0.010607527568936348\n",
      "epoch: 7 step: 1657, loss is 0.018983380869030952\n",
      "epoch: 7 step: 1658, loss is 0.1678301990032196\n",
      "epoch: 7 step: 1659, loss is 0.0012779543176293373\n",
      "epoch: 7 step: 1660, loss is 0.006128022447228432\n",
      "epoch: 7 step: 1661, loss is 0.011863229796290398\n",
      "epoch: 7 step: 1662, loss is 0.004649792332202196\n",
      "epoch: 7 step: 1663, loss is 0.0013841032050549984\n",
      "epoch: 7 step: 1664, loss is 0.05071535333991051\n",
      "epoch: 7 step: 1665, loss is 0.008768409490585327\n",
      "epoch: 7 step: 1666, loss is 0.0017324392683804035\n",
      "epoch: 7 step: 1667, loss is 0.01420726627111435\n",
      "epoch: 7 step: 1668, loss is 0.00023717623844277114\n",
      "epoch: 7 step: 1669, loss is 0.0009636795148253441\n",
      "epoch: 7 step: 1670, loss is 0.00035663825110532343\n",
      "epoch: 7 step: 1671, loss is 0.0013214526697993279\n",
      "epoch: 7 step: 1672, loss is 0.01648019813001156\n",
      "epoch: 7 step: 1673, loss is 0.0017534828511998057\n",
      "epoch: 7 step: 1674, loss is 0.028076626360416412\n",
      "epoch: 7 step: 1675, loss is 0.0002307301911059767\n",
      "epoch: 7 step: 1676, loss is 0.023452479392290115\n",
      "epoch: 7 step: 1677, loss is 0.0038975304923951626\n",
      "epoch: 7 step: 1678, loss is 0.0014626559568569064\n",
      "epoch: 7 step: 1679, loss is 0.01768551394343376\n",
      "epoch: 7 step: 1680, loss is 0.0011388498824089766\n",
      "epoch: 7 step: 1681, loss is 0.15774643421173096\n",
      "epoch: 7 step: 1682, loss is 0.0027437880635261536\n",
      "epoch: 7 step: 1683, loss is 0.004406545776873827\n",
      "epoch: 7 step: 1684, loss is 0.0032898704521358013\n",
      "epoch: 7 step: 1685, loss is 0.0031500798650085926\n",
      "epoch: 7 step: 1686, loss is 0.0711033046245575\n",
      "epoch: 7 step: 1687, loss is 0.0005866956780664623\n",
      "epoch: 7 step: 1688, loss is 0.0008696395088918507\n",
      "epoch: 7 step: 1689, loss is 0.0003348133759573102\n",
      "epoch: 7 step: 1690, loss is 0.00028496928280219436\n",
      "epoch: 7 step: 1691, loss is 0.0001841279154177755\n",
      "epoch: 7 step: 1692, loss is 4.1726481867954135e-05\n",
      "epoch: 7 step: 1693, loss is 0.00405957642942667\n",
      "epoch: 7 step: 1694, loss is 0.003239639336243272\n",
      "epoch: 7 step: 1695, loss is 0.16999027132987976\n",
      "epoch: 7 step: 1696, loss is 0.0016565518453717232\n",
      "epoch: 7 step: 1697, loss is 0.00030827027512714267\n",
      "epoch: 7 step: 1698, loss is 0.00410037487745285\n",
      "epoch: 7 step: 1699, loss is 0.0003548391396179795\n",
      "epoch: 7 step: 1700, loss is 0.001079433597624302\n",
      "epoch: 7 step: 1701, loss is 8.642295142635703e-05\n",
      "epoch: 7 step: 1702, loss is 0.0094164302572608\n",
      "epoch: 7 step: 1703, loss is 0.00016231728659477085\n",
      "epoch: 7 step: 1704, loss is 0.044245846569538116\n",
      "epoch: 7 step: 1705, loss is 0.006447391118854284\n",
      "epoch: 7 step: 1706, loss is 0.05346689000725746\n",
      "epoch: 7 step: 1707, loss is 0.0130701195448637\n",
      "epoch: 7 step: 1708, loss is 0.0007580773672088981\n",
      "epoch: 7 step: 1709, loss is 0.002635696204379201\n",
      "epoch: 7 step: 1710, loss is 0.020870452746748924\n",
      "epoch: 7 step: 1711, loss is 0.0013761561131104827\n",
      "epoch: 7 step: 1712, loss is 0.029082760214805603\n",
      "epoch: 7 step: 1713, loss is 0.00023123653954826295\n",
      "epoch: 7 step: 1714, loss is 0.0027340822853147984\n",
      "epoch: 7 step: 1715, loss is 0.07105030119419098\n",
      "epoch: 7 step: 1716, loss is 0.0016607155557721853\n",
      "epoch: 7 step: 1717, loss is 0.1846010386943817\n",
      "epoch: 7 step: 1718, loss is 0.0001742500317050144\n",
      "epoch: 7 step: 1719, loss is 0.06715226918458939\n",
      "epoch: 7 step: 1720, loss is 0.00029438038473017514\n",
      "epoch: 7 step: 1721, loss is 0.0004464012454263866\n",
      "epoch: 7 step: 1722, loss is 0.0005980175919830799\n",
      "epoch: 7 step: 1723, loss is 0.00010103388922289014\n",
      "epoch: 7 step: 1724, loss is 0.005163456778973341\n",
      "epoch: 7 step: 1725, loss is 0.004310266580432653\n",
      "epoch: 7 step: 1726, loss is 0.0002674073330126703\n",
      "epoch: 7 step: 1727, loss is 0.004282698500901461\n",
      "epoch: 7 step: 1728, loss is 0.023307878524065018\n",
      "epoch: 7 step: 1729, loss is 0.0013767917407676578\n",
      "epoch: 7 step: 1730, loss is 0.000529420212842524\n",
      "epoch: 7 step: 1731, loss is 0.00021590171672869474\n",
      "epoch: 7 step: 1732, loss is 0.017182854935526848\n",
      "epoch: 7 step: 1733, loss is 0.0073235249146819115\n",
      "epoch: 7 step: 1734, loss is 0.24245116114616394\n",
      "epoch: 7 step: 1735, loss is 0.009778808802366257\n",
      "epoch: 7 step: 1736, loss is 0.0618680939078331\n",
      "epoch: 7 step: 1737, loss is 0.002193232299759984\n",
      "epoch: 7 step: 1738, loss is 0.0023508169688284397\n",
      "epoch: 7 step: 1739, loss is 0.1334546059370041\n",
      "epoch: 7 step: 1740, loss is 0.0008988442132249475\n",
      "epoch: 7 step: 1741, loss is 0.07106711715459824\n",
      "epoch: 7 step: 1742, loss is 0.0024464318994432688\n",
      "epoch: 7 step: 1743, loss is 0.1107337474822998\n",
      "epoch: 7 step: 1744, loss is 0.0002493613283149898\n",
      "epoch: 7 step: 1745, loss is 0.009403885342180729\n",
      "epoch: 7 step: 1746, loss is 0.0003688306314870715\n",
      "epoch: 7 step: 1747, loss is 0.0167362280189991\n",
      "epoch: 7 step: 1748, loss is 0.0024835029616951942\n",
      "epoch: 7 step: 1749, loss is 0.000352585717337206\n",
      "epoch: 7 step: 1750, loss is 0.0015883215237408876\n",
      "epoch: 7 step: 1751, loss is 0.01755121350288391\n",
      "epoch: 7 step: 1752, loss is 0.002449631690979004\n",
      "epoch: 7 step: 1753, loss is 0.0006825760938227177\n",
      "epoch: 7 step: 1754, loss is 0.1072985902428627\n",
      "epoch: 7 step: 1755, loss is 0.11763830482959747\n",
      "epoch: 7 step: 1756, loss is 0.0017746897647157311\n",
      "epoch: 7 step: 1757, loss is 0.13763642311096191\n",
      "epoch: 7 step: 1758, loss is 0.05137769132852554\n",
      "epoch: 7 step: 1759, loss is 0.008728035725653172\n",
      "epoch: 7 step: 1760, loss is 0.020192919299006462\n",
      "epoch: 7 step: 1761, loss is 0.0033897957764565945\n",
      "epoch: 7 step: 1762, loss is 0.00819210335612297\n",
      "epoch: 7 step: 1763, loss is 0.010199191980063915\n",
      "epoch: 7 step: 1764, loss is 0.0002444684796500951\n",
      "epoch: 7 step: 1765, loss is 0.03884068876504898\n",
      "epoch: 7 step: 1766, loss is 0.0002543501032050699\n",
      "epoch: 7 step: 1767, loss is 0.008147919550538063\n",
      "epoch: 7 step: 1768, loss is 0.0005667329533025622\n",
      "epoch: 7 step: 1769, loss is 0.11466457694768906\n",
      "epoch: 7 step: 1770, loss is 0.03603975474834442\n",
      "epoch: 7 step: 1771, loss is 0.0026851100847125053\n",
      "epoch: 7 step: 1772, loss is 0.11204975098371506\n",
      "epoch: 7 step: 1773, loss is 0.009191294200718403\n",
      "epoch: 7 step: 1774, loss is 0.006409151945263147\n",
      "epoch: 7 step: 1775, loss is 0.02706173062324524\n",
      "epoch: 7 step: 1776, loss is 0.00016005110228434205\n",
      "epoch: 7 step: 1777, loss is 0.012753934599459171\n",
      "epoch: 7 step: 1778, loss is 0.0006339389365166426\n",
      "epoch: 7 step: 1779, loss is 0.0026286086067557335\n",
      "epoch: 7 step: 1780, loss is 0.12566107511520386\n",
      "epoch: 7 step: 1781, loss is 0.0021539090666919947\n",
      "epoch: 7 step: 1782, loss is 0.04499952867627144\n",
      "epoch: 7 step: 1783, loss is 0.08950413763523102\n",
      "epoch: 7 step: 1784, loss is 0.003654556116089225\n",
      "epoch: 7 step: 1785, loss is 0.0023732823319733143\n",
      "epoch: 7 step: 1786, loss is 0.001341120689176023\n",
      "epoch: 7 step: 1787, loss is 0.0312594510614872\n",
      "epoch: 7 step: 1788, loss is 0.012885695323348045\n",
      "epoch: 7 step: 1789, loss is 0.004227404948323965\n",
      "epoch: 7 step: 1790, loss is 0.051859673112630844\n",
      "epoch: 7 step: 1791, loss is 0.21692056953907013\n",
      "epoch: 7 step: 1792, loss is 0.0007531497976742685\n",
      "epoch: 7 step: 1793, loss is 0.0027086485642939806\n",
      "epoch: 7 step: 1794, loss is 0.0015028868801891804\n",
      "epoch: 7 step: 1795, loss is 0.00036173337139189243\n",
      "epoch: 7 step: 1796, loss is 0.003225933760404587\n",
      "epoch: 7 step: 1797, loss is 0.004019598476588726\n",
      "epoch: 7 step: 1798, loss is 0.006735661532729864\n",
      "epoch: 7 step: 1799, loss is 0.012232288718223572\n",
      "epoch: 7 step: 1800, loss is 0.007837112061679363\n",
      "epoch: 7 step: 1801, loss is 0.046275682747364044\n",
      "epoch: 7 step: 1802, loss is 0.01887666806578636\n",
      "epoch: 7 step: 1803, loss is 0.0008121515857055783\n",
      "epoch: 7 step: 1804, loss is 0.01503246184438467\n",
      "epoch: 7 step: 1805, loss is 0.0004475732857827097\n",
      "epoch: 7 step: 1806, loss is 0.07316292822360992\n",
      "epoch: 7 step: 1807, loss is 0.00038976361975073814\n",
      "epoch: 7 step: 1808, loss is 0.0364324152469635\n",
      "epoch: 7 step: 1809, loss is 0.03816935792565346\n",
      "epoch: 7 step: 1810, loss is 0.0018243480008095503\n",
      "epoch: 7 step: 1811, loss is 0.0005941122071817517\n",
      "epoch: 7 step: 1812, loss is 0.23851589858531952\n",
      "epoch: 7 step: 1813, loss is 0.00044707042980007827\n",
      "epoch: 7 step: 1814, loss is 0.0006719563971273601\n",
      "epoch: 7 step: 1815, loss is 0.07073070853948593\n",
      "epoch: 7 step: 1816, loss is 0.18290238082408905\n",
      "epoch: 7 step: 1817, loss is 0.07004361599683762\n",
      "epoch: 7 step: 1818, loss is 0.0012950380332767963\n",
      "epoch: 7 step: 1819, loss is 0.015266346745193005\n",
      "epoch: 7 step: 1820, loss is 0.0003872272791340947\n",
      "epoch: 7 step: 1821, loss is 0.00024205294903367758\n",
      "epoch: 7 step: 1822, loss is 0.0009304860141128302\n",
      "epoch: 7 step: 1823, loss is 0.03827325627207756\n",
      "epoch: 7 step: 1824, loss is 0.0008004255942068994\n",
      "epoch: 7 step: 1825, loss is 0.18856190145015717\n",
      "epoch: 7 step: 1826, loss is 0.000326069857692346\n",
      "epoch: 7 step: 1827, loss is 0.19003431499004364\n",
      "epoch: 7 step: 1828, loss is 0.0006827613105997443\n",
      "epoch: 7 step: 1829, loss is 0.11128921061754227\n",
      "epoch: 7 step: 1830, loss is 0.0033622663468122482\n",
      "epoch: 7 step: 1831, loss is 0.032974500209093094\n",
      "epoch: 7 step: 1832, loss is 0.01118281576782465\n",
      "epoch: 7 step: 1833, loss is 0.12879608571529388\n",
      "epoch: 7 step: 1834, loss is 0.09430430084466934\n",
      "epoch: 7 step: 1835, loss is 0.003543845610693097\n",
      "epoch: 7 step: 1836, loss is 0.007146050222218037\n",
      "epoch: 7 step: 1837, loss is 0.008254610002040863\n",
      "epoch: 7 step: 1838, loss is 0.11957542598247528\n",
      "epoch: 7 step: 1839, loss is 0.00019792086095549166\n",
      "epoch: 7 step: 1840, loss is 0.0011511363554745913\n",
      "epoch: 7 step: 1841, loss is 0.0014027499128133059\n",
      "epoch: 7 step: 1842, loss is 0.022357776761054993\n",
      "epoch: 7 step: 1843, loss is 0.11798559874296188\n",
      "epoch: 7 step: 1844, loss is 0.05241966247558594\n",
      "epoch: 7 step: 1845, loss is 0.001750205410644412\n",
      "epoch: 7 step: 1846, loss is 0.018065571784973145\n",
      "epoch: 7 step: 1847, loss is 0.00129728764295578\n",
      "epoch: 7 step: 1848, loss is 0.0030976992566138506\n",
      "epoch: 7 step: 1849, loss is 0.007844826206564903\n",
      "epoch: 7 step: 1850, loss is 0.012607117183506489\n",
      "epoch: 7 step: 1851, loss is 0.04856719821691513\n",
      "epoch: 7 step: 1852, loss is 0.008759650401771069\n",
      "epoch: 7 step: 1853, loss is 0.01622709631919861\n",
      "epoch: 7 step: 1854, loss is 0.0019042787607759237\n",
      "epoch: 7 step: 1855, loss is 0.00491810217499733\n",
      "epoch: 7 step: 1856, loss is 0.17728665471076965\n",
      "epoch: 7 step: 1857, loss is 0.03641560673713684\n",
      "epoch: 7 step: 1858, loss is 0.000879341212566942\n",
      "epoch: 7 step: 1859, loss is 0.00040521431947126985\n",
      "epoch: 7 step: 1860, loss is 0.0029637599363923073\n",
      "epoch: 7 step: 1861, loss is 0.010451367124915123\n",
      "epoch: 7 step: 1862, loss is 0.0002872607728932053\n",
      "epoch: 7 step: 1863, loss is 0.012957022525370121\n",
      "epoch: 7 step: 1864, loss is 0.010000981390476227\n",
      "epoch: 7 step: 1865, loss is 0.0018627052195370197\n",
      "epoch: 7 step: 1866, loss is 4.2041985579999164e-05\n",
      "epoch: 7 step: 1867, loss is 0.01250376645475626\n",
      "epoch: 7 step: 1868, loss is 0.00950013380497694\n",
      "epoch: 7 step: 1869, loss is 0.0011672478867694736\n",
      "epoch: 7 step: 1870, loss is 0.08477312326431274\n",
      "epoch: 7 step: 1871, loss is 0.0005967989563941956\n",
      "epoch: 7 step: 1872, loss is 3.8704467442585155e-05\n",
      "epoch: 7 step: 1873, loss is 0.01786176674067974\n",
      "epoch: 7 step: 1874, loss is 0.0006892439560033381\n",
      "epoch: 7 step: 1875, loss is 0.02155776135623455\n",
      "epoch: 8 step: 1, loss is 0.00028690334875136614\n",
      "epoch: 8 step: 2, loss is 0.009089598432183266\n",
      "epoch: 8 step: 3, loss is 0.00013033389404881746\n",
      "epoch: 8 step: 4, loss is 0.06214071065187454\n",
      "epoch: 8 step: 5, loss is 0.025996465235948563\n",
      "epoch: 8 step: 6, loss is 0.03508085384964943\n",
      "epoch: 8 step: 7, loss is 0.07598487287759781\n",
      "epoch: 8 step: 8, loss is 0.14782123267650604\n",
      "epoch: 8 step: 9, loss is 0.0017680086893960834\n",
      "epoch: 8 step: 10, loss is 0.00020608474733307958\n",
      "epoch: 8 step: 11, loss is 0.04595092311501503\n",
      "epoch: 8 step: 12, loss is 0.004589047282934189\n",
      "epoch: 8 step: 13, loss is 0.0009850813075900078\n",
      "epoch: 8 step: 14, loss is 0.0003901328600477427\n",
      "epoch: 8 step: 15, loss is 0.007023768033832312\n",
      "epoch: 8 step: 16, loss is 0.032306332141160965\n",
      "epoch: 8 step: 17, loss is 0.0004177530063316226\n",
      "epoch: 8 step: 18, loss is 0.00017258411389775574\n",
      "epoch: 8 step: 19, loss is 0.006408943794667721\n",
      "epoch: 8 step: 20, loss is 0.007205619942396879\n",
      "epoch: 8 step: 21, loss is 0.0022257775999605656\n",
      "epoch: 8 step: 22, loss is 0.014121251180768013\n",
      "epoch: 8 step: 23, loss is 0.0015161974588409066\n",
      "epoch: 8 step: 24, loss is 0.011074752546846867\n",
      "epoch: 8 step: 25, loss is 0.00016998001956380904\n",
      "epoch: 8 step: 26, loss is 0.08301424980163574\n",
      "epoch: 8 step: 27, loss is 0.002247406868264079\n",
      "epoch: 8 step: 28, loss is 0.0014989727642387152\n",
      "epoch: 8 step: 29, loss is 0.001995135797187686\n",
      "epoch: 8 step: 30, loss is 0.011335263028740883\n",
      "epoch: 8 step: 31, loss is 0.0026657581329345703\n",
      "epoch: 8 step: 32, loss is 0.0006381520652212203\n",
      "epoch: 8 step: 33, loss is 0.030506018549203873\n",
      "epoch: 8 step: 34, loss is 0.0024276357144117355\n",
      "epoch: 8 step: 35, loss is 0.0015205772360786796\n",
      "epoch: 8 step: 36, loss is 0.0007682990399189293\n",
      "epoch: 8 step: 37, loss is 0.000471264444058761\n",
      "epoch: 8 step: 38, loss is 0.002602573949843645\n",
      "epoch: 8 step: 39, loss is 0.00029547844314947724\n",
      "epoch: 8 step: 40, loss is 0.006939814426004887\n",
      "epoch: 8 step: 41, loss is 0.0033708522096276283\n",
      "epoch: 8 step: 42, loss is 0.00880320742726326\n",
      "epoch: 8 step: 43, loss is 0.0006457790150307119\n",
      "epoch: 8 step: 44, loss is 0.001491837203502655\n",
      "epoch: 8 step: 45, loss is 0.00024187790404539555\n",
      "epoch: 8 step: 46, loss is 0.00036352136521600187\n",
      "epoch: 8 step: 47, loss is 0.003851728979498148\n",
      "epoch: 8 step: 48, loss is 0.039589665830135345\n",
      "epoch: 8 step: 49, loss is 0.0003468949580565095\n",
      "epoch: 8 step: 50, loss is 0.0006414463859982789\n",
      "epoch: 8 step: 51, loss is 0.0001456953614251688\n",
      "epoch: 8 step: 52, loss is 0.0005298046744428575\n",
      "epoch: 8 step: 53, loss is 0.0005227968213148415\n",
      "epoch: 8 step: 54, loss is 0.00025238472153432667\n",
      "epoch: 8 step: 55, loss is 0.025358550250530243\n",
      "epoch: 8 step: 56, loss is 0.045952096581459045\n",
      "epoch: 8 step: 57, loss is 2.0132427380303852e-05\n",
      "epoch: 8 step: 58, loss is 0.00013888307148590684\n",
      "epoch: 8 step: 59, loss is 0.016502145677804947\n",
      "epoch: 8 step: 60, loss is 0.035876233130693436\n",
      "epoch: 8 step: 61, loss is 0.002744947327300906\n",
      "epoch: 8 step: 62, loss is 0.0003801461134571582\n",
      "epoch: 8 step: 63, loss is 0.008941656909883022\n",
      "epoch: 8 step: 64, loss is 0.005691195372492075\n",
      "epoch: 8 step: 65, loss is 0.0005351725849322975\n",
      "epoch: 8 step: 66, loss is 0.225293830037117\n",
      "epoch: 8 step: 67, loss is 0.00010406687943032011\n",
      "epoch: 8 step: 68, loss is 0.004357750061899424\n",
      "epoch: 8 step: 69, loss is 0.043425269424915314\n",
      "epoch: 8 step: 70, loss is 0.0006294882041402161\n",
      "epoch: 8 step: 71, loss is 0.0012361044064164162\n",
      "epoch: 8 step: 72, loss is 0.002542724134400487\n",
      "epoch: 8 step: 73, loss is 0.0019266492454335093\n",
      "epoch: 8 step: 74, loss is 0.013854923658072948\n",
      "epoch: 8 step: 75, loss is 0.0013845820212736726\n",
      "epoch: 8 step: 76, loss is 0.0007275048410519958\n",
      "epoch: 8 step: 77, loss is 0.025316203013062477\n",
      "epoch: 8 step: 78, loss is 0.0018086312338709831\n",
      "epoch: 8 step: 79, loss is 0.0008141744765453041\n",
      "epoch: 8 step: 80, loss is 0.0373331755399704\n",
      "epoch: 8 step: 81, loss is 0.0015283457469195127\n",
      "epoch: 8 step: 82, loss is 0.002095799194648862\n",
      "epoch: 8 step: 83, loss is 0.008533651009202003\n",
      "epoch: 8 step: 84, loss is 6.700362428091466e-05\n",
      "epoch: 8 step: 85, loss is 4.573367550619878e-05\n",
      "epoch: 8 step: 86, loss is 0.00013288654736243188\n",
      "epoch: 8 step: 87, loss is 9.978640446206555e-05\n",
      "epoch: 8 step: 88, loss is 0.026545492932200432\n",
      "epoch: 8 step: 89, loss is 0.0013605792773887515\n",
      "epoch: 8 step: 90, loss is 0.0029045541305094957\n",
      "epoch: 8 step: 91, loss is 0.0002203398325946182\n",
      "epoch: 8 step: 92, loss is 0.0008117620018310845\n",
      "epoch: 8 step: 93, loss is 0.001645364100113511\n",
      "epoch: 8 step: 94, loss is 0.07897337526082993\n",
      "epoch: 8 step: 95, loss is 0.0012798869283869863\n",
      "epoch: 8 step: 96, loss is 0.06004025414586067\n",
      "epoch: 8 step: 97, loss is 6.864991155453026e-05\n",
      "epoch: 8 step: 98, loss is 0.0025565929245203733\n",
      "epoch: 8 step: 99, loss is 0.028943665325641632\n",
      "epoch: 8 step: 100, loss is 3.2962019758997485e-05\n",
      "epoch: 8 step: 101, loss is 0.018313182517886162\n",
      "epoch: 8 step: 102, loss is 0.0007073356537148356\n",
      "epoch: 8 step: 103, loss is 0.00011174763494636863\n",
      "epoch: 8 step: 104, loss is 0.003879929892718792\n",
      "epoch: 8 step: 105, loss is 0.000192349441931583\n",
      "epoch: 8 step: 106, loss is 0.000966253865044564\n",
      "epoch: 8 step: 107, loss is 0.004437820985913277\n",
      "epoch: 8 step: 108, loss is 0.001638917252421379\n",
      "epoch: 8 step: 109, loss is 0.00036603177431970835\n",
      "epoch: 8 step: 110, loss is 0.14152854681015015\n",
      "epoch: 8 step: 111, loss is 0.0025053664576262236\n",
      "epoch: 8 step: 112, loss is 0.05707462877035141\n",
      "epoch: 8 step: 113, loss is 0.00042266753735020757\n",
      "epoch: 8 step: 114, loss is 0.004869807977229357\n",
      "epoch: 8 step: 115, loss is 0.005684359930455685\n",
      "epoch: 8 step: 116, loss is 0.002987385494634509\n",
      "epoch: 8 step: 117, loss is 0.006341414526104927\n",
      "epoch: 8 step: 118, loss is 0.09476995468139648\n",
      "epoch: 8 step: 119, loss is 0.044902779161930084\n",
      "epoch: 8 step: 120, loss is 0.02043965272605419\n",
      "epoch: 8 step: 121, loss is 0.000557207502424717\n",
      "epoch: 8 step: 122, loss is 0.002717204624786973\n",
      "epoch: 8 step: 123, loss is 0.0004821878974325955\n",
      "epoch: 8 step: 124, loss is 0.22569017112255096\n",
      "epoch: 8 step: 125, loss is 0.005868447478860617\n",
      "epoch: 8 step: 126, loss is 0.0003105132782366127\n",
      "epoch: 8 step: 127, loss is 0.0150953633710742\n",
      "epoch: 8 step: 128, loss is 0.05439303070306778\n",
      "epoch: 8 step: 129, loss is 0.00033792416797950864\n",
      "epoch: 8 step: 130, loss is 0.00016216686344705522\n",
      "epoch: 8 step: 131, loss is 0.0007841857732273638\n",
      "epoch: 8 step: 132, loss is 9.36863652896136e-05\n",
      "epoch: 8 step: 133, loss is 0.0003731891920324415\n",
      "epoch: 8 step: 134, loss is 0.0002630820672493428\n",
      "epoch: 8 step: 135, loss is 0.0549057200551033\n",
      "epoch: 8 step: 136, loss is 0.0387592539191246\n",
      "epoch: 8 step: 137, loss is 0.0033312556333839893\n",
      "epoch: 8 step: 138, loss is 0.0002789713616948575\n",
      "epoch: 8 step: 139, loss is 0.027457205578684807\n",
      "epoch: 8 step: 140, loss is 0.0009596735471859574\n",
      "epoch: 8 step: 141, loss is 0.131267711520195\n",
      "epoch: 8 step: 142, loss is 0.0003014035173691809\n",
      "epoch: 8 step: 143, loss is 0.0010114392498508096\n",
      "epoch: 8 step: 144, loss is 0.033379651606082916\n",
      "epoch: 8 step: 145, loss is 0.06978092342615128\n",
      "epoch: 8 step: 146, loss is 9.188732656184584e-05\n",
      "epoch: 8 step: 147, loss is 8.270778198493645e-05\n",
      "epoch: 8 step: 148, loss is 0.0017871647141873837\n",
      "epoch: 8 step: 149, loss is 0.007964585907757282\n",
      "epoch: 8 step: 150, loss is 0.00021872534125577658\n",
      "epoch: 8 step: 151, loss is 0.004779466427862644\n",
      "epoch: 8 step: 152, loss is 0.0006046310300007463\n",
      "epoch: 8 step: 153, loss is 0.14067155122756958\n",
      "epoch: 8 step: 154, loss is 0.014093506149947643\n",
      "epoch: 8 step: 155, loss is 0.034091826528310776\n",
      "epoch: 8 step: 156, loss is 0.35148346424102783\n",
      "epoch: 8 step: 157, loss is 0.0016316730761900544\n",
      "epoch: 8 step: 158, loss is 0.007220139726996422\n",
      "epoch: 8 step: 159, loss is 0.04163466766476631\n",
      "epoch: 8 step: 160, loss is 6.468532228609547e-05\n",
      "epoch: 8 step: 161, loss is 0.00025993038434535265\n",
      "epoch: 8 step: 162, loss is 0.026430778205394745\n",
      "epoch: 8 step: 163, loss is 0.009496945887804031\n",
      "epoch: 8 step: 164, loss is 0.0005379188223741949\n",
      "epoch: 8 step: 165, loss is 0.08877202123403549\n",
      "epoch: 8 step: 166, loss is 0.006757373921573162\n",
      "epoch: 8 step: 167, loss is 0.0020240333396941423\n",
      "epoch: 8 step: 168, loss is 0.0005285825463943183\n",
      "epoch: 8 step: 169, loss is 0.08532223850488663\n",
      "epoch: 8 step: 170, loss is 0.015936603769659996\n",
      "epoch: 8 step: 171, loss is 0.15143541991710663\n",
      "epoch: 8 step: 172, loss is 0.0038608277682214975\n",
      "epoch: 8 step: 173, loss is 0.0063505955040454865\n",
      "epoch: 8 step: 174, loss is 0.007810794282704592\n",
      "epoch: 8 step: 175, loss is 0.0035245970357209444\n",
      "epoch: 8 step: 176, loss is 0.0005366296391002834\n",
      "epoch: 8 step: 177, loss is 0.0005629515508189797\n",
      "epoch: 8 step: 178, loss is 0.002886379137635231\n",
      "epoch: 8 step: 179, loss is 0.05180862918496132\n",
      "epoch: 8 step: 180, loss is 0.02667398750782013\n",
      "epoch: 8 step: 181, loss is 0.001339521026238799\n",
      "epoch: 8 step: 182, loss is 0.005159563850611448\n",
      "epoch: 8 step: 183, loss is 0.001274938229471445\n",
      "epoch: 8 step: 184, loss is 0.0004737969138659537\n",
      "epoch: 8 step: 185, loss is 0.0014256645226851106\n",
      "epoch: 8 step: 186, loss is 0.004537438973784447\n",
      "epoch: 8 step: 187, loss is 0.03075561299920082\n",
      "epoch: 8 step: 188, loss is 0.14206045866012573\n",
      "epoch: 8 step: 189, loss is 0.005752368364483118\n",
      "epoch: 8 step: 190, loss is 0.003554715309292078\n",
      "epoch: 8 step: 191, loss is 0.009008198045194149\n",
      "epoch: 8 step: 192, loss is 0.1218365728855133\n",
      "epoch: 8 step: 193, loss is 0.0004161900142207742\n",
      "epoch: 8 step: 194, loss is 0.002702313009649515\n",
      "epoch: 8 step: 195, loss is 0.002153603592887521\n",
      "epoch: 8 step: 196, loss is 0.00017648248467594385\n",
      "epoch: 8 step: 197, loss is 0.07668376713991165\n",
      "epoch: 8 step: 198, loss is 0.029528846964240074\n",
      "epoch: 8 step: 199, loss is 0.00015083297330420464\n",
      "epoch: 8 step: 200, loss is 0.009548801928758621\n",
      "epoch: 8 step: 201, loss is 0.0035239048302173615\n",
      "epoch: 8 step: 202, loss is 0.17997333407402039\n",
      "epoch: 8 step: 203, loss is 0.0027252808213233948\n",
      "epoch: 8 step: 204, loss is 0.00038013921584933996\n",
      "epoch: 8 step: 205, loss is 0.007542596198618412\n",
      "epoch: 8 step: 206, loss is 0.0018263093661516905\n",
      "epoch: 8 step: 207, loss is 0.028319843113422394\n",
      "epoch: 8 step: 208, loss is 0.003787935245782137\n",
      "epoch: 8 step: 209, loss is 0.048575036227703094\n",
      "epoch: 8 step: 210, loss is 0.0038849390111863613\n",
      "epoch: 8 step: 211, loss is 0.0011093160137534142\n",
      "epoch: 8 step: 212, loss is 0.03202829509973526\n",
      "epoch: 8 step: 213, loss is 0.013987218029797077\n",
      "epoch: 8 step: 214, loss is 0.003790065413340926\n",
      "epoch: 8 step: 215, loss is 0.0008985376334749162\n",
      "epoch: 8 step: 216, loss is 0.10476875305175781\n",
      "epoch: 8 step: 217, loss is 0.0020208831410855055\n",
      "epoch: 8 step: 218, loss is 0.0016576587222516537\n",
      "epoch: 8 step: 219, loss is 0.00025563029339537024\n",
      "epoch: 8 step: 220, loss is 0.00030545360641554\n",
      "epoch: 8 step: 221, loss is 0.0032185919117182493\n",
      "epoch: 8 step: 222, loss is 0.003589614061638713\n",
      "epoch: 8 step: 223, loss is 0.0009110019891522825\n",
      "epoch: 8 step: 224, loss is 0.02548183873295784\n",
      "epoch: 8 step: 225, loss is 0.0007887468091212213\n",
      "epoch: 8 step: 226, loss is 0.029632896184921265\n",
      "epoch: 8 step: 227, loss is 0.007367132697254419\n",
      "epoch: 8 step: 228, loss is 0.00530598871409893\n",
      "epoch: 8 step: 229, loss is 0.0072667780332267284\n",
      "epoch: 8 step: 230, loss is 0.09783412516117096\n",
      "epoch: 8 step: 231, loss is 0.007072927430272102\n",
      "epoch: 8 step: 232, loss is 0.03380792587995529\n",
      "epoch: 8 step: 233, loss is 0.03220770135521889\n",
      "epoch: 8 step: 234, loss is 0.0008428602013736963\n",
      "epoch: 8 step: 235, loss is 0.02574426680803299\n",
      "epoch: 8 step: 236, loss is 0.0007287155603989959\n",
      "epoch: 8 step: 237, loss is 0.012407111935317516\n",
      "epoch: 8 step: 238, loss is 0.0002818501670844853\n",
      "epoch: 8 step: 239, loss is 0.0014837177004665136\n",
      "epoch: 8 step: 240, loss is 0.15479475259780884\n",
      "epoch: 8 step: 241, loss is 0.13530701398849487\n",
      "epoch: 8 step: 242, loss is 0.003933812491595745\n",
      "epoch: 8 step: 243, loss is 0.0007602765108458698\n",
      "epoch: 8 step: 244, loss is 0.00022641070245299488\n",
      "epoch: 8 step: 245, loss is 0.0013884127838537097\n",
      "epoch: 8 step: 246, loss is 0.0010990918381139636\n",
      "epoch: 8 step: 247, loss is 0.0005993004888296127\n",
      "epoch: 8 step: 248, loss is 0.014791314490139484\n",
      "epoch: 8 step: 249, loss is 0.03491206839680672\n",
      "epoch: 8 step: 250, loss is 0.014148227870464325\n",
      "epoch: 8 step: 251, loss is 0.002174261724576354\n",
      "epoch: 8 step: 252, loss is 0.005135667510330677\n",
      "epoch: 8 step: 253, loss is 0.000525946612469852\n",
      "epoch: 8 step: 254, loss is 0.00531886238604784\n",
      "epoch: 8 step: 255, loss is 0.004194239154458046\n",
      "epoch: 8 step: 256, loss is 0.010100344195961952\n",
      "epoch: 8 step: 257, loss is 0.0015553763369098306\n",
      "epoch: 8 step: 258, loss is 0.0035122104454785585\n",
      "epoch: 8 step: 259, loss is 0.00404152600094676\n",
      "epoch: 8 step: 260, loss is 0.00017623922030907124\n",
      "epoch: 8 step: 261, loss is 0.00029259358416311443\n",
      "epoch: 8 step: 262, loss is 0.0019771833904087543\n",
      "epoch: 8 step: 263, loss is 0.0011401999508962035\n",
      "epoch: 8 step: 264, loss is 0.0018558230949565768\n",
      "epoch: 8 step: 265, loss is 0.0022039341274648905\n",
      "epoch: 8 step: 266, loss is 0.01901467889547348\n",
      "epoch: 8 step: 267, loss is 0.002829595934599638\n",
      "epoch: 8 step: 268, loss is 0.0024759049993008375\n",
      "epoch: 8 step: 269, loss is 0.0042932769283652306\n",
      "epoch: 8 step: 270, loss is 0.007354896515607834\n",
      "epoch: 8 step: 271, loss is 0.00012227438855916262\n",
      "epoch: 8 step: 272, loss is 0.0003697561623994261\n",
      "epoch: 8 step: 273, loss is 0.0006142249912954867\n",
      "epoch: 8 step: 274, loss is 0.0034162397496402264\n",
      "epoch: 8 step: 275, loss is 0.00656321132555604\n",
      "epoch: 8 step: 276, loss is 0.1627216935157776\n",
      "epoch: 8 step: 277, loss is 0.026982607319951057\n",
      "epoch: 8 step: 278, loss is 0.00020363192015793175\n",
      "epoch: 8 step: 279, loss is 0.028891952708363533\n",
      "epoch: 8 step: 280, loss is 0.0024218137841671705\n",
      "epoch: 8 step: 281, loss is 0.00012100148887839168\n",
      "epoch: 8 step: 282, loss is 0.00010898097389144823\n",
      "epoch: 8 step: 283, loss is 0.06540492177009583\n",
      "epoch: 8 step: 284, loss is 0.006493289023637772\n",
      "epoch: 8 step: 285, loss is 0.0007875838200561702\n",
      "epoch: 8 step: 286, loss is 0.00030132842948660254\n",
      "epoch: 8 step: 287, loss is 0.021770067512989044\n",
      "epoch: 8 step: 288, loss is 0.01690342277288437\n",
      "epoch: 8 step: 289, loss is 0.0016447033267468214\n",
      "epoch: 8 step: 290, loss is 0.006177127361297607\n",
      "epoch: 8 step: 291, loss is 0.07690846920013428\n",
      "epoch: 8 step: 292, loss is 0.009712358936667442\n",
      "epoch: 8 step: 293, loss is 0.0003621216455940157\n",
      "epoch: 8 step: 294, loss is 0.012082374654710293\n",
      "epoch: 8 step: 295, loss is 0.020690124481916428\n",
      "epoch: 8 step: 296, loss is 0.0004061596991959959\n",
      "epoch: 8 step: 297, loss is 0.016507860273122787\n",
      "epoch: 8 step: 298, loss is 0.007513192482292652\n",
      "epoch: 8 step: 299, loss is 0.003927966579794884\n",
      "epoch: 8 step: 300, loss is 0.003728114068508148\n",
      "epoch: 8 step: 301, loss is 0.0005364103708416224\n",
      "epoch: 8 step: 302, loss is 0.03521551564335823\n",
      "epoch: 8 step: 303, loss is 0.0029304097406566143\n",
      "epoch: 8 step: 304, loss is 0.007650122046470642\n",
      "epoch: 8 step: 305, loss is 0.023036370053887367\n",
      "epoch: 8 step: 306, loss is 0.0005257737939245999\n",
      "epoch: 8 step: 307, loss is 0.0012360636610537767\n",
      "epoch: 8 step: 308, loss is 0.001981943380087614\n",
      "epoch: 8 step: 309, loss is 0.000407501618610695\n",
      "epoch: 8 step: 310, loss is 0.0025988181587308645\n",
      "epoch: 8 step: 311, loss is 0.10395167768001556\n",
      "epoch: 8 step: 312, loss is 0.0003251468588132411\n",
      "epoch: 8 step: 313, loss is 0.002354099415242672\n",
      "epoch: 8 step: 314, loss is 0.007361164316534996\n",
      "epoch: 8 step: 315, loss is 0.0003306898579467088\n",
      "epoch: 8 step: 316, loss is 0.05828801915049553\n",
      "epoch: 8 step: 317, loss is 0.0011382338125258684\n",
      "epoch: 8 step: 318, loss is 6.487379141617566e-05\n",
      "epoch: 8 step: 319, loss is 0.26457616686820984\n",
      "epoch: 8 step: 320, loss is 0.10335370153188705\n",
      "epoch: 8 step: 321, loss is 8.55803518788889e-05\n",
      "epoch: 8 step: 322, loss is 0.002072475850582123\n",
      "epoch: 8 step: 323, loss is 0.00024629762629047036\n",
      "epoch: 8 step: 324, loss is 0.027544237673282623\n",
      "epoch: 8 step: 325, loss is 0.006710925605148077\n",
      "epoch: 8 step: 326, loss is 0.002488266909494996\n",
      "epoch: 8 step: 327, loss is 0.04423202574253082\n",
      "epoch: 8 step: 328, loss is 0.006185526493936777\n",
      "epoch: 8 step: 329, loss is 0.0058863661251962185\n",
      "epoch: 8 step: 330, loss is 0.0018996945582330227\n",
      "epoch: 8 step: 331, loss is 0.001739179715514183\n",
      "epoch: 8 step: 332, loss is 0.0009879316203296185\n",
      "epoch: 8 step: 333, loss is 0.00908779539167881\n",
      "epoch: 8 step: 334, loss is 0.012087797746062279\n",
      "epoch: 8 step: 335, loss is 0.00046033048420213163\n",
      "epoch: 8 step: 336, loss is 0.00022332130174618214\n",
      "epoch: 8 step: 337, loss is 0.0006266669370234013\n",
      "epoch: 8 step: 338, loss is 0.004195335786789656\n",
      "epoch: 8 step: 339, loss is 0.0008745583472773433\n",
      "epoch: 8 step: 340, loss is 0.08741927146911621\n",
      "epoch: 8 step: 341, loss is 0.0011035992065444589\n",
      "epoch: 8 step: 342, loss is 0.000582828011829406\n",
      "epoch: 8 step: 343, loss is 0.007910097017884254\n",
      "epoch: 8 step: 344, loss is 0.02564425766468048\n",
      "epoch: 8 step: 345, loss is 0.028119387105107307\n",
      "epoch: 8 step: 346, loss is 0.0007662255666218698\n",
      "epoch: 8 step: 347, loss is 5.283253631205298e-05\n",
      "epoch: 8 step: 348, loss is 0.0003791596391238272\n",
      "epoch: 8 step: 349, loss is 0.0005606963532045484\n",
      "epoch: 8 step: 350, loss is 0.0002177113783545792\n",
      "epoch: 8 step: 351, loss is 0.00043436119449324906\n",
      "epoch: 8 step: 352, loss is 0.0022289028856903315\n",
      "epoch: 8 step: 353, loss is 0.00020521963597275317\n",
      "epoch: 8 step: 354, loss is 0.00022710089979227632\n",
      "epoch: 8 step: 355, loss is 0.0044609676115214825\n",
      "epoch: 8 step: 356, loss is 0.0018408659379929304\n",
      "epoch: 8 step: 357, loss is 0.003812073729932308\n",
      "epoch: 8 step: 358, loss is 0.00016098229389172047\n",
      "epoch: 8 step: 359, loss is 0.004149109125137329\n",
      "epoch: 8 step: 360, loss is 0.06072520092129707\n",
      "epoch: 8 step: 361, loss is 0.0009668248821981251\n",
      "epoch: 8 step: 362, loss is 0.0002362889499636367\n",
      "epoch: 8 step: 363, loss is 0.04331732913851738\n",
      "epoch: 8 step: 364, loss is 0.00015468522906303406\n",
      "epoch: 8 step: 365, loss is 0.0008832783787511289\n",
      "epoch: 8 step: 366, loss is 0.0035743655171245337\n",
      "epoch: 8 step: 367, loss is 0.0028824028559029102\n",
      "epoch: 8 step: 368, loss is 0.0008056610822677612\n",
      "epoch: 8 step: 369, loss is 0.003383421106263995\n",
      "epoch: 8 step: 370, loss is 0.00021534702682401985\n",
      "epoch: 8 step: 371, loss is 0.0005858120857737958\n",
      "epoch: 8 step: 372, loss is 0.0006977197481319308\n",
      "epoch: 8 step: 373, loss is 0.0032347894739359617\n",
      "epoch: 8 step: 374, loss is 3.461287633399479e-05\n",
      "epoch: 8 step: 375, loss is 0.0008292821003124118\n",
      "epoch: 8 step: 376, loss is 0.004971567075699568\n",
      "epoch: 8 step: 377, loss is 0.0008741134079173207\n",
      "epoch: 8 step: 378, loss is 0.02547006867825985\n",
      "epoch: 8 step: 379, loss is 0.01914166659116745\n",
      "epoch: 8 step: 380, loss is 0.0015726580750197172\n",
      "epoch: 8 step: 381, loss is 0.0017114864895120263\n",
      "epoch: 8 step: 382, loss is 0.011910670436918736\n",
      "epoch: 8 step: 383, loss is 0.011831811629235744\n",
      "epoch: 8 step: 384, loss is 0.0038035190664231777\n",
      "epoch: 8 step: 385, loss is 0.008915731683373451\n",
      "epoch: 8 step: 386, loss is 0.0007540208171121776\n",
      "epoch: 8 step: 387, loss is 0.00016770942602306604\n",
      "epoch: 8 step: 388, loss is 0.002758278511464596\n",
      "epoch: 8 step: 389, loss is 0.0027964096516370773\n",
      "epoch: 8 step: 390, loss is 0.003925495781004429\n",
      "epoch: 8 step: 391, loss is 0.0018137345323339105\n",
      "epoch: 8 step: 392, loss is 0.0025893384590744972\n",
      "epoch: 8 step: 393, loss is 0.019732344895601273\n",
      "epoch: 8 step: 394, loss is 0.009733191691339016\n",
      "epoch: 8 step: 395, loss is 0.000974380353000015\n",
      "epoch: 8 step: 396, loss is 1.8791006368701346e-05\n",
      "epoch: 8 step: 397, loss is 0.011346345767378807\n",
      "epoch: 8 step: 398, loss is 0.0002900365216191858\n",
      "epoch: 8 step: 399, loss is 0.00030506725306622684\n",
      "epoch: 8 step: 400, loss is 0.002959001110866666\n",
      "epoch: 8 step: 401, loss is 0.002357248682528734\n",
      "epoch: 8 step: 402, loss is 0.0004234599764458835\n",
      "epoch: 8 step: 403, loss is 0.0020529821049422026\n",
      "epoch: 8 step: 404, loss is 0.0005977199762128294\n",
      "epoch: 8 step: 405, loss is 0.00026958194212056696\n",
      "epoch: 8 step: 406, loss is 0.024228159338235855\n",
      "epoch: 8 step: 407, loss is 0.000224198229261674\n",
      "epoch: 8 step: 408, loss is 6.763901910744607e-05\n",
      "epoch: 8 step: 409, loss is 1.1265626199019607e-05\n",
      "epoch: 8 step: 410, loss is 3.436919359955937e-05\n",
      "epoch: 8 step: 411, loss is 0.00453598378226161\n",
      "epoch: 8 step: 412, loss is 0.001751038944348693\n",
      "epoch: 8 step: 413, loss is 0.0005194361438043416\n",
      "epoch: 8 step: 414, loss is 0.011216256767511368\n",
      "epoch: 8 step: 415, loss is 0.0008296623127534986\n",
      "epoch: 8 step: 416, loss is 0.0013810684904456139\n",
      "epoch: 8 step: 417, loss is 0.006467732600867748\n",
      "epoch: 8 step: 418, loss is 0.01640007086098194\n",
      "epoch: 8 step: 419, loss is 0.018048658967018127\n",
      "epoch: 8 step: 420, loss is 0.20958508551120758\n",
      "epoch: 8 step: 421, loss is 0.001932868268340826\n",
      "epoch: 8 step: 422, loss is 0.00020715653954539448\n",
      "epoch: 8 step: 423, loss is 0.018387334421277046\n",
      "epoch: 8 step: 424, loss is 0.00048739317571744323\n",
      "epoch: 8 step: 425, loss is 0.0006697527132928371\n",
      "epoch: 8 step: 426, loss is 5.342693839338608e-05\n",
      "epoch: 8 step: 427, loss is 0.0024886566679924726\n",
      "epoch: 8 step: 428, loss is 0.005891595035791397\n",
      "epoch: 8 step: 429, loss is 0.002040100982412696\n",
      "epoch: 8 step: 430, loss is 0.00018143295892514288\n",
      "epoch: 8 step: 431, loss is 0.0004954758915118873\n",
      "epoch: 8 step: 432, loss is 0.0009866947075352073\n",
      "epoch: 8 step: 433, loss is 2.871104152291082e-05\n",
      "epoch: 8 step: 434, loss is 0.004045237321406603\n",
      "epoch: 8 step: 435, loss is 0.10369489341974258\n",
      "epoch: 8 step: 436, loss is 0.0003118201857432723\n",
      "epoch: 8 step: 437, loss is 0.004061864223331213\n",
      "epoch: 8 step: 438, loss is 0.0032859505154192448\n",
      "epoch: 8 step: 439, loss is 0.000831135839689523\n",
      "epoch: 8 step: 440, loss is 0.001617924775928259\n",
      "epoch: 8 step: 441, loss is 0.0005893283523619175\n",
      "epoch: 8 step: 442, loss is 0.06158582866191864\n",
      "epoch: 8 step: 443, loss is 0.0011703514028340578\n",
      "epoch: 8 step: 444, loss is 0.007562621962279081\n",
      "epoch: 8 step: 445, loss is 0.0027581010945141315\n",
      "epoch: 8 step: 446, loss is 0.00034335264354012907\n",
      "epoch: 8 step: 447, loss is 0.016861552372574806\n",
      "epoch: 8 step: 448, loss is 0.0001066042241291143\n",
      "epoch: 8 step: 449, loss is 0.00901106558740139\n",
      "epoch: 8 step: 450, loss is 0.011853983625769615\n",
      "epoch: 8 step: 451, loss is 0.0006322108674794436\n",
      "epoch: 8 step: 452, loss is 0.0013748626224696636\n",
      "epoch: 8 step: 453, loss is 0.0033980044536292553\n",
      "epoch: 8 step: 454, loss is 8.728287502890453e-05\n",
      "epoch: 8 step: 455, loss is 0.000441706768469885\n",
      "epoch: 8 step: 456, loss is 0.0002804283576551825\n",
      "epoch: 8 step: 457, loss is 0.008979648351669312\n",
      "epoch: 8 step: 458, loss is 0.011996746994554996\n",
      "epoch: 8 step: 459, loss is 0.00017173148808069527\n",
      "epoch: 8 step: 460, loss is 0.0009711231687106192\n",
      "epoch: 8 step: 461, loss is 1.5593108400935307e-05\n",
      "epoch: 8 step: 462, loss is 0.014143139123916626\n",
      "epoch: 8 step: 463, loss is 0.016029013320803642\n",
      "epoch: 8 step: 464, loss is 0.0010742752347141504\n",
      "epoch: 8 step: 465, loss is 0.04721343517303467\n",
      "epoch: 8 step: 466, loss is 1.8977396393893287e-05\n",
      "epoch: 8 step: 467, loss is 0.002612682990729809\n",
      "epoch: 8 step: 468, loss is 7.937950431369245e-05\n",
      "epoch: 8 step: 469, loss is 8.707625966053456e-05\n",
      "epoch: 8 step: 470, loss is 0.01641915738582611\n",
      "epoch: 8 step: 471, loss is 0.00010905706585617736\n",
      "epoch: 8 step: 472, loss is 0.004071262665092945\n",
      "epoch: 8 step: 473, loss is 6.515438144560903e-05\n",
      "epoch: 8 step: 474, loss is 5.756745667895302e-05\n",
      "epoch: 8 step: 475, loss is 0.0004936268087476492\n",
      "epoch: 8 step: 476, loss is 0.019822388887405396\n",
      "epoch: 8 step: 477, loss is 3.50529735442251e-05\n",
      "epoch: 8 step: 478, loss is 0.0005866438150405884\n",
      "epoch: 8 step: 479, loss is 0.0008018515654839575\n",
      "epoch: 8 step: 480, loss is 0.04144344851374626\n",
      "epoch: 8 step: 481, loss is 8.239797170972452e-05\n",
      "epoch: 8 step: 482, loss is 0.0005909806350246072\n",
      "epoch: 8 step: 483, loss is 0.00023501995019614697\n",
      "epoch: 8 step: 484, loss is 0.0001571420580148697\n",
      "epoch: 8 step: 485, loss is 0.016996877267956734\n",
      "epoch: 8 step: 486, loss is 0.00024764350382611156\n",
      "epoch: 8 step: 487, loss is 0.0012878314591944218\n",
      "epoch: 8 step: 488, loss is 0.008767050690948963\n",
      "epoch: 8 step: 489, loss is 0.0002665323845576495\n",
      "epoch: 8 step: 490, loss is 0.0005273693241178989\n",
      "epoch: 8 step: 491, loss is 0.011682492680847645\n",
      "epoch: 8 step: 492, loss is 0.016206536442041397\n",
      "epoch: 8 step: 493, loss is 0.00010297750850440934\n",
      "epoch: 8 step: 494, loss is 0.005525656975805759\n",
      "epoch: 8 step: 495, loss is 0.0013425312936306\n",
      "epoch: 8 step: 496, loss is 0.056816112250089645\n",
      "epoch: 8 step: 497, loss is 0.00861388724297285\n",
      "epoch: 8 step: 498, loss is 0.010293569415807724\n",
      "epoch: 8 step: 499, loss is 0.07881343364715576\n",
      "epoch: 8 step: 500, loss is 0.00036602342152036726\n",
      "epoch: 8 step: 501, loss is 0.007124449126422405\n",
      "epoch: 8 step: 502, loss is 0.00098556955344975\n",
      "epoch: 8 step: 503, loss is 0.0002823931572493166\n",
      "epoch: 8 step: 504, loss is 0.000712015840690583\n",
      "epoch: 8 step: 505, loss is 0.00016051337297540158\n",
      "epoch: 8 step: 506, loss is 0.0007455893792212009\n",
      "epoch: 8 step: 507, loss is 0.001145168673247099\n",
      "epoch: 8 step: 508, loss is 0.019320448860526085\n",
      "epoch: 8 step: 509, loss is 0.00020754014258272946\n",
      "epoch: 8 step: 510, loss is 0.014584660530090332\n",
      "epoch: 8 step: 511, loss is 0.004868361167609692\n",
      "epoch: 8 step: 512, loss is 0.019948258996009827\n",
      "epoch: 8 step: 513, loss is 0.002947099506855011\n",
      "epoch: 8 step: 514, loss is 0.004866480827331543\n",
      "epoch: 8 step: 515, loss is 0.0005035082576796412\n",
      "epoch: 8 step: 516, loss is 0.019254250451922417\n",
      "epoch: 8 step: 517, loss is 0.0003057386784348637\n",
      "epoch: 8 step: 518, loss is 0.030034862458705902\n",
      "epoch: 8 step: 519, loss is 0.0020061468239873648\n",
      "epoch: 8 step: 520, loss is 0.0006391326314769685\n",
      "epoch: 8 step: 521, loss is 0.0033738482743501663\n",
      "epoch: 8 step: 522, loss is 0.0019920780323445797\n",
      "epoch: 8 step: 523, loss is 0.025821272283792496\n",
      "epoch: 8 step: 524, loss is 0.000320419087074697\n",
      "epoch: 8 step: 525, loss is 0.0036904928274452686\n",
      "epoch: 8 step: 526, loss is 0.0015611826675012708\n",
      "epoch: 8 step: 527, loss is 0.1555858999490738\n",
      "epoch: 8 step: 528, loss is 0.012897412292659283\n",
      "epoch: 8 step: 529, loss is 0.0336548313498497\n",
      "epoch: 8 step: 530, loss is 5.5635024182265624e-05\n",
      "epoch: 8 step: 531, loss is 0.012591855600476265\n",
      "epoch: 8 step: 532, loss is 4.017054016003385e-05\n",
      "epoch: 8 step: 533, loss is 0.024434685707092285\n",
      "epoch: 8 step: 534, loss is 0.0021928157657384872\n",
      "epoch: 8 step: 535, loss is 0.0007875077426433563\n",
      "epoch: 8 step: 536, loss is 0.06398886442184448\n",
      "epoch: 8 step: 537, loss is 0.015595683827996254\n",
      "epoch: 8 step: 538, loss is 0.003161668311804533\n",
      "epoch: 8 step: 539, loss is 0.00414645578712225\n",
      "epoch: 8 step: 540, loss is 0.08525659143924713\n",
      "epoch: 8 step: 541, loss is 0.0386849120259285\n",
      "epoch: 8 step: 542, loss is 0.013656063005328178\n",
      "epoch: 8 step: 543, loss is 0.005079261027276516\n",
      "epoch: 8 step: 544, loss is 0.01024794951081276\n",
      "epoch: 8 step: 545, loss is 0.00018127582734450698\n",
      "epoch: 8 step: 546, loss is 6.10728093306534e-05\n",
      "epoch: 8 step: 547, loss is 0.01117496844381094\n",
      "epoch: 8 step: 548, loss is 0.00016105629038065672\n",
      "epoch: 8 step: 549, loss is 0.0004681670106947422\n",
      "epoch: 8 step: 550, loss is 0.0017514503560960293\n",
      "epoch: 8 step: 551, loss is 0.0005632793763652444\n",
      "epoch: 8 step: 552, loss is 0.09038596600294113\n",
      "epoch: 8 step: 553, loss is 9.615218732506037e-05\n",
      "epoch: 8 step: 554, loss is 0.0005629019578918815\n",
      "epoch: 8 step: 555, loss is 0.00010107691923622042\n",
      "epoch: 8 step: 556, loss is 0.02379109151661396\n",
      "epoch: 8 step: 557, loss is 5.171455268282443e-05\n",
      "epoch: 8 step: 558, loss is 0.005335668101906776\n",
      "epoch: 8 step: 559, loss is 0.0036873123608529568\n",
      "epoch: 8 step: 560, loss is 0.13794714212417603\n",
      "epoch: 8 step: 561, loss is 0.005290244240313768\n",
      "epoch: 8 step: 562, loss is 0.0836748331785202\n",
      "epoch: 8 step: 563, loss is 0.032361242920160294\n",
      "epoch: 8 step: 564, loss is 0.020787853747606277\n",
      "epoch: 8 step: 565, loss is 0.011540494859218597\n",
      "epoch: 8 step: 566, loss is 0.0013116765767335892\n",
      "epoch: 8 step: 567, loss is 5.169110227143392e-05\n",
      "epoch: 8 step: 568, loss is 0.0071635860949754715\n",
      "epoch: 8 step: 569, loss is 0.005734420381486416\n",
      "epoch: 8 step: 570, loss is 0.0006345807923935354\n",
      "epoch: 8 step: 571, loss is 0.01068190298974514\n",
      "epoch: 8 step: 572, loss is 0.002168891718611121\n",
      "epoch: 8 step: 573, loss is 0.04657893255352974\n",
      "epoch: 8 step: 574, loss is 0.00428078044205904\n",
      "epoch: 8 step: 575, loss is 0.0055634742602705956\n",
      "epoch: 8 step: 576, loss is 0.004622455686330795\n",
      "epoch: 8 step: 577, loss is 0.006867959629744291\n",
      "epoch: 8 step: 578, loss is 0.0007308346685022116\n",
      "epoch: 8 step: 579, loss is 0.001097457716241479\n",
      "epoch: 8 step: 580, loss is 0.0012403620639815927\n",
      "epoch: 8 step: 581, loss is 0.003600779687985778\n",
      "epoch: 8 step: 582, loss is 0.002481469651684165\n",
      "epoch: 8 step: 583, loss is 0.14917056262493134\n",
      "epoch: 8 step: 584, loss is 0.24943186342716217\n",
      "epoch: 8 step: 585, loss is 0.0006715059280395508\n",
      "epoch: 8 step: 586, loss is 0.006012242753058672\n",
      "epoch: 8 step: 587, loss is 0.1533435434103012\n",
      "epoch: 8 step: 588, loss is 0.0008158514392562211\n",
      "epoch: 8 step: 589, loss is 0.0005465458962135017\n",
      "epoch: 8 step: 590, loss is 0.06633539497852325\n",
      "epoch: 8 step: 591, loss is 0.0005551801878027618\n",
      "epoch: 8 step: 592, loss is 0.007700448390096426\n",
      "epoch: 8 step: 593, loss is 0.00043789323535747826\n",
      "epoch: 8 step: 594, loss is 0.008629650808870792\n",
      "epoch: 8 step: 595, loss is 0.005274651106446981\n",
      "epoch: 8 step: 596, loss is 0.034772150218486786\n",
      "epoch: 8 step: 597, loss is 0.0003054120752494782\n",
      "epoch: 8 step: 598, loss is 0.00011905444989679381\n",
      "epoch: 8 step: 599, loss is 0.01396044623106718\n",
      "epoch: 8 step: 600, loss is 0.11587962508201599\n",
      "epoch: 8 step: 601, loss is 0.02815222553908825\n",
      "epoch: 8 step: 602, loss is 0.0006431526853702962\n",
      "epoch: 8 step: 603, loss is 0.002754645422101021\n",
      "epoch: 8 step: 604, loss is 0.0025974041782319546\n",
      "epoch: 8 step: 605, loss is 0.0012030178913846612\n",
      "epoch: 8 step: 606, loss is 0.0038861462380737066\n",
      "epoch: 8 step: 607, loss is 0.024089645594358444\n",
      "epoch: 8 step: 608, loss is 0.004932069219648838\n",
      "epoch: 8 step: 609, loss is 0.0007751391385681927\n",
      "epoch: 8 step: 610, loss is 0.000246175768552348\n",
      "epoch: 8 step: 611, loss is 0.008211902342736721\n",
      "epoch: 8 step: 612, loss is 0.1568979173898697\n",
      "epoch: 8 step: 613, loss is 0.09129214286804199\n",
      "epoch: 8 step: 614, loss is 0.010314795188605785\n",
      "epoch: 8 step: 615, loss is 0.003005053149536252\n",
      "epoch: 8 step: 616, loss is 0.001434633624739945\n",
      "epoch: 8 step: 617, loss is 0.0002291546406922862\n",
      "epoch: 8 step: 618, loss is 0.003528161207213998\n",
      "epoch: 8 step: 619, loss is 0.002259231870993972\n",
      "epoch: 8 step: 620, loss is 0.00013851793482899666\n",
      "epoch: 8 step: 621, loss is 0.02779301069676876\n",
      "epoch: 8 step: 622, loss is 0.00034647612483240664\n",
      "epoch: 8 step: 623, loss is 0.0029970521572977304\n",
      "epoch: 8 step: 624, loss is 0.00013153563486412168\n",
      "epoch: 8 step: 625, loss is 0.029932677745819092\n",
      "epoch: 8 step: 626, loss is 0.00041116736247204244\n",
      "epoch: 8 step: 627, loss is 9.69535467447713e-05\n",
      "epoch: 8 step: 628, loss is 0.02470860630273819\n",
      "epoch: 8 step: 629, loss is 0.011646744795143604\n",
      "epoch: 8 step: 630, loss is 0.002826038748025894\n",
      "epoch: 8 step: 631, loss is 0.00013682317512575537\n",
      "epoch: 8 step: 632, loss is 0.0022531584836542606\n",
      "epoch: 8 step: 633, loss is 0.006690799258649349\n",
      "epoch: 8 step: 634, loss is 0.056772831827402115\n",
      "epoch: 8 step: 635, loss is 0.0004049292765557766\n",
      "epoch: 8 step: 636, loss is 0.037779953330755234\n",
      "epoch: 8 step: 637, loss is 0.011530760675668716\n",
      "epoch: 8 step: 638, loss is 0.0016956652980297804\n",
      "epoch: 8 step: 639, loss is 0.00041251949733123183\n",
      "epoch: 8 step: 640, loss is 0.00268843537196517\n",
      "epoch: 8 step: 641, loss is 0.00032609724439680576\n",
      "epoch: 8 step: 642, loss is 0.006425525993108749\n",
      "epoch: 8 step: 643, loss is 0.04287857562303543\n",
      "epoch: 8 step: 644, loss is 0.0008079514955170453\n",
      "epoch: 8 step: 645, loss is 0.001682926551438868\n",
      "epoch: 8 step: 646, loss is 0.0003073247498832643\n",
      "epoch: 8 step: 647, loss is 0.08251786977052689\n",
      "epoch: 8 step: 648, loss is 0.0011967290192842484\n",
      "epoch: 8 step: 649, loss is 0.0002086499735014513\n",
      "epoch: 8 step: 650, loss is 0.0007472092984244227\n",
      "epoch: 8 step: 651, loss is 0.004881715402007103\n",
      "epoch: 8 step: 652, loss is 3.7091293052071705e-05\n",
      "epoch: 8 step: 653, loss is 0.0067318095825612545\n",
      "epoch: 8 step: 654, loss is 0.0007187075680121779\n",
      "epoch: 8 step: 655, loss is 0.0009556346922181547\n",
      "epoch: 8 step: 656, loss is 0.046415504068136215\n",
      "epoch: 8 step: 657, loss is 0.009596340358257294\n",
      "epoch: 8 step: 658, loss is 0.00042498737457208335\n",
      "epoch: 8 step: 659, loss is 0.0010167298605665565\n",
      "epoch: 8 step: 660, loss is 0.09271368384361267\n",
      "epoch: 8 step: 661, loss is 0.004542511887848377\n",
      "epoch: 8 step: 662, loss is 0.013630671426653862\n",
      "epoch: 8 step: 663, loss is 0.000767640070989728\n",
      "epoch: 8 step: 664, loss is 2.732250140979886e-05\n",
      "epoch: 8 step: 665, loss is 0.0007531255250796676\n",
      "epoch: 8 step: 666, loss is 7.20266907592304e-05\n",
      "epoch: 8 step: 667, loss is 0.0019106809049844742\n",
      "epoch: 8 step: 668, loss is 0.003541067009791732\n",
      "epoch: 8 step: 669, loss is 0.02709188498556614\n",
      "epoch: 8 step: 670, loss is 0.004721165169030428\n",
      "epoch: 8 step: 671, loss is 0.0006179516203701496\n",
      "epoch: 8 step: 672, loss is 9.771232726052403e-05\n",
      "epoch: 8 step: 673, loss is 6.428462802432477e-05\n",
      "epoch: 8 step: 674, loss is 0.002040429273620248\n",
      "epoch: 8 step: 675, loss is 0.00046169618144631386\n",
      "epoch: 8 step: 676, loss is 0.0014877578942105174\n",
      "epoch: 8 step: 677, loss is 0.006261975038796663\n",
      "epoch: 8 step: 678, loss is 0.00375717063434422\n",
      "epoch: 8 step: 679, loss is 0.0024397678207606077\n",
      "epoch: 8 step: 680, loss is 0.004426509141921997\n",
      "epoch: 8 step: 681, loss is 0.00047281422303058207\n",
      "epoch: 8 step: 682, loss is 0.0041086566634476185\n",
      "epoch: 8 step: 683, loss is 0.0020657337736338377\n",
      "epoch: 8 step: 684, loss is 0.0008799158968031406\n",
      "epoch: 8 step: 685, loss is 0.0006932691903784871\n",
      "epoch: 8 step: 686, loss is 0.0016242245910689235\n",
      "epoch: 8 step: 687, loss is 0.0025534348096698523\n",
      "epoch: 8 step: 688, loss is 0.0017146223690360785\n",
      "epoch: 8 step: 689, loss is 0.014873813837766647\n",
      "epoch: 8 step: 690, loss is 0.00018338352674618363\n",
      "epoch: 8 step: 691, loss is 0.010881082154810429\n",
      "epoch: 8 step: 692, loss is 0.0014913218328729272\n",
      "epoch: 8 step: 693, loss is 0.013576440513134003\n",
      "epoch: 8 step: 694, loss is 0.026886194944381714\n",
      "epoch: 8 step: 695, loss is 2.7778289222624153e-05\n",
      "epoch: 8 step: 696, loss is 0.02020687237381935\n",
      "epoch: 8 step: 697, loss is 0.1357126533985138\n",
      "epoch: 8 step: 698, loss is 0.0010686317691579461\n",
      "epoch: 8 step: 699, loss is 0.11928699165582657\n",
      "epoch: 8 step: 700, loss is 6.943149492144585e-05\n",
      "epoch: 8 step: 701, loss is 0.10209859907627106\n",
      "epoch: 8 step: 702, loss is 4.477023321669549e-05\n",
      "epoch: 8 step: 703, loss is 0.0009241355583071709\n",
      "epoch: 8 step: 704, loss is 7.557628123322502e-05\n",
      "epoch: 8 step: 705, loss is 0.0008562514558434486\n",
      "epoch: 8 step: 706, loss is 0.002135968767106533\n",
      "epoch: 8 step: 707, loss is 0.0023923602420836687\n",
      "epoch: 8 step: 708, loss is 0.0022374403197318316\n",
      "epoch: 8 step: 709, loss is 0.00312309293076396\n",
      "epoch: 8 step: 710, loss is 0.0023985428269952536\n",
      "epoch: 8 step: 711, loss is 0.010894037783145905\n",
      "epoch: 8 step: 712, loss is 0.00011593214003369212\n",
      "epoch: 8 step: 713, loss is 0.00010865640069823712\n",
      "epoch: 8 step: 714, loss is 0.0004833203856833279\n",
      "epoch: 8 step: 715, loss is 0.010812229476869106\n",
      "epoch: 8 step: 716, loss is 0.00509006017819047\n",
      "epoch: 8 step: 717, loss is 0.0006514870910905302\n",
      "epoch: 8 step: 718, loss is 0.0008678033482283354\n",
      "epoch: 8 step: 719, loss is 0.054114930331707\n",
      "epoch: 8 step: 720, loss is 0.004521057475358248\n",
      "epoch: 8 step: 721, loss is 1.7050071619451046e-05\n",
      "epoch: 8 step: 722, loss is 0.0016373124672099948\n",
      "epoch: 8 step: 723, loss is 0.0021041943691670895\n",
      "epoch: 8 step: 724, loss is 0.05891988426446915\n",
      "epoch: 8 step: 725, loss is 0.0011092049535363913\n",
      "epoch: 8 step: 726, loss is 0.026335498318076134\n",
      "epoch: 8 step: 727, loss is 0.018391365185379982\n",
      "epoch: 8 step: 728, loss is 0.011158199049532413\n",
      "epoch: 8 step: 729, loss is 0.006864003837108612\n",
      "epoch: 8 step: 730, loss is 4.2202824261039495e-05\n",
      "epoch: 8 step: 731, loss is 4.121179517824203e-05\n",
      "epoch: 8 step: 732, loss is 3.183012086083181e-05\n",
      "epoch: 8 step: 733, loss is 0.089316226541996\n",
      "epoch: 8 step: 734, loss is 0.0031470328103750944\n",
      "epoch: 8 step: 735, loss is 0.0004545223491732031\n",
      "epoch: 8 step: 736, loss is 0.0007494318997487426\n",
      "epoch: 8 step: 737, loss is 0.0002803305978886783\n",
      "epoch: 8 step: 738, loss is 0.00023265178606379777\n",
      "epoch: 8 step: 739, loss is 0.03308062627911568\n",
      "epoch: 8 step: 740, loss is 0.0003959869791287929\n",
      "epoch: 8 step: 741, loss is 0.016881918534636497\n",
      "epoch: 8 step: 742, loss is 0.00011776998871937394\n",
      "epoch: 8 step: 743, loss is 0.0036646949592977762\n",
      "epoch: 8 step: 744, loss is 3.3289481507381424e-05\n",
      "epoch: 8 step: 745, loss is 0.0001962163660209626\n",
      "epoch: 8 step: 746, loss is 0.001858717412687838\n",
      "epoch: 8 step: 747, loss is 0.0016798702999949455\n",
      "epoch: 8 step: 748, loss is 5.058031092630699e-05\n",
      "epoch: 8 step: 749, loss is 0.022767195478081703\n",
      "epoch: 8 step: 750, loss is 0.0004428613174241036\n",
      "epoch: 8 step: 751, loss is 0.10215234011411667\n",
      "epoch: 8 step: 752, loss is 0.02553604170680046\n",
      "epoch: 8 step: 753, loss is 0.012062758207321167\n",
      "epoch: 8 step: 754, loss is 0.002881062449887395\n",
      "epoch: 8 step: 755, loss is 0.01518533006310463\n",
      "epoch: 8 step: 756, loss is 0.0006496947025880218\n",
      "epoch: 8 step: 757, loss is 0.015517042949795723\n",
      "epoch: 8 step: 758, loss is 0.0017575935926288366\n",
      "epoch: 8 step: 759, loss is 0.20130623877048492\n",
      "epoch: 8 step: 760, loss is 0.0006782377604395151\n",
      "epoch: 8 step: 761, loss is 0.011009898968040943\n",
      "epoch: 8 step: 762, loss is 0.02537398785352707\n",
      "epoch: 8 step: 763, loss is 3.094359999522567e-05\n",
      "epoch: 8 step: 764, loss is 0.004756065085530281\n",
      "epoch: 8 step: 765, loss is 0.016462497413158417\n",
      "epoch: 8 step: 766, loss is 0.001443961402401328\n",
      "epoch: 8 step: 767, loss is 0.001446126727387309\n",
      "epoch: 8 step: 768, loss is 0.009529356844723225\n",
      "epoch: 8 step: 769, loss is 8.19592532934621e-05\n",
      "epoch: 8 step: 770, loss is 0.00018967023061122745\n",
      "epoch: 8 step: 771, loss is 0.0014733836287632585\n",
      "epoch: 8 step: 772, loss is 0.005515288095921278\n",
      "epoch: 8 step: 773, loss is 0.0006149020628072321\n",
      "epoch: 8 step: 774, loss is 0.0021952204406261444\n",
      "epoch: 8 step: 775, loss is 0.028414443135261536\n",
      "epoch: 8 step: 776, loss is 0.009445484727621078\n",
      "epoch: 8 step: 777, loss is 0.0003273448091931641\n",
      "epoch: 8 step: 778, loss is 0.0025069823022931814\n",
      "epoch: 8 step: 779, loss is 0.08361631631851196\n",
      "epoch: 8 step: 780, loss is 0.013601244427263737\n",
      "epoch: 8 step: 781, loss is 8.543945295969024e-05\n",
      "epoch: 8 step: 782, loss is 0.0183094535022974\n",
      "epoch: 8 step: 783, loss is 0.0007761815213598311\n",
      "epoch: 8 step: 784, loss is 0.15270589292049408\n",
      "epoch: 8 step: 785, loss is 0.0036550485529005527\n",
      "epoch: 8 step: 786, loss is 0.068060964345932\n",
      "epoch: 8 step: 787, loss is 0.004937600344419479\n",
      "epoch: 8 step: 788, loss is 0.001437741913832724\n",
      "epoch: 8 step: 789, loss is 0.025195490568876266\n",
      "epoch: 8 step: 790, loss is 0.00016741588478907943\n",
      "epoch: 8 step: 791, loss is 0.021417496725916862\n",
      "epoch: 8 step: 792, loss is 0.011717703193426132\n",
      "epoch: 8 step: 793, loss is 0.02847418747842312\n",
      "epoch: 8 step: 794, loss is 0.04011746868491173\n",
      "epoch: 8 step: 795, loss is 0.018248431384563446\n",
      "epoch: 8 step: 796, loss is 0.0035324294585734606\n",
      "epoch: 8 step: 797, loss is 0.024802464991807938\n",
      "epoch: 8 step: 798, loss is 0.0006746338913217187\n",
      "epoch: 8 step: 799, loss is 0.0026233706157654524\n",
      "epoch: 8 step: 800, loss is 0.0005564341554418206\n",
      "epoch: 8 step: 801, loss is 0.0008223107433877885\n",
      "epoch: 8 step: 802, loss is 0.005051418673247099\n",
      "epoch: 8 step: 803, loss is 0.0001568544830661267\n",
      "epoch: 8 step: 804, loss is 0.00020114520157221705\n",
      "epoch: 8 step: 805, loss is 0.00030973798129707575\n",
      "epoch: 8 step: 806, loss is 0.002742073033004999\n",
      "epoch: 8 step: 807, loss is 0.003134949831292033\n",
      "epoch: 8 step: 808, loss is 0.006016821134835482\n",
      "epoch: 8 step: 809, loss is 0.05193137750029564\n",
      "epoch: 8 step: 810, loss is 0.016373205929994583\n",
      "epoch: 8 step: 811, loss is 0.059909887611866\n",
      "epoch: 8 step: 812, loss is 0.026545602828264236\n",
      "epoch: 8 step: 813, loss is 0.0061448137275874615\n",
      "epoch: 8 step: 814, loss is 0.031848229467868805\n",
      "epoch: 8 step: 815, loss is 0.0005809745634905994\n",
      "epoch: 8 step: 816, loss is 0.00046944920904934406\n",
      "epoch: 8 step: 817, loss is 0.10828454792499542\n",
      "epoch: 8 step: 818, loss is 0.00018315378110855818\n",
      "epoch: 8 step: 819, loss is 9.668376878835261e-05\n",
      "epoch: 8 step: 820, loss is 0.03424543887376785\n",
      "epoch: 8 step: 821, loss is 0.06281424313783646\n",
      "epoch: 8 step: 822, loss is 0.0050258636474609375\n",
      "epoch: 8 step: 823, loss is 0.0007436898304149508\n",
      "epoch: 8 step: 824, loss is 0.00023489964951295406\n",
      "epoch: 8 step: 825, loss is 0.011641423217952251\n",
      "epoch: 8 step: 826, loss is 0.0007871189736761153\n",
      "epoch: 8 step: 827, loss is 2.56230887316633e-05\n",
      "epoch: 8 step: 828, loss is 0.0009608275722712278\n",
      "epoch: 8 step: 829, loss is 0.0006923570181243122\n",
      "epoch: 8 step: 830, loss is 0.00029659306164830923\n",
      "epoch: 8 step: 831, loss is 0.00024864834267646074\n",
      "epoch: 8 step: 832, loss is 0.00044456831528805196\n",
      "epoch: 8 step: 833, loss is 0.0001355228596366942\n",
      "epoch: 8 step: 834, loss is 0.03129716217517853\n",
      "epoch: 8 step: 835, loss is 0.04107038676738739\n",
      "epoch: 8 step: 836, loss is 0.001195906545035541\n",
      "epoch: 8 step: 837, loss is 0.0004815126594621688\n",
      "epoch: 8 step: 838, loss is 0.12839023768901825\n",
      "epoch: 8 step: 839, loss is 0.00835461262613535\n",
      "epoch: 8 step: 840, loss is 0.0034440874587744474\n",
      "epoch: 8 step: 841, loss is 0.0003546372172422707\n",
      "epoch: 8 step: 842, loss is 0.00010514787572901696\n",
      "epoch: 8 step: 843, loss is 0.0004903454682789743\n",
      "epoch: 8 step: 844, loss is 0.0024569060187786818\n",
      "epoch: 8 step: 845, loss is 0.00020831151050515473\n",
      "epoch: 8 step: 846, loss is 0.000484497839352116\n",
      "epoch: 8 step: 847, loss is 0.0015878630802035332\n",
      "epoch: 8 step: 848, loss is 0.0013204703573137522\n",
      "epoch: 8 step: 849, loss is 0.00034300494007766247\n",
      "epoch: 8 step: 850, loss is 0.00013424656935967505\n",
      "epoch: 8 step: 851, loss is 0.027359705418348312\n",
      "epoch: 8 step: 852, loss is 0.1525121033191681\n",
      "epoch: 8 step: 853, loss is 0.0002944564039353281\n",
      "epoch: 8 step: 854, loss is 0.000144113291753456\n",
      "epoch: 8 step: 855, loss is 0.0006234355387277901\n",
      "epoch: 8 step: 856, loss is 0.0010262842988595366\n",
      "epoch: 8 step: 857, loss is 0.00013621844118461013\n",
      "epoch: 8 step: 858, loss is 0.0039270855486392975\n",
      "epoch: 8 step: 859, loss is 0.00047925178660079837\n",
      "epoch: 8 step: 860, loss is 0.12638774514198303\n",
      "epoch: 8 step: 861, loss is 0.007604457437992096\n",
      "epoch: 8 step: 862, loss is 0.008815736509859562\n",
      "epoch: 8 step: 863, loss is 0.06200745329260826\n",
      "epoch: 8 step: 864, loss is 0.0028762195724993944\n",
      "epoch: 8 step: 865, loss is 0.00030594010604545474\n",
      "epoch: 8 step: 866, loss is 0.0016869519604369998\n",
      "epoch: 8 step: 867, loss is 0.002285612979903817\n",
      "epoch: 8 step: 868, loss is 7.340685988310724e-05\n",
      "epoch: 8 step: 869, loss is 0.00582182127982378\n",
      "epoch: 8 step: 870, loss is 0.02528134174644947\n",
      "epoch: 8 step: 871, loss is 0.06103223189711571\n",
      "epoch: 8 step: 872, loss is 0.022916585206985474\n",
      "epoch: 8 step: 873, loss is 0.028125643730163574\n",
      "epoch: 8 step: 874, loss is 0.04853794723749161\n",
      "epoch: 8 step: 875, loss is 0.018875300884246826\n",
      "epoch: 8 step: 876, loss is 0.002847875701263547\n",
      "epoch: 8 step: 877, loss is 0.004349255468696356\n",
      "epoch: 8 step: 878, loss is 0.004997148644179106\n",
      "epoch: 8 step: 879, loss is 0.005719173699617386\n",
      "epoch: 8 step: 880, loss is 0.000237752465181984\n",
      "epoch: 8 step: 881, loss is 0.00039698256296105683\n",
      "epoch: 8 step: 882, loss is 0.0001764382905093953\n",
      "epoch: 8 step: 883, loss is 0.0018202190985903144\n",
      "epoch: 8 step: 884, loss is 0.0007269122288562357\n",
      "epoch: 8 step: 885, loss is 0.0006473743706010282\n",
      "epoch: 8 step: 886, loss is 0.0014982122229412198\n",
      "epoch: 8 step: 887, loss is 0.000528541742824018\n",
      "epoch: 8 step: 888, loss is 0.001848786836490035\n",
      "epoch: 8 step: 889, loss is 0.08743695169687271\n",
      "epoch: 8 step: 890, loss is 0.0016215457580983639\n",
      "epoch: 8 step: 891, loss is 0.13083480298519135\n",
      "epoch: 8 step: 892, loss is 8.22056972538121e-05\n",
      "epoch: 8 step: 893, loss is 0.004792994353920221\n",
      "epoch: 8 step: 894, loss is 9.177928586723283e-05\n",
      "epoch: 8 step: 895, loss is 0.0003953358391299844\n",
      "epoch: 8 step: 896, loss is 0.044113386422395706\n",
      "epoch: 8 step: 897, loss is 0.08140292018651962\n",
      "epoch: 8 step: 898, loss is 0.02127942256629467\n",
      "epoch: 8 step: 899, loss is 0.05527983605861664\n",
      "epoch: 8 step: 900, loss is 6.438636046368629e-05\n",
      "epoch: 8 step: 901, loss is 0.0001325562916463241\n",
      "epoch: 8 step: 902, loss is 0.0012193783186376095\n",
      "epoch: 8 step: 903, loss is 0.022186819463968277\n",
      "epoch: 8 step: 904, loss is 0.012521124444901943\n",
      "epoch: 8 step: 905, loss is 0.004159457050263882\n",
      "epoch: 8 step: 906, loss is 0.0032019729260355234\n",
      "epoch: 8 step: 907, loss is 0.012964196503162384\n",
      "epoch: 8 step: 908, loss is 5.321110802469775e-05\n",
      "epoch: 8 step: 909, loss is 0.009277797304093838\n",
      "epoch: 8 step: 910, loss is 0.002454089466482401\n",
      "epoch: 8 step: 911, loss is 0.0023842400405555964\n",
      "epoch: 8 step: 912, loss is 0.00012604689982254058\n",
      "epoch: 8 step: 913, loss is 0.00022552377777174115\n",
      "epoch: 8 step: 914, loss is 0.0023833257146179676\n",
      "epoch: 8 step: 915, loss is 0.12349043786525726\n",
      "epoch: 8 step: 916, loss is 0.012419851496815681\n",
      "epoch: 8 step: 917, loss is 0.0006537913577631116\n",
      "epoch: 8 step: 918, loss is 0.001234611263498664\n",
      "epoch: 8 step: 919, loss is 3.5708009818335995e-05\n",
      "epoch: 8 step: 920, loss is 0.005031228065490723\n",
      "epoch: 8 step: 921, loss is 0.0020539441611617804\n",
      "epoch: 8 step: 922, loss is 0.002247268334031105\n",
      "epoch: 8 step: 923, loss is 0.020433545112609863\n",
      "epoch: 8 step: 924, loss is 0.019837554544210434\n",
      "epoch: 8 step: 925, loss is 0.0992019921541214\n",
      "epoch: 8 step: 926, loss is 0.0007817126461304724\n",
      "epoch: 8 step: 927, loss is 0.0034095775336027145\n",
      "epoch: 8 step: 928, loss is 0.0021632525604218245\n",
      "epoch: 8 step: 929, loss is 0.018083693459630013\n",
      "epoch: 8 step: 930, loss is 0.00037607166450470686\n",
      "epoch: 8 step: 931, loss is 0.0011048950254917145\n",
      "epoch: 8 step: 932, loss is 0.0037676410283893347\n",
      "epoch: 8 step: 933, loss is 0.0006408711196854711\n",
      "epoch: 8 step: 934, loss is 0.23848241567611694\n",
      "epoch: 8 step: 935, loss is 0.22170390188694\n",
      "epoch: 8 step: 936, loss is 0.0013193164486438036\n",
      "epoch: 8 step: 937, loss is 0.0003605901438277215\n",
      "epoch: 8 step: 938, loss is 0.0012785859871655703\n",
      "epoch: 8 step: 939, loss is 0.0017736139707267284\n",
      "epoch: 8 step: 940, loss is 0.0026785857044160366\n",
      "epoch: 8 step: 941, loss is 0.0043927570804953575\n",
      "epoch: 8 step: 942, loss is 0.00046260125236585736\n",
      "epoch: 8 step: 943, loss is 0.002989796921610832\n",
      "epoch: 8 step: 944, loss is 0.0041533056646585464\n",
      "epoch: 8 step: 945, loss is 0.006790287792682648\n",
      "epoch: 8 step: 946, loss is 0.03258476033806801\n",
      "epoch: 8 step: 947, loss is 0.0009308848530054092\n",
      "epoch: 8 step: 948, loss is 0.001525459811091423\n",
      "epoch: 8 step: 949, loss is 0.001509574125520885\n",
      "epoch: 8 step: 950, loss is 4.159886884735897e-05\n",
      "epoch: 8 step: 951, loss is 0.00047243310837075114\n",
      "epoch: 8 step: 952, loss is 0.0002808443387039006\n",
      "epoch: 8 step: 953, loss is 0.018137583509087563\n",
      "epoch: 8 step: 954, loss is 0.06209171935915947\n",
      "epoch: 8 step: 955, loss is 0.0007719096611253917\n",
      "epoch: 8 step: 956, loss is 0.000789218524005264\n",
      "epoch: 8 step: 957, loss is 0.0011389630381017923\n",
      "epoch: 8 step: 958, loss is 0.001243049860931933\n",
      "epoch: 8 step: 959, loss is 0.01542912982404232\n",
      "epoch: 8 step: 960, loss is 0.007958097383379936\n",
      "epoch: 8 step: 961, loss is 0.011392845772206783\n",
      "epoch: 8 step: 962, loss is 0.00019583186076488346\n",
      "epoch: 8 step: 963, loss is 0.005184573587030172\n",
      "epoch: 8 step: 964, loss is 0.005802552215754986\n",
      "epoch: 8 step: 965, loss is 0.012167416512966156\n",
      "epoch: 8 step: 966, loss is 0.0039268252439796925\n",
      "epoch: 8 step: 967, loss is 0.026621967554092407\n",
      "epoch: 8 step: 968, loss is 8.868738950695843e-05\n",
      "epoch: 8 step: 969, loss is 3.805213054874912e-05\n",
      "epoch: 8 step: 970, loss is 0.0014129369519650936\n",
      "epoch: 8 step: 971, loss is 0.00012545008212327957\n",
      "epoch: 8 step: 972, loss is 0.009885392151772976\n",
      "epoch: 8 step: 973, loss is 0.0010116936173290014\n",
      "epoch: 8 step: 974, loss is 0.002039074432104826\n",
      "epoch: 8 step: 975, loss is 0.0006027850904501975\n",
      "epoch: 8 step: 976, loss is 0.0015503932954743505\n",
      "epoch: 8 step: 977, loss is 0.0019604386761784554\n",
      "epoch: 8 step: 978, loss is 0.00030903631704859436\n",
      "epoch: 8 step: 979, loss is 6.232598389033228e-05\n",
      "epoch: 8 step: 980, loss is 0.00010719379497459158\n",
      "epoch: 8 step: 981, loss is 0.006934777367860079\n",
      "epoch: 8 step: 982, loss is 0.013574592769145966\n",
      "epoch: 8 step: 983, loss is 0.0006964356289245188\n",
      "epoch: 8 step: 984, loss is 0.0003722880792338401\n",
      "epoch: 8 step: 985, loss is 0.018987758085131645\n",
      "epoch: 8 step: 986, loss is 0.005561927333474159\n",
      "epoch: 8 step: 987, loss is 7.348862709477544e-05\n",
      "epoch: 8 step: 988, loss is 0.005119824782013893\n",
      "epoch: 8 step: 989, loss is 0.011875364929437637\n",
      "epoch: 8 step: 990, loss is 0.0017754863947629929\n",
      "epoch: 8 step: 991, loss is 0.0005419939989224076\n",
      "epoch: 8 step: 992, loss is 0.003491729963570833\n",
      "epoch: 8 step: 993, loss is 0.001218805555254221\n",
      "epoch: 8 step: 994, loss is 0.02829642780125141\n",
      "epoch: 8 step: 995, loss is 0.028064284473657608\n",
      "epoch: 8 step: 996, loss is 1.2907080417789984e-05\n",
      "epoch: 8 step: 997, loss is 0.13081198930740356\n",
      "epoch: 8 step: 998, loss is 0.0017653799150139093\n",
      "epoch: 8 step: 999, loss is 0.00621664896607399\n",
      "epoch: 8 step: 1000, loss is 0.00033367524156346917\n",
      "epoch: 8 step: 1001, loss is 3.723198460647836e-05\n",
      "epoch: 8 step: 1002, loss is 0.00033227159292437136\n",
      "epoch: 8 step: 1003, loss is 0.00024843995925039053\n",
      "epoch: 8 step: 1004, loss is 0.010979009792208672\n",
      "epoch: 8 step: 1005, loss is 0.0013489664997905493\n",
      "epoch: 8 step: 1006, loss is 0.0002750782296061516\n",
      "epoch: 8 step: 1007, loss is 0.09887296706438065\n",
      "epoch: 8 step: 1008, loss is 6.37174307485111e-05\n",
      "epoch: 8 step: 1009, loss is 0.0733393058180809\n",
      "epoch: 8 step: 1010, loss is 0.00056071812286973\n",
      "epoch: 8 step: 1011, loss is 0.0012137960875406861\n",
      "epoch: 8 step: 1012, loss is 0.0018918307032436132\n",
      "epoch: 8 step: 1013, loss is 0.0001732878154143691\n",
      "epoch: 8 step: 1014, loss is 0.0002608209615573287\n",
      "epoch: 8 step: 1015, loss is 0.0002907374291680753\n",
      "epoch: 8 step: 1016, loss is 0.0028774624224752188\n",
      "epoch: 8 step: 1017, loss is 2.2016458387952298e-05\n",
      "epoch: 8 step: 1018, loss is 0.004916712176054716\n",
      "epoch: 8 step: 1019, loss is 2.3674097974435426e-05\n",
      "epoch: 8 step: 1020, loss is 0.015699371695518494\n",
      "epoch: 8 step: 1021, loss is 0.0033732056617736816\n",
      "epoch: 8 step: 1022, loss is 0.0017514827195554972\n",
      "epoch: 8 step: 1023, loss is 0.10180462896823883\n",
      "epoch: 8 step: 1024, loss is 0.0007480664644390345\n",
      "epoch: 8 step: 1025, loss is 0.001071483246050775\n",
      "epoch: 8 step: 1026, loss is 0.0020463825203478336\n",
      "epoch: 8 step: 1027, loss is 0.00010537931666476652\n",
      "epoch: 8 step: 1028, loss is 0.0033652607817202806\n",
      "epoch: 8 step: 1029, loss is 0.007588743232190609\n",
      "epoch: 8 step: 1030, loss is 0.025916559621691704\n",
      "epoch: 8 step: 1031, loss is 0.007812854833900928\n",
      "epoch: 8 step: 1032, loss is 0.02024245634675026\n",
      "epoch: 8 step: 1033, loss is 0.0002653667179401964\n",
      "epoch: 8 step: 1034, loss is 0.00030524865724146366\n",
      "epoch: 8 step: 1035, loss is 0.004045270849019289\n",
      "epoch: 8 step: 1036, loss is 0.0015990094980224967\n",
      "epoch: 8 step: 1037, loss is 0.0005555757088586688\n",
      "epoch: 8 step: 1038, loss is 0.0003620866918936372\n",
      "epoch: 8 step: 1039, loss is 0.0014688334194943309\n",
      "epoch: 8 step: 1040, loss is 0.000580325722694397\n",
      "epoch: 8 step: 1041, loss is 0.08403933048248291\n",
      "epoch: 8 step: 1042, loss is 0.2888522148132324\n",
      "epoch: 8 step: 1043, loss is 0.005126481410115957\n",
      "epoch: 8 step: 1044, loss is 0.0040920269675552845\n",
      "epoch: 8 step: 1045, loss is 0.00215274840593338\n",
      "epoch: 8 step: 1046, loss is 0.010019115172326565\n",
      "epoch: 8 step: 1047, loss is 0.021661700680851936\n",
      "epoch: 8 step: 1048, loss is 6.170244159875438e-05\n",
      "epoch: 8 step: 1049, loss is 0.00037318479735404253\n",
      "epoch: 8 step: 1050, loss is 0.0013194849016144872\n",
      "epoch: 8 step: 1051, loss is 0.0733395665884018\n",
      "epoch: 8 step: 1052, loss is 0.0005805560504086316\n",
      "epoch: 8 step: 1053, loss is 0.182631716132164\n",
      "epoch: 8 step: 1054, loss is 0.0008443040424026549\n",
      "epoch: 8 step: 1055, loss is 0.00036758254282176495\n",
      "epoch: 8 step: 1056, loss is 0.04973459988832474\n",
      "epoch: 8 step: 1057, loss is 0.002702013123780489\n",
      "epoch: 8 step: 1058, loss is 0.00035399955231696367\n",
      "epoch: 8 step: 1059, loss is 0.005044816993176937\n",
      "epoch: 8 step: 1060, loss is 0.0008103755535557866\n",
      "epoch: 8 step: 1061, loss is 0.0067824712023139\n",
      "epoch: 8 step: 1062, loss is 0.030827254056930542\n",
      "epoch: 8 step: 1063, loss is 0.0018981242319568992\n",
      "epoch: 8 step: 1064, loss is 0.04315784573554993\n",
      "epoch: 8 step: 1065, loss is 0.006186974234879017\n",
      "epoch: 8 step: 1066, loss is 0.00024689489509910345\n",
      "epoch: 8 step: 1067, loss is 0.0022283210419118404\n",
      "epoch: 8 step: 1068, loss is 0.005477414932101965\n",
      "epoch: 8 step: 1069, loss is 0.008280599489808083\n",
      "epoch: 8 step: 1070, loss is 0.005996331572532654\n",
      "epoch: 8 step: 1071, loss is 0.0028645077254623175\n",
      "epoch: 8 step: 1072, loss is 0.003504642751067877\n",
      "epoch: 8 step: 1073, loss is 0.02236134000122547\n",
      "epoch: 8 step: 1074, loss is 0.010035717859864235\n",
      "epoch: 8 step: 1075, loss is 0.00028845344786532223\n",
      "epoch: 8 step: 1076, loss is 0.0033145828638225794\n",
      "epoch: 8 step: 1077, loss is 0.0006941204192116857\n",
      "epoch: 8 step: 1078, loss is 0.00019883837376255542\n",
      "epoch: 8 step: 1079, loss is 0.014403905719518661\n",
      "epoch: 8 step: 1080, loss is 0.0016683225985616446\n",
      "epoch: 8 step: 1081, loss is 0.00014331936836242676\n",
      "epoch: 8 step: 1082, loss is 9.651090658735484e-05\n",
      "epoch: 8 step: 1083, loss is 0.0006598448962904513\n",
      "epoch: 8 step: 1084, loss is 0.04045835882425308\n",
      "epoch: 8 step: 1085, loss is 0.0005037098308093846\n",
      "epoch: 8 step: 1086, loss is 0.03220796957612038\n",
      "epoch: 8 step: 1087, loss is 0.01749352179467678\n",
      "epoch: 8 step: 1088, loss is 1.9062868886976503e-05\n",
      "epoch: 8 step: 1089, loss is 0.0011704113567247987\n",
      "epoch: 8 step: 1090, loss is 0.10073769837617874\n",
      "epoch: 8 step: 1091, loss is 0.0012046125484630466\n",
      "epoch: 8 step: 1092, loss is 0.0003437157429289073\n",
      "epoch: 8 step: 1093, loss is 0.07209950685501099\n",
      "epoch: 8 step: 1094, loss is 0.0007478873012587428\n",
      "epoch: 8 step: 1095, loss is 0.0008953246288001537\n",
      "epoch: 8 step: 1096, loss is 6.312076584436e-05\n",
      "epoch: 8 step: 1097, loss is 0.012025627307593822\n",
      "epoch: 8 step: 1098, loss is 0.002036858582869172\n",
      "epoch: 8 step: 1099, loss is 0.0019951406866312027\n",
      "epoch: 8 step: 1100, loss is 0.0022411721292883158\n",
      "epoch: 8 step: 1101, loss is 0.009429381228983402\n",
      "epoch: 8 step: 1102, loss is 5.403803152148612e-05\n",
      "epoch: 8 step: 1103, loss is 3.800105332629755e-05\n",
      "epoch: 8 step: 1104, loss is 0.0007944410317577422\n",
      "epoch: 8 step: 1105, loss is 0.02643471583724022\n",
      "epoch: 8 step: 1106, loss is 0.0015919174766167998\n",
      "epoch: 8 step: 1107, loss is 0.006751006003469229\n",
      "epoch: 8 step: 1108, loss is 0.006860384251922369\n",
      "epoch: 8 step: 1109, loss is 0.0038469743449240923\n",
      "epoch: 8 step: 1110, loss is 0.020256953313946724\n",
      "epoch: 8 step: 1111, loss is 0.00021443174045998603\n",
      "epoch: 8 step: 1112, loss is 0.0002693897986318916\n",
      "epoch: 8 step: 1113, loss is 0.06270116567611694\n",
      "epoch: 8 step: 1114, loss is 0.00011268005619058385\n",
      "epoch: 8 step: 1115, loss is 3.280721648479812e-05\n",
      "epoch: 8 step: 1116, loss is 0.0109246876090765\n",
      "epoch: 8 step: 1117, loss is 0.0017828536219894886\n",
      "epoch: 8 step: 1118, loss is 0.0002962232392746955\n",
      "epoch: 8 step: 1119, loss is 0.016568182036280632\n",
      "epoch: 8 step: 1120, loss is 0.07569534331560135\n",
      "epoch: 8 step: 1121, loss is 0.0016348777571693063\n",
      "epoch: 8 step: 1122, loss is 0.009774995036423206\n",
      "epoch: 8 step: 1123, loss is 0.004513927269726992\n",
      "epoch: 8 step: 1124, loss is 0.0018279198557138443\n",
      "epoch: 8 step: 1125, loss is 0.19409465789794922\n",
      "epoch: 8 step: 1126, loss is 0.004360416904091835\n",
      "epoch: 8 step: 1127, loss is 0.006556291598826647\n",
      "epoch: 8 step: 1128, loss is 0.0002255148283438757\n",
      "epoch: 8 step: 1129, loss is 0.003451354568824172\n",
      "epoch: 8 step: 1130, loss is 0.05497773736715317\n",
      "epoch: 8 step: 1131, loss is 0.027349276468157768\n",
      "epoch: 8 step: 1132, loss is 0.0056167468428611755\n",
      "epoch: 8 step: 1133, loss is 0.0007186284055933356\n",
      "epoch: 8 step: 1134, loss is 0.007433517370373011\n",
      "epoch: 8 step: 1135, loss is 0.028468430042266846\n",
      "epoch: 8 step: 1136, loss is 0.0004033076111227274\n",
      "epoch: 8 step: 1137, loss is 0.0017700284952297807\n",
      "epoch: 8 step: 1138, loss is 0.0539693608880043\n",
      "epoch: 8 step: 1139, loss is 0.01582670398056507\n",
      "epoch: 8 step: 1140, loss is 0.03212329372763634\n",
      "epoch: 8 step: 1141, loss is 0.003964993637055159\n",
      "epoch: 8 step: 1142, loss is 0.042802292853593826\n",
      "epoch: 8 step: 1143, loss is 0.004689048044383526\n",
      "epoch: 8 step: 1144, loss is 0.02820749022066593\n",
      "epoch: 8 step: 1145, loss is 0.24368083477020264\n",
      "epoch: 8 step: 1146, loss is 0.02803080528974533\n",
      "epoch: 8 step: 1147, loss is 0.0037782893050462008\n",
      "epoch: 8 step: 1148, loss is 0.008378277532756329\n",
      "epoch: 8 step: 1149, loss is 0.0006826737662777305\n",
      "epoch: 8 step: 1150, loss is 0.0016056785825639963\n",
      "epoch: 8 step: 1151, loss is 0.001264556311070919\n",
      "epoch: 8 step: 1152, loss is 0.00975804217159748\n",
      "epoch: 8 step: 1153, loss is 0.0013692957581952214\n",
      "epoch: 8 step: 1154, loss is 0.00017983795260079205\n",
      "epoch: 8 step: 1155, loss is 0.010653247125446796\n",
      "epoch: 8 step: 1156, loss is 0.005513877607882023\n",
      "epoch: 8 step: 1157, loss is 0.009285731241106987\n",
      "epoch: 8 step: 1158, loss is 0.011044581420719624\n",
      "epoch: 8 step: 1159, loss is 0.000980406068265438\n",
      "epoch: 8 step: 1160, loss is 0.04415729269385338\n",
      "epoch: 8 step: 1161, loss is 0.024977728724479675\n",
      "epoch: 8 step: 1162, loss is 0.030133668333292007\n",
      "epoch: 8 step: 1163, loss is 4.4974909542361274e-05\n",
      "epoch: 8 step: 1164, loss is 0.0001535779592813924\n",
      "epoch: 8 step: 1165, loss is 0.0058557079173624516\n",
      "epoch: 8 step: 1166, loss is 0.07447060197591782\n",
      "epoch: 8 step: 1167, loss is 0.06883393228054047\n",
      "epoch: 8 step: 1168, loss is 0.00034074991708621383\n",
      "epoch: 8 step: 1169, loss is 0.03342188522219658\n",
      "epoch: 8 step: 1170, loss is 0.02301616221666336\n",
      "epoch: 8 step: 1171, loss is 0.011776684783399105\n",
      "epoch: 8 step: 1172, loss is 0.001281707314774394\n",
      "epoch: 8 step: 1173, loss is 0.0025967878755182028\n",
      "epoch: 8 step: 1174, loss is 0.00045982125448063016\n",
      "epoch: 8 step: 1175, loss is 0.0007873771828599274\n",
      "epoch: 8 step: 1176, loss is 0.013310370035469532\n",
      "epoch: 8 step: 1177, loss is 0.0023680422455072403\n",
      "epoch: 8 step: 1178, loss is 0.0003540756879374385\n",
      "epoch: 8 step: 1179, loss is 0.00020467797003220767\n",
      "epoch: 8 step: 1180, loss is 0.15891586244106293\n",
      "epoch: 8 step: 1181, loss is 0.00023043816327117383\n",
      "epoch: 8 step: 1182, loss is 0.09742981195449829\n",
      "epoch: 8 step: 1183, loss is 0.00042328890413045883\n",
      "epoch: 8 step: 1184, loss is 0.004141457844525576\n",
      "epoch: 8 step: 1185, loss is 0.002653334988281131\n",
      "epoch: 8 step: 1186, loss is 0.09356728196144104\n",
      "epoch: 8 step: 1187, loss is 0.030650801956653595\n",
      "epoch: 8 step: 1188, loss is 0.0006941985921002924\n",
      "epoch: 8 step: 1189, loss is 6.551573460455984e-05\n",
      "epoch: 8 step: 1190, loss is 0.1516076922416687\n",
      "epoch: 8 step: 1191, loss is 0.000802734459284693\n",
      "epoch: 8 step: 1192, loss is 0.00580892339348793\n",
      "epoch: 8 step: 1193, loss is 0.05934280529618263\n",
      "epoch: 8 step: 1194, loss is 0.0032369159162044525\n",
      "epoch: 8 step: 1195, loss is 0.02170412242412567\n",
      "epoch: 8 step: 1196, loss is 0.04626273363828659\n",
      "epoch: 8 step: 1197, loss is 0.00013781133748125285\n",
      "epoch: 8 step: 1198, loss is 0.0012843759031966329\n",
      "epoch: 8 step: 1199, loss is 0.005466722883284092\n",
      "epoch: 8 step: 1200, loss is 0.09965092688798904\n",
      "epoch: 8 step: 1201, loss is 0.1970439851284027\n",
      "epoch: 8 step: 1202, loss is 0.07904623448848724\n",
      "epoch: 8 step: 1203, loss is 0.00017091688641812652\n",
      "epoch: 8 step: 1204, loss is 0.0015809852629899979\n",
      "epoch: 8 step: 1205, loss is 0.20737792551517487\n",
      "epoch: 8 step: 1206, loss is 0.0035738381557166576\n",
      "epoch: 8 step: 1207, loss is 2.3022095774649642e-05\n",
      "epoch: 8 step: 1208, loss is 0.003084116615355015\n",
      "epoch: 8 step: 1209, loss is 0.0007949484861455858\n",
      "epoch: 8 step: 1210, loss is 0.0005402240203693509\n",
      "epoch: 8 step: 1211, loss is 0.08925428241491318\n",
      "epoch: 8 step: 1212, loss is 0.0001935342006618157\n",
      "epoch: 8 step: 1213, loss is 0.008885392919182777\n",
      "epoch: 8 step: 1214, loss is 0.005492186639457941\n",
      "epoch: 8 step: 1215, loss is 0.0007882103673182428\n",
      "epoch: 8 step: 1216, loss is 0.0006013921229168773\n",
      "epoch: 8 step: 1217, loss is 0.023639043793082237\n",
      "epoch: 8 step: 1218, loss is 0.005440283101052046\n",
      "epoch: 8 step: 1219, loss is 0.0921880230307579\n",
      "epoch: 8 step: 1220, loss is 0.1203184723854065\n",
      "epoch: 8 step: 1221, loss is 0.07895276695489883\n",
      "epoch: 8 step: 1222, loss is 0.008013426326215267\n",
      "epoch: 8 step: 1223, loss is 0.004827222786843777\n",
      "epoch: 8 step: 1224, loss is 0.006589088123291731\n",
      "epoch: 8 step: 1225, loss is 0.00024309451691806316\n",
      "epoch: 8 step: 1226, loss is 0.031299490481615067\n",
      "epoch: 8 step: 1227, loss is 0.000129976513562724\n",
      "epoch: 8 step: 1228, loss is 0.003933974076062441\n",
      "epoch: 8 step: 1229, loss is 0.0007337753195315599\n",
      "epoch: 8 step: 1230, loss is 0.010587156750261784\n",
      "epoch: 8 step: 1231, loss is 0.002546923700720072\n",
      "epoch: 8 step: 1232, loss is 0.0017739104805514216\n",
      "epoch: 8 step: 1233, loss is 0.002361356047913432\n",
      "epoch: 8 step: 1234, loss is 0.002684400649741292\n",
      "epoch: 8 step: 1235, loss is 0.003370255697518587\n",
      "epoch: 8 step: 1236, loss is 0.01775367185473442\n",
      "epoch: 8 step: 1237, loss is 0.0004304739413782954\n",
      "epoch: 8 step: 1238, loss is 0.007268471643328667\n",
      "epoch: 8 step: 1239, loss is 0.01145018171519041\n",
      "epoch: 8 step: 1240, loss is 0.00014203984756022692\n",
      "epoch: 8 step: 1241, loss is 0.020782966166734695\n",
      "epoch: 8 step: 1242, loss is 0.14358803629875183\n",
      "epoch: 8 step: 1243, loss is 0.13680905103683472\n",
      "epoch: 8 step: 1244, loss is 0.00281691481359303\n",
      "epoch: 8 step: 1245, loss is 0.003843024605885148\n",
      "epoch: 8 step: 1246, loss is 0.0022179391235113144\n",
      "epoch: 8 step: 1247, loss is 0.03058898262679577\n",
      "epoch: 8 step: 1248, loss is 0.0005888851592317224\n",
      "epoch: 8 step: 1249, loss is 0.04983890801668167\n",
      "epoch: 8 step: 1250, loss is 0.09906352311372757\n",
      "epoch: 8 step: 1251, loss is 0.0011879472294822335\n",
      "epoch: 8 step: 1252, loss is 0.00015837463433854282\n",
      "epoch: 8 step: 1253, loss is 0.004300164524465799\n",
      "epoch: 8 step: 1254, loss is 0.005763937719166279\n",
      "epoch: 8 step: 1255, loss is 0.0034244884736835957\n",
      "epoch: 8 step: 1256, loss is 0.07221122831106186\n",
      "epoch: 8 step: 1257, loss is 0.002134686568751931\n",
      "epoch: 8 step: 1258, loss is 0.07940927147865295\n",
      "epoch: 8 step: 1259, loss is 0.002346884226426482\n",
      "epoch: 8 step: 1260, loss is 0.0071814702823758125\n",
      "epoch: 8 step: 1261, loss is 0.001620959839783609\n",
      "epoch: 8 step: 1262, loss is 0.003497852012515068\n",
      "epoch: 8 step: 1263, loss is 0.001281380420550704\n",
      "epoch: 8 step: 1264, loss is 9.182807843899354e-05\n",
      "epoch: 8 step: 1265, loss is 0.14067701995372772\n",
      "epoch: 8 step: 1266, loss is 0.00024573830887675285\n",
      "epoch: 8 step: 1267, loss is 0.0028757683467119932\n",
      "epoch: 8 step: 1268, loss is 0.06957299262285233\n",
      "epoch: 8 step: 1269, loss is 0.004314762074500322\n",
      "epoch: 8 step: 1270, loss is 0.0029178718104958534\n",
      "epoch: 8 step: 1271, loss is 0.0851508378982544\n",
      "epoch: 8 step: 1272, loss is 0.001189305679872632\n",
      "epoch: 8 step: 1273, loss is 0.018588431179523468\n",
      "epoch: 8 step: 1274, loss is 0.009174124337732792\n",
      "epoch: 8 step: 1275, loss is 0.0001463499356759712\n",
      "epoch: 8 step: 1276, loss is 0.033082135021686554\n",
      "epoch: 8 step: 1277, loss is 0.02730642445385456\n",
      "epoch: 8 step: 1278, loss is 0.0006149547407403588\n",
      "epoch: 8 step: 1279, loss is 0.0017252310644835234\n",
      "epoch: 8 step: 1280, loss is 0.00552239827811718\n",
      "epoch: 8 step: 1281, loss is 0.002435679780319333\n",
      "epoch: 8 step: 1282, loss is 0.0003286403079982847\n",
      "epoch: 8 step: 1283, loss is 0.036650147289037704\n",
      "epoch: 8 step: 1284, loss is 0.03930375725030899\n",
      "epoch: 8 step: 1285, loss is 0.03301655873656273\n",
      "epoch: 8 step: 1286, loss is 0.059889599680900574\n",
      "epoch: 8 step: 1287, loss is 0.147104412317276\n",
      "epoch: 8 step: 1288, loss is 0.04226582869887352\n",
      "epoch: 8 step: 1289, loss is 0.008937143720686436\n",
      "epoch: 8 step: 1290, loss is 0.03725519776344299\n",
      "epoch: 8 step: 1291, loss is 0.0010227714665234089\n",
      "epoch: 8 step: 1292, loss is 5.4892701882636175e-05\n",
      "epoch: 8 step: 1293, loss is 0.0875808596611023\n",
      "epoch: 8 step: 1294, loss is 0.008203862234950066\n",
      "epoch: 8 step: 1295, loss is 0.09099235385656357\n",
      "epoch: 8 step: 1296, loss is 0.00399738596752286\n",
      "epoch: 8 step: 1297, loss is 0.0015742543619126081\n",
      "epoch: 8 step: 1298, loss is 0.24828088283538818\n",
      "epoch: 8 step: 1299, loss is 0.00023187781334854662\n",
      "epoch: 8 step: 1300, loss is 0.05018973350524902\n",
      "epoch: 8 step: 1301, loss is 0.005887736566364765\n",
      "epoch: 8 step: 1302, loss is 0.1979583501815796\n",
      "epoch: 8 step: 1303, loss is 0.0006764901336282492\n",
      "epoch: 8 step: 1304, loss is 0.0012827174505218863\n",
      "epoch: 8 step: 1305, loss is 0.0005966550088487566\n",
      "epoch: 8 step: 1306, loss is 0.02053811028599739\n",
      "epoch: 8 step: 1307, loss is 0.005945720709860325\n",
      "epoch: 8 step: 1308, loss is 0.0035499408841133118\n",
      "epoch: 8 step: 1309, loss is 0.03181283921003342\n",
      "epoch: 8 step: 1310, loss is 0.0005248209345154464\n",
      "epoch: 8 step: 1311, loss is 0.013843467459082603\n",
      "epoch: 8 step: 1312, loss is 0.014162137173116207\n",
      "epoch: 8 step: 1313, loss is 0.040203556418418884\n",
      "epoch: 8 step: 1314, loss is 0.002828547265380621\n",
      "epoch: 8 step: 1315, loss is 0.0009265466942451894\n",
      "epoch: 8 step: 1316, loss is 0.004903524648398161\n",
      "epoch: 8 step: 1317, loss is 0.0004569808079395443\n",
      "epoch: 8 step: 1318, loss is 0.000282036024145782\n",
      "epoch: 8 step: 1319, loss is 0.0037726538721472025\n",
      "epoch: 8 step: 1320, loss is 0.00046642706729471684\n",
      "epoch: 8 step: 1321, loss is 0.013041488826274872\n",
      "epoch: 8 step: 1322, loss is 0.00036447489401325583\n",
      "epoch: 8 step: 1323, loss is 0.014339670538902283\n",
      "epoch: 8 step: 1324, loss is 0.02167995274066925\n",
      "epoch: 8 step: 1325, loss is 0.04348308965563774\n",
      "epoch: 8 step: 1326, loss is 0.0004437777679413557\n",
      "epoch: 8 step: 1327, loss is 0.007921181619167328\n",
      "epoch: 8 step: 1328, loss is 0.0019286846509203315\n",
      "epoch: 8 step: 1329, loss is 0.0006702381651848555\n",
      "epoch: 8 step: 1330, loss is 0.001120101660490036\n",
      "epoch: 8 step: 1331, loss is 9.384109580423683e-05\n",
      "epoch: 8 step: 1332, loss is 0.004624278284609318\n",
      "epoch: 8 step: 1333, loss is 0.12198057770729065\n",
      "epoch: 8 step: 1334, loss is 0.0002766721008811146\n",
      "epoch: 8 step: 1335, loss is 0.0009554995922371745\n",
      "epoch: 8 step: 1336, loss is 0.010946784168481827\n",
      "epoch: 8 step: 1337, loss is 1.3056376701570116e-05\n",
      "epoch: 8 step: 1338, loss is 0.0017290334217250347\n",
      "epoch: 8 step: 1339, loss is 0.00027490840875543654\n",
      "epoch: 8 step: 1340, loss is 0.0025190627202391624\n",
      "epoch: 8 step: 1341, loss is 0.0006098460871726274\n",
      "epoch: 8 step: 1342, loss is 0.0029890278819948435\n",
      "epoch: 8 step: 1343, loss is 0.05236167460680008\n",
      "epoch: 8 step: 1344, loss is 0.0013882623752579093\n",
      "epoch: 8 step: 1345, loss is 0.0034818127751350403\n",
      "epoch: 8 step: 1346, loss is 0.10419466346502304\n",
      "epoch: 8 step: 1347, loss is 0.09886396676301956\n",
      "epoch: 8 step: 1348, loss is 0.0020215788390487432\n",
      "epoch: 8 step: 1349, loss is 0.012501521036028862\n",
      "epoch: 8 step: 1350, loss is 0.0031397128477692604\n",
      "epoch: 8 step: 1351, loss is 0.0002345315442653373\n",
      "epoch: 8 step: 1352, loss is 0.027646953240036964\n",
      "epoch: 8 step: 1353, loss is 0.0008353808079846203\n",
      "epoch: 8 step: 1354, loss is 0.003005532082170248\n",
      "epoch: 8 step: 1355, loss is 0.05508551001548767\n",
      "epoch: 8 step: 1356, loss is 0.0001389319950249046\n",
      "epoch: 8 step: 1357, loss is 0.0063497694209218025\n",
      "epoch: 8 step: 1358, loss is 0.011665773577988148\n",
      "epoch: 8 step: 1359, loss is 0.05545613169670105\n",
      "epoch: 8 step: 1360, loss is 0.0026412764564156532\n",
      "epoch: 8 step: 1361, loss is 0.0023030228912830353\n",
      "epoch: 8 step: 1362, loss is 0.00967768020927906\n",
      "epoch: 8 step: 1363, loss is 0.0004544005496427417\n",
      "epoch: 8 step: 1364, loss is 0.012067163363099098\n",
      "epoch: 8 step: 1365, loss is 4.6648689021822065e-05\n",
      "epoch: 8 step: 1366, loss is 4.497616828302853e-05\n",
      "epoch: 8 step: 1367, loss is 0.0004578698717523366\n",
      "epoch: 8 step: 1368, loss is 0.013070535846054554\n",
      "epoch: 8 step: 1369, loss is 0.021134810522198677\n",
      "epoch: 8 step: 1370, loss is 0.04574868828058243\n",
      "epoch: 8 step: 1371, loss is 0.00035247195046395063\n",
      "epoch: 8 step: 1372, loss is 0.044319894164800644\n",
      "epoch: 8 step: 1373, loss is 0.017081907019019127\n",
      "epoch: 8 step: 1374, loss is 0.004595586098730564\n",
      "epoch: 8 step: 1375, loss is 0.008981737308204174\n",
      "epoch: 8 step: 1376, loss is 0.0019361384911462665\n",
      "epoch: 8 step: 1377, loss is 0.00015256753249559551\n",
      "epoch: 8 step: 1378, loss is 0.00018352127517573535\n",
      "epoch: 8 step: 1379, loss is 0.15868139266967773\n",
      "epoch: 8 step: 1380, loss is 0.0050803073681890965\n",
      "epoch: 8 step: 1381, loss is 0.07321909070014954\n",
      "epoch: 8 step: 1382, loss is 0.0002634878910612315\n",
      "epoch: 8 step: 1383, loss is 0.004350584466010332\n",
      "epoch: 8 step: 1384, loss is 0.0011483699781820178\n",
      "epoch: 8 step: 1385, loss is 0.10767178237438202\n",
      "epoch: 8 step: 1386, loss is 0.21118798851966858\n",
      "epoch: 8 step: 1387, loss is 0.045657724142074585\n",
      "epoch: 8 step: 1388, loss is 0.0013887633103877306\n",
      "epoch: 8 step: 1389, loss is 0.029094357043504715\n",
      "epoch: 8 step: 1390, loss is 0.10562556982040405\n",
      "epoch: 8 step: 1391, loss is 0.001198057783767581\n",
      "epoch: 8 step: 1392, loss is 0.0005653385305777192\n",
      "epoch: 8 step: 1393, loss is 0.0024639861658215523\n",
      "epoch: 8 step: 1394, loss is 0.0006190661224536598\n",
      "epoch: 8 step: 1395, loss is 9.480641165282577e-05\n",
      "epoch: 8 step: 1396, loss is 0.0006439663120545447\n",
      "epoch: 8 step: 1397, loss is 0.021944474428892136\n",
      "epoch: 8 step: 1398, loss is 0.001869011903181672\n",
      "epoch: 8 step: 1399, loss is 0.01183297298848629\n",
      "epoch: 8 step: 1400, loss is 7.105481927283108e-05\n",
      "epoch: 8 step: 1401, loss is 0.13618694245815277\n",
      "epoch: 8 step: 1402, loss is 0.03235965594649315\n",
      "epoch: 8 step: 1403, loss is 0.0052435328252613544\n",
      "epoch: 8 step: 1404, loss is 0.00019184201664756984\n",
      "epoch: 8 step: 1405, loss is 0.00088896369561553\n",
      "epoch: 8 step: 1406, loss is 0.00013858365127816796\n",
      "epoch: 8 step: 1407, loss is 0.0037742392159998417\n",
      "epoch: 8 step: 1408, loss is 0.007218601647764444\n",
      "epoch: 8 step: 1409, loss is 0.008082431741058826\n",
      "epoch: 8 step: 1410, loss is 8.517201786162332e-05\n",
      "epoch: 8 step: 1411, loss is 0.007669560611248016\n",
      "epoch: 8 step: 1412, loss is 0.005633094813674688\n",
      "epoch: 8 step: 1413, loss is 0.0004139009688515216\n",
      "epoch: 8 step: 1414, loss is 0.007462168578058481\n",
      "epoch: 8 step: 1415, loss is 0.06186637654900551\n",
      "epoch: 8 step: 1416, loss is 5.412836617324501e-05\n",
      "epoch: 8 step: 1417, loss is 0.009345877915620804\n",
      "epoch: 8 step: 1418, loss is 0.0012067754287272692\n",
      "epoch: 8 step: 1419, loss is 0.002149837790057063\n",
      "epoch: 8 step: 1420, loss is 0.08668485283851624\n",
      "epoch: 8 step: 1421, loss is 6.213631422724575e-05\n",
      "epoch: 8 step: 1422, loss is 0.0069882553070783615\n",
      "epoch: 8 step: 1423, loss is 0.0007895045564509928\n",
      "epoch: 8 step: 1424, loss is 0.08595592528581619\n",
      "epoch: 8 step: 1425, loss is 0.0008078614482656121\n",
      "epoch: 8 step: 1426, loss is 0.00040748974424786866\n",
      "epoch: 8 step: 1427, loss is 0.00030472042271867394\n",
      "epoch: 8 step: 1428, loss is 0.0015965590719133615\n",
      "epoch: 8 step: 1429, loss is 0.01170938927680254\n",
      "epoch: 8 step: 1430, loss is 0.018694661557674408\n",
      "epoch: 8 step: 1431, loss is 0.0008100776467472315\n",
      "epoch: 8 step: 1432, loss is 0.009164763614535332\n",
      "epoch: 8 step: 1433, loss is 0.01049008034169674\n",
      "epoch: 8 step: 1434, loss is 0.00046150892740115523\n",
      "epoch: 8 step: 1435, loss is 0.004334069788455963\n",
      "epoch: 8 step: 1436, loss is 0.0019178068032488227\n",
      "epoch: 8 step: 1437, loss is 0.0016773648094385862\n",
      "epoch: 8 step: 1438, loss is 0.030869126319885254\n",
      "epoch: 8 step: 1439, loss is 0.013753870502114296\n",
      "epoch: 8 step: 1440, loss is 0.1342574656009674\n",
      "epoch: 8 step: 1441, loss is 0.0023682033643126488\n",
      "epoch: 8 step: 1442, loss is 0.0025988230481743813\n",
      "epoch: 8 step: 1443, loss is 0.00017611906514503062\n",
      "epoch: 8 step: 1444, loss is 0.0006516947760246694\n",
      "epoch: 8 step: 1445, loss is 0.010109176859259605\n",
      "epoch: 8 step: 1446, loss is 0.003918071277439594\n",
      "epoch: 8 step: 1447, loss is 0.0010433676652610302\n",
      "epoch: 8 step: 1448, loss is 0.0001440859487047419\n",
      "epoch: 8 step: 1449, loss is 0.014707041904330254\n",
      "epoch: 8 step: 1450, loss is 0.006923270877450705\n",
      "epoch: 8 step: 1451, loss is 0.002085760235786438\n",
      "epoch: 8 step: 1452, loss is 3.40366896125488e-05\n",
      "epoch: 8 step: 1453, loss is 0.1224067360162735\n",
      "epoch: 8 step: 1454, loss is 0.001256803167052567\n",
      "epoch: 8 step: 1455, loss is 0.0020118304528295994\n",
      "epoch: 8 step: 1456, loss is 0.0012925001792609692\n",
      "epoch: 8 step: 1457, loss is 0.0007813161937519908\n",
      "epoch: 8 step: 1458, loss is 0.0010455151787027717\n",
      "epoch: 8 step: 1459, loss is 0.021296648308634758\n",
      "epoch: 8 step: 1460, loss is 0.10222965478897095\n",
      "epoch: 8 step: 1461, loss is 0.002413140144199133\n",
      "epoch: 8 step: 1462, loss is 0.0066859303042292595\n",
      "epoch: 8 step: 1463, loss is 0.00043681153329089284\n",
      "epoch: 8 step: 1464, loss is 0.002538672648370266\n",
      "epoch: 8 step: 1465, loss is 0.0029113194905221462\n",
      "epoch: 8 step: 1466, loss is 0.008388396352529526\n",
      "epoch: 8 step: 1467, loss is 0.018655981868505478\n",
      "epoch: 8 step: 1468, loss is 0.00744427228346467\n",
      "epoch: 8 step: 1469, loss is 0.005417431239038706\n",
      "epoch: 8 step: 1470, loss is 0.05613935738801956\n",
      "epoch: 8 step: 1471, loss is 0.018421974033117294\n",
      "epoch: 8 step: 1472, loss is 0.03050130233168602\n",
      "epoch: 8 step: 1473, loss is 0.03356984257698059\n",
      "epoch: 8 step: 1474, loss is 0.04316689819097519\n",
      "epoch: 8 step: 1475, loss is 0.0017992507200688124\n",
      "epoch: 8 step: 1476, loss is 7.860024197725579e-05\n",
      "epoch: 8 step: 1477, loss is 0.02203075960278511\n",
      "epoch: 8 step: 1478, loss is 0.00021758400544058532\n",
      "epoch: 8 step: 1479, loss is 0.004574042744934559\n",
      "epoch: 8 step: 1480, loss is 0.0038141116965562105\n",
      "epoch: 8 step: 1481, loss is 0.06252797693014145\n",
      "epoch: 8 step: 1482, loss is 0.003955073654651642\n",
      "epoch: 8 step: 1483, loss is 0.006767742335796356\n",
      "epoch: 8 step: 1484, loss is 0.03438583016395569\n",
      "epoch: 8 step: 1485, loss is 0.015567164868116379\n",
      "epoch: 8 step: 1486, loss is 0.002006119815632701\n",
      "epoch: 8 step: 1487, loss is 0.020330354571342468\n",
      "epoch: 8 step: 1488, loss is 0.0008799268398433924\n",
      "epoch: 8 step: 1489, loss is 3.377274697413668e-05\n",
      "epoch: 8 step: 1490, loss is 7.623370765941218e-05\n",
      "epoch: 8 step: 1491, loss is 0.00020990885968785733\n",
      "epoch: 8 step: 1492, loss is 0.0008591338410042226\n",
      "epoch: 8 step: 1493, loss is 0.003659047419205308\n",
      "epoch: 8 step: 1494, loss is 0.001992760458961129\n",
      "epoch: 8 step: 1495, loss is 0.0007148677832446992\n",
      "epoch: 8 step: 1496, loss is 0.002288412069901824\n",
      "epoch: 8 step: 1497, loss is 0.008272442035377026\n",
      "epoch: 8 step: 1498, loss is 0.0008554196683689952\n",
      "epoch: 8 step: 1499, loss is 0.10067461431026459\n",
      "epoch: 8 step: 1500, loss is 0.002599958796054125\n",
      "epoch: 8 step: 1501, loss is 0.00554294977337122\n",
      "epoch: 8 step: 1502, loss is 0.0038076182827353477\n",
      "epoch: 8 step: 1503, loss is 0.008333663456141949\n",
      "epoch: 8 step: 1504, loss is 4.943120075040497e-05\n",
      "epoch: 8 step: 1505, loss is 0.00020024653349537402\n",
      "epoch: 8 step: 1506, loss is 0.0011579070705920458\n",
      "epoch: 8 step: 1507, loss is 0.011204030364751816\n",
      "epoch: 8 step: 1508, loss is 0.006664613261818886\n",
      "epoch: 8 step: 1509, loss is 0.005089064594358206\n",
      "epoch: 8 step: 1510, loss is 0.00023950790637172759\n",
      "epoch: 8 step: 1511, loss is 0.0031687726732343435\n",
      "epoch: 8 step: 1512, loss is 0.00030642925412394106\n",
      "epoch: 8 step: 1513, loss is 0.006224814336746931\n",
      "epoch: 8 step: 1514, loss is 0.08631293475627899\n",
      "epoch: 8 step: 1515, loss is 4.0069477108772844e-05\n",
      "epoch: 8 step: 1516, loss is 0.0009057105635292828\n",
      "epoch: 8 step: 1517, loss is 0.0001251102949026972\n",
      "epoch: 8 step: 1518, loss is 0.00034182119998149574\n",
      "epoch: 8 step: 1519, loss is 0.00015162241470534354\n",
      "epoch: 8 step: 1520, loss is 0.030233141034841537\n",
      "epoch: 8 step: 1521, loss is 0.0011506137670949101\n",
      "epoch: 8 step: 1522, loss is 0.04647161066532135\n",
      "epoch: 8 step: 1523, loss is 0.001409525633789599\n",
      "epoch: 8 step: 1524, loss is 0.00043152409489266574\n",
      "epoch: 8 step: 1525, loss is 0.002108263084664941\n",
      "epoch: 8 step: 1526, loss is 0.020758483558893204\n",
      "epoch: 8 step: 1527, loss is 0.08772489428520203\n",
      "epoch: 8 step: 1528, loss is 0.008530575782060623\n",
      "epoch: 8 step: 1529, loss is 0.01336193922907114\n",
      "epoch: 8 step: 1530, loss is 0.05201584845781326\n",
      "epoch: 8 step: 1531, loss is 0.002337707206606865\n",
      "epoch: 8 step: 1532, loss is 0.013449410907924175\n",
      "epoch: 8 step: 1533, loss is 0.0012685471447184682\n",
      "epoch: 8 step: 1534, loss is 0.026181455701589584\n",
      "epoch: 8 step: 1535, loss is 0.004107983782887459\n",
      "epoch: 8 step: 1536, loss is 0.008674168959259987\n",
      "epoch: 8 step: 1537, loss is 0.0006920426967553794\n",
      "epoch: 8 step: 1538, loss is 0.007390732876956463\n",
      "epoch: 8 step: 1539, loss is 0.0074422117322683334\n",
      "epoch: 8 step: 1540, loss is 0.0016460965853184462\n",
      "epoch: 8 step: 1541, loss is 0.00030175011488609016\n",
      "epoch: 8 step: 1542, loss is 0.07500344514846802\n",
      "epoch: 8 step: 1543, loss is 4.6542074414901435e-05\n",
      "epoch: 8 step: 1544, loss is 0.00024823390413075686\n",
      "epoch: 8 step: 1545, loss is 0.024556048214435577\n",
      "epoch: 8 step: 1546, loss is 0.0019635166972875595\n",
      "epoch: 8 step: 1547, loss is 0.0003206266846973449\n",
      "epoch: 8 step: 1548, loss is 0.00021592892881017178\n",
      "epoch: 8 step: 1549, loss is 0.08015615493059158\n",
      "epoch: 8 step: 1550, loss is 0.04218258336186409\n",
      "epoch: 8 step: 1551, loss is 0.0031343121081590652\n",
      "epoch: 8 step: 1552, loss is 0.005345460027456284\n",
      "epoch: 8 step: 1553, loss is 0.19125160574913025\n",
      "epoch: 8 step: 1554, loss is 0.01533842645585537\n",
      "epoch: 8 step: 1555, loss is 0.00019209334277547896\n",
      "epoch: 8 step: 1556, loss is 2.5269982870668173e-05\n",
      "epoch: 8 step: 1557, loss is 0.0005993444938212633\n",
      "epoch: 8 step: 1558, loss is 0.0006319524254649878\n",
      "epoch: 8 step: 1559, loss is 0.0001816738222260028\n",
      "epoch: 8 step: 1560, loss is 1.9107883417746052e-05\n",
      "epoch: 8 step: 1561, loss is 0.00010843670315807685\n",
      "epoch: 8 step: 1562, loss is 0.00933978520333767\n",
      "epoch: 8 step: 1563, loss is 0.11841662973165512\n",
      "epoch: 8 step: 1564, loss is 0.003399062203243375\n",
      "epoch: 8 step: 1565, loss is 0.0006754918722435832\n",
      "epoch: 8 step: 1566, loss is 0.002663605846464634\n",
      "epoch: 8 step: 1567, loss is 0.0029768268577754498\n",
      "epoch: 8 step: 1568, loss is 0.01742098666727543\n",
      "epoch: 8 step: 1569, loss is 0.0023271231912076473\n",
      "epoch: 8 step: 1570, loss is 0.001957467757165432\n",
      "epoch: 8 step: 1571, loss is 0.03928031027317047\n",
      "epoch: 8 step: 1572, loss is 0.0004715145332738757\n",
      "epoch: 8 step: 1573, loss is 0.0007862589554861188\n",
      "epoch: 8 step: 1574, loss is 0.02557401917874813\n",
      "epoch: 8 step: 1575, loss is 0.09899617731571198\n",
      "epoch: 8 step: 1576, loss is 2.4909197236411273e-05\n",
      "epoch: 8 step: 1577, loss is 0.00020572691573761404\n",
      "epoch: 8 step: 1578, loss is 0.023883739486336708\n",
      "epoch: 8 step: 1579, loss is 0.0003991220146417618\n",
      "epoch: 8 step: 1580, loss is 0.0022939660120755434\n",
      "epoch: 8 step: 1581, loss is 0.0029631878715008497\n",
      "epoch: 8 step: 1582, loss is 1.6539588614250533e-05\n",
      "epoch: 8 step: 1583, loss is 0.0013505012029781938\n",
      "epoch: 8 step: 1584, loss is 0.0008258784073404968\n",
      "epoch: 8 step: 1585, loss is 0.012293865904211998\n",
      "epoch: 8 step: 1586, loss is 0.001438512816093862\n",
      "epoch: 8 step: 1587, loss is 0.0007681883871555328\n",
      "epoch: 8 step: 1588, loss is 0.005817513447254896\n",
      "epoch: 8 step: 1589, loss is 0.03920508176088333\n",
      "epoch: 8 step: 1590, loss is 0.0001513120805611834\n",
      "epoch: 8 step: 1591, loss is 0.005282807629555464\n",
      "epoch: 8 step: 1592, loss is 0.10116580873727798\n",
      "epoch: 8 step: 1593, loss is 0.0011214491678401828\n",
      "epoch: 8 step: 1594, loss is 0.001587546314112842\n",
      "epoch: 8 step: 1595, loss is 0.00011367681145202368\n",
      "epoch: 8 step: 1596, loss is 0.004986865445971489\n",
      "epoch: 8 step: 1597, loss is 3.2902189559536055e-05\n",
      "epoch: 8 step: 1598, loss is 0.2861786484718323\n",
      "epoch: 8 step: 1599, loss is 0.009287403896450996\n",
      "epoch: 8 step: 1600, loss is 0.00014362158253788948\n",
      "epoch: 8 step: 1601, loss is 0.00031707901507616043\n",
      "epoch: 8 step: 1602, loss is 0.015485826879739761\n",
      "epoch: 8 step: 1603, loss is 0.005186877679079771\n",
      "epoch: 8 step: 1604, loss is 0.006259287241846323\n",
      "epoch: 8 step: 1605, loss is 0.007721328642219305\n",
      "epoch: 8 step: 1606, loss is 0.15043537318706512\n",
      "epoch: 8 step: 1607, loss is 0.0014919511741027236\n",
      "epoch: 8 step: 1608, loss is 0.014572634361684322\n",
      "epoch: 8 step: 1609, loss is 0.002120762364938855\n",
      "epoch: 8 step: 1610, loss is 0.00010135128104593605\n",
      "epoch: 8 step: 1611, loss is 0.011152775026857853\n",
      "epoch: 8 step: 1612, loss is 0.0013143268879503012\n",
      "epoch: 8 step: 1613, loss is 0.062480662018060684\n",
      "epoch: 8 step: 1614, loss is 0.16747909784317017\n",
      "epoch: 8 step: 1615, loss is 0.06153406575322151\n",
      "epoch: 8 step: 1616, loss is 0.0007577123469673097\n",
      "epoch: 8 step: 1617, loss is 0.0007958645583130419\n",
      "epoch: 8 step: 1618, loss is 0.014141117222607136\n",
      "epoch: 8 step: 1619, loss is 0.016367729753255844\n",
      "epoch: 8 step: 1620, loss is 0.08307338505983353\n",
      "epoch: 8 step: 1621, loss is 0.013788774609565735\n",
      "epoch: 8 step: 1622, loss is 0.004806459881365299\n",
      "epoch: 8 step: 1623, loss is 0.014003584161400795\n",
      "epoch: 8 step: 1624, loss is 0.007035424467176199\n",
      "epoch: 8 step: 1625, loss is 0.0005654062260873616\n",
      "epoch: 8 step: 1626, loss is 0.007250659633427858\n",
      "epoch: 8 step: 1627, loss is 0.0011292824055999517\n",
      "epoch: 8 step: 1628, loss is 0.0100160613656044\n",
      "epoch: 8 step: 1629, loss is 0.0005617606220766902\n",
      "epoch: 8 step: 1630, loss is 0.011354382149875164\n",
      "epoch: 8 step: 1631, loss is 0.11106649041175842\n",
      "epoch: 8 step: 1632, loss is 0.0014987410977482796\n",
      "epoch: 8 step: 1633, loss is 0.001793858828023076\n",
      "epoch: 8 step: 1634, loss is 0.0028839220758527517\n",
      "epoch: 8 step: 1635, loss is 0.3315165340900421\n",
      "epoch: 8 step: 1636, loss is 0.0006487234495580196\n",
      "epoch: 8 step: 1637, loss is 0.014530682936310768\n",
      "epoch: 8 step: 1638, loss is 0.0009286069544032216\n",
      "epoch: 8 step: 1639, loss is 0.058822065591812134\n",
      "epoch: 8 step: 1640, loss is 0.00051386613631621\n",
      "epoch: 8 step: 1641, loss is 0.001478058286011219\n",
      "epoch: 8 step: 1642, loss is 0.048816051334142685\n",
      "epoch: 8 step: 1643, loss is 0.0003776189114432782\n",
      "epoch: 8 step: 1644, loss is 0.0006888254429213703\n",
      "epoch: 8 step: 1645, loss is 0.0030362955294549465\n",
      "epoch: 8 step: 1646, loss is 0.004205483477562666\n",
      "epoch: 8 step: 1647, loss is 0.020873818546533585\n",
      "epoch: 8 step: 1648, loss is 0.02066764608025551\n",
      "epoch: 8 step: 1649, loss is 0.006333858240395784\n",
      "epoch: 8 step: 1650, loss is 0.01406032219529152\n",
      "epoch: 8 step: 1651, loss is 0.022168753668665886\n",
      "epoch: 8 step: 1652, loss is 0.0001581395190441981\n",
      "epoch: 8 step: 1653, loss is 1.1306588021398056e-05\n",
      "epoch: 8 step: 1654, loss is 0.0009938741568475962\n",
      "epoch: 8 step: 1655, loss is 0.040157582610845566\n",
      "epoch: 8 step: 1656, loss is 0.00024788049631752074\n",
      "epoch: 8 step: 1657, loss is 0.0012327581644058228\n",
      "epoch: 8 step: 1658, loss is 0.0028484021313488483\n",
      "epoch: 8 step: 1659, loss is 0.0034331735223531723\n",
      "epoch: 8 step: 1660, loss is 0.1607646346092224\n",
      "epoch: 8 step: 1661, loss is 0.00012251923908479512\n",
      "epoch: 8 step: 1662, loss is 0.0009438652778044343\n",
      "epoch: 8 step: 1663, loss is 0.0005828259163536131\n",
      "epoch: 8 step: 1664, loss is 0.004216571804136038\n",
      "epoch: 8 step: 1665, loss is 0.003206392517313361\n",
      "epoch: 8 step: 1666, loss is 0.0020389354322105646\n",
      "epoch: 8 step: 1667, loss is 0.07926821708679199\n",
      "epoch: 8 step: 1668, loss is 0.0007224652217701077\n",
      "epoch: 8 step: 1669, loss is 0.0006531589897349477\n",
      "epoch: 8 step: 1670, loss is 0.0036536557599902153\n",
      "epoch: 8 step: 1671, loss is 0.002788613550364971\n",
      "epoch: 8 step: 1672, loss is 0.000825936091132462\n",
      "epoch: 8 step: 1673, loss is 0.009463543072342873\n",
      "epoch: 8 step: 1674, loss is 0.027320193126797676\n",
      "epoch: 8 step: 1675, loss is 0.00031106258393265307\n",
      "epoch: 8 step: 1676, loss is 0.0291588194668293\n",
      "epoch: 8 step: 1677, loss is 0.0007955342880450189\n",
      "epoch: 8 step: 1678, loss is 0.004585688002407551\n",
      "epoch: 8 step: 1679, loss is 0.010088278912007809\n",
      "epoch: 8 step: 1680, loss is 0.031147271394729614\n",
      "epoch: 8 step: 1681, loss is 0.08694976568222046\n",
      "epoch: 8 step: 1682, loss is 0.027785280719399452\n",
      "epoch: 8 step: 1683, loss is 0.021136686205863953\n",
      "epoch: 8 step: 1684, loss is 0.0026601054705679417\n",
      "epoch: 8 step: 1685, loss is 0.0005506600718945265\n",
      "epoch: 8 step: 1686, loss is 0.0005028459709137678\n",
      "epoch: 8 step: 1687, loss is 0.001429746043868363\n",
      "epoch: 8 step: 1688, loss is 0.036081742495298386\n",
      "epoch: 8 step: 1689, loss is 0.007302640471607447\n",
      "epoch: 8 step: 1690, loss is 0.0005549041670747101\n",
      "epoch: 8 step: 1691, loss is 0.00017077573284041137\n",
      "epoch: 8 step: 1692, loss is 0.010457386262714863\n",
      "epoch: 8 step: 1693, loss is 0.0035436961334198713\n",
      "epoch: 8 step: 1694, loss is 0.01842082105576992\n",
      "epoch: 8 step: 1695, loss is 0.04123995453119278\n",
      "epoch: 8 step: 1696, loss is 0.00043764806468971074\n",
      "epoch: 8 step: 1697, loss is 0.02961120940744877\n",
      "epoch: 8 step: 1698, loss is 0.008236142806708813\n",
      "epoch: 8 step: 1699, loss is 0.009297718293964863\n",
      "epoch: 8 step: 1700, loss is 0.004052732139825821\n",
      "epoch: 8 step: 1701, loss is 0.00949003268033266\n",
      "epoch: 8 step: 1702, loss is 0.000247849035076797\n",
      "epoch: 8 step: 1703, loss is 0.001591009320691228\n",
      "epoch: 8 step: 1704, loss is 0.00032272134558297694\n",
      "epoch: 8 step: 1705, loss is 0.008219008333981037\n",
      "epoch: 8 step: 1706, loss is 0.025204313918948174\n",
      "epoch: 8 step: 1707, loss is 0.006984195671975613\n",
      "epoch: 8 step: 1708, loss is 0.03366454690694809\n",
      "epoch: 8 step: 1709, loss is 0.0007708651828579605\n",
      "epoch: 8 step: 1710, loss is 0.0005700546898879111\n",
      "epoch: 8 step: 1711, loss is 0.01472788117825985\n",
      "epoch: 8 step: 1712, loss is 0.000639385893009603\n",
      "epoch: 8 step: 1713, loss is 0.05204566940665245\n",
      "epoch: 8 step: 1714, loss is 0.008685747161507607\n",
      "epoch: 8 step: 1715, loss is 0.0003815386153291911\n",
      "epoch: 8 step: 1716, loss is 0.018419725820422173\n",
      "epoch: 8 step: 1717, loss is 0.0025325207971036434\n",
      "epoch: 8 step: 1718, loss is 0.0007283233571797609\n",
      "epoch: 8 step: 1719, loss is 0.00026209678617306054\n",
      "epoch: 8 step: 1720, loss is 0.014533257111907005\n",
      "epoch: 8 step: 1721, loss is 0.011469260789453983\n",
      "epoch: 8 step: 1722, loss is 0.14642709493637085\n",
      "epoch: 8 step: 1723, loss is 0.006695857271552086\n",
      "epoch: 8 step: 1724, loss is 0.0014438870130106807\n",
      "epoch: 8 step: 1725, loss is 9.474138641962782e-05\n",
      "epoch: 8 step: 1726, loss is 0.033465031534433365\n",
      "epoch: 8 step: 1727, loss is 0.00024970967206172645\n",
      "epoch: 8 step: 1728, loss is 0.0006790728075429797\n",
      "epoch: 8 step: 1729, loss is 0.012772493995726109\n",
      "epoch: 8 step: 1730, loss is 0.000477854220662266\n",
      "epoch: 8 step: 1731, loss is 0.029233356937766075\n",
      "epoch: 8 step: 1732, loss is 0.047804612666368484\n",
      "epoch: 8 step: 1733, loss is 0.00036547190393321216\n",
      "epoch: 8 step: 1734, loss is 0.39029818773269653\n",
      "epoch: 8 step: 1735, loss is 2.8333797672530636e-05\n",
      "epoch: 8 step: 1736, loss is 6.558046152349561e-05\n",
      "epoch: 8 step: 1737, loss is 0.0014906602445989847\n",
      "epoch: 8 step: 1738, loss is 0.020130539312958717\n",
      "epoch: 8 step: 1739, loss is 0.007189474999904633\n",
      "epoch: 8 step: 1740, loss is 0.0008801419171504676\n",
      "epoch: 8 step: 1741, loss is 0.007302527781575918\n",
      "epoch: 8 step: 1742, loss is 0.0013313086237758398\n",
      "epoch: 8 step: 1743, loss is 0.0016900287009775639\n",
      "epoch: 8 step: 1744, loss is 0.036784812808036804\n",
      "epoch: 8 step: 1745, loss is 0.03687973693013191\n",
      "epoch: 8 step: 1746, loss is 0.0022931431885808706\n",
      "epoch: 8 step: 1747, loss is 0.0038837571628391743\n",
      "epoch: 8 step: 1748, loss is 0.0015938195865601301\n",
      "epoch: 8 step: 1749, loss is 0.0002126046601915732\n",
      "epoch: 8 step: 1750, loss is 0.06357691437005997\n",
      "epoch: 8 step: 1751, loss is 0.06530823558568954\n",
      "epoch: 8 step: 1752, loss is 0.22322016954421997\n",
      "epoch: 8 step: 1753, loss is 0.0017435353947803378\n",
      "epoch: 8 step: 1754, loss is 0.01698346994817257\n",
      "epoch: 8 step: 1755, loss is 0.039867620915174484\n",
      "epoch: 8 step: 1756, loss is 0.010261406190693378\n",
      "epoch: 8 step: 1757, loss is 0.053961534053087234\n",
      "epoch: 8 step: 1758, loss is 0.0017129810294136405\n",
      "epoch: 8 step: 1759, loss is 0.0003816334647126496\n",
      "epoch: 8 step: 1760, loss is 0.011514751240611076\n",
      "epoch: 8 step: 1761, loss is 0.04475369304418564\n",
      "epoch: 8 step: 1762, loss is 0.00016397147555835545\n",
      "epoch: 8 step: 1763, loss is 0.00015583851200062782\n",
      "epoch: 8 step: 1764, loss is 1.0028733413491864e-05\n",
      "epoch: 8 step: 1765, loss is 0.001747503294609487\n",
      "epoch: 8 step: 1766, loss is 0.0001609800528967753\n",
      "epoch: 8 step: 1767, loss is 0.0004922085208818316\n",
      "epoch: 8 step: 1768, loss is 0.0009291658643633127\n",
      "epoch: 8 step: 1769, loss is 0.002885770285502076\n",
      "epoch: 8 step: 1770, loss is 0.0023283662740141153\n",
      "epoch: 8 step: 1771, loss is 0.014246870763599873\n",
      "epoch: 8 step: 1772, loss is 0.0004623972927220166\n",
      "epoch: 8 step: 1773, loss is 0.007482124958187342\n",
      "epoch: 8 step: 1774, loss is 9.177916217595339e-05\n",
      "epoch: 8 step: 1775, loss is 0.22586838901042938\n",
      "epoch: 8 step: 1776, loss is 0.025812989100813866\n",
      "epoch: 8 step: 1777, loss is 0.00020508166926447302\n",
      "epoch: 8 step: 1778, loss is 0.1514533907175064\n",
      "epoch: 8 step: 1779, loss is 0.022702740505337715\n",
      "epoch: 8 step: 1780, loss is 0.00017263161134906113\n",
      "epoch: 8 step: 1781, loss is 0.0010998270008713007\n",
      "epoch: 8 step: 1782, loss is 0.005145486444234848\n",
      "epoch: 8 step: 1783, loss is 0.18401525914669037\n",
      "epoch: 8 step: 1784, loss is 0.013748749159276485\n",
      "epoch: 8 step: 1785, loss is 0.007280741818249226\n",
      "epoch: 8 step: 1786, loss is 0.004761837422847748\n",
      "epoch: 8 step: 1787, loss is 0.00584328081458807\n",
      "epoch: 8 step: 1788, loss is 0.0005332643049769104\n",
      "epoch: 8 step: 1789, loss is 0.008817139081656933\n",
      "epoch: 8 step: 1790, loss is 0.003949194680899382\n",
      "epoch: 8 step: 1791, loss is 0.002851591445505619\n",
      "epoch: 8 step: 1792, loss is 0.000154697205289267\n",
      "epoch: 8 step: 1793, loss is 0.015746276825666428\n",
      "epoch: 8 step: 1794, loss is 7.616671791765839e-05\n",
      "epoch: 8 step: 1795, loss is 0.006254859734326601\n",
      "epoch: 8 step: 1796, loss is 0.0693972259759903\n",
      "epoch: 8 step: 1797, loss is 0.0005470727919600904\n",
      "epoch: 8 step: 1798, loss is 0.0009127975790761411\n",
      "epoch: 8 step: 1799, loss is 0.0006574319559149444\n",
      "epoch: 8 step: 1800, loss is 0.027845710515975952\n",
      "epoch: 8 step: 1801, loss is 0.0049063400365412235\n",
      "epoch: 8 step: 1802, loss is 0.04266874119639397\n",
      "epoch: 8 step: 1803, loss is 0.03746085986495018\n",
      "epoch: 8 step: 1804, loss is 0.00014126926544122398\n",
      "epoch: 8 step: 1805, loss is 0.05826710909605026\n",
      "epoch: 8 step: 1806, loss is 0.2284843921661377\n",
      "epoch: 8 step: 1807, loss is 4.575515413307585e-05\n",
      "epoch: 8 step: 1808, loss is 0.00013360704178921878\n",
      "epoch: 8 step: 1809, loss is 5.23496164532844e-05\n",
      "epoch: 8 step: 1810, loss is 0.002134900074452162\n",
      "epoch: 8 step: 1811, loss is 0.0002841619134414941\n",
      "epoch: 8 step: 1812, loss is 0.01053231954574585\n",
      "epoch: 8 step: 1813, loss is 6.399511767085642e-05\n",
      "epoch: 8 step: 1814, loss is 0.07342378795146942\n",
      "epoch: 8 step: 1815, loss is 0.0002164343313779682\n",
      "epoch: 8 step: 1816, loss is 0.0003177202306687832\n",
      "epoch: 8 step: 1817, loss is 0.1796872615814209\n",
      "epoch: 8 step: 1818, loss is 0.003213409334421158\n",
      "epoch: 8 step: 1819, loss is 0.0011914066271856427\n",
      "epoch: 8 step: 1820, loss is 0.00034412016975693405\n",
      "epoch: 8 step: 1821, loss is 0.003994603641331196\n",
      "epoch: 8 step: 1822, loss is 0.007135485764592886\n",
      "epoch: 8 step: 1823, loss is 0.01622828096151352\n",
      "epoch: 8 step: 1824, loss is 0.0012552981497719884\n",
      "epoch: 8 step: 1825, loss is 0.0012088487856090069\n",
      "epoch: 8 step: 1826, loss is 0.0017837585182860494\n",
      "epoch: 8 step: 1827, loss is 0.008050861768424511\n",
      "epoch: 8 step: 1828, loss is 0.0005734056467190385\n",
      "epoch: 8 step: 1829, loss is 0.0009311931789852679\n",
      "epoch: 8 step: 1830, loss is 0.00014442684187088162\n",
      "epoch: 8 step: 1831, loss is 0.018150342628359795\n",
      "epoch: 8 step: 1832, loss is 0.0007928874110803008\n",
      "epoch: 8 step: 1833, loss is 0.007420369889587164\n",
      "epoch: 8 step: 1834, loss is 0.00834100041538477\n",
      "epoch: 8 step: 1835, loss is 0.00012899602006655186\n",
      "epoch: 8 step: 1836, loss is 0.04632801562547684\n",
      "epoch: 8 step: 1837, loss is 0.0012011351063847542\n",
      "epoch: 8 step: 1838, loss is 0.0003469425719231367\n",
      "epoch: 8 step: 1839, loss is 0.05254489928483963\n",
      "epoch: 8 step: 1840, loss is 0.003894151421263814\n",
      "epoch: 8 step: 1841, loss is 0.0031153711024671793\n",
      "epoch: 8 step: 1842, loss is 0.0001287656486965716\n",
      "epoch: 8 step: 1843, loss is 0.0005765289533883333\n",
      "epoch: 8 step: 1844, loss is 0.00041552403126843274\n",
      "epoch: 8 step: 1845, loss is 9.616191528039053e-05\n",
      "epoch: 8 step: 1846, loss is 0.07368828356266022\n",
      "epoch: 8 step: 1847, loss is 0.0991256907582283\n",
      "epoch: 8 step: 1848, loss is 0.0001613510394236073\n",
      "epoch: 8 step: 1849, loss is 0.058169782161712646\n",
      "epoch: 8 step: 1850, loss is 0.0009887168416753411\n",
      "epoch: 8 step: 1851, loss is 0.01356505323201418\n",
      "epoch: 8 step: 1852, loss is 0.15209515392780304\n",
      "epoch: 8 step: 1853, loss is 0.00043105537770316005\n",
      "epoch: 8 step: 1854, loss is 0.2655711770057678\n",
      "epoch: 8 step: 1855, loss is 0.0025079769548028708\n",
      "epoch: 8 step: 1856, loss is 0.00047016498865559697\n",
      "epoch: 8 step: 1857, loss is 0.005078676622360945\n",
      "epoch: 8 step: 1858, loss is 0.026243558153510094\n",
      "epoch: 8 step: 1859, loss is 0.0488576665520668\n",
      "epoch: 8 step: 1860, loss is 0.002603891072794795\n",
      "epoch: 8 step: 1861, loss is 0.001167728565633297\n",
      "epoch: 8 step: 1862, loss is 0.0003158378240186721\n",
      "epoch: 8 step: 1863, loss is 0.011314812116324902\n",
      "epoch: 8 step: 1864, loss is 0.02246517315506935\n",
      "epoch: 8 step: 1865, loss is 0.059069473296403885\n",
      "epoch: 8 step: 1866, loss is 0.0015491500962525606\n",
      "epoch: 8 step: 1867, loss is 0.003042361466214061\n",
      "epoch: 8 step: 1868, loss is 0.058028630912303925\n",
      "epoch: 8 step: 1869, loss is 0.12021850049495697\n",
      "epoch: 8 step: 1870, loss is 4.5526579924626276e-05\n",
      "epoch: 8 step: 1871, loss is 0.007690221071243286\n",
      "epoch: 8 step: 1872, loss is 0.0031134728342294693\n",
      "epoch: 8 step: 1873, loss is 0.10428155213594437\n",
      "epoch: 8 step: 1874, loss is 0.032135970890522\n",
      "epoch: 8 step: 1875, loss is 0.004551338031888008\n",
      "epoch: 9 step: 1, loss is 0.0024350499734282494\n",
      "epoch: 9 step: 2, loss is 0.003057404886931181\n",
      "epoch: 9 step: 3, loss is 0.0036643859930336475\n",
      "epoch: 9 step: 4, loss is 0.04789132997393608\n",
      "epoch: 9 step: 5, loss is 0.00013549016148317605\n",
      "epoch: 9 step: 6, loss is 0.0013771508820354939\n",
      "epoch: 9 step: 7, loss is 0.0016877762973308563\n",
      "epoch: 9 step: 8, loss is 0.0001607469457667321\n",
      "epoch: 9 step: 9, loss is 0.0695204809308052\n",
      "epoch: 9 step: 10, loss is 0.0004021788772661239\n",
      "epoch: 9 step: 11, loss is 0.023137852549552917\n",
      "epoch: 9 step: 12, loss is 0.00026897864881902933\n",
      "epoch: 9 step: 13, loss is 0.0017787945689633489\n",
      "epoch: 9 step: 14, loss is 5.282416532281786e-05\n",
      "epoch: 9 step: 15, loss is 0.0034001623280346394\n",
      "epoch: 9 step: 16, loss is 0.0018197085009887815\n",
      "epoch: 9 step: 17, loss is 0.03887954726815224\n",
      "epoch: 9 step: 18, loss is 0.0020337484311312437\n",
      "epoch: 9 step: 19, loss is 0.0027054576203227043\n",
      "epoch: 9 step: 20, loss is 0.0004348131187725812\n",
      "epoch: 9 step: 21, loss is 0.0017047529108822346\n",
      "epoch: 9 step: 22, loss is 0.024746011942625046\n",
      "epoch: 9 step: 23, loss is 0.003579534823074937\n",
      "epoch: 9 step: 24, loss is 0.0007270016940310597\n",
      "epoch: 9 step: 25, loss is 0.09278863668441772\n",
      "epoch: 9 step: 26, loss is 5.822723687742837e-05\n",
      "epoch: 9 step: 27, loss is 0.08757615089416504\n",
      "epoch: 9 step: 28, loss is 0.0002765021054074168\n",
      "epoch: 9 step: 29, loss is 0.010378249920904636\n",
      "epoch: 9 step: 30, loss is 0.026414655148983\n",
      "epoch: 9 step: 31, loss is 0.053720008581876755\n",
      "epoch: 9 step: 32, loss is 0.012762775644659996\n",
      "epoch: 9 step: 33, loss is 0.0014939967077225447\n",
      "epoch: 9 step: 34, loss is 0.010517864488065243\n",
      "epoch: 9 step: 35, loss is 0.0003240845398977399\n",
      "epoch: 9 step: 36, loss is 0.011967295780777931\n",
      "epoch: 9 step: 37, loss is 0.0009302482358179986\n",
      "epoch: 9 step: 38, loss is 0.00013670303451362997\n",
      "epoch: 9 step: 39, loss is 0.0007717238622717559\n",
      "epoch: 9 step: 40, loss is 0.006448217201977968\n",
      "epoch: 9 step: 41, loss is 0.0007393992273136973\n",
      "epoch: 9 step: 42, loss is 0.013768135569989681\n",
      "epoch: 9 step: 43, loss is 7.33893975848332e-05\n",
      "epoch: 9 step: 44, loss is 0.0002398679353063926\n",
      "epoch: 9 step: 45, loss is 0.06258971244096756\n",
      "epoch: 9 step: 46, loss is 0.05232159420847893\n",
      "epoch: 9 step: 47, loss is 0.00015915502444840968\n",
      "epoch: 9 step: 48, loss is 0.0008459141245111823\n",
      "epoch: 9 step: 49, loss is 0.0164955984801054\n",
      "epoch: 9 step: 50, loss is 5.165880065760575e-05\n",
      "epoch: 9 step: 51, loss is 0.0011312763672322035\n",
      "epoch: 9 step: 52, loss is 0.020866595208644867\n",
      "epoch: 9 step: 53, loss is 0.00010009814286604524\n",
      "epoch: 9 step: 54, loss is 0.03792818263173103\n",
      "epoch: 9 step: 55, loss is 0.00019232917111366987\n",
      "epoch: 9 step: 56, loss is 0.00015961886674631387\n",
      "epoch: 9 step: 57, loss is 0.0013270076597109437\n",
      "epoch: 9 step: 58, loss is 0.0003490026865620166\n",
      "epoch: 9 step: 59, loss is 0.002849425422027707\n",
      "epoch: 9 step: 60, loss is 0.00810463447123766\n",
      "epoch: 9 step: 61, loss is 0.0001854333095252514\n",
      "epoch: 9 step: 62, loss is 0.004020249471068382\n",
      "epoch: 9 step: 63, loss is 0.014490051195025444\n",
      "epoch: 9 step: 64, loss is 0.02952161803841591\n",
      "epoch: 9 step: 65, loss is 0.0028456461150199175\n",
      "epoch: 9 step: 66, loss is 4.9209047574549913e-05\n",
      "epoch: 9 step: 67, loss is 0.008845729753375053\n",
      "epoch: 9 step: 68, loss is 0.0011566936736926436\n",
      "epoch: 9 step: 69, loss is 0.011369593441486359\n",
      "epoch: 9 step: 70, loss is 0.002951959380879998\n",
      "epoch: 9 step: 71, loss is 0.00013870486873202026\n",
      "epoch: 9 step: 72, loss is 0.0032501546666026115\n",
      "epoch: 9 step: 73, loss is 0.002019445411860943\n",
      "epoch: 9 step: 74, loss is 0.00017067913722712547\n",
      "epoch: 9 step: 75, loss is 0.01784593053162098\n",
      "epoch: 9 step: 76, loss is 0.029341477900743484\n",
      "epoch: 9 step: 77, loss is 0.04089926928281784\n",
      "epoch: 9 step: 78, loss is 0.0024659777991473675\n",
      "epoch: 9 step: 79, loss is 0.0016227682353928685\n",
      "epoch: 9 step: 80, loss is 0.00011599632125580683\n",
      "epoch: 9 step: 81, loss is 4.51511368737556e-06\n",
      "epoch: 9 step: 82, loss is 0.0006247875862754881\n",
      "epoch: 9 step: 83, loss is 4.536988853942603e-05\n",
      "epoch: 9 step: 84, loss is 4.7813315177336335e-05\n",
      "epoch: 9 step: 85, loss is 3.8365349610103294e-05\n",
      "epoch: 9 step: 86, loss is 0.07718992978334427\n",
      "epoch: 9 step: 87, loss is 0.00041110534220933914\n",
      "epoch: 9 step: 88, loss is 0.0019227864686399698\n",
      "epoch: 9 step: 89, loss is 0.00914465170353651\n",
      "epoch: 9 step: 90, loss is 0.00025933238794095814\n",
      "epoch: 9 step: 91, loss is 0.12954217195510864\n",
      "epoch: 9 step: 92, loss is 5.247024091659114e-05\n",
      "epoch: 9 step: 93, loss is 0.0011693228734657168\n",
      "epoch: 9 step: 94, loss is 0.001676900777965784\n",
      "epoch: 9 step: 95, loss is 0.0003768325550481677\n",
      "epoch: 9 step: 96, loss is 0.007127342280000448\n",
      "epoch: 9 step: 97, loss is 0.0035842645447701216\n",
      "epoch: 9 step: 98, loss is 0.007527741603553295\n",
      "epoch: 9 step: 99, loss is 0.0003341116535011679\n",
      "epoch: 9 step: 100, loss is 0.0005524332518689334\n",
      "epoch: 9 step: 101, loss is 0.0016775588737800717\n",
      "epoch: 9 step: 102, loss is 0.0008923448622226715\n",
      "epoch: 9 step: 103, loss is 0.007483069784939289\n",
      "epoch: 9 step: 104, loss is 0.005316981580108404\n",
      "epoch: 9 step: 105, loss is 0.05359813943505287\n",
      "epoch: 9 step: 106, loss is 0.3118470311164856\n",
      "epoch: 9 step: 107, loss is 0.00021596885926555842\n",
      "epoch: 9 step: 108, loss is 0.00029443931998685\n",
      "epoch: 9 step: 109, loss is 9.513399709248915e-05\n",
      "epoch: 9 step: 110, loss is 0.0008153939852491021\n",
      "epoch: 9 step: 111, loss is 0.0011535685043781996\n",
      "epoch: 9 step: 112, loss is 0.0010946642141789198\n",
      "epoch: 9 step: 113, loss is 0.002409847918897867\n",
      "epoch: 9 step: 114, loss is 0.0003818572440650314\n",
      "epoch: 9 step: 115, loss is 1.6822850739117712e-05\n",
      "epoch: 9 step: 116, loss is 0.00017542587011121213\n",
      "epoch: 9 step: 117, loss is 4.07027910114266e-05\n",
      "epoch: 9 step: 118, loss is 0.0010381682077422738\n",
      "epoch: 9 step: 119, loss is 0.01516735926270485\n",
      "epoch: 9 step: 120, loss is 0.0003519979363773018\n",
      "epoch: 9 step: 121, loss is 0.0018980047898367047\n",
      "epoch: 9 step: 122, loss is 0.176542267203331\n",
      "epoch: 9 step: 123, loss is 0.0016165648121386766\n",
      "epoch: 9 step: 124, loss is 6.463656063715462e-06\n",
      "epoch: 9 step: 125, loss is 0.001967131160199642\n",
      "epoch: 9 step: 126, loss is 0.00013105306425131857\n",
      "epoch: 9 step: 127, loss is 9.551181574352086e-05\n",
      "epoch: 9 step: 128, loss is 0.0014514169888570905\n",
      "epoch: 9 step: 129, loss is 0.0008700638427399099\n",
      "epoch: 9 step: 130, loss is 0.03385094925761223\n",
      "epoch: 9 step: 131, loss is 0.0461963415145874\n",
      "epoch: 9 step: 132, loss is 0.22845499217510223\n",
      "epoch: 9 step: 133, loss is 0.004335056524723768\n",
      "epoch: 9 step: 134, loss is 0.0891592875123024\n",
      "epoch: 9 step: 135, loss is 0.006499505136162043\n",
      "epoch: 9 step: 136, loss is 0.0002852088073268533\n",
      "epoch: 9 step: 137, loss is 0.013757724314928055\n",
      "epoch: 9 step: 138, loss is 0.001160193933174014\n",
      "epoch: 9 step: 139, loss is 0.00872310996055603\n",
      "epoch: 9 step: 140, loss is 0.0021963792387396097\n",
      "epoch: 9 step: 141, loss is 3.023581302841194e-05\n",
      "epoch: 9 step: 142, loss is 0.0003660169313661754\n",
      "epoch: 9 step: 143, loss is 0.0008672800613567233\n",
      "epoch: 9 step: 144, loss is 0.012514805421233177\n",
      "epoch: 9 step: 145, loss is 1.3605403182737064e-05\n",
      "epoch: 9 step: 146, loss is 0.014947392046451569\n",
      "epoch: 9 step: 147, loss is 7.407624798361212e-05\n",
      "epoch: 9 step: 148, loss is 0.00029695287230424583\n",
      "epoch: 9 step: 149, loss is 0.020271746441721916\n",
      "epoch: 9 step: 150, loss is 0.013367997482419014\n",
      "epoch: 9 step: 151, loss is 0.00012911380326841027\n",
      "epoch: 9 step: 152, loss is 0.042054805904626846\n",
      "epoch: 9 step: 153, loss is 0.00041292081004939973\n",
      "epoch: 9 step: 154, loss is 0.019697243347764015\n",
      "epoch: 9 step: 155, loss is 0.00741311302408576\n",
      "epoch: 9 step: 156, loss is 0.003945209085941315\n",
      "epoch: 9 step: 157, loss is 0.0012407094473019242\n",
      "epoch: 9 step: 158, loss is 0.013369214721024036\n",
      "epoch: 9 step: 159, loss is 4.4656080717686564e-05\n",
      "epoch: 9 step: 160, loss is 0.00015024299500510097\n",
      "epoch: 9 step: 161, loss is 0.0003854745882563293\n",
      "epoch: 9 step: 162, loss is 0.004020273219794035\n",
      "epoch: 9 step: 163, loss is 0.00045599209261126816\n",
      "epoch: 9 step: 164, loss is 8.085928129730746e-05\n",
      "epoch: 9 step: 165, loss is 0.00021813288913108408\n",
      "epoch: 9 step: 166, loss is 0.008547182194888592\n",
      "epoch: 9 step: 167, loss is 0.0013194296043366194\n",
      "epoch: 9 step: 168, loss is 0.0338328592479229\n",
      "epoch: 9 step: 169, loss is 0.0015434724045917392\n",
      "epoch: 9 step: 170, loss is 9.836630488280207e-05\n",
      "epoch: 9 step: 171, loss is 0.0007993538165464997\n",
      "epoch: 9 step: 172, loss is 0.002375681884586811\n",
      "epoch: 9 step: 173, loss is 0.0002843698894139379\n",
      "epoch: 9 step: 174, loss is 0.016428114846348763\n",
      "epoch: 9 step: 175, loss is 0.005866531282663345\n",
      "epoch: 9 step: 176, loss is 0.0011832858435809612\n",
      "epoch: 9 step: 177, loss is 0.0009359521791338921\n",
      "epoch: 9 step: 178, loss is 6.625075911870226e-05\n",
      "epoch: 9 step: 179, loss is 0.0005256539443507791\n",
      "epoch: 9 step: 180, loss is 0.0004329529474489391\n",
      "epoch: 9 step: 181, loss is 0.006812466308474541\n",
      "epoch: 9 step: 182, loss is 0.020926419645547867\n",
      "epoch: 9 step: 183, loss is 0.0001981839450309053\n",
      "epoch: 9 step: 184, loss is 0.0005585418548434973\n",
      "epoch: 9 step: 185, loss is 0.0005212505348026752\n",
      "epoch: 9 step: 186, loss is 0.0033964512404054403\n",
      "epoch: 9 step: 187, loss is 0.005678479094058275\n",
      "epoch: 9 step: 188, loss is 8.278515451820567e-05\n",
      "epoch: 9 step: 189, loss is 0.01913630962371826\n",
      "epoch: 9 step: 190, loss is 0.0034346955362707376\n",
      "epoch: 9 step: 191, loss is 0.00015409836487378925\n",
      "epoch: 9 step: 192, loss is 0.00010889619443332776\n",
      "epoch: 9 step: 193, loss is 0.002447145525366068\n",
      "epoch: 9 step: 194, loss is 0.011720605194568634\n",
      "epoch: 9 step: 195, loss is 0.0028837055433541536\n",
      "epoch: 9 step: 196, loss is 0.0007630619802512228\n",
      "epoch: 9 step: 197, loss is 0.003296676790341735\n",
      "epoch: 9 step: 198, loss is 0.010752965696156025\n",
      "epoch: 9 step: 199, loss is 0.0006748829619027674\n",
      "epoch: 9 step: 200, loss is 0.04000882804393768\n",
      "epoch: 9 step: 201, loss is 0.00039747441769577563\n",
      "epoch: 9 step: 202, loss is 0.005802325904369354\n",
      "epoch: 9 step: 203, loss is 5.247447552392259e-05\n",
      "epoch: 9 step: 204, loss is 0.0001394990540575236\n",
      "epoch: 9 step: 205, loss is 0.0015163716161623597\n",
      "epoch: 9 step: 206, loss is 0.0020471857860684395\n",
      "epoch: 9 step: 207, loss is 0.0017053643241524696\n",
      "epoch: 9 step: 208, loss is 0.0002166564663639292\n",
      "epoch: 9 step: 209, loss is 0.12186145037412643\n",
      "epoch: 9 step: 210, loss is 0.006189366802573204\n",
      "epoch: 9 step: 211, loss is 0.0007587528671137989\n",
      "epoch: 9 step: 212, loss is 0.00020962856069672853\n",
      "epoch: 9 step: 213, loss is 0.007842586375772953\n",
      "epoch: 9 step: 214, loss is 0.011927585117518902\n",
      "epoch: 9 step: 215, loss is 0.0012447331100702286\n",
      "epoch: 9 step: 216, loss is 0.06567198783159256\n",
      "epoch: 9 step: 217, loss is 0.001368055585771799\n",
      "epoch: 9 step: 218, loss is 0.026930689811706543\n",
      "epoch: 9 step: 219, loss is 0.008022630587220192\n",
      "epoch: 9 step: 220, loss is 0.0003793364157900214\n",
      "epoch: 9 step: 221, loss is 0.0023491925094276667\n",
      "epoch: 9 step: 222, loss is 6.042312452336773e-05\n",
      "epoch: 9 step: 223, loss is 0.09414347261190414\n",
      "epoch: 9 step: 224, loss is 0.005252187140285969\n",
      "epoch: 9 step: 225, loss is 0.12738992273807526\n",
      "epoch: 9 step: 226, loss is 0.01289944164454937\n",
      "epoch: 9 step: 227, loss is 0.0001877883041743189\n",
      "epoch: 9 step: 228, loss is 0.008208675310015678\n",
      "epoch: 9 step: 229, loss is 0.008993111550807953\n",
      "epoch: 9 step: 230, loss is 0.0010892314603552222\n",
      "epoch: 9 step: 231, loss is 0.001277096220292151\n",
      "epoch: 9 step: 232, loss is 0.00013704082812182605\n",
      "epoch: 9 step: 233, loss is 0.0011307952227070928\n",
      "epoch: 9 step: 234, loss is 0.00024024337471928447\n",
      "epoch: 9 step: 235, loss is 0.0002913388016168028\n",
      "epoch: 9 step: 236, loss is 0.0007999552180990577\n",
      "epoch: 9 step: 237, loss is 0.0032466568518429995\n",
      "epoch: 9 step: 238, loss is 4.556110525300028e-06\n",
      "epoch: 9 step: 239, loss is 0.0822163000702858\n",
      "epoch: 9 step: 240, loss is 0.0001199598191305995\n",
      "epoch: 9 step: 241, loss is 0.00031899873283691704\n",
      "epoch: 9 step: 242, loss is 0.0003842463484033942\n",
      "epoch: 9 step: 243, loss is 0.2264907956123352\n",
      "epoch: 9 step: 244, loss is 0.00032315246062353253\n",
      "epoch: 9 step: 245, loss is 0.0015013081720098853\n",
      "epoch: 9 step: 246, loss is 0.002576526254415512\n",
      "epoch: 9 step: 247, loss is 0.0005020001553930342\n",
      "epoch: 9 step: 248, loss is 0.0016792501555755734\n",
      "epoch: 9 step: 249, loss is 0.005193786229938269\n",
      "epoch: 9 step: 250, loss is 6.116394797572866e-05\n",
      "epoch: 9 step: 251, loss is 0.010921340435743332\n",
      "epoch: 9 step: 252, loss is 0.02804245799779892\n",
      "epoch: 9 step: 253, loss is 0.0002656016149558127\n",
      "epoch: 9 step: 254, loss is 0.004738337825983763\n",
      "epoch: 9 step: 255, loss is 0.00040236019412986934\n",
      "epoch: 9 step: 256, loss is 0.0011996411485597491\n",
      "epoch: 9 step: 257, loss is 0.002300792606547475\n",
      "epoch: 9 step: 258, loss is 0.0014775630552321672\n",
      "epoch: 9 step: 259, loss is 0.058094996958971024\n",
      "epoch: 9 step: 260, loss is 0.0006876763072796166\n",
      "epoch: 9 step: 261, loss is 0.004640690051019192\n",
      "epoch: 9 step: 262, loss is 0.0005819876678287983\n",
      "epoch: 9 step: 263, loss is 0.003158415900543332\n",
      "epoch: 9 step: 264, loss is 0.07101581990718842\n",
      "epoch: 9 step: 265, loss is 0.008989643305540085\n",
      "epoch: 9 step: 266, loss is 0.0006145925144664943\n",
      "epoch: 9 step: 267, loss is 0.005175064317882061\n",
      "epoch: 9 step: 268, loss is 0.00034918004530481994\n",
      "epoch: 9 step: 269, loss is 0.005276146344840527\n",
      "epoch: 9 step: 270, loss is 0.002129898639395833\n",
      "epoch: 9 step: 271, loss is 0.04522698000073433\n",
      "epoch: 9 step: 272, loss is 0.0023983947467058897\n",
      "epoch: 9 step: 273, loss is 0.14154674112796783\n",
      "epoch: 9 step: 274, loss is 0.0004126425483264029\n",
      "epoch: 9 step: 275, loss is 0.0003379244590178132\n",
      "epoch: 9 step: 276, loss is 0.0002258679742226377\n",
      "epoch: 9 step: 277, loss is 0.00010210554319201037\n",
      "epoch: 9 step: 278, loss is 0.0016741454601287842\n",
      "epoch: 9 step: 279, loss is 0.0010944365058094263\n",
      "epoch: 9 step: 280, loss is 6.477587157860398e-05\n",
      "epoch: 9 step: 281, loss is 0.00023140298435464501\n",
      "epoch: 9 step: 282, loss is 0.02361786924302578\n",
      "epoch: 9 step: 283, loss is 0.0006473772809840739\n",
      "epoch: 9 step: 284, loss is 0.0001325162302237004\n",
      "epoch: 9 step: 285, loss is 0.01668936014175415\n",
      "epoch: 9 step: 286, loss is 0.0005862084799446166\n",
      "epoch: 9 step: 287, loss is 0.00025818057474680245\n",
      "epoch: 9 step: 288, loss is 0.14529404044151306\n",
      "epoch: 9 step: 289, loss is 0.016373569145798683\n",
      "epoch: 9 step: 290, loss is 0.0005671743419952691\n",
      "epoch: 9 step: 291, loss is 0.001510700909420848\n",
      "epoch: 9 step: 292, loss is 0.006508793216198683\n",
      "epoch: 9 step: 293, loss is 0.001954467035830021\n",
      "epoch: 9 step: 294, loss is 0.016339993104338646\n",
      "epoch: 9 step: 295, loss is 0.003995880484580994\n",
      "epoch: 9 step: 296, loss is 0.006214960478246212\n",
      "epoch: 9 step: 297, loss is 0.018366128206253052\n",
      "epoch: 9 step: 298, loss is 0.18582621216773987\n",
      "epoch: 9 step: 299, loss is 0.0023089812602847815\n",
      "epoch: 9 step: 300, loss is 0.0007960849907249212\n",
      "epoch: 9 step: 301, loss is 0.0017970134504139423\n",
      "epoch: 9 step: 302, loss is 0.004285396076738834\n",
      "epoch: 9 step: 303, loss is 0.011971424333751202\n",
      "epoch: 9 step: 304, loss is 0.001484467415139079\n",
      "epoch: 9 step: 305, loss is 0.0012276474153622985\n",
      "epoch: 9 step: 306, loss is 0.038494937121868134\n",
      "epoch: 9 step: 307, loss is 0.060261212289333344\n",
      "epoch: 9 step: 308, loss is 0.004269611556082964\n",
      "epoch: 9 step: 309, loss is 0.05293939262628555\n",
      "epoch: 9 step: 310, loss is 0.0325523242354393\n",
      "epoch: 9 step: 311, loss is 1.4707116861245595e-05\n",
      "epoch: 9 step: 312, loss is 0.0035223583690822124\n",
      "epoch: 9 step: 313, loss is 0.0007544130785390735\n",
      "epoch: 9 step: 314, loss is 0.021952981129288673\n",
      "epoch: 9 step: 315, loss is 0.0014344193041324615\n",
      "epoch: 9 step: 316, loss is 0.030446894466876984\n",
      "epoch: 9 step: 317, loss is 0.0013940520584583282\n",
      "epoch: 9 step: 318, loss is 0.04112149029970169\n",
      "epoch: 9 step: 319, loss is 0.0035958909429609776\n",
      "epoch: 9 step: 320, loss is 0.060576215386390686\n",
      "epoch: 9 step: 321, loss is 0.009848485700786114\n",
      "epoch: 9 step: 322, loss is 0.0003279162629041821\n",
      "epoch: 9 step: 323, loss is 0.033542949706315994\n",
      "epoch: 9 step: 324, loss is 0.004915696103125811\n",
      "epoch: 9 step: 325, loss is 0.03770904242992401\n",
      "epoch: 9 step: 326, loss is 0.017954718321561813\n",
      "epoch: 9 step: 327, loss is 8.386099216295406e-05\n",
      "epoch: 9 step: 328, loss is 0.0048628635704517365\n",
      "epoch: 9 step: 329, loss is 3.202536390745081e-05\n",
      "epoch: 9 step: 330, loss is 0.001297195442020893\n",
      "epoch: 9 step: 331, loss is 0.1203458309173584\n",
      "epoch: 9 step: 332, loss is 0.004008146934211254\n",
      "epoch: 9 step: 333, loss is 7.565086707472801e-05\n",
      "epoch: 9 step: 334, loss is 0.02510666474699974\n",
      "epoch: 9 step: 335, loss is 5.178119317861274e-05\n",
      "epoch: 9 step: 336, loss is 0.0007787838694639504\n",
      "epoch: 9 step: 337, loss is 0.00822528824210167\n",
      "epoch: 9 step: 338, loss is 0.00027015700470656157\n",
      "epoch: 9 step: 339, loss is 0.00045099243288859725\n",
      "epoch: 9 step: 340, loss is 0.00021815818035975099\n",
      "epoch: 9 step: 341, loss is 0.005255411379039288\n",
      "epoch: 9 step: 342, loss is 0.00432793702930212\n",
      "epoch: 9 step: 343, loss is 0.0004500928334891796\n",
      "epoch: 9 step: 344, loss is 0.0008045437862165272\n",
      "epoch: 9 step: 345, loss is 0.01881781592965126\n",
      "epoch: 9 step: 346, loss is 0.0253731831908226\n",
      "epoch: 9 step: 347, loss is 0.007600870449095964\n",
      "epoch: 9 step: 348, loss is 0.0007999718654900789\n",
      "epoch: 9 step: 349, loss is 0.00035713682882487774\n",
      "epoch: 9 step: 350, loss is 0.006242751143872738\n",
      "epoch: 9 step: 351, loss is 0.0005979074048809707\n",
      "epoch: 9 step: 352, loss is 0.0008552689105272293\n",
      "epoch: 9 step: 353, loss is 0.0009241197258234024\n",
      "epoch: 9 step: 354, loss is 0.0007167405565269291\n",
      "epoch: 9 step: 355, loss is 0.003423091256991029\n",
      "epoch: 9 step: 356, loss is 0.0022022672928869724\n",
      "epoch: 9 step: 357, loss is 0.034943364560604095\n",
      "epoch: 9 step: 358, loss is 0.00026280072052031755\n",
      "epoch: 9 step: 359, loss is 0.027036042883992195\n",
      "epoch: 9 step: 360, loss is 0.0003210696449968964\n",
      "epoch: 9 step: 361, loss is 0.004482392221689224\n",
      "epoch: 9 step: 362, loss is 4.533737956080586e-05\n",
      "epoch: 9 step: 363, loss is 0.0015167791862040758\n",
      "epoch: 9 step: 364, loss is 0.06822888553142548\n",
      "epoch: 9 step: 365, loss is 0.0004680432321038097\n",
      "epoch: 9 step: 366, loss is 0.03916803002357483\n",
      "epoch: 9 step: 367, loss is 0.0006857722764834762\n",
      "epoch: 9 step: 368, loss is 0.0005928240134380758\n",
      "epoch: 9 step: 369, loss is 0.0001931917213369161\n",
      "epoch: 9 step: 370, loss is 0.017289776355028152\n",
      "epoch: 9 step: 371, loss is 0.010632784105837345\n",
      "epoch: 9 step: 372, loss is 0.0008688565576449037\n",
      "epoch: 9 step: 373, loss is 0.03911217674612999\n",
      "epoch: 9 step: 374, loss is 0.0033585361670702696\n",
      "epoch: 9 step: 375, loss is 0.00019714399240911007\n",
      "epoch: 9 step: 376, loss is 0.00028362544253468513\n",
      "epoch: 9 step: 377, loss is 0.041956719011068344\n",
      "epoch: 9 step: 378, loss is 0.00038356782170012593\n",
      "epoch: 9 step: 379, loss is 0.006322083529084921\n",
      "epoch: 9 step: 380, loss is 0.028252022340893745\n",
      "epoch: 9 step: 381, loss is 0.05541977286338806\n",
      "epoch: 9 step: 382, loss is 0.0043186647817492485\n",
      "epoch: 9 step: 383, loss is 0.00047688811901025474\n",
      "epoch: 9 step: 384, loss is 0.0013745496980845928\n",
      "epoch: 9 step: 385, loss is 0.012998479418456554\n",
      "epoch: 9 step: 386, loss is 0.015145507641136646\n",
      "epoch: 9 step: 387, loss is 0.002086917171254754\n",
      "epoch: 9 step: 388, loss is 0.00026792543940246105\n",
      "epoch: 9 step: 389, loss is 1.0055186976387631e-05\n",
      "epoch: 9 step: 390, loss is 7.652033673366532e-05\n",
      "epoch: 9 step: 391, loss is 0.059844255447387695\n",
      "epoch: 9 step: 392, loss is 0.04544944688677788\n",
      "epoch: 9 step: 393, loss is 0.002582286251708865\n",
      "epoch: 9 step: 394, loss is 6.229219434317201e-05\n",
      "epoch: 9 step: 395, loss is 0.0006317711668089032\n",
      "epoch: 9 step: 396, loss is 0.0005247515509836376\n",
      "epoch: 9 step: 397, loss is 0.0001094757390092127\n",
      "epoch: 9 step: 398, loss is 0.000663082639221102\n",
      "epoch: 9 step: 399, loss is 0.009390247985720634\n",
      "epoch: 9 step: 400, loss is 0.003116118023172021\n",
      "epoch: 9 step: 401, loss is 0.0002674130373634398\n",
      "epoch: 9 step: 402, loss is 0.0019049415132030845\n",
      "epoch: 9 step: 403, loss is 0.0004752653476316482\n",
      "epoch: 9 step: 404, loss is 0.0024271118454635143\n",
      "epoch: 9 step: 405, loss is 0.01159876212477684\n",
      "epoch: 9 step: 406, loss is 0.0004332760290708393\n",
      "epoch: 9 step: 407, loss is 0.03853892907500267\n",
      "epoch: 9 step: 408, loss is 0.0042154984548687935\n",
      "epoch: 9 step: 409, loss is 0.03598836809396744\n",
      "epoch: 9 step: 410, loss is 0.023108644410967827\n",
      "epoch: 9 step: 411, loss is 0.00038527935976162553\n",
      "epoch: 9 step: 412, loss is 7.826740329619497e-05\n",
      "epoch: 9 step: 413, loss is 0.00021019393170718104\n",
      "epoch: 9 step: 414, loss is 0.002865664893761277\n",
      "epoch: 9 step: 415, loss is 0.0014990655472502112\n",
      "epoch: 9 step: 416, loss is 0.022651994600892067\n",
      "epoch: 9 step: 417, loss is 0.00020052402396686375\n",
      "epoch: 9 step: 418, loss is 0.0029169167391955853\n",
      "epoch: 9 step: 419, loss is 0.03809944540262222\n",
      "epoch: 9 step: 420, loss is 0.0005948528996668756\n",
      "epoch: 9 step: 421, loss is 0.007017819676548243\n",
      "epoch: 9 step: 422, loss is 0.0003479434235487133\n",
      "epoch: 9 step: 423, loss is 0.0009673316380940378\n",
      "epoch: 9 step: 424, loss is 0.00017630426737014204\n",
      "epoch: 9 step: 425, loss is 0.0015533530386164784\n",
      "epoch: 9 step: 426, loss is 0.011250806972384453\n",
      "epoch: 9 step: 427, loss is 0.00033743662061169744\n",
      "epoch: 9 step: 428, loss is 0.001621816074475646\n",
      "epoch: 9 step: 429, loss is 0.0013018049066886306\n",
      "epoch: 9 step: 430, loss is 0.005821628961712122\n",
      "epoch: 9 step: 431, loss is 0.021529700607061386\n",
      "epoch: 9 step: 432, loss is 0.005888634826987982\n",
      "epoch: 9 step: 433, loss is 0.003184504108503461\n",
      "epoch: 9 step: 434, loss is 1.6558564311708324e-05\n",
      "epoch: 9 step: 435, loss is 0.00020980462431907654\n",
      "epoch: 9 step: 436, loss is 0.030862074345350266\n",
      "epoch: 9 step: 437, loss is 0.0018011527135968208\n",
      "epoch: 9 step: 438, loss is 0.0016598602524027228\n",
      "epoch: 9 step: 439, loss is 0.00029080946114845574\n",
      "epoch: 9 step: 440, loss is 0.017901811748743057\n",
      "epoch: 9 step: 441, loss is 0.0057239895686507225\n",
      "epoch: 9 step: 442, loss is 0.006855986546725035\n",
      "epoch: 9 step: 443, loss is 0.37356579303741455\n",
      "epoch: 9 step: 444, loss is 0.0001484649837948382\n",
      "epoch: 9 step: 445, loss is 0.004061652347445488\n",
      "epoch: 9 step: 446, loss is 0.0026168720796704292\n",
      "epoch: 9 step: 447, loss is 7.920615462353453e-05\n",
      "epoch: 9 step: 448, loss is 0.003186721121892333\n",
      "epoch: 9 step: 449, loss is 0.03410619497299194\n",
      "epoch: 9 step: 450, loss is 0.0004098463978152722\n",
      "epoch: 9 step: 451, loss is 0.0009753340273164213\n",
      "epoch: 9 step: 452, loss is 8.14816594356671e-05\n",
      "epoch: 9 step: 453, loss is 0.0006455093389376998\n",
      "epoch: 9 step: 454, loss is 0.0012375074438750744\n",
      "epoch: 9 step: 455, loss is 0.06074979528784752\n",
      "epoch: 9 step: 456, loss is 0.017162475734949112\n",
      "epoch: 9 step: 457, loss is 6.0368372942321e-05\n",
      "epoch: 9 step: 458, loss is 0.001808328554034233\n",
      "epoch: 9 step: 459, loss is 0.002202671952545643\n",
      "epoch: 9 step: 460, loss is 0.00018414195801597089\n",
      "epoch: 9 step: 461, loss is 0.08734707534313202\n",
      "epoch: 9 step: 462, loss is 0.0005411089514382184\n",
      "epoch: 9 step: 463, loss is 0.034804899245500565\n",
      "epoch: 9 step: 464, loss is 0.007501137908548117\n",
      "epoch: 9 step: 465, loss is 0.036419130861759186\n",
      "epoch: 9 step: 466, loss is 0.00013165545533411205\n",
      "epoch: 9 step: 467, loss is 0.0019627627916634083\n",
      "epoch: 9 step: 468, loss is 0.055453889071941376\n",
      "epoch: 9 step: 469, loss is 0.012583363801240921\n",
      "epoch: 9 step: 470, loss is 3.5734879929805174e-05\n",
      "epoch: 9 step: 471, loss is 0.00018883615848608315\n",
      "epoch: 9 step: 472, loss is 8.310407429235056e-05\n",
      "epoch: 9 step: 473, loss is 0.12337811291217804\n",
      "epoch: 9 step: 474, loss is 0.01262256596237421\n",
      "epoch: 9 step: 475, loss is 0.02028987742960453\n",
      "epoch: 9 step: 476, loss is 3.621486757765524e-05\n",
      "epoch: 9 step: 477, loss is 0.12496212124824524\n",
      "epoch: 9 step: 478, loss is 0.007176168728619814\n",
      "epoch: 9 step: 479, loss is 0.034471213817596436\n",
      "epoch: 9 step: 480, loss is 0.00022546746185980737\n",
      "epoch: 9 step: 481, loss is 0.0007360797608271241\n",
      "epoch: 9 step: 482, loss is 0.05752066895365715\n",
      "epoch: 9 step: 483, loss is 0.001716798753477633\n",
      "epoch: 9 step: 484, loss is 0.0930991917848587\n",
      "epoch: 9 step: 485, loss is 0.001690733595751226\n",
      "epoch: 9 step: 486, loss is 0.0003982437483500689\n",
      "epoch: 9 step: 487, loss is 0.008625137619674206\n",
      "epoch: 9 step: 488, loss is 0.018832536414265633\n",
      "epoch: 9 step: 489, loss is 0.2816234827041626\n",
      "epoch: 9 step: 490, loss is 5.00197711517103e-05\n",
      "epoch: 9 step: 491, loss is 0.0005680097383446991\n",
      "epoch: 9 step: 492, loss is 0.04674885794520378\n",
      "epoch: 9 step: 493, loss is 0.18070848286151886\n",
      "epoch: 9 step: 494, loss is 0.00021817011293023825\n",
      "epoch: 9 step: 495, loss is 0.0002312418509973213\n",
      "epoch: 9 step: 496, loss is 0.00022632384207099676\n",
      "epoch: 9 step: 497, loss is 0.0027175319846719503\n",
      "epoch: 9 step: 498, loss is 0.0014395660255104303\n",
      "epoch: 9 step: 499, loss is 0.012025199830532074\n",
      "epoch: 9 step: 500, loss is 0.0005937813548371196\n",
      "epoch: 9 step: 501, loss is 0.03536725416779518\n",
      "epoch: 9 step: 502, loss is 0.0002906864683609456\n",
      "epoch: 9 step: 503, loss is 0.01440349593758583\n",
      "epoch: 9 step: 504, loss is 0.002924248343333602\n",
      "epoch: 9 step: 505, loss is 0.0027901986613869667\n",
      "epoch: 9 step: 506, loss is 0.05565323680639267\n",
      "epoch: 9 step: 507, loss is 0.0006293103215284646\n",
      "epoch: 9 step: 508, loss is 0.014816221781075\n",
      "epoch: 9 step: 509, loss is 0.0005291179986670613\n",
      "epoch: 9 step: 510, loss is 0.14438508450984955\n",
      "epoch: 9 step: 511, loss is 0.000807439733762294\n",
      "epoch: 9 step: 512, loss is 0.0745374783873558\n",
      "epoch: 9 step: 513, loss is 0.0010147819994017482\n",
      "epoch: 9 step: 514, loss is 0.009819027036428452\n",
      "epoch: 9 step: 515, loss is 0.0002231796970590949\n",
      "epoch: 9 step: 516, loss is 0.00024289560678880662\n",
      "epoch: 9 step: 517, loss is 0.0004818201996386051\n",
      "epoch: 9 step: 518, loss is 0.0040779416449368\n",
      "epoch: 9 step: 519, loss is 0.0013933475129306316\n",
      "epoch: 9 step: 520, loss is 0.0006986536900512874\n",
      "epoch: 9 step: 521, loss is 0.008365893736481667\n",
      "epoch: 9 step: 522, loss is 0.1974443793296814\n",
      "epoch: 9 step: 523, loss is 0.025633350014686584\n",
      "epoch: 9 step: 524, loss is 0.0019011590629816055\n",
      "epoch: 9 step: 525, loss is 0.0026526176370680332\n",
      "epoch: 9 step: 526, loss is 0.00038402696372941136\n",
      "epoch: 9 step: 527, loss is 0.0018081152811646461\n",
      "epoch: 9 step: 528, loss is 0.0011695199646055698\n",
      "epoch: 9 step: 529, loss is 0.0023457324132323265\n",
      "epoch: 9 step: 530, loss is 0.01773959957063198\n",
      "epoch: 9 step: 531, loss is 0.00012674630852416158\n",
      "epoch: 9 step: 532, loss is 0.001493188552558422\n",
      "epoch: 9 step: 533, loss is 0.03862552344799042\n",
      "epoch: 9 step: 534, loss is 0.00016719763516448438\n",
      "epoch: 9 step: 535, loss is 0.0008759012562222779\n",
      "epoch: 9 step: 536, loss is 0.0012940966989845037\n",
      "epoch: 9 step: 537, loss is 0.0025141900405287743\n",
      "epoch: 9 step: 538, loss is 0.0017247078940272331\n",
      "epoch: 9 step: 539, loss is 0.009273398667573929\n",
      "epoch: 9 step: 540, loss is 0.04981599748134613\n",
      "epoch: 9 step: 541, loss is 0.0635712668299675\n",
      "epoch: 9 step: 542, loss is 0.0018415803788229823\n",
      "epoch: 9 step: 543, loss is 0.0012327969307079911\n",
      "epoch: 9 step: 544, loss is 0.00046592368744313717\n",
      "epoch: 9 step: 545, loss is 0.00012228672858327627\n",
      "epoch: 9 step: 546, loss is 0.0006908197537995875\n",
      "epoch: 9 step: 547, loss is 0.0002852661127690226\n",
      "epoch: 9 step: 548, loss is 0.0159916989505291\n",
      "epoch: 9 step: 549, loss is 0.0005909989704377949\n",
      "epoch: 9 step: 550, loss is 5.434976992546581e-05\n",
      "epoch: 9 step: 551, loss is 0.02267572283744812\n",
      "epoch: 9 step: 552, loss is 0.000722006952855736\n",
      "epoch: 9 step: 553, loss is 0.09936319291591644\n",
      "epoch: 9 step: 554, loss is 0.005619735922664404\n",
      "epoch: 9 step: 555, loss is 0.001444830559194088\n",
      "epoch: 9 step: 556, loss is 0.004912171047180891\n",
      "epoch: 9 step: 557, loss is 0.001489019487053156\n",
      "epoch: 9 step: 558, loss is 0.13938145339488983\n",
      "epoch: 9 step: 559, loss is 0.0026409828569740057\n",
      "epoch: 9 step: 560, loss is 0.002797759836539626\n",
      "epoch: 9 step: 561, loss is 2.975395182147622e-05\n",
      "epoch: 9 step: 562, loss is 0.021671591326594353\n",
      "epoch: 9 step: 563, loss is 0.033503226935863495\n",
      "epoch: 9 step: 564, loss is 0.0008008754812180996\n",
      "epoch: 9 step: 565, loss is 0.0009094778797589242\n",
      "epoch: 9 step: 566, loss is 0.0771738663315773\n",
      "epoch: 9 step: 567, loss is 0.14613856375217438\n",
      "epoch: 9 step: 568, loss is 0.02231043577194214\n",
      "epoch: 9 step: 569, loss is 0.0014687911607325077\n",
      "epoch: 9 step: 570, loss is 0.003150024451315403\n",
      "epoch: 9 step: 571, loss is 0.0017720254836604\n",
      "epoch: 9 step: 572, loss is 0.0016385219059884548\n",
      "epoch: 9 step: 573, loss is 0.0009759956737980247\n",
      "epoch: 9 step: 574, loss is 0.0010725940810516477\n",
      "epoch: 9 step: 575, loss is 0.00013955144095234573\n",
      "epoch: 9 step: 576, loss is 0.012861561961472034\n",
      "epoch: 9 step: 577, loss is 0.003984402399510145\n",
      "epoch: 9 step: 578, loss is 0.007022133097052574\n",
      "epoch: 9 step: 579, loss is 0.00012655399041250348\n",
      "epoch: 9 step: 580, loss is 0.00013071740977466106\n",
      "epoch: 9 step: 581, loss is 0.0002469346218276769\n",
      "epoch: 9 step: 582, loss is 0.0009383057476952672\n",
      "epoch: 9 step: 583, loss is 0.00043367838952690363\n",
      "epoch: 9 step: 584, loss is 0.04546239227056503\n",
      "epoch: 9 step: 585, loss is 0.0019869899842888117\n",
      "epoch: 9 step: 586, loss is 0.00026389656704850495\n",
      "epoch: 9 step: 587, loss is 0.025273684412240982\n",
      "epoch: 9 step: 588, loss is 0.0032894983887672424\n",
      "epoch: 9 step: 589, loss is 0.00011193736281711608\n",
      "epoch: 9 step: 590, loss is 9.241134830517694e-05\n",
      "epoch: 9 step: 591, loss is 0.00021917933190707117\n",
      "epoch: 9 step: 592, loss is 0.04192645847797394\n",
      "epoch: 9 step: 593, loss is 0.006896496284753084\n",
      "epoch: 9 step: 594, loss is 0.0013661631383001804\n",
      "epoch: 9 step: 595, loss is 0.0037733844947069883\n",
      "epoch: 9 step: 596, loss is 0.00022617487411480397\n",
      "epoch: 9 step: 597, loss is 8.009839075384662e-05\n",
      "epoch: 9 step: 598, loss is 8.431713649770245e-05\n",
      "epoch: 9 step: 599, loss is 0.009572832845151424\n",
      "epoch: 9 step: 600, loss is 0.07099924236536026\n",
      "epoch: 9 step: 601, loss is 0.008135104551911354\n",
      "epoch: 9 step: 602, loss is 0.004848306067287922\n",
      "epoch: 9 step: 603, loss is 0.0017711564432829618\n",
      "epoch: 9 step: 604, loss is 0.003836007323116064\n",
      "epoch: 9 step: 605, loss is 0.0017206162447109818\n",
      "epoch: 9 step: 606, loss is 0.000340782105922699\n",
      "epoch: 9 step: 607, loss is 0.03766651824116707\n",
      "epoch: 9 step: 608, loss is 0.0014786726096644998\n",
      "epoch: 9 step: 609, loss is 0.005949830170720816\n",
      "epoch: 9 step: 610, loss is 0.0004384070634841919\n",
      "epoch: 9 step: 611, loss is 0.018262939527630806\n",
      "epoch: 9 step: 612, loss is 0.0003151505661662668\n",
      "epoch: 9 step: 613, loss is 0.0018556858412921429\n",
      "epoch: 9 step: 614, loss is 0.001675330218859017\n",
      "epoch: 9 step: 615, loss is 0.0070243580266833305\n",
      "epoch: 9 step: 616, loss is 0.0001568092411616817\n",
      "epoch: 9 step: 617, loss is 0.01612028107047081\n",
      "epoch: 9 step: 618, loss is 0.0941925123333931\n",
      "epoch: 9 step: 619, loss is 0.0629723072052002\n",
      "epoch: 9 step: 620, loss is 0.0004909296403639019\n",
      "epoch: 9 step: 621, loss is 0.0004572384350467473\n",
      "epoch: 9 step: 622, loss is 0.000507434771861881\n",
      "epoch: 9 step: 623, loss is 0.02222917228937149\n",
      "epoch: 9 step: 624, loss is 1.9146935301250778e-05\n",
      "epoch: 9 step: 625, loss is 0.060124315321445465\n",
      "epoch: 9 step: 626, loss is 0.003006350714713335\n",
      "epoch: 9 step: 627, loss is 0.0009451968944631517\n",
      "epoch: 9 step: 628, loss is 0.01904476247727871\n",
      "epoch: 9 step: 629, loss is 0.0006910916417837143\n",
      "epoch: 9 step: 630, loss is 0.006338601931929588\n",
      "epoch: 9 step: 631, loss is 0.005985286086797714\n",
      "epoch: 9 step: 632, loss is 0.008397514000535011\n",
      "epoch: 9 step: 633, loss is 0.00032154552172869444\n",
      "epoch: 9 step: 634, loss is 0.021743938326835632\n",
      "epoch: 9 step: 635, loss is 0.02570030651986599\n",
      "epoch: 9 step: 636, loss is 0.00017512706108391285\n",
      "epoch: 9 step: 637, loss is 1.1835830264317337e-05\n",
      "epoch: 9 step: 638, loss is 0.004404356703162193\n",
      "epoch: 9 step: 639, loss is 0.00017120617849286646\n",
      "epoch: 9 step: 640, loss is 0.00123183976393193\n",
      "epoch: 9 step: 641, loss is 0.0011830647708848119\n",
      "epoch: 9 step: 642, loss is 0.003725003683939576\n",
      "epoch: 9 step: 643, loss is 6.149559339974076e-05\n",
      "epoch: 9 step: 644, loss is 0.0001839873439166695\n",
      "epoch: 9 step: 645, loss is 0.0001414113212376833\n",
      "epoch: 9 step: 646, loss is 8.281883492600173e-05\n",
      "epoch: 9 step: 647, loss is 0.015366950072348118\n",
      "epoch: 9 step: 648, loss is 0.001004043035209179\n",
      "epoch: 9 step: 649, loss is 0.013423283584415913\n",
      "epoch: 9 step: 650, loss is 0.0006123640341684222\n",
      "epoch: 9 step: 651, loss is 3.91233988921158e-05\n",
      "epoch: 9 step: 652, loss is 0.0017733379499986768\n",
      "epoch: 9 step: 653, loss is 0.003256362397223711\n",
      "epoch: 9 step: 654, loss is 0.07259783148765564\n",
      "epoch: 9 step: 655, loss is 0.0001460923522245139\n",
      "epoch: 9 step: 656, loss is 0.0006403111619874835\n",
      "epoch: 9 step: 657, loss is 0.00222962349653244\n",
      "epoch: 9 step: 658, loss is 0.13938748836517334\n",
      "epoch: 9 step: 659, loss is 0.00020279138698242605\n",
      "epoch: 9 step: 660, loss is 0.02537432499229908\n",
      "epoch: 9 step: 661, loss is 0.006851515732705593\n",
      "epoch: 9 step: 662, loss is 0.0015960634918883443\n",
      "epoch: 9 step: 663, loss is 0.03976631909608841\n",
      "epoch: 9 step: 664, loss is 0.00012038091517752036\n",
      "epoch: 9 step: 665, loss is 0.0007849465473555028\n",
      "epoch: 9 step: 666, loss is 0.006210670340806246\n",
      "epoch: 9 step: 667, loss is 0.025372110307216644\n",
      "epoch: 9 step: 668, loss is 0.006647762376815081\n",
      "epoch: 9 step: 669, loss is 0.35439664125442505\n",
      "epoch: 9 step: 670, loss is 0.02774943970143795\n",
      "epoch: 9 step: 671, loss is 0.0015535015845671296\n",
      "epoch: 9 step: 672, loss is 0.034781794995069504\n",
      "epoch: 9 step: 673, loss is 0.00022016909497324377\n",
      "epoch: 9 step: 674, loss is 0.00020904249686282128\n",
      "epoch: 9 step: 675, loss is 0.006220540963113308\n",
      "epoch: 9 step: 676, loss is 0.002359529258683324\n",
      "epoch: 9 step: 677, loss is 0.00021874858066439629\n",
      "epoch: 9 step: 678, loss is 0.0011081271804869175\n",
      "epoch: 9 step: 679, loss is 0.007538573816418648\n",
      "epoch: 9 step: 680, loss is 0.03201796114444733\n",
      "epoch: 9 step: 681, loss is 0.0043921214528381824\n",
      "epoch: 9 step: 682, loss is 0.0016031393315643072\n",
      "epoch: 9 step: 683, loss is 0.007129108998924494\n",
      "epoch: 9 step: 684, loss is 9.867903281701729e-05\n",
      "epoch: 9 step: 685, loss is 0.028997084125876427\n",
      "epoch: 9 step: 686, loss is 0.00023236838751472533\n",
      "epoch: 9 step: 687, loss is 0.005097436718642712\n",
      "epoch: 9 step: 688, loss is 0.0029773027636110783\n",
      "epoch: 9 step: 689, loss is 0.003760193707421422\n",
      "epoch: 9 step: 690, loss is 0.000352298142388463\n",
      "epoch: 9 step: 691, loss is 1.6763126041041687e-05\n",
      "epoch: 9 step: 692, loss is 0.004298778250813484\n",
      "epoch: 9 step: 693, loss is 0.0008849333971738815\n",
      "epoch: 9 step: 694, loss is 0.0016436964506283402\n",
      "epoch: 9 step: 695, loss is 0.00030967421480454504\n",
      "epoch: 9 step: 696, loss is 0.00023151820641942322\n",
      "epoch: 9 step: 697, loss is 6.373463838826865e-05\n",
      "epoch: 9 step: 698, loss is 0.0005762531072832644\n",
      "epoch: 9 step: 699, loss is 0.028143616393208504\n",
      "epoch: 9 step: 700, loss is 0.01009804755449295\n",
      "epoch: 9 step: 701, loss is 0.00023710375535301864\n",
      "epoch: 9 step: 702, loss is 6.057546215743059e-06\n",
      "epoch: 9 step: 703, loss is 0.017450598999857903\n",
      "epoch: 9 step: 704, loss is 0.003130786120891571\n",
      "epoch: 9 step: 705, loss is 0.0027569597586989403\n",
      "epoch: 9 step: 706, loss is 3.313666820758954e-05\n",
      "epoch: 9 step: 707, loss is 0.012211894616484642\n",
      "epoch: 9 step: 708, loss is 0.0012634643353521824\n",
      "epoch: 9 step: 709, loss is 0.00041889341082423925\n",
      "epoch: 9 step: 710, loss is 0.0001164775385404937\n",
      "epoch: 9 step: 711, loss is 0.00023607285402249545\n",
      "epoch: 9 step: 712, loss is 0.02705509215593338\n",
      "epoch: 9 step: 713, loss is 0.0003546658263076097\n",
      "epoch: 9 step: 714, loss is 0.0006956162396818399\n",
      "epoch: 9 step: 715, loss is 8.466099825454876e-05\n",
      "epoch: 9 step: 716, loss is 0.0006843875162303448\n",
      "epoch: 9 step: 717, loss is 0.053252268582582474\n",
      "epoch: 9 step: 718, loss is 0.00251536606810987\n",
      "epoch: 9 step: 719, loss is 0.006185666657984257\n",
      "epoch: 9 step: 720, loss is 0.005694682244211435\n",
      "epoch: 9 step: 721, loss is 0.007475777994841337\n",
      "epoch: 9 step: 722, loss is 0.11746907234191895\n",
      "epoch: 9 step: 723, loss is 0.0717463344335556\n",
      "epoch: 9 step: 724, loss is 0.0004928256385028362\n",
      "epoch: 9 step: 725, loss is 0.032417748123407364\n",
      "epoch: 9 step: 726, loss is 0.007939423434436321\n",
      "epoch: 9 step: 727, loss is 0.001580529031343758\n",
      "epoch: 9 step: 728, loss is 0.014881329610943794\n",
      "epoch: 9 step: 729, loss is 0.0017464675474911928\n",
      "epoch: 9 step: 730, loss is 0.00044366493239067495\n",
      "epoch: 9 step: 731, loss is 0.00014645388000644743\n",
      "epoch: 9 step: 732, loss is 3.98390693590045e-05\n",
      "epoch: 9 step: 733, loss is 0.015554022043943405\n",
      "epoch: 9 step: 734, loss is 4.357979923952371e-05\n",
      "epoch: 9 step: 735, loss is 0.010635114274919033\n",
      "epoch: 9 step: 736, loss is 0.000646475120447576\n",
      "epoch: 9 step: 737, loss is 0.001635402673855424\n",
      "epoch: 9 step: 738, loss is 7.89791374700144e-05\n",
      "epoch: 9 step: 739, loss is 0.00015993296983651817\n",
      "epoch: 9 step: 740, loss is 0.012484652921557426\n",
      "epoch: 9 step: 741, loss is 0.01188652217388153\n",
      "epoch: 9 step: 742, loss is 0.10580451786518097\n",
      "epoch: 9 step: 743, loss is 0.0012844951124861836\n",
      "epoch: 9 step: 744, loss is 9.726296411827207e-05\n",
      "epoch: 9 step: 745, loss is 7.616936636622995e-05\n",
      "epoch: 9 step: 746, loss is 0.00040497875306755304\n",
      "epoch: 9 step: 747, loss is 0.002022307366132736\n",
      "epoch: 9 step: 748, loss is 7.613700290676206e-05\n",
      "epoch: 9 step: 749, loss is 7.285390893230215e-05\n",
      "epoch: 9 step: 750, loss is 0.00471735093742609\n",
      "epoch: 9 step: 751, loss is 2.1463272787514143e-05\n",
      "epoch: 9 step: 752, loss is 0.0005474787321873009\n",
      "epoch: 9 step: 753, loss is 0.0010088373674079776\n",
      "epoch: 9 step: 754, loss is 0.0016147778369486332\n",
      "epoch: 9 step: 755, loss is 0.0005322422948665917\n",
      "epoch: 9 step: 756, loss is 0.05152709037065506\n",
      "epoch: 9 step: 757, loss is 0.0020314662251621485\n",
      "epoch: 9 step: 758, loss is 0.0002956913667730987\n",
      "epoch: 9 step: 759, loss is 0.016219567507505417\n",
      "epoch: 9 step: 760, loss is 0.0006537234876304865\n",
      "epoch: 9 step: 761, loss is 4.0227925637736917e-05\n",
      "epoch: 9 step: 762, loss is 1.913130472530611e-05\n",
      "epoch: 9 step: 763, loss is 0.001431899145245552\n",
      "epoch: 9 step: 764, loss is 0.000364589475793764\n",
      "epoch: 9 step: 765, loss is 0.031532686203718185\n",
      "epoch: 9 step: 766, loss is 0.002516055479645729\n",
      "epoch: 9 step: 767, loss is 0.00485617108643055\n",
      "epoch: 9 step: 768, loss is 0.001497275778092444\n",
      "epoch: 9 step: 769, loss is 0.0004471847787499428\n",
      "epoch: 9 step: 770, loss is 0.0016702862922102213\n",
      "epoch: 9 step: 771, loss is 0.02085968852043152\n",
      "epoch: 9 step: 772, loss is 0.00022461249318439513\n",
      "epoch: 9 step: 773, loss is 0.0002781710063572973\n",
      "epoch: 9 step: 774, loss is 0.003041244111955166\n",
      "epoch: 9 step: 775, loss is 0.2018987536430359\n",
      "epoch: 9 step: 776, loss is 0.008838712237775326\n",
      "epoch: 9 step: 777, loss is 0.00023177194816526026\n",
      "epoch: 9 step: 778, loss is 0.00017362364451400936\n",
      "epoch: 9 step: 779, loss is 0.014545174315571785\n",
      "epoch: 9 step: 780, loss is 1.6182097169803455e-05\n",
      "epoch: 9 step: 781, loss is 0.0007148126023821533\n",
      "epoch: 9 step: 782, loss is 0.11661131680011749\n",
      "epoch: 9 step: 783, loss is 0.0007016504532657564\n",
      "epoch: 9 step: 784, loss is 7.128557626856491e-05\n",
      "epoch: 9 step: 785, loss is 0.005307862535119057\n",
      "epoch: 9 step: 786, loss is 8.363501365238335e-06\n",
      "epoch: 9 step: 787, loss is 0.00021443335572257638\n",
      "epoch: 9 step: 788, loss is 0.0017231246456503868\n",
      "epoch: 9 step: 789, loss is 0.0022696442902088165\n",
      "epoch: 9 step: 790, loss is 0.0015026459004729986\n",
      "epoch: 9 step: 791, loss is 8.795750181889161e-05\n",
      "epoch: 9 step: 792, loss is 0.19330409169197083\n",
      "epoch: 9 step: 793, loss is 2.6792358767124824e-05\n",
      "epoch: 9 step: 794, loss is 0.0007859744364395738\n",
      "epoch: 9 step: 795, loss is 0.0021500408183783293\n",
      "epoch: 9 step: 796, loss is 0.014208695851266384\n",
      "epoch: 9 step: 797, loss is 0.0023390462156385183\n",
      "epoch: 9 step: 798, loss is 0.0015142018673941493\n",
      "epoch: 9 step: 799, loss is 0.0001705887116258964\n",
      "epoch: 9 step: 800, loss is 7.004462531767786e-05\n",
      "epoch: 9 step: 801, loss is 0.043773263692855835\n",
      "epoch: 9 step: 802, loss is 0.002556745195761323\n",
      "epoch: 9 step: 803, loss is 0.05635729432106018\n",
      "epoch: 9 step: 804, loss is 0.03466913849115372\n",
      "epoch: 9 step: 805, loss is 0.0003848907945211977\n",
      "epoch: 9 step: 806, loss is 0.03003876842558384\n",
      "epoch: 9 step: 807, loss is 0.000809676421340555\n",
      "epoch: 9 step: 808, loss is 0.15659241378307343\n",
      "epoch: 9 step: 809, loss is 0.002176560927182436\n",
      "epoch: 9 step: 810, loss is 0.06457103043794632\n",
      "epoch: 9 step: 811, loss is 0.003113533603027463\n",
      "epoch: 9 step: 812, loss is 6.155890150694177e-05\n",
      "epoch: 9 step: 813, loss is 0.0077124182134866714\n",
      "epoch: 9 step: 814, loss is 0.007657845504581928\n",
      "epoch: 9 step: 815, loss is 0.0002535328676458448\n",
      "epoch: 9 step: 816, loss is 0.00031483499333262444\n",
      "epoch: 9 step: 817, loss is 0.023612933233380318\n",
      "epoch: 9 step: 818, loss is 0.0012230168795213103\n",
      "epoch: 9 step: 819, loss is 0.000871888070832938\n",
      "epoch: 9 step: 820, loss is 0.023763129487633705\n",
      "epoch: 9 step: 821, loss is 0.00029997198726050556\n",
      "epoch: 9 step: 822, loss is 0.009304175153374672\n",
      "epoch: 9 step: 823, loss is 0.015726085752248764\n",
      "epoch: 9 step: 824, loss is 0.009520089253783226\n",
      "epoch: 9 step: 825, loss is 0.20771680772304535\n",
      "epoch: 9 step: 826, loss is 0.00023827208497095853\n",
      "epoch: 9 step: 827, loss is 3.164496229146607e-05\n",
      "epoch: 9 step: 828, loss is 0.020971613004803658\n",
      "epoch: 9 step: 829, loss is 0.03107565827667713\n",
      "epoch: 9 step: 830, loss is 0.06337537616491318\n",
      "epoch: 9 step: 831, loss is 0.0368884839117527\n",
      "epoch: 9 step: 832, loss is 0.0016816571587696671\n",
      "epoch: 9 step: 833, loss is 0.0017139792907983065\n",
      "epoch: 9 step: 834, loss is 0.006499978248029947\n",
      "epoch: 9 step: 835, loss is 0.003327887272462249\n",
      "epoch: 9 step: 836, loss is 0.002061246894299984\n",
      "epoch: 9 step: 837, loss is 0.00012524884368758649\n",
      "epoch: 9 step: 838, loss is 0.0017136975657194853\n",
      "epoch: 9 step: 839, loss is 0.00023587915347889066\n",
      "epoch: 9 step: 840, loss is 0.0035431126598268747\n",
      "epoch: 9 step: 841, loss is 0.0009834807133302093\n",
      "epoch: 9 step: 842, loss is 2.3219643480842933e-05\n",
      "epoch: 9 step: 843, loss is 0.0007326621562242508\n",
      "epoch: 9 step: 844, loss is 0.00015303143300116062\n",
      "epoch: 9 step: 845, loss is 0.0008698165183886886\n",
      "epoch: 9 step: 846, loss is 0.06876219809055328\n",
      "epoch: 9 step: 847, loss is 0.0005998826818540692\n",
      "epoch: 9 step: 848, loss is 0.09203372150659561\n",
      "epoch: 9 step: 849, loss is 0.0008697450975887477\n",
      "epoch: 9 step: 850, loss is 0.002060291590169072\n",
      "epoch: 9 step: 851, loss is 0.0038772081024944782\n",
      "epoch: 9 step: 852, loss is 0.0026157163083553314\n",
      "epoch: 9 step: 853, loss is 0.003295790869742632\n",
      "epoch: 9 step: 854, loss is 0.0010486636310815811\n",
      "epoch: 9 step: 855, loss is 1.3927960935689043e-05\n",
      "epoch: 9 step: 856, loss is 5.407066782936454e-05\n",
      "epoch: 9 step: 857, loss is 0.03831319510936737\n",
      "epoch: 9 step: 858, loss is 9.436446998734027e-05\n",
      "epoch: 9 step: 859, loss is 0.017619213089346886\n",
      "epoch: 9 step: 860, loss is 0.00018840715347323567\n",
      "epoch: 9 step: 861, loss is 0.0003785489243455231\n",
      "epoch: 9 step: 862, loss is 0.024283956736326218\n",
      "epoch: 9 step: 863, loss is 0.00022749672643840313\n",
      "epoch: 9 step: 864, loss is 0.028221435844898224\n",
      "epoch: 9 step: 865, loss is 0.0017812313744798303\n",
      "epoch: 9 step: 866, loss is 0.0019306909525766969\n",
      "epoch: 9 step: 867, loss is 0.0036879018880426884\n",
      "epoch: 9 step: 868, loss is 0.00014232743706088513\n",
      "epoch: 9 step: 869, loss is 0.019486846402287483\n",
      "epoch: 9 step: 870, loss is 0.0001612807100173086\n",
      "epoch: 9 step: 871, loss is 0.01830822229385376\n",
      "epoch: 9 step: 872, loss is 0.0001860427437350154\n",
      "epoch: 9 step: 873, loss is 0.004999830387532711\n",
      "epoch: 9 step: 874, loss is 0.01196314301341772\n",
      "epoch: 9 step: 875, loss is 0.0004136475035920739\n",
      "epoch: 9 step: 876, loss is 0.00040423753671348095\n",
      "epoch: 9 step: 877, loss is 0.0005111694918014109\n",
      "epoch: 9 step: 878, loss is 0.0011791303986683488\n",
      "epoch: 9 step: 879, loss is 0.004152171313762665\n",
      "epoch: 9 step: 880, loss is 0.001646086573600769\n",
      "epoch: 9 step: 881, loss is 5.7258053857367486e-05\n",
      "epoch: 9 step: 882, loss is 0.010484146885573864\n",
      "epoch: 9 step: 883, loss is 0.006761797238141298\n",
      "epoch: 9 step: 884, loss is 0.005302549339830875\n",
      "epoch: 9 step: 885, loss is 0.00022219399397727102\n",
      "epoch: 9 step: 886, loss is 0.0001919549104059115\n",
      "epoch: 9 step: 887, loss is 0.13718412816524506\n",
      "epoch: 9 step: 888, loss is 0.0021219151094555855\n",
      "epoch: 9 step: 889, loss is 0.0003040288866031915\n",
      "epoch: 9 step: 890, loss is 0.0004683174774982035\n",
      "epoch: 9 step: 891, loss is 0.002484204713255167\n",
      "epoch: 9 step: 892, loss is 0.0003901607997249812\n",
      "epoch: 9 step: 893, loss is 0.016700588166713715\n",
      "epoch: 9 step: 894, loss is 0.00018322936375625432\n",
      "epoch: 9 step: 895, loss is 0.00036979815922677517\n",
      "epoch: 9 step: 896, loss is 0.006618955172598362\n",
      "epoch: 9 step: 897, loss is 0.02191842906177044\n",
      "epoch: 9 step: 898, loss is 2.9544655262725428e-05\n",
      "epoch: 9 step: 899, loss is 0.00010131623275810853\n",
      "epoch: 9 step: 900, loss is 0.004730990156531334\n",
      "epoch: 9 step: 901, loss is 0.0015430942876264453\n",
      "epoch: 9 step: 902, loss is 0.00010037366882897913\n",
      "epoch: 9 step: 903, loss is 7.410954276565462e-05\n",
      "epoch: 9 step: 904, loss is 6.701982783852145e-05\n",
      "epoch: 9 step: 905, loss is 0.0005830875597894192\n",
      "epoch: 9 step: 906, loss is 0.00018281253869645298\n",
      "epoch: 9 step: 907, loss is 0.00018447954789735377\n",
      "epoch: 9 step: 908, loss is 0.00012091560347471386\n",
      "epoch: 9 step: 909, loss is 6.442895391955972e-05\n",
      "epoch: 9 step: 910, loss is 3.593873407226056e-05\n",
      "epoch: 9 step: 911, loss is 0.00031183662940748036\n",
      "epoch: 9 step: 912, loss is 0.12618683278560638\n",
      "epoch: 9 step: 913, loss is 0.03130640834569931\n",
      "epoch: 9 step: 914, loss is 0.0002019040985032916\n",
      "epoch: 9 step: 915, loss is 0.0015102046309038997\n",
      "epoch: 9 step: 916, loss is 0.01621357537806034\n",
      "epoch: 9 step: 917, loss is 0.035730354487895966\n",
      "epoch: 9 step: 918, loss is 0.0011394817847758532\n",
      "epoch: 9 step: 919, loss is 0.00019046320812776685\n",
      "epoch: 9 step: 920, loss is 0.008099584840238094\n",
      "epoch: 9 step: 921, loss is 0.0001078780842362903\n",
      "epoch: 9 step: 922, loss is 3.4778302506310865e-05\n",
      "epoch: 9 step: 923, loss is 2.1533405742957257e-05\n",
      "epoch: 9 step: 924, loss is 3.3923148293979466e-05\n",
      "epoch: 9 step: 925, loss is 0.038619983941316605\n",
      "epoch: 9 step: 926, loss is 0.00011335849558236077\n",
      "epoch: 9 step: 927, loss is 0.0022350852377712727\n",
      "epoch: 9 step: 928, loss is 0.00010311433288734406\n",
      "epoch: 9 step: 929, loss is 0.033034149557352066\n",
      "epoch: 9 step: 930, loss is 0.002017777878791094\n",
      "epoch: 9 step: 931, loss is 0.0515378937125206\n",
      "epoch: 9 step: 932, loss is 0.002934981370344758\n",
      "epoch: 9 step: 933, loss is 0.0003365185984876007\n",
      "epoch: 9 step: 934, loss is 0.0017328691901639104\n",
      "epoch: 9 step: 935, loss is 0.0016067522810772061\n",
      "epoch: 9 step: 936, loss is 0.002441592048853636\n",
      "epoch: 9 step: 937, loss is 0.0002582829911261797\n",
      "epoch: 9 step: 938, loss is 7.157075015129521e-05\n",
      "epoch: 9 step: 939, loss is 0.30997395515441895\n",
      "epoch: 9 step: 940, loss is 0.014509418979287148\n",
      "epoch: 9 step: 941, loss is 0.0003401183639653027\n",
      "epoch: 9 step: 942, loss is 0.003906603436917067\n",
      "epoch: 9 step: 943, loss is 0.0004510572471190244\n",
      "epoch: 9 step: 944, loss is 0.034319352358579636\n",
      "epoch: 9 step: 945, loss is 0.039591770619153976\n",
      "epoch: 9 step: 946, loss is 0.04505468159914017\n",
      "epoch: 9 step: 947, loss is 0.13318078219890594\n",
      "epoch: 9 step: 948, loss is 0.0005169108044356108\n",
      "epoch: 9 step: 949, loss is 5.265033905743621e-05\n",
      "epoch: 9 step: 950, loss is 0.0015831429045647383\n",
      "epoch: 9 step: 951, loss is 0.001149281277321279\n",
      "epoch: 9 step: 952, loss is 0.02194110117852688\n",
      "epoch: 9 step: 953, loss is 0.0007874540751799941\n",
      "epoch: 9 step: 954, loss is 0.00014466697757598013\n",
      "epoch: 9 step: 955, loss is 0.0010101539082825184\n",
      "epoch: 9 step: 956, loss is 0.008219114504754543\n",
      "epoch: 9 step: 957, loss is 0.0005362813826650381\n",
      "epoch: 9 step: 958, loss is 0.0002701047924347222\n",
      "epoch: 9 step: 959, loss is 0.00015123073535505682\n",
      "epoch: 9 step: 960, loss is 0.0009894870454445481\n",
      "epoch: 9 step: 961, loss is 0.11680170148611069\n",
      "epoch: 9 step: 962, loss is 0.0008955380762927234\n",
      "epoch: 9 step: 963, loss is 0.08256218582391739\n",
      "epoch: 9 step: 964, loss is 0.00023758772294968367\n",
      "epoch: 9 step: 965, loss is 0.0026020356453955173\n",
      "epoch: 9 step: 966, loss is 0.00036004773573949933\n",
      "epoch: 9 step: 967, loss is 7.0446885729325e-06\n",
      "epoch: 9 step: 968, loss is 0.0006099909078329802\n",
      "epoch: 9 step: 969, loss is 0.01919771544635296\n",
      "epoch: 9 step: 970, loss is 0.0020255132112652063\n",
      "epoch: 9 step: 971, loss is 0.030969731509685516\n",
      "epoch: 9 step: 972, loss is 0.001547826686874032\n",
      "epoch: 9 step: 973, loss is 0.0010503089288249612\n",
      "epoch: 9 step: 974, loss is 0.000980424927547574\n",
      "epoch: 9 step: 975, loss is 0.001423756475560367\n",
      "epoch: 9 step: 976, loss is 0.058503564447164536\n",
      "epoch: 9 step: 977, loss is 0.00027740621590055525\n",
      "epoch: 9 step: 978, loss is 7.912894216133282e-05\n",
      "epoch: 9 step: 979, loss is 0.0007755437982268631\n",
      "epoch: 9 step: 980, loss is 0.020557837560772896\n",
      "epoch: 9 step: 981, loss is 7.052046566968784e-05\n",
      "epoch: 9 step: 982, loss is 0.0002210672973887995\n",
      "epoch: 9 step: 983, loss is 4.292931407690048e-05\n",
      "epoch: 9 step: 984, loss is 0.007426257710903883\n",
      "epoch: 9 step: 985, loss is 0.001094004837796092\n",
      "epoch: 9 step: 986, loss is 0.016045114025473595\n",
      "epoch: 9 step: 987, loss is 0.027897922322154045\n",
      "epoch: 9 step: 988, loss is 0.0008732940768823028\n",
      "epoch: 9 step: 989, loss is 0.04148772731423378\n",
      "epoch: 9 step: 990, loss is 0.0017118173418566585\n",
      "epoch: 9 step: 991, loss is 0.005810272414237261\n",
      "epoch: 9 step: 992, loss is 7.352157990681008e-05\n",
      "epoch: 9 step: 993, loss is 0.05862579122185707\n",
      "epoch: 9 step: 994, loss is 0.001174609875306487\n",
      "epoch: 9 step: 995, loss is 0.0007395999855361879\n",
      "epoch: 9 step: 996, loss is 0.0031844640616327524\n",
      "epoch: 9 step: 997, loss is 0.0008526364108547568\n",
      "epoch: 9 step: 998, loss is 0.052473124116659164\n",
      "epoch: 9 step: 999, loss is 0.0001253029186045751\n",
      "epoch: 9 step: 1000, loss is 0.0005071586347185075\n",
      "epoch: 9 step: 1001, loss is 6.354029756039381e-05\n",
      "epoch: 9 step: 1002, loss is 0.001773305470123887\n",
      "epoch: 9 step: 1003, loss is 0.016093220561742783\n",
      "epoch: 9 step: 1004, loss is 2.4362154363188893e-05\n",
      "epoch: 9 step: 1005, loss is 0.00010365147318225354\n",
      "epoch: 9 step: 1006, loss is 0.0005032318294979632\n",
      "epoch: 9 step: 1007, loss is 0.027949953451752663\n",
      "epoch: 9 step: 1008, loss is 0.00011219760199310258\n",
      "epoch: 9 step: 1009, loss is 0.05738475173711777\n",
      "epoch: 9 step: 1010, loss is 0.00789227057248354\n",
      "epoch: 9 step: 1011, loss is 0.001487657893449068\n",
      "epoch: 9 step: 1012, loss is 2.497609966667369e-05\n",
      "epoch: 9 step: 1013, loss is 0.08503810316324234\n",
      "epoch: 9 step: 1014, loss is 0.21145592629909515\n",
      "epoch: 9 step: 1015, loss is 0.00019944473751820624\n",
      "epoch: 9 step: 1016, loss is 0.005960904527455568\n",
      "epoch: 9 step: 1017, loss is 0.0019148836145177484\n",
      "epoch: 9 step: 1018, loss is 0.02368471398949623\n",
      "epoch: 9 step: 1019, loss is 0.0019637919031083584\n",
      "epoch: 9 step: 1020, loss is 0.010021713562309742\n",
      "epoch: 9 step: 1021, loss is 0.006156377959996462\n",
      "epoch: 9 step: 1022, loss is 0.0016491973074153066\n",
      "epoch: 9 step: 1023, loss is 0.004134304355829954\n",
      "epoch: 9 step: 1024, loss is 0.12349031865596771\n",
      "epoch: 9 step: 1025, loss is 0.00013402696640696377\n",
      "epoch: 9 step: 1026, loss is 0.00020681103342212737\n",
      "epoch: 9 step: 1027, loss is 0.11800162494182587\n",
      "epoch: 9 step: 1028, loss is 0.0004918901831842959\n",
      "epoch: 9 step: 1029, loss is 0.0008305764640681446\n",
      "epoch: 9 step: 1030, loss is 0.028037510812282562\n",
      "epoch: 9 step: 1031, loss is 0.00019528850680217147\n",
      "epoch: 9 step: 1032, loss is 0.050265226513147354\n",
      "epoch: 9 step: 1033, loss is 0.0006265533156692982\n",
      "epoch: 9 step: 1034, loss is 0.0014104916481301188\n",
      "epoch: 9 step: 1035, loss is 0.0003374424995854497\n",
      "epoch: 9 step: 1036, loss is 0.0017001002561300993\n",
      "epoch: 9 step: 1037, loss is 0.00012867627083323896\n",
      "epoch: 9 step: 1038, loss is 0.00010080725769512355\n",
      "epoch: 9 step: 1039, loss is 0.0021453723311424255\n",
      "epoch: 9 step: 1040, loss is 0.0003621063369791955\n",
      "epoch: 9 step: 1041, loss is 0.0004641589766833931\n",
      "epoch: 9 step: 1042, loss is 0.009881190955638885\n",
      "epoch: 9 step: 1043, loss is 0.000754235137719661\n",
      "epoch: 9 step: 1044, loss is 2.115567804139573e-05\n",
      "epoch: 9 step: 1045, loss is 0.0007222386193461716\n",
      "epoch: 9 step: 1046, loss is 3.3874428481794894e-05\n",
      "epoch: 9 step: 1047, loss is 0.0028657587245106697\n",
      "epoch: 9 step: 1048, loss is 0.0014632884413003922\n",
      "epoch: 9 step: 1049, loss is 0.003039398230612278\n",
      "epoch: 9 step: 1050, loss is 0.010484918020665646\n",
      "epoch: 9 step: 1051, loss is 0.00382913276553154\n",
      "epoch: 9 step: 1052, loss is 0.0853373110294342\n",
      "epoch: 9 step: 1053, loss is 0.0683542862534523\n",
      "epoch: 9 step: 1054, loss is 0.08861476182937622\n",
      "epoch: 9 step: 1055, loss is 7.949014980113134e-05\n",
      "epoch: 9 step: 1056, loss is 0.16074813902378082\n",
      "epoch: 9 step: 1057, loss is 0.0009587492677383125\n",
      "epoch: 9 step: 1058, loss is 0.0025139707140624523\n",
      "epoch: 9 step: 1059, loss is 0.001745325163938105\n",
      "epoch: 9 step: 1060, loss is 0.004929686430841684\n",
      "epoch: 9 step: 1061, loss is 0.011491574347019196\n",
      "epoch: 9 step: 1062, loss is 0.0082065649330616\n",
      "epoch: 9 step: 1063, loss is 0.006762861739844084\n",
      "epoch: 9 step: 1064, loss is 0.000978912110440433\n",
      "epoch: 9 step: 1065, loss is 0.0027348061557859182\n",
      "epoch: 9 step: 1066, loss is 0.0002935778466053307\n",
      "epoch: 9 step: 1067, loss is 0.03941649943590164\n",
      "epoch: 9 step: 1068, loss is 0.0005219968152232468\n",
      "epoch: 9 step: 1069, loss is 0.0020085766445845366\n",
      "epoch: 9 step: 1070, loss is 0.0010216579539701343\n",
      "epoch: 9 step: 1071, loss is 0.038427188992500305\n",
      "epoch: 9 step: 1072, loss is 0.026284031569957733\n",
      "epoch: 9 step: 1073, loss is 0.0034047935623675585\n",
      "epoch: 9 step: 1074, loss is 0.01131468079984188\n",
      "epoch: 9 step: 1075, loss is 0.004097881261259317\n",
      "epoch: 9 step: 1076, loss is 0.001546868123114109\n",
      "epoch: 9 step: 1077, loss is 0.0037362936418503523\n",
      "epoch: 9 step: 1078, loss is 0.040739431977272034\n",
      "epoch: 9 step: 1079, loss is 0.009981034323573112\n",
      "epoch: 9 step: 1080, loss is 0.010678182356059551\n",
      "epoch: 9 step: 1081, loss is 0.01631680689752102\n",
      "epoch: 9 step: 1082, loss is 0.004652311559766531\n",
      "epoch: 9 step: 1083, loss is 0.006822230760008097\n",
      "epoch: 9 step: 1084, loss is 0.02012651041150093\n",
      "epoch: 9 step: 1085, loss is 0.11996898800134659\n",
      "epoch: 9 step: 1086, loss is 0.0068788048811256886\n",
      "epoch: 9 step: 1087, loss is 0.00794154591858387\n",
      "epoch: 9 step: 1088, loss is 0.00012927164789289236\n",
      "epoch: 9 step: 1089, loss is 0.0033489533234387636\n",
      "epoch: 9 step: 1090, loss is 0.003985765390098095\n",
      "epoch: 9 step: 1091, loss is 0.0005002586985938251\n",
      "epoch: 9 step: 1092, loss is 0.07504341006278992\n",
      "epoch: 9 step: 1093, loss is 0.00016439511091448367\n",
      "epoch: 9 step: 1094, loss is 0.0018983599729835987\n",
      "epoch: 9 step: 1095, loss is 0.0016954009188339114\n",
      "epoch: 9 step: 1096, loss is 0.00036015798104926944\n",
      "epoch: 9 step: 1097, loss is 0.001533284317702055\n",
      "epoch: 9 step: 1098, loss is 0.13677625358104706\n",
      "epoch: 9 step: 1099, loss is 0.000489566009491682\n",
      "epoch: 9 step: 1100, loss is 0.11752396821975708\n",
      "epoch: 9 step: 1101, loss is 0.0011202475288882852\n",
      "epoch: 9 step: 1102, loss is 0.003523703431710601\n",
      "epoch: 9 step: 1103, loss is 2.7995931304758415e-05\n",
      "epoch: 9 step: 1104, loss is 0.01301545463502407\n",
      "epoch: 9 step: 1105, loss is 0.00017727771773934364\n",
      "epoch: 9 step: 1106, loss is 0.012593467719852924\n",
      "epoch: 9 step: 1107, loss is 0.013921234756708145\n",
      "epoch: 9 step: 1108, loss is 0.003754770616069436\n",
      "epoch: 9 step: 1109, loss is 0.000308962888084352\n",
      "epoch: 9 step: 1110, loss is 0.0004594335041474551\n",
      "epoch: 9 step: 1111, loss is 0.0003936414432246238\n",
      "epoch: 9 step: 1112, loss is 5.901000804442447e-06\n",
      "epoch: 9 step: 1113, loss is 0.0023539795074611902\n",
      "epoch: 9 step: 1114, loss is 0.0011903520207852125\n",
      "epoch: 9 step: 1115, loss is 0.001543204067274928\n",
      "epoch: 9 step: 1116, loss is 0.0005593688110820949\n",
      "epoch: 9 step: 1117, loss is 0.12886086106300354\n",
      "epoch: 9 step: 1118, loss is 4.442919453140348e-05\n",
      "epoch: 9 step: 1119, loss is 0.0012116088764742017\n",
      "epoch: 9 step: 1120, loss is 0.00034305264125578105\n",
      "epoch: 9 step: 1121, loss is 5.136745312483981e-05\n",
      "epoch: 9 step: 1122, loss is 0.0004619954852387309\n",
      "epoch: 9 step: 1123, loss is 0.006611029617488384\n",
      "epoch: 9 step: 1124, loss is 0.00022122316295281053\n",
      "epoch: 9 step: 1125, loss is 0.0038119431119412184\n",
      "epoch: 9 step: 1126, loss is 0.019779112190008163\n",
      "epoch: 9 step: 1127, loss is 8.380802319152281e-05\n",
      "epoch: 9 step: 1128, loss is 0.014597253873944283\n",
      "epoch: 9 step: 1129, loss is 0.011527865193784237\n",
      "epoch: 9 step: 1130, loss is 7.431090489262715e-05\n",
      "epoch: 9 step: 1131, loss is 0.0013507007388398051\n",
      "epoch: 9 step: 1132, loss is 0.00020154559751972556\n",
      "epoch: 9 step: 1133, loss is 0.00010717545956140384\n",
      "epoch: 9 step: 1134, loss is 0.007989514619112015\n",
      "epoch: 9 step: 1135, loss is 0.0015301676467061043\n",
      "epoch: 9 step: 1136, loss is 0.00010739958088379353\n",
      "epoch: 9 step: 1137, loss is 0.0016639810055494308\n",
      "epoch: 9 step: 1138, loss is 0.0007644633878953755\n",
      "epoch: 9 step: 1139, loss is 0.000403757207095623\n",
      "epoch: 9 step: 1140, loss is 0.0008133781375363469\n",
      "epoch: 9 step: 1141, loss is 0.01973116584122181\n",
      "epoch: 9 step: 1142, loss is 0.001547047751955688\n",
      "epoch: 9 step: 1143, loss is 0.002867835108190775\n",
      "epoch: 9 step: 1144, loss is 0.0241098590195179\n",
      "epoch: 9 step: 1145, loss is 0.03379172831773758\n",
      "epoch: 9 step: 1146, loss is 0.21426333487033844\n",
      "epoch: 9 step: 1147, loss is 0.0027325237169861794\n",
      "epoch: 9 step: 1148, loss is 0.0034050808753818274\n",
      "epoch: 9 step: 1149, loss is 0.00012843555305153131\n",
      "epoch: 9 step: 1150, loss is 0.0669095367193222\n",
      "epoch: 9 step: 1151, loss is 0.004539420362561941\n",
      "epoch: 9 step: 1152, loss is 0.00021109037334099412\n",
      "epoch: 9 step: 1153, loss is 0.0014367784606292844\n",
      "epoch: 9 step: 1154, loss is 0.0042678010649979115\n",
      "epoch: 9 step: 1155, loss is 0.003096403321251273\n",
      "epoch: 9 step: 1156, loss is 0.00222352659329772\n",
      "epoch: 9 step: 1157, loss is 0.012594463303685188\n",
      "epoch: 9 step: 1158, loss is 0.01070808619260788\n",
      "epoch: 9 step: 1159, loss is 0.00032440602080896497\n",
      "epoch: 9 step: 1160, loss is 0.009015331976115704\n",
      "epoch: 9 step: 1161, loss is 0.07556582242250443\n",
      "epoch: 9 step: 1162, loss is 0.0003484056214801967\n",
      "epoch: 9 step: 1163, loss is 0.0032319058664143085\n",
      "epoch: 9 step: 1164, loss is 0.00468287942931056\n",
      "epoch: 9 step: 1165, loss is 0.00019141040684189647\n",
      "epoch: 9 step: 1166, loss is 0.0010464483639225364\n",
      "epoch: 9 step: 1167, loss is 0.016868336126208305\n",
      "epoch: 9 step: 1168, loss is 0.001923440839163959\n",
      "epoch: 9 step: 1169, loss is 0.0004348650691099465\n",
      "epoch: 9 step: 1170, loss is 5.285166116664186e-05\n",
      "epoch: 9 step: 1171, loss is 0.00754973292350769\n",
      "epoch: 9 step: 1172, loss is 0.0033627552911639214\n",
      "epoch: 9 step: 1173, loss is 0.0003707747964654118\n",
      "epoch: 9 step: 1174, loss is 0.11970388144254684\n",
      "epoch: 9 step: 1175, loss is 0.000809844525065273\n",
      "epoch: 9 step: 1176, loss is 9.9101998785045e-05\n",
      "epoch: 9 step: 1177, loss is 0.0076174382120370865\n",
      "epoch: 9 step: 1178, loss is 0.013183663599193096\n",
      "epoch: 9 step: 1179, loss is 0.0045298985205590725\n",
      "epoch: 9 step: 1180, loss is 0.002871672622859478\n",
      "epoch: 9 step: 1181, loss is 0.033320214599370956\n",
      "epoch: 9 step: 1182, loss is 0.0003750156320165843\n",
      "epoch: 9 step: 1183, loss is 0.10966666787862778\n",
      "epoch: 9 step: 1184, loss is 0.00044012092985212803\n",
      "epoch: 9 step: 1185, loss is 0.0022492411080747843\n",
      "epoch: 9 step: 1186, loss is 0.0012010815553367138\n",
      "epoch: 9 step: 1187, loss is 0.0005398431676439941\n",
      "epoch: 9 step: 1188, loss is 0.18858429789543152\n",
      "epoch: 9 step: 1189, loss is 0.00014135798846837133\n",
      "epoch: 9 step: 1190, loss is 0.002471540356054902\n",
      "epoch: 9 step: 1191, loss is 0.024072596803307533\n",
      "epoch: 9 step: 1192, loss is 0.1340400129556656\n",
      "epoch: 9 step: 1193, loss is 0.006088909227401018\n",
      "epoch: 9 step: 1194, loss is 0.002457308117300272\n",
      "epoch: 9 step: 1195, loss is 0.007387426681816578\n",
      "epoch: 9 step: 1196, loss is 0.0008052763296291232\n",
      "epoch: 9 step: 1197, loss is 0.005268082953989506\n",
      "epoch: 9 step: 1198, loss is 0.01605011522769928\n",
      "epoch: 9 step: 1199, loss is 0.01967460662126541\n",
      "epoch: 9 step: 1200, loss is 0.046830229461193085\n",
      "epoch: 9 step: 1201, loss is 0.1716667115688324\n",
      "epoch: 9 step: 1202, loss is 9.08259789866861e-06\n",
      "epoch: 9 step: 1203, loss is 0.0014138631522655487\n",
      "epoch: 9 step: 1204, loss is 0.006804382894188166\n",
      "epoch: 9 step: 1205, loss is 0.01833449676632881\n",
      "epoch: 9 step: 1206, loss is 0.0018828605534508824\n",
      "epoch: 9 step: 1207, loss is 0.051200997084379196\n",
      "epoch: 9 step: 1208, loss is 0.04421193152666092\n",
      "epoch: 9 step: 1209, loss is 0.0030862956773489714\n",
      "epoch: 9 step: 1210, loss is 0.0004413960559759289\n",
      "epoch: 9 step: 1211, loss is 6.317296356428415e-05\n",
      "epoch: 9 step: 1212, loss is 0.0012809778563678265\n",
      "epoch: 9 step: 1213, loss is 0.003935201559215784\n",
      "epoch: 9 step: 1214, loss is 0.0004918068298138678\n",
      "epoch: 9 step: 1215, loss is 7.30750325601548e-05\n",
      "epoch: 9 step: 1216, loss is 0.027643024921417236\n",
      "epoch: 9 step: 1217, loss is 0.0007023848593235016\n",
      "epoch: 9 step: 1218, loss is 0.0017281181644648314\n",
      "epoch: 9 step: 1219, loss is 0.0003947594668716192\n",
      "epoch: 9 step: 1220, loss is 0.021048912778496742\n",
      "epoch: 9 step: 1221, loss is 0.0013019040925428271\n",
      "epoch: 9 step: 1222, loss is 0.12678295373916626\n",
      "epoch: 9 step: 1223, loss is 0.0101873017847538\n",
      "epoch: 9 step: 1224, loss is 0.003990731202065945\n",
      "epoch: 9 step: 1225, loss is 0.007073099724948406\n",
      "epoch: 9 step: 1226, loss is 0.00010104873217642307\n",
      "epoch: 9 step: 1227, loss is 0.00011838287173304707\n",
      "epoch: 9 step: 1228, loss is 1.2982187399757095e-05\n",
      "epoch: 9 step: 1229, loss is 0.005475153680890799\n",
      "epoch: 9 step: 1230, loss is 0.0014056939398869872\n",
      "epoch: 9 step: 1231, loss is 0.022391343489289284\n",
      "epoch: 9 step: 1232, loss is 0.00031056234729476273\n",
      "epoch: 9 step: 1233, loss is 0.05493061989545822\n",
      "epoch: 9 step: 1234, loss is 0.022098179906606674\n",
      "epoch: 9 step: 1235, loss is 0.004731790162622929\n",
      "epoch: 9 step: 1236, loss is 0.05673806369304657\n",
      "epoch: 9 step: 1237, loss is 0.06497930735349655\n",
      "epoch: 9 step: 1238, loss is 0.0012471125228330493\n",
      "epoch: 9 step: 1239, loss is 0.0002367197594139725\n",
      "epoch: 9 step: 1240, loss is 0.0031801178120076656\n",
      "epoch: 9 step: 1241, loss is 0.0003950787358917296\n",
      "epoch: 9 step: 1242, loss is 0.00043933294364251196\n",
      "epoch: 9 step: 1243, loss is 0.00022765141329728067\n",
      "epoch: 9 step: 1244, loss is 0.06705520302057266\n",
      "epoch: 9 step: 1245, loss is 0.023401372134685516\n",
      "epoch: 9 step: 1246, loss is 0.07544929534196854\n",
      "epoch: 9 step: 1247, loss is 0.2303689569234848\n",
      "epoch: 9 step: 1248, loss is 0.013369978405535221\n",
      "epoch: 9 step: 1249, loss is 1.2501282981247641e-05\n",
      "epoch: 9 step: 1250, loss is 0.01374914962798357\n",
      "epoch: 9 step: 1251, loss is 0.004795435816049576\n",
      "epoch: 9 step: 1252, loss is 0.0431843064725399\n",
      "epoch: 9 step: 1253, loss is 4.683794031734578e-05\n",
      "epoch: 9 step: 1254, loss is 0.000649025896564126\n",
      "epoch: 9 step: 1255, loss is 0.01004678662866354\n",
      "epoch: 9 step: 1256, loss is 0.0009155170409940183\n",
      "epoch: 9 step: 1257, loss is 0.007320684380829334\n",
      "epoch: 9 step: 1258, loss is 0.009504202753305435\n",
      "epoch: 9 step: 1259, loss is 0.005497627425938845\n",
      "epoch: 9 step: 1260, loss is 0.000145633181091398\n",
      "epoch: 9 step: 1261, loss is 0.017274335026741028\n",
      "epoch: 9 step: 1262, loss is 0.0008370042196474969\n",
      "epoch: 9 step: 1263, loss is 0.00033163477201014757\n",
      "epoch: 9 step: 1264, loss is 0.0008847665740177035\n",
      "epoch: 9 step: 1265, loss is 0.02059200033545494\n",
      "epoch: 9 step: 1266, loss is 0.002103987382724881\n",
      "epoch: 9 step: 1267, loss is 0.0013194128405302763\n",
      "epoch: 9 step: 1268, loss is 0.026924457401037216\n",
      "epoch: 9 step: 1269, loss is 0.04036622494459152\n",
      "epoch: 9 step: 1270, loss is 0.0013371529057621956\n",
      "epoch: 9 step: 1271, loss is 0.00020039596711285412\n",
      "epoch: 9 step: 1272, loss is 0.025117458775639534\n",
      "epoch: 9 step: 1273, loss is 0.001863974961452186\n",
      "epoch: 9 step: 1274, loss is 0.020837847143411636\n",
      "epoch: 9 step: 1275, loss is 0.03454272449016571\n",
      "epoch: 9 step: 1276, loss is 0.0467224083840847\n",
      "epoch: 9 step: 1277, loss is 0.00422263890504837\n",
      "epoch: 9 step: 1278, loss is 6.263556861085817e-05\n",
      "epoch: 9 step: 1279, loss is 0.01776735670864582\n",
      "epoch: 9 step: 1280, loss is 5.5817316024331376e-05\n",
      "epoch: 9 step: 1281, loss is 0.008815232664346695\n",
      "epoch: 9 step: 1282, loss is 0.0020695605780929327\n",
      "epoch: 9 step: 1283, loss is 0.09182374179363251\n",
      "epoch: 9 step: 1284, loss is 0.0006053310935385525\n",
      "epoch: 9 step: 1285, loss is 0.0036190294194966555\n",
      "epoch: 9 step: 1286, loss is 0.0003025214246008545\n",
      "epoch: 9 step: 1287, loss is 0.001867740647867322\n",
      "epoch: 9 step: 1288, loss is 0.006233520805835724\n",
      "epoch: 9 step: 1289, loss is 0.08882420510053635\n",
      "epoch: 9 step: 1290, loss is 0.016170555725693703\n",
      "epoch: 9 step: 1291, loss is 2.469042556185741e-05\n",
      "epoch: 9 step: 1292, loss is 0.000739577692002058\n",
      "epoch: 9 step: 1293, loss is 0.1536317616701126\n",
      "epoch: 9 step: 1294, loss is 0.008648836053907871\n",
      "epoch: 9 step: 1295, loss is 0.004671762697398663\n",
      "epoch: 9 step: 1296, loss is 0.0040887524373829365\n",
      "epoch: 9 step: 1297, loss is 0.0008891718462109566\n",
      "epoch: 9 step: 1298, loss is 0.017191028222441673\n",
      "epoch: 9 step: 1299, loss is 0.000178945017978549\n",
      "epoch: 9 step: 1300, loss is 0.07109951227903366\n",
      "epoch: 9 step: 1301, loss is 0.0003929060767404735\n",
      "epoch: 9 step: 1302, loss is 0.019946308806538582\n",
      "epoch: 9 step: 1303, loss is 3.372198261786252e-05\n",
      "epoch: 9 step: 1304, loss is 0.07952660322189331\n",
      "epoch: 9 step: 1305, loss is 9.623884398024529e-05\n",
      "epoch: 9 step: 1306, loss is 0.0002085059677483514\n",
      "epoch: 9 step: 1307, loss is 0.0001516819029347971\n",
      "epoch: 9 step: 1308, loss is 0.04187152907252312\n",
      "epoch: 9 step: 1309, loss is 0.00014372108853422105\n",
      "epoch: 9 step: 1310, loss is 0.001248574466444552\n",
      "epoch: 9 step: 1311, loss is 0.00015523213369306177\n",
      "epoch: 9 step: 1312, loss is 0.005905347876250744\n",
      "epoch: 9 step: 1313, loss is 0.0009353968198411167\n",
      "epoch: 9 step: 1314, loss is 0.0006212388398125768\n",
      "epoch: 9 step: 1315, loss is 0.0005488409660756588\n",
      "epoch: 9 step: 1316, loss is 0.0013832139084115624\n",
      "epoch: 9 step: 1317, loss is 0.0015219238121062517\n",
      "epoch: 9 step: 1318, loss is 0.00044477524352259934\n",
      "epoch: 9 step: 1319, loss is 0.0017869676230475307\n",
      "epoch: 9 step: 1320, loss is 0.004448875784873962\n",
      "epoch: 9 step: 1321, loss is 0.0002861273824237287\n",
      "epoch: 9 step: 1322, loss is 0.00013424325152300298\n",
      "epoch: 9 step: 1323, loss is 0.0008494844078086317\n",
      "epoch: 9 step: 1324, loss is 0.0007350124069489539\n",
      "epoch: 9 step: 1325, loss is 6.453388778027147e-05\n",
      "epoch: 9 step: 1326, loss is 0.027265368029475212\n",
      "epoch: 9 step: 1327, loss is 0.00022302324941847473\n",
      "epoch: 9 step: 1328, loss is 0.0001888917904580012\n",
      "epoch: 9 step: 1329, loss is 0.0005654639098793268\n",
      "epoch: 9 step: 1330, loss is 0.00011657238792395219\n",
      "epoch: 9 step: 1331, loss is 0.0017845812253654003\n",
      "epoch: 9 step: 1332, loss is 0.016467995941638947\n",
      "epoch: 9 step: 1333, loss is 0.018490634858608246\n",
      "epoch: 9 step: 1334, loss is 0.01231073122471571\n",
      "epoch: 9 step: 1335, loss is 0.00025696781813167036\n",
      "epoch: 9 step: 1336, loss is 0.00013254789519123733\n",
      "epoch: 9 step: 1337, loss is 0.0014074008213356137\n",
      "epoch: 9 step: 1338, loss is 0.00026085955323651433\n",
      "epoch: 9 step: 1339, loss is 0.00020526396110653877\n",
      "epoch: 9 step: 1340, loss is 0.07240556925535202\n",
      "epoch: 9 step: 1341, loss is 0.0003282345714978874\n",
      "epoch: 9 step: 1342, loss is 0.0005706380470655859\n",
      "epoch: 9 step: 1343, loss is 0.0032566185109317303\n",
      "epoch: 9 step: 1344, loss is 0.0006531404796987772\n",
      "epoch: 9 step: 1345, loss is 0.00028553177253343165\n",
      "epoch: 9 step: 1346, loss is 0.0010648395400494337\n",
      "epoch: 9 step: 1347, loss is 0.0005989276105538011\n",
      "epoch: 9 step: 1348, loss is 5.852220056112856e-05\n",
      "epoch: 9 step: 1349, loss is 4.636395533452742e-05\n",
      "epoch: 9 step: 1350, loss is 5.643177428282797e-05\n",
      "epoch: 9 step: 1351, loss is 0.00013594540359918028\n",
      "epoch: 9 step: 1352, loss is 0.0002558426931500435\n",
      "epoch: 9 step: 1353, loss is 9.566889275447465e-06\n",
      "epoch: 9 step: 1354, loss is 0.0016080338973551989\n",
      "epoch: 9 step: 1355, loss is 0.008762476965785027\n",
      "epoch: 9 step: 1356, loss is 0.0037907271180301905\n",
      "epoch: 9 step: 1357, loss is 0.06281405687332153\n",
      "epoch: 9 step: 1358, loss is 0.047240301966667175\n",
      "epoch: 9 step: 1359, loss is 0.0001972815953195095\n",
      "epoch: 9 step: 1360, loss is 8.787340630078688e-05\n",
      "epoch: 9 step: 1361, loss is 7.973375613801181e-05\n",
      "epoch: 9 step: 1362, loss is 0.012663140892982483\n",
      "epoch: 9 step: 1363, loss is 0.0001637292152736336\n",
      "epoch: 9 step: 1364, loss is 0.03359442204236984\n",
      "epoch: 9 step: 1365, loss is 0.0003534310089889914\n",
      "epoch: 9 step: 1366, loss is 0.06973245739936829\n",
      "epoch: 9 step: 1367, loss is 0.0006100474274717271\n",
      "epoch: 9 step: 1368, loss is 0.001792931230738759\n",
      "epoch: 9 step: 1369, loss is 0.011734658852219582\n",
      "epoch: 9 step: 1370, loss is 4.7563335101585835e-05\n",
      "epoch: 9 step: 1371, loss is 0.0007486828835681081\n",
      "epoch: 9 step: 1372, loss is 5.6187913287431e-05\n",
      "epoch: 9 step: 1373, loss is 0.0006673694006167352\n",
      "epoch: 9 step: 1374, loss is 0.0047365170903503895\n",
      "epoch: 9 step: 1375, loss is 0.0036771507002413273\n",
      "epoch: 9 step: 1376, loss is 0.022850340232253075\n",
      "epoch: 9 step: 1377, loss is 7.975128391990438e-05\n",
      "epoch: 9 step: 1378, loss is 0.02560965158045292\n",
      "epoch: 9 step: 1379, loss is 0.0003298868832644075\n",
      "epoch: 9 step: 1380, loss is 0.0004234983935020864\n",
      "epoch: 9 step: 1381, loss is 0.00021466714679263532\n",
      "epoch: 9 step: 1382, loss is 0.012847420759499073\n",
      "epoch: 9 step: 1383, loss is 0.00045365921687334776\n",
      "epoch: 9 step: 1384, loss is 0.034127045422792435\n",
      "epoch: 9 step: 1385, loss is 0.0006963694468140602\n",
      "epoch: 9 step: 1386, loss is 0.018255548551678658\n",
      "epoch: 9 step: 1387, loss is 0.03906494379043579\n",
      "epoch: 9 step: 1388, loss is 0.00024280845536850393\n",
      "epoch: 9 step: 1389, loss is 0.0020342376083135605\n",
      "epoch: 9 step: 1390, loss is 0.0017182803712785244\n",
      "epoch: 9 step: 1391, loss is 0.004500136710703373\n",
      "epoch: 9 step: 1392, loss is 0.00938575342297554\n",
      "epoch: 9 step: 1393, loss is 0.03133073076605797\n",
      "epoch: 9 step: 1394, loss is 0.005816856864839792\n",
      "epoch: 9 step: 1395, loss is 0.00013487778778653592\n",
      "epoch: 9 step: 1396, loss is 0.0020718073938041925\n",
      "epoch: 9 step: 1397, loss is 0.008773577399551868\n",
      "epoch: 9 step: 1398, loss is 0.035220418125391006\n",
      "epoch: 9 step: 1399, loss is 0.0020467089489102364\n",
      "epoch: 9 step: 1400, loss is 0.0014443269465118647\n",
      "epoch: 9 step: 1401, loss is 0.0018711172742769122\n",
      "epoch: 9 step: 1402, loss is 0.0011691759573295712\n",
      "epoch: 9 step: 1403, loss is 0.0025641077663749456\n",
      "epoch: 9 step: 1404, loss is 0.0008831663290038705\n",
      "epoch: 9 step: 1405, loss is 0.0038034035824239254\n",
      "epoch: 9 step: 1406, loss is 4.1464707464911044e-05\n",
      "epoch: 9 step: 1407, loss is 7.33001870685257e-05\n",
      "epoch: 9 step: 1408, loss is 0.000630176451522857\n",
      "epoch: 9 step: 1409, loss is 0.09126490354537964\n",
      "epoch: 9 step: 1410, loss is 8.956292731454596e-05\n",
      "epoch: 9 step: 1411, loss is 0.00026719432207755744\n",
      "epoch: 9 step: 1412, loss is 0.00020436898921616375\n",
      "epoch: 9 step: 1413, loss is 1.2507290193752851e-05\n",
      "epoch: 9 step: 1414, loss is 0.1005893126130104\n",
      "epoch: 9 step: 1415, loss is 0.0005541229038499296\n",
      "epoch: 9 step: 1416, loss is 0.0016245964216068387\n",
      "epoch: 9 step: 1417, loss is 0.046614035964012146\n",
      "epoch: 9 step: 1418, loss is 6.478350314864656e-06\n",
      "epoch: 9 step: 1419, loss is 2.518341898394283e-05\n",
      "epoch: 9 step: 1420, loss is 0.0006550362450070679\n",
      "epoch: 9 step: 1421, loss is 0.0008826127159409225\n",
      "epoch: 9 step: 1422, loss is 0.0016924247611314058\n",
      "epoch: 9 step: 1423, loss is 3.416254912735894e-05\n",
      "epoch: 9 step: 1424, loss is 0.030528131872415543\n",
      "epoch: 9 step: 1425, loss is 0.0007793866097927094\n",
      "epoch: 9 step: 1426, loss is 0.010651551187038422\n",
      "epoch: 9 step: 1427, loss is 0.0004637207312043756\n",
      "epoch: 9 step: 1428, loss is 0.0006029140204191208\n",
      "epoch: 9 step: 1429, loss is 0.0006881312583573163\n",
      "epoch: 9 step: 1430, loss is 0.005172958131879568\n",
      "epoch: 9 step: 1431, loss is 0.02723662741482258\n",
      "epoch: 9 step: 1432, loss is 0.10189986228942871\n",
      "epoch: 9 step: 1433, loss is 0.0016993959434330463\n",
      "epoch: 9 step: 1434, loss is 0.0004368130466900766\n",
      "epoch: 9 step: 1435, loss is 0.00324967410415411\n",
      "epoch: 9 step: 1436, loss is 0.00021901793661527336\n",
      "epoch: 9 step: 1437, loss is 0.001134483958594501\n",
      "epoch: 9 step: 1438, loss is 0.000935829826630652\n",
      "epoch: 9 step: 1439, loss is 0.005297437775880098\n",
      "epoch: 9 step: 1440, loss is 0.0014693811535835266\n",
      "epoch: 9 step: 1441, loss is 0.05895959585905075\n",
      "epoch: 9 step: 1442, loss is 7.116951746866107e-05\n",
      "epoch: 9 step: 1443, loss is 0.00465031573548913\n",
      "epoch: 9 step: 1444, loss is 0.002029992174357176\n",
      "epoch: 9 step: 1445, loss is 0.03302387520670891\n",
      "epoch: 9 step: 1446, loss is 0.004085299093276262\n",
      "epoch: 9 step: 1447, loss is 0.0018392599886283278\n",
      "epoch: 9 step: 1448, loss is 0.0012292071478441358\n",
      "epoch: 9 step: 1449, loss is 0.009449755772948265\n",
      "epoch: 9 step: 1450, loss is 0.00015878159319981933\n",
      "epoch: 9 step: 1451, loss is 0.010564623400568962\n",
      "epoch: 9 step: 1452, loss is 0.009266966953873634\n",
      "epoch: 9 step: 1453, loss is 0.0006560382898896933\n",
      "epoch: 9 step: 1454, loss is 0.0005468285526148975\n",
      "epoch: 9 step: 1455, loss is 0.0029738883022218943\n",
      "epoch: 9 step: 1456, loss is 0.014009600505232811\n",
      "epoch: 9 step: 1457, loss is 0.0006498615839518607\n",
      "epoch: 9 step: 1458, loss is 0.10871770977973938\n",
      "epoch: 9 step: 1459, loss is 0.0004761019954457879\n",
      "epoch: 9 step: 1460, loss is 0.0008305060327984393\n",
      "epoch: 9 step: 1461, loss is 0.0002613432297948748\n",
      "epoch: 9 step: 1462, loss is 0.0056481994688510895\n",
      "epoch: 9 step: 1463, loss is 0.007613099180161953\n",
      "epoch: 9 step: 1464, loss is 0.023739630356431007\n",
      "epoch: 9 step: 1465, loss is 0.0025677429512143135\n",
      "epoch: 9 step: 1466, loss is 0.02349264547228813\n",
      "epoch: 9 step: 1467, loss is 0.006990568246692419\n",
      "epoch: 9 step: 1468, loss is 0.00113488151691854\n",
      "epoch: 9 step: 1469, loss is 0.00016203966515604407\n",
      "epoch: 9 step: 1470, loss is 0.0004997496725991368\n",
      "epoch: 9 step: 1471, loss is 0.000646167085506022\n",
      "epoch: 9 step: 1472, loss is 0.0001481026702094823\n",
      "epoch: 9 step: 1473, loss is 0.021160107105970383\n",
      "epoch: 9 step: 1474, loss is 0.005814495030790567\n",
      "epoch: 9 step: 1475, loss is 0.007429706864058971\n",
      "epoch: 9 step: 1476, loss is 0.27045321464538574\n",
      "epoch: 9 step: 1477, loss is 3.5357545129954815e-05\n",
      "epoch: 9 step: 1478, loss is 0.06250189989805222\n",
      "epoch: 9 step: 1479, loss is 0.0028267819434404373\n",
      "epoch: 9 step: 1480, loss is 0.00020951518672518432\n",
      "epoch: 9 step: 1481, loss is 0.09941881895065308\n",
      "epoch: 9 step: 1482, loss is 8.64080066094175e-05\n",
      "epoch: 9 step: 1483, loss is 0.039386581629514694\n",
      "epoch: 9 step: 1484, loss is 0.014870457351207733\n",
      "epoch: 9 step: 1485, loss is 0.12374834716320038\n",
      "epoch: 9 step: 1486, loss is 0.001084491959773004\n",
      "epoch: 9 step: 1487, loss is 0.013032604940235615\n",
      "epoch: 9 step: 1488, loss is 0.0005122232250869274\n",
      "epoch: 9 step: 1489, loss is 0.00030404466087929904\n",
      "epoch: 9 step: 1490, loss is 0.00010668561299098656\n",
      "epoch: 9 step: 1491, loss is 0.015037025325000286\n",
      "epoch: 9 step: 1492, loss is 0.021436266601085663\n",
      "epoch: 9 step: 1493, loss is 0.001507237320765853\n",
      "epoch: 9 step: 1494, loss is 0.32927748560905457\n",
      "epoch: 9 step: 1495, loss is 0.0047422414645552635\n",
      "epoch: 9 step: 1496, loss is 0.0013202860718593001\n",
      "epoch: 9 step: 1497, loss is 0.0008283549104817212\n",
      "epoch: 9 step: 1498, loss is 0.001115894760005176\n",
      "epoch: 9 step: 1499, loss is 0.00021408488100860268\n",
      "epoch: 9 step: 1500, loss is 0.0005504486616700888\n",
      "epoch: 9 step: 1501, loss is 7.852720591472462e-05\n",
      "epoch: 9 step: 1502, loss is 2.6123501811525784e-05\n",
      "epoch: 9 step: 1503, loss is 0.00033346828422509134\n",
      "epoch: 9 step: 1504, loss is 0.0011084028519690037\n",
      "epoch: 9 step: 1505, loss is 0.046353138983249664\n",
      "epoch: 9 step: 1506, loss is 0.00011744086077669635\n",
      "epoch: 9 step: 1507, loss is 0.03299688175320625\n",
      "epoch: 9 step: 1508, loss is 0.16302236914634705\n",
      "epoch: 9 step: 1509, loss is 0.00010264267621096224\n",
      "epoch: 9 step: 1510, loss is 0.0008937621023505926\n",
      "epoch: 9 step: 1511, loss is 0.0004918060731142759\n",
      "epoch: 9 step: 1512, loss is 0.0012398769613355398\n",
      "epoch: 9 step: 1513, loss is 0.0005955395754426718\n",
      "epoch: 9 step: 1514, loss is 0.004061955027282238\n",
      "epoch: 9 step: 1515, loss is 0.0011521178530529141\n",
      "epoch: 9 step: 1516, loss is 0.001376103376969695\n",
      "epoch: 9 step: 1517, loss is 0.0719856396317482\n",
      "epoch: 9 step: 1518, loss is 0.03908649832010269\n",
      "epoch: 9 step: 1519, loss is 0.04356592521071434\n",
      "epoch: 9 step: 1520, loss is 0.0020440875086933374\n",
      "epoch: 9 step: 1521, loss is 0.0007789747323840857\n",
      "epoch: 9 step: 1522, loss is 0.09474281221628189\n",
      "epoch: 9 step: 1523, loss is 0.005384477321058512\n",
      "epoch: 9 step: 1524, loss is 0.0015537546714767814\n",
      "epoch: 9 step: 1525, loss is 0.0005577791598625481\n",
      "epoch: 9 step: 1526, loss is 1.733453245833516e-05\n",
      "epoch: 9 step: 1527, loss is 0.0005451836623251438\n",
      "epoch: 9 step: 1528, loss is 0.0006988079403527081\n",
      "epoch: 9 step: 1529, loss is 0.000162081079906784\n",
      "epoch: 9 step: 1530, loss is 1.2031767255393788e-05\n",
      "epoch: 9 step: 1531, loss is 0.0056609418243169785\n",
      "epoch: 9 step: 1532, loss is 0.01751723140478134\n",
      "epoch: 9 step: 1533, loss is 0.01602313481271267\n",
      "epoch: 9 step: 1534, loss is 1.0278334229951724e-05\n",
      "epoch: 9 step: 1535, loss is 0.0020115391816943884\n",
      "epoch: 9 step: 1536, loss is 0.020797308534383774\n",
      "epoch: 9 step: 1537, loss is 0.0009231119765900075\n",
      "epoch: 9 step: 1538, loss is 0.003307637060061097\n",
      "epoch: 9 step: 1539, loss is 9.68795211520046e-05\n",
      "epoch: 9 step: 1540, loss is 0.0031727831810712814\n",
      "epoch: 9 step: 1541, loss is 0.0009698525536805391\n",
      "epoch: 9 step: 1542, loss is 0.00020866552949883044\n",
      "epoch: 9 step: 1543, loss is 1.112032350647496e-05\n",
      "epoch: 9 step: 1544, loss is 0.003853757167235017\n",
      "epoch: 9 step: 1545, loss is 0.012874159961938858\n",
      "epoch: 9 step: 1546, loss is 0.07397348433732986\n",
      "epoch: 9 step: 1547, loss is 0.1028078943490982\n",
      "epoch: 9 step: 1548, loss is 2.3456937924493104e-05\n",
      "epoch: 9 step: 1549, loss is 0.006256386172026396\n",
      "epoch: 9 step: 1550, loss is 2.0305389625718817e-05\n",
      "epoch: 9 step: 1551, loss is 0.010347750037908554\n",
      "epoch: 9 step: 1552, loss is 0.00032963871490210295\n",
      "epoch: 9 step: 1553, loss is 0.01703893579542637\n",
      "epoch: 9 step: 1554, loss is 5.757248800364323e-05\n",
      "epoch: 9 step: 1555, loss is 0.005787582602351904\n",
      "epoch: 9 step: 1556, loss is 0.005274979863315821\n",
      "epoch: 9 step: 1557, loss is 0.00031354735256172717\n",
      "epoch: 9 step: 1558, loss is 0.00751876924186945\n",
      "epoch: 9 step: 1559, loss is 0.0005642467876896262\n",
      "epoch: 9 step: 1560, loss is 0.00205979379825294\n",
      "epoch: 9 step: 1561, loss is 0.016347873955965042\n",
      "epoch: 9 step: 1562, loss is 0.036879558116197586\n",
      "epoch: 9 step: 1563, loss is 0.010355332866311073\n",
      "epoch: 9 step: 1564, loss is 0.00015172004350461066\n",
      "epoch: 9 step: 1565, loss is 0.011962948366999626\n",
      "epoch: 9 step: 1566, loss is 0.0017697678413242102\n",
      "epoch: 9 step: 1567, loss is 0.017688490450382233\n",
      "epoch: 9 step: 1568, loss is 0.042828693985939026\n",
      "epoch: 9 step: 1569, loss is 0.004141473211348057\n",
      "epoch: 9 step: 1570, loss is 0.004804844968020916\n",
      "epoch: 9 step: 1571, loss is 0.00027164025232195854\n",
      "epoch: 9 step: 1572, loss is 6.387420580722392e-05\n",
      "epoch: 9 step: 1573, loss is 0.0006787051097489893\n",
      "epoch: 9 step: 1574, loss is 7.490151620004326e-05\n",
      "epoch: 9 step: 1575, loss is 0.0322076790034771\n",
      "epoch: 9 step: 1576, loss is 0.32419025897979736\n",
      "epoch: 9 step: 1577, loss is 0.00046105365618132055\n",
      "epoch: 9 step: 1578, loss is 0.0011515312362462282\n",
      "epoch: 9 step: 1579, loss is 0.00041706219781190157\n",
      "epoch: 9 step: 1580, loss is 0.004340454936027527\n",
      "epoch: 9 step: 1581, loss is 0.0024012706708163023\n",
      "epoch: 9 step: 1582, loss is 0.0005348568083718419\n",
      "epoch: 9 step: 1583, loss is 0.0005468385643325746\n",
      "epoch: 9 step: 1584, loss is 0.0006605117814615369\n",
      "epoch: 9 step: 1585, loss is 0.006629789248108864\n",
      "epoch: 9 step: 1586, loss is 0.00022753058874513954\n",
      "epoch: 9 step: 1587, loss is 0.0006463110912591219\n",
      "epoch: 9 step: 1588, loss is 1.822956073738169e-05\n",
      "epoch: 9 step: 1589, loss is 0.010550207458436489\n",
      "epoch: 9 step: 1590, loss is 0.03447909653186798\n",
      "epoch: 9 step: 1591, loss is 0.0067850444465875626\n",
      "epoch: 9 step: 1592, loss is 0.007930144667625427\n",
      "epoch: 9 step: 1593, loss is 0.06322843581438065\n",
      "epoch: 9 step: 1594, loss is 0.000625259184744209\n",
      "epoch: 9 step: 1595, loss is 1.7985057638725266e-05\n",
      "epoch: 9 step: 1596, loss is 0.024669792503118515\n",
      "epoch: 9 step: 1597, loss is 0.009995776228606701\n",
      "epoch: 9 step: 1598, loss is 2.9596567401313223e-05\n",
      "epoch: 9 step: 1599, loss is 0.00012063474423484877\n",
      "epoch: 9 step: 1600, loss is 0.05010020360350609\n",
      "epoch: 9 step: 1601, loss is 0.010787805542349815\n",
      "epoch: 9 step: 1602, loss is 0.009687979705631733\n",
      "epoch: 9 step: 1603, loss is 0.07921557873487473\n",
      "epoch: 9 step: 1604, loss is 0.198403000831604\n",
      "epoch: 9 step: 1605, loss is 0.007632070686668158\n",
      "epoch: 9 step: 1606, loss is 0.005236970726400614\n",
      "epoch: 9 step: 1607, loss is 0.009360861964523792\n",
      "epoch: 9 step: 1608, loss is 0.013504506088793278\n",
      "epoch: 9 step: 1609, loss is 0.009976595640182495\n",
      "epoch: 9 step: 1610, loss is 0.00023820607748348266\n",
      "epoch: 9 step: 1611, loss is 0.03165363892912865\n",
      "epoch: 9 step: 1612, loss is 0.0034453461412340403\n",
      "epoch: 9 step: 1613, loss is 0.0006058617145754397\n",
      "epoch: 9 step: 1614, loss is 0.004688132554292679\n",
      "epoch: 9 step: 1615, loss is 0.006938288453966379\n",
      "epoch: 9 step: 1616, loss is 0.012385094538331032\n",
      "epoch: 9 step: 1617, loss is 0.07556678354740143\n",
      "epoch: 9 step: 1618, loss is 0.01742170751094818\n",
      "epoch: 9 step: 1619, loss is 0.03685923293232918\n",
      "epoch: 9 step: 1620, loss is 0.0003199332859367132\n",
      "epoch: 9 step: 1621, loss is 0.0006735690403729677\n",
      "epoch: 9 step: 1622, loss is 0.0017043253174051642\n",
      "epoch: 9 step: 1623, loss is 4.873582656728104e-05\n",
      "epoch: 9 step: 1624, loss is 0.0003948755329474807\n",
      "epoch: 9 step: 1625, loss is 0.0057855406776070595\n",
      "epoch: 9 step: 1626, loss is 0.004092031624168158\n",
      "epoch: 9 step: 1627, loss is 0.003938411362469196\n",
      "epoch: 9 step: 1628, loss is 0.0009947579819709063\n",
      "epoch: 9 step: 1629, loss is 0.00015356994117610157\n",
      "epoch: 9 step: 1630, loss is 0.03208057954907417\n",
      "epoch: 9 step: 1631, loss is 1.2502448043960612e-05\n",
      "epoch: 9 step: 1632, loss is 0.0014341697096824646\n",
      "epoch: 9 step: 1633, loss is 0.014724676497280598\n",
      "epoch: 9 step: 1634, loss is 5.420210800366476e-05\n",
      "epoch: 9 step: 1635, loss is 0.003874358022585511\n",
      "epoch: 9 step: 1636, loss is 0.0028541034553200006\n",
      "epoch: 9 step: 1637, loss is 0.0003691343008540571\n",
      "epoch: 9 step: 1638, loss is 0.025016820058226585\n",
      "epoch: 9 step: 1639, loss is 0.07310798764228821\n",
      "epoch: 9 step: 1640, loss is 0.0024675370659679174\n",
      "epoch: 9 step: 1641, loss is 0.002520243637263775\n",
      "epoch: 9 step: 1642, loss is 0.002288008341565728\n",
      "epoch: 9 step: 1643, loss is 0.00027982090250588953\n",
      "epoch: 9 step: 1644, loss is 8.706389780854806e-05\n",
      "epoch: 9 step: 1645, loss is 0.06054060161113739\n",
      "epoch: 9 step: 1646, loss is 0.0800020694732666\n",
      "epoch: 9 step: 1647, loss is 0.13688665628433228\n",
      "epoch: 9 step: 1648, loss is 0.0031696187797933817\n",
      "epoch: 9 step: 1649, loss is 0.009821438230574131\n",
      "epoch: 9 step: 1650, loss is 0.001042678253725171\n",
      "epoch: 9 step: 1651, loss is 2.4276778276544064e-05\n",
      "epoch: 9 step: 1652, loss is 0.027391865849494934\n",
      "epoch: 9 step: 1653, loss is 7.208505849121138e-05\n",
      "epoch: 9 step: 1654, loss is 5.410242374637164e-05\n",
      "epoch: 9 step: 1655, loss is 0.04299186170101166\n",
      "epoch: 9 step: 1656, loss is 0.15331703424453735\n",
      "epoch: 9 step: 1657, loss is 0.015044936910271645\n",
      "epoch: 9 step: 1658, loss is 0.059449534863233566\n",
      "epoch: 9 step: 1659, loss is 8.333004370797426e-05\n",
      "epoch: 9 step: 1660, loss is 3.488808943075128e-05\n",
      "epoch: 9 step: 1661, loss is 0.0002592985692899674\n",
      "epoch: 9 step: 1662, loss is 0.00012480681471060961\n",
      "epoch: 9 step: 1663, loss is 0.0007920429925434291\n",
      "epoch: 9 step: 1664, loss is 0.0007836938602849841\n",
      "epoch: 9 step: 1665, loss is 0.01160519290715456\n",
      "epoch: 9 step: 1666, loss is 0.0007275411044247448\n",
      "epoch: 9 step: 1667, loss is 0.0010651808697730303\n",
      "epoch: 9 step: 1668, loss is 0.0014364812523126602\n",
      "epoch: 9 step: 1669, loss is 0.001240411656908691\n",
      "epoch: 9 step: 1670, loss is 0.09792621433734894\n",
      "epoch: 9 step: 1671, loss is 0.0021271645091474056\n",
      "epoch: 9 step: 1672, loss is 0.0018244768725708127\n",
      "epoch: 9 step: 1673, loss is 0.0018686779076233506\n",
      "epoch: 9 step: 1674, loss is 0.041905153542757034\n",
      "epoch: 9 step: 1675, loss is 0.17567169666290283\n",
      "epoch: 9 step: 1676, loss is 0.03817235305905342\n",
      "epoch: 9 step: 1677, loss is 0.053703900426626205\n",
      "epoch: 9 step: 1678, loss is 0.013350981287658215\n",
      "epoch: 9 step: 1679, loss is 0.016466807574033737\n",
      "epoch: 9 step: 1680, loss is 0.0053240275010466576\n",
      "epoch: 9 step: 1681, loss is 0.019566919654607773\n",
      "epoch: 9 step: 1682, loss is 0.12304245680570602\n",
      "epoch: 9 step: 1683, loss is 0.02920874021947384\n",
      "epoch: 9 step: 1684, loss is 0.000852505094371736\n",
      "epoch: 9 step: 1685, loss is 0.00023562010028399527\n",
      "epoch: 9 step: 1686, loss is 0.00502740265801549\n",
      "epoch: 9 step: 1687, loss is 3.28464011545293e-05\n",
      "epoch: 9 step: 1688, loss is 0.0004215818189550191\n",
      "epoch: 9 step: 1689, loss is 0.07111966609954834\n",
      "epoch: 9 step: 1690, loss is 0.0007040081545710564\n",
      "epoch: 9 step: 1691, loss is 0.004778173286467791\n",
      "epoch: 9 step: 1692, loss is 0.00213213125243783\n",
      "epoch: 9 step: 1693, loss is 0.001780028105713427\n",
      "epoch: 9 step: 1694, loss is 0.014058662578463554\n",
      "epoch: 9 step: 1695, loss is 0.0005495285149663687\n",
      "epoch: 9 step: 1696, loss is 0.00045751265133731067\n",
      "epoch: 9 step: 1697, loss is 6.304353883024305e-05\n",
      "epoch: 9 step: 1698, loss is 0.0006628779228776693\n",
      "epoch: 9 step: 1699, loss is 6.550292164320126e-05\n",
      "epoch: 9 step: 1700, loss is 0.0038342808838933706\n",
      "epoch: 9 step: 1701, loss is 0.014249265193939209\n",
      "epoch: 9 step: 1702, loss is 0.00746498815715313\n",
      "epoch: 9 step: 1703, loss is 0.0009770173346623778\n",
      "epoch: 9 step: 1704, loss is 0.08702041953802109\n",
      "epoch: 9 step: 1705, loss is 0.0003004552563652396\n",
      "epoch: 9 step: 1706, loss is 0.25839200615882874\n",
      "epoch: 9 step: 1707, loss is 0.0029250907246023417\n",
      "epoch: 9 step: 1708, loss is 0.29715386033058167\n",
      "epoch: 9 step: 1709, loss is 0.008165838196873665\n",
      "epoch: 9 step: 1710, loss is 0.001728220609948039\n",
      "epoch: 9 step: 1711, loss is 0.00446334108710289\n",
      "epoch: 9 step: 1712, loss is 0.00011792014993261546\n",
      "epoch: 9 step: 1713, loss is 0.012127521447837353\n",
      "epoch: 9 step: 1714, loss is 0.0004428429820109159\n",
      "epoch: 9 step: 1715, loss is 0.011085274629294872\n",
      "epoch: 9 step: 1716, loss is 0.0036316877231001854\n",
      "epoch: 9 step: 1717, loss is 0.0597340427339077\n",
      "epoch: 9 step: 1718, loss is 0.005587346851825714\n",
      "epoch: 9 step: 1719, loss is 0.004126595798879862\n",
      "epoch: 9 step: 1720, loss is 0.0018651193240657449\n",
      "epoch: 9 step: 1721, loss is 0.00014530078624375165\n",
      "epoch: 9 step: 1722, loss is 0.006748109590262175\n",
      "epoch: 9 step: 1723, loss is 0.0014509260654449463\n",
      "epoch: 9 step: 1724, loss is 0.14127379655838013\n",
      "epoch: 9 step: 1725, loss is 0.0025095471646636724\n",
      "epoch: 9 step: 1726, loss is 0.015404476784169674\n",
      "epoch: 9 step: 1727, loss is 0.013545092195272446\n",
      "epoch: 9 step: 1728, loss is 0.004443750251084566\n",
      "epoch: 9 step: 1729, loss is 0.006934879347681999\n",
      "epoch: 9 step: 1730, loss is 0.007281876169145107\n",
      "epoch: 9 step: 1731, loss is 0.00017492067127022892\n",
      "epoch: 9 step: 1732, loss is 0.0007738469867035747\n",
      "epoch: 9 step: 1733, loss is 0.0004239744448568672\n",
      "epoch: 9 step: 1734, loss is 0.002134295180439949\n",
      "epoch: 9 step: 1735, loss is 0.022350911051034927\n",
      "epoch: 9 step: 1736, loss is 0.00026155656087212265\n",
      "epoch: 9 step: 1737, loss is 0.0006454854155890644\n",
      "epoch: 9 step: 1738, loss is 0.00264884065836668\n",
      "epoch: 9 step: 1739, loss is 0.004347429145127535\n",
      "epoch: 9 step: 1740, loss is 0.0001515705807833001\n",
      "epoch: 9 step: 1741, loss is 0.0005065178265795112\n",
      "epoch: 9 step: 1742, loss is 0.00011864483531098813\n",
      "epoch: 9 step: 1743, loss is 0.01062180008739233\n",
      "epoch: 9 step: 1744, loss is 0.0014122218126431108\n",
      "epoch: 9 step: 1745, loss is 0.012567443773150444\n",
      "epoch: 9 step: 1746, loss is 0.002928295638412237\n",
      "epoch: 9 step: 1747, loss is 0.04180918633937836\n",
      "epoch: 9 step: 1748, loss is 0.0032652616500854492\n",
      "epoch: 9 step: 1749, loss is 0.028026152402162552\n",
      "epoch: 9 step: 1750, loss is 0.004886153619736433\n",
      "epoch: 9 step: 1751, loss is 0.007499867118895054\n",
      "epoch: 9 step: 1752, loss is 0.0012410671915858984\n",
      "epoch: 9 step: 1753, loss is 3.166206442983821e-05\n",
      "epoch: 9 step: 1754, loss is 0.000792923616245389\n",
      "epoch: 9 step: 1755, loss is 0.01181513350456953\n",
      "epoch: 9 step: 1756, loss is 0.000517687585670501\n",
      "epoch: 9 step: 1757, loss is 0.0001473324664402753\n",
      "epoch: 9 step: 1758, loss is 0.006236291956156492\n",
      "epoch: 9 step: 1759, loss is 0.03818812221288681\n",
      "epoch: 9 step: 1760, loss is 1.97712033696007e-05\n",
      "epoch: 9 step: 1761, loss is 0.0015729507431387901\n",
      "epoch: 9 step: 1762, loss is 0.005461450200527906\n",
      "epoch: 9 step: 1763, loss is 0.0026625259779393673\n",
      "epoch: 9 step: 1764, loss is 0.00014697108417749405\n",
      "epoch: 9 step: 1765, loss is 0.003736016573384404\n",
      "epoch: 9 step: 1766, loss is 0.0019821603782474995\n",
      "epoch: 9 step: 1767, loss is 6.521198520204052e-05\n",
      "epoch: 9 step: 1768, loss is 0.017899226397275925\n",
      "epoch: 9 step: 1769, loss is 6.881928129587322e-05\n",
      "epoch: 9 step: 1770, loss is 0.02492385357618332\n",
      "epoch: 9 step: 1771, loss is 9.562026389176026e-05\n",
      "epoch: 9 step: 1772, loss is 0.0010609579039737582\n",
      "epoch: 9 step: 1773, loss is 0.0013331930385902524\n",
      "epoch: 9 step: 1774, loss is 0.01588914915919304\n",
      "epoch: 9 step: 1775, loss is 0.025199631229043007\n",
      "epoch: 9 step: 1776, loss is 0.020549669861793518\n",
      "epoch: 9 step: 1777, loss is 0.0003204782260581851\n",
      "epoch: 9 step: 1778, loss is 0.0005627550999633968\n",
      "epoch: 9 step: 1779, loss is 0.14024722576141357\n",
      "epoch: 9 step: 1780, loss is 0.006719930563122034\n",
      "epoch: 9 step: 1781, loss is 0.001928306301124394\n",
      "epoch: 9 step: 1782, loss is 0.00016330920334439725\n",
      "epoch: 9 step: 1783, loss is 0.01783706061542034\n",
      "epoch: 9 step: 1784, loss is 7.650920451851562e-05\n",
      "epoch: 9 step: 1785, loss is 0.003642067778855562\n",
      "epoch: 9 step: 1786, loss is 0.0001669854245847091\n",
      "epoch: 9 step: 1787, loss is 0.0001764487533364445\n",
      "epoch: 9 step: 1788, loss is 4.832387276110239e-05\n",
      "epoch: 9 step: 1789, loss is 0.0017564988229423761\n",
      "epoch: 9 step: 1790, loss is 0.008728954009711742\n",
      "epoch: 9 step: 1791, loss is 0.10208143293857574\n",
      "epoch: 9 step: 1792, loss is 0.000820022018160671\n",
      "epoch: 9 step: 1793, loss is 0.00038034588214941323\n",
      "epoch: 9 step: 1794, loss is 0.0038030361756682396\n",
      "epoch: 9 step: 1795, loss is 0.0011847640853375196\n",
      "epoch: 9 step: 1796, loss is 0.0210302472114563\n",
      "epoch: 9 step: 1797, loss is 0.00019315516692586243\n",
      "epoch: 9 step: 1798, loss is 0.0006573714781552553\n",
      "epoch: 9 step: 1799, loss is 0.000291209522401914\n",
      "epoch: 9 step: 1800, loss is 0.08654652535915375\n",
      "epoch: 9 step: 1801, loss is 0.00015900608559604734\n",
      "epoch: 9 step: 1802, loss is 0.0003894528781529516\n",
      "epoch: 9 step: 1803, loss is 0.14721906185150146\n",
      "epoch: 9 step: 1804, loss is 0.0006452309899032116\n",
      "epoch: 9 step: 1805, loss is 0.00018873915541917086\n",
      "epoch: 9 step: 1806, loss is 0.051390137523412704\n",
      "epoch: 9 step: 1807, loss is 0.05262254923582077\n",
      "epoch: 9 step: 1808, loss is 0.004411265254020691\n",
      "epoch: 9 step: 1809, loss is 0.0028364716563373804\n",
      "epoch: 9 step: 1810, loss is 0.0027464849408715963\n",
      "epoch: 9 step: 1811, loss is 0.008615178056061268\n",
      "epoch: 9 step: 1812, loss is 0.0033188117668032646\n",
      "epoch: 9 step: 1813, loss is 0.005083241034299135\n",
      "epoch: 9 step: 1814, loss is 0.004154498223215342\n",
      "epoch: 9 step: 1815, loss is 0.007693145424127579\n",
      "epoch: 9 step: 1816, loss is 0.0018802366685122252\n",
      "epoch: 9 step: 1817, loss is 0.003848536405712366\n",
      "epoch: 9 step: 1818, loss is 0.008654812350869179\n",
      "epoch: 9 step: 1819, loss is 0.002936433767899871\n",
      "epoch: 9 step: 1820, loss is 0.045668020844459534\n",
      "epoch: 9 step: 1821, loss is 0.0010208571329712868\n",
      "epoch: 9 step: 1822, loss is 0.004924202803522348\n",
      "epoch: 9 step: 1823, loss is 0.00019272323697805405\n",
      "epoch: 9 step: 1824, loss is 0.0035917365457862616\n",
      "epoch: 9 step: 1825, loss is 0.004014056175947189\n",
      "epoch: 9 step: 1826, loss is 0.20765484869480133\n",
      "epoch: 9 step: 1827, loss is 0.26446473598480225\n",
      "epoch: 9 step: 1828, loss is 0.021257784217596054\n",
      "epoch: 9 step: 1829, loss is 0.053723566234111786\n",
      "epoch: 9 step: 1830, loss is 0.0025444028433412313\n",
      "epoch: 9 step: 1831, loss is 0.0024808987509459257\n",
      "epoch: 9 step: 1832, loss is 0.002188987098634243\n",
      "epoch: 9 step: 1833, loss is 0.020606989040970802\n",
      "epoch: 9 step: 1834, loss is 0.002591582015156746\n",
      "epoch: 9 step: 1835, loss is 0.006665506865829229\n",
      "epoch: 9 step: 1836, loss is 0.004976678639650345\n",
      "epoch: 9 step: 1837, loss is 0.00021720165386795998\n",
      "epoch: 9 step: 1838, loss is 0.0068238056264817715\n",
      "epoch: 9 step: 1839, loss is 7.088024722179398e-05\n",
      "epoch: 9 step: 1840, loss is 0.00022521583014167845\n",
      "epoch: 9 step: 1841, loss is 0.009015589021146297\n",
      "epoch: 9 step: 1842, loss is 0.00010909231059486046\n",
      "epoch: 9 step: 1843, loss is 0.00149369181599468\n",
      "epoch: 9 step: 1844, loss is 0.0001963308168342337\n",
      "epoch: 9 step: 1845, loss is 0.0006075195269659162\n",
      "epoch: 9 step: 1846, loss is 0.01461775228381157\n",
      "epoch: 9 step: 1847, loss is 0.023007584735751152\n",
      "epoch: 9 step: 1848, loss is 0.00013302687148097903\n",
      "epoch: 9 step: 1849, loss is 0.004325957503169775\n",
      "epoch: 9 step: 1850, loss is 0.10859693586826324\n",
      "epoch: 9 step: 1851, loss is 0.0017740963958203793\n",
      "epoch: 9 step: 1852, loss is 0.005929458886384964\n",
      "epoch: 9 step: 1853, loss is 0.0004501936782617122\n",
      "epoch: 9 step: 1854, loss is 0.0027378688100725412\n",
      "epoch: 9 step: 1855, loss is 0.00019168980361428112\n",
      "epoch: 9 step: 1856, loss is 0.02319825254380703\n",
      "epoch: 9 step: 1857, loss is 0.002037713536992669\n",
      "epoch: 9 step: 1858, loss is 0.014004291035234928\n",
      "epoch: 9 step: 1859, loss is 0.01113566942512989\n",
      "epoch: 9 step: 1860, loss is 0.00022966141114011407\n",
      "epoch: 9 step: 1861, loss is 0.0008835266344249249\n",
      "epoch: 9 step: 1862, loss is 0.004383357707411051\n",
      "epoch: 9 step: 1863, loss is 0.0018464477034285665\n",
      "epoch: 9 step: 1864, loss is 0.00015554408309981227\n",
      "epoch: 9 step: 1865, loss is 0.01622146926820278\n",
      "epoch: 9 step: 1866, loss is 4.145092316321097e-05\n",
      "epoch: 9 step: 1867, loss is 0.0375283844769001\n",
      "epoch: 9 step: 1868, loss is 0.0009859882993623614\n",
      "epoch: 9 step: 1869, loss is 0.0007897825562395155\n",
      "epoch: 9 step: 1870, loss is 0.0033213866408914328\n",
      "epoch: 9 step: 1871, loss is 0.006448029074817896\n",
      "epoch: 9 step: 1872, loss is 0.003813639748841524\n",
      "epoch: 9 step: 1873, loss is 0.009593479335308075\n",
      "epoch: 9 step: 1874, loss is 0.004037398379296064\n",
      "epoch: 9 step: 1875, loss is 0.018653780221939087\n",
      "epoch: 10 step: 1, loss is 0.020442185923457146\n",
      "epoch: 10 step: 2, loss is 0.002213249681517482\n",
      "epoch: 10 step: 3, loss is 0.00017823820235207677\n",
      "epoch: 10 step: 4, loss is 0.00019515240273904055\n",
      "epoch: 10 step: 5, loss is 0.005132391583174467\n",
      "epoch: 10 step: 6, loss is 3.765535802813247e-05\n",
      "epoch: 10 step: 7, loss is 0.1055903285741806\n",
      "epoch: 10 step: 8, loss is 5.3620260587194934e-05\n",
      "epoch: 10 step: 9, loss is 0.0010746591724455357\n",
      "epoch: 10 step: 10, loss is 0.00010246022429782897\n",
      "epoch: 10 step: 11, loss is 0.01697445847094059\n",
      "epoch: 10 step: 12, loss is 0.0013079644413664937\n",
      "epoch: 10 step: 13, loss is 9.901286830427125e-05\n",
      "epoch: 10 step: 14, loss is 0.0010806607315316796\n",
      "epoch: 10 step: 15, loss is 0.0029177262913435698\n",
      "epoch: 10 step: 16, loss is 0.00021983760234434158\n",
      "epoch: 10 step: 17, loss is 0.00025304092559963465\n",
      "epoch: 10 step: 18, loss is 0.0034164567478001118\n",
      "epoch: 10 step: 19, loss is 0.0018496315460652113\n",
      "epoch: 10 step: 20, loss is 0.0072379340417683125\n",
      "epoch: 10 step: 21, loss is 0.0004939387436024845\n",
      "epoch: 10 step: 22, loss is 0.08680383861064911\n",
      "epoch: 10 step: 23, loss is 0.0007997341454029083\n",
      "epoch: 10 step: 24, loss is 0.001536098076030612\n",
      "epoch: 10 step: 25, loss is 0.0001598785602254793\n",
      "epoch: 10 step: 26, loss is 0.00016432697884738445\n",
      "epoch: 10 step: 27, loss is 0.17346704006195068\n",
      "epoch: 10 step: 28, loss is 0.000434532412327826\n",
      "epoch: 10 step: 29, loss is 0.051800437271595\n",
      "epoch: 10 step: 30, loss is 0.004193970002233982\n",
      "epoch: 10 step: 31, loss is 0.04547173157334328\n",
      "epoch: 10 step: 32, loss is 0.0028670362662523985\n",
      "epoch: 10 step: 33, loss is 0.00043411419028416276\n",
      "epoch: 10 step: 34, loss is 0.0027749266009777784\n",
      "epoch: 10 step: 35, loss is 0.0005297800526022911\n",
      "epoch: 10 step: 36, loss is 0.0009351406479254365\n",
      "epoch: 10 step: 37, loss is 0.00030843084095977247\n",
      "epoch: 10 step: 38, loss is 0.012543916702270508\n",
      "epoch: 10 step: 39, loss is 0.03527263551950455\n",
      "epoch: 10 step: 40, loss is 0.2527787685394287\n",
      "epoch: 10 step: 41, loss is 0.012959380634129047\n",
      "epoch: 10 step: 42, loss is 0.0001646796299610287\n",
      "epoch: 10 step: 43, loss is 0.01738632470369339\n",
      "epoch: 10 step: 44, loss is 0.007558816112577915\n",
      "epoch: 10 step: 45, loss is 0.007370959967374802\n",
      "epoch: 10 step: 46, loss is 0.0002111018548021093\n",
      "epoch: 10 step: 47, loss is 0.00033231551060453057\n",
      "epoch: 10 step: 48, loss is 0.022344250231981277\n",
      "epoch: 10 step: 49, loss is 4.175482172286138e-05\n",
      "epoch: 10 step: 50, loss is 0.05254112184047699\n",
      "epoch: 10 step: 51, loss is 0.0006134948343969882\n",
      "epoch: 10 step: 52, loss is 0.00044232234358787537\n",
      "epoch: 10 step: 53, loss is 0.006617266684770584\n",
      "epoch: 10 step: 54, loss is 0.0027771564200520515\n",
      "epoch: 10 step: 55, loss is 0.003092160215601325\n",
      "epoch: 10 step: 56, loss is 0.05938450247049332\n",
      "epoch: 10 step: 57, loss is 6.461197335738689e-05\n",
      "epoch: 10 step: 58, loss is 0.012873171828687191\n",
      "epoch: 10 step: 59, loss is 0.019192712381482124\n",
      "epoch: 10 step: 60, loss is 0.006047454662621021\n",
      "epoch: 10 step: 61, loss is 0.0004740408039651811\n",
      "epoch: 10 step: 62, loss is 0.003646108554676175\n",
      "epoch: 10 step: 63, loss is 0.00011001091479556635\n",
      "epoch: 10 step: 64, loss is 0.001933608902618289\n",
      "epoch: 10 step: 65, loss is 0.04321460798382759\n",
      "epoch: 10 step: 66, loss is 0.0003907260252162814\n",
      "epoch: 10 step: 67, loss is 0.00095731345936656\n",
      "epoch: 10 step: 68, loss is 4.6790002670604736e-05\n",
      "epoch: 10 step: 69, loss is 0.0001078577188309282\n",
      "epoch: 10 step: 70, loss is 0.0014727463712915778\n",
      "epoch: 10 step: 71, loss is 0.0006834387895651162\n",
      "epoch: 10 step: 72, loss is 0.00465477304533124\n",
      "epoch: 10 step: 73, loss is 0.0012703754473477602\n",
      "epoch: 10 step: 74, loss is 0.06966672837734222\n",
      "epoch: 10 step: 75, loss is 0.00018618744798004627\n",
      "epoch: 10 step: 76, loss is 0.02678076922893524\n",
      "epoch: 10 step: 77, loss is 0.004001637455075979\n",
      "epoch: 10 step: 78, loss is 0.0006368067115545273\n",
      "epoch: 10 step: 79, loss is 0.016625303775072098\n",
      "epoch: 10 step: 80, loss is 0.007127780932933092\n",
      "epoch: 10 step: 81, loss is 0.001137681188993156\n",
      "epoch: 10 step: 82, loss is 0.01864413544535637\n",
      "epoch: 10 step: 83, loss is 0.018464449793100357\n",
      "epoch: 10 step: 84, loss is 0.0002608385984785855\n",
      "epoch: 10 step: 85, loss is 3.062250107177533e-05\n",
      "epoch: 10 step: 86, loss is 0.031112074851989746\n",
      "epoch: 10 step: 87, loss is 0.00014716143778059632\n",
      "epoch: 10 step: 88, loss is 0.0010424155043438077\n",
      "epoch: 10 step: 89, loss is 0.01931154727935791\n",
      "epoch: 10 step: 90, loss is 0.001360989874228835\n",
      "epoch: 10 step: 91, loss is 5.829679503221996e-05\n",
      "epoch: 10 step: 92, loss is 0.0005788399139419198\n",
      "epoch: 10 step: 93, loss is 0.0004476796311791986\n",
      "epoch: 10 step: 94, loss is 0.011793411336839199\n",
      "epoch: 10 step: 95, loss is 0.00024234398733824492\n",
      "epoch: 10 step: 96, loss is 0.01621655561029911\n",
      "epoch: 10 step: 97, loss is 0.0041113621555268764\n",
      "epoch: 10 step: 98, loss is 0.00017995393136516213\n",
      "epoch: 10 step: 99, loss is 0.0035326078068464994\n",
      "epoch: 10 step: 100, loss is 0.007906057871878147\n",
      "epoch: 10 step: 101, loss is 0.00012957124272361398\n",
      "epoch: 10 step: 102, loss is 5.6483946536900476e-05\n",
      "epoch: 10 step: 103, loss is 0.0008200478041544557\n",
      "epoch: 10 step: 104, loss is 5.525903543457389e-05\n",
      "epoch: 10 step: 105, loss is 0.004552298691123724\n",
      "epoch: 10 step: 106, loss is 0.0006366297602653503\n",
      "epoch: 10 step: 107, loss is 0.0007936923648230731\n",
      "epoch: 10 step: 108, loss is 0.0012249507708474994\n",
      "epoch: 10 step: 109, loss is 2.167820457543712e-05\n",
      "epoch: 10 step: 110, loss is 8.754933332966175e-06\n",
      "epoch: 10 step: 111, loss is 4.384373823995702e-05\n",
      "epoch: 10 step: 112, loss is 2.7423902793088928e-05\n",
      "epoch: 10 step: 113, loss is 2.9627568437717855e-05\n",
      "epoch: 10 step: 114, loss is 0.011282083578407764\n",
      "epoch: 10 step: 115, loss is 0.02916140668094158\n",
      "epoch: 10 step: 116, loss is 0.0002747069811448455\n",
      "epoch: 10 step: 117, loss is 0.006117901299148798\n",
      "epoch: 10 step: 118, loss is 0.0001975430059246719\n",
      "epoch: 10 step: 119, loss is 0.0004428399261087179\n",
      "epoch: 10 step: 120, loss is 0.00020273479458410293\n",
      "epoch: 10 step: 121, loss is 0.0009875432588160038\n",
      "epoch: 10 step: 122, loss is 0.0009707007557153702\n",
      "epoch: 10 step: 123, loss is 0.000318038190016523\n",
      "epoch: 10 step: 124, loss is 0.0005163139430806041\n",
      "epoch: 10 step: 125, loss is 0.0021683068480342627\n",
      "epoch: 10 step: 126, loss is 4.5862248953199014e-05\n",
      "epoch: 10 step: 127, loss is 0.00022410263773053885\n",
      "epoch: 10 step: 128, loss is 0.00044003984658047557\n",
      "epoch: 10 step: 129, loss is 0.029395977035164833\n",
      "epoch: 10 step: 130, loss is 0.06583897769451141\n",
      "epoch: 10 step: 131, loss is 0.0015379423275589943\n",
      "epoch: 10 step: 132, loss is 0.00029471557354554534\n",
      "epoch: 10 step: 133, loss is 0.002502442104741931\n",
      "epoch: 10 step: 134, loss is 2.9545732104452327e-05\n",
      "epoch: 10 step: 135, loss is 0.0010996954515576363\n",
      "epoch: 10 step: 136, loss is 0.02761967107653618\n",
      "epoch: 10 step: 137, loss is 0.10853627324104309\n",
      "epoch: 10 step: 138, loss is 1.9002663975697942e-05\n",
      "epoch: 10 step: 139, loss is 0.000265183683950454\n",
      "epoch: 10 step: 140, loss is 0.0004001546767540276\n",
      "epoch: 10 step: 141, loss is 7.788077346049249e-05\n",
      "epoch: 10 step: 142, loss is 7.296234252862632e-05\n",
      "epoch: 10 step: 143, loss is 0.00019356222765054554\n",
      "epoch: 10 step: 144, loss is 0.001531635643914342\n",
      "epoch: 10 step: 145, loss is 0.00015958536823745817\n",
      "epoch: 10 step: 146, loss is 0.07664045691490173\n",
      "epoch: 10 step: 147, loss is 0.0009986632503569126\n",
      "epoch: 10 step: 148, loss is 8.001602691365406e-05\n",
      "epoch: 10 step: 149, loss is 0.0006109508103691041\n",
      "epoch: 10 step: 150, loss is 0.0332246795296669\n",
      "epoch: 10 step: 151, loss is 0.000993605935946107\n",
      "epoch: 10 step: 152, loss is 0.00022892802371643484\n",
      "epoch: 10 step: 153, loss is 0.00048114691162481904\n",
      "epoch: 10 step: 154, loss is 0.0022012058179825544\n",
      "epoch: 10 step: 155, loss is 0.0028322970028966665\n",
      "epoch: 10 step: 156, loss is 0.021545933559536934\n",
      "epoch: 10 step: 157, loss is 0.028494244441390038\n",
      "epoch: 10 step: 158, loss is 0.00021629055845551193\n",
      "epoch: 10 step: 159, loss is 0.00016112146840896457\n",
      "epoch: 10 step: 160, loss is 7.632698543602601e-05\n",
      "epoch: 10 step: 161, loss is 0.0031283171847462654\n",
      "epoch: 10 step: 162, loss is 0.0160981435328722\n",
      "epoch: 10 step: 163, loss is 0.004654929507523775\n",
      "epoch: 10 step: 164, loss is 0.000661744677927345\n",
      "epoch: 10 step: 165, loss is 0.02945210039615631\n",
      "epoch: 10 step: 166, loss is 0.0002033115888480097\n",
      "epoch: 10 step: 167, loss is 0.0010526332771405578\n",
      "epoch: 10 step: 168, loss is 0.006217148154973984\n",
      "epoch: 10 step: 169, loss is 2.3218823116621934e-05\n",
      "epoch: 10 step: 170, loss is 3.347288293298334e-05\n",
      "epoch: 10 step: 171, loss is 5.750694981543347e-05\n",
      "epoch: 10 step: 172, loss is 0.0006084969500079751\n",
      "epoch: 10 step: 173, loss is 4.209648068353999e-06\n",
      "epoch: 10 step: 174, loss is 2.2203209937288193e-06\n",
      "epoch: 10 step: 175, loss is 0.004839915782213211\n",
      "epoch: 10 step: 176, loss is 5.882302502868697e-05\n",
      "epoch: 10 step: 177, loss is 0.0009671200532466173\n",
      "epoch: 10 step: 178, loss is 0.005718905478715897\n",
      "epoch: 10 step: 179, loss is 0.0021369843743741512\n",
      "epoch: 10 step: 180, loss is 1.1424461263231933e-05\n",
      "epoch: 10 step: 181, loss is 0.003472666023299098\n",
      "epoch: 10 step: 182, loss is 0.016011985018849373\n",
      "epoch: 10 step: 183, loss is 0.009037097916007042\n",
      "epoch: 10 step: 184, loss is 3.9898518480185885e-06\n",
      "epoch: 10 step: 185, loss is 0.003995509352535009\n",
      "epoch: 10 step: 186, loss is 0.0013219163520261645\n",
      "epoch: 10 step: 187, loss is 0.0014066316653043032\n",
      "epoch: 10 step: 188, loss is 0.0038349046371877193\n",
      "epoch: 10 step: 189, loss is 9.938725997926667e-05\n",
      "epoch: 10 step: 190, loss is 0.006373116746544838\n",
      "epoch: 10 step: 191, loss is 0.002487750956788659\n",
      "epoch: 10 step: 192, loss is 0.001555097522214055\n",
      "epoch: 10 step: 193, loss is 0.012052935548126698\n",
      "epoch: 10 step: 194, loss is 2.4079323338810354e-05\n",
      "epoch: 10 step: 195, loss is 0.009340024553239346\n",
      "epoch: 10 step: 196, loss is 0.04363003745675087\n",
      "epoch: 10 step: 197, loss is 0.0006404667510651052\n",
      "epoch: 10 step: 198, loss is 0.0010167460422962904\n",
      "epoch: 10 step: 199, loss is 0.003410942852497101\n",
      "epoch: 10 step: 200, loss is 0.00487501360476017\n",
      "epoch: 10 step: 201, loss is 0.0002443278790451586\n",
      "epoch: 10 step: 202, loss is 0.00014028727309778333\n",
      "epoch: 10 step: 203, loss is 0.006888538133352995\n",
      "epoch: 10 step: 204, loss is 0.07479674369096756\n",
      "epoch: 10 step: 205, loss is 0.00018995092250406742\n",
      "epoch: 10 step: 206, loss is 6.724557351844851e-06\n",
      "epoch: 10 step: 207, loss is 3.6421686672838405e-05\n",
      "epoch: 10 step: 208, loss is 8.444426202913746e-05\n",
      "epoch: 10 step: 209, loss is 0.0012339615495875478\n",
      "epoch: 10 step: 210, loss is 0.007191840559244156\n",
      "epoch: 10 step: 211, loss is 0.0002208142395829782\n",
      "epoch: 10 step: 212, loss is 1.009222978609614e-05\n",
      "epoch: 10 step: 213, loss is 0.0011184468166902661\n",
      "epoch: 10 step: 214, loss is 0.046739887446165085\n",
      "epoch: 10 step: 215, loss is 0.00037270376924425364\n",
      "epoch: 10 step: 216, loss is 0.004739031661301851\n",
      "epoch: 10 step: 217, loss is 0.0005540207494050264\n",
      "epoch: 10 step: 218, loss is 0.00018799699319060892\n",
      "epoch: 10 step: 219, loss is 0.0010399606544524431\n",
      "epoch: 10 step: 220, loss is 0.00015363651618827134\n",
      "epoch: 10 step: 221, loss is 0.004124931991100311\n",
      "epoch: 10 step: 222, loss is 4.809474830835825e-06\n",
      "epoch: 10 step: 223, loss is 0.006296944804489613\n",
      "epoch: 10 step: 224, loss is 0.001326786819845438\n",
      "epoch: 10 step: 225, loss is 0.00019163875549566\n",
      "epoch: 10 step: 226, loss is 9.114638487517368e-06\n",
      "epoch: 10 step: 227, loss is 0.028284316882491112\n",
      "epoch: 10 step: 228, loss is 0.027054429054260254\n",
      "epoch: 10 step: 229, loss is 3.889325853378978e-06\n",
      "epoch: 10 step: 230, loss is 0.03313775733113289\n",
      "epoch: 10 step: 231, loss is 0.0035978525411337614\n",
      "epoch: 10 step: 232, loss is 0.0005017637740820646\n",
      "epoch: 10 step: 233, loss is 0.00030258402694016695\n",
      "epoch: 10 step: 234, loss is 0.00340675818733871\n",
      "epoch: 10 step: 235, loss is 0.25578027963638306\n",
      "epoch: 10 step: 236, loss is 1.3317297089088243e-05\n",
      "epoch: 10 step: 237, loss is 0.0006378288962878287\n",
      "epoch: 10 step: 238, loss is 0.03954726830124855\n",
      "epoch: 10 step: 239, loss is 0.0008893258636817336\n",
      "epoch: 10 step: 240, loss is 0.0002618724829517305\n",
      "epoch: 10 step: 241, loss is 0.003940304275602102\n",
      "epoch: 10 step: 242, loss is 0.0008291251142509282\n",
      "epoch: 10 step: 243, loss is 0.00023151497589424253\n",
      "epoch: 10 step: 244, loss is 6.168811523821205e-05\n",
      "epoch: 10 step: 245, loss is 0.0022823037579655647\n",
      "epoch: 10 step: 246, loss is 0.015746785327792168\n",
      "epoch: 10 step: 247, loss is 0.00404291832819581\n",
      "epoch: 10 step: 248, loss is 0.001302298391237855\n",
      "epoch: 10 step: 249, loss is 0.0034879613667726517\n",
      "epoch: 10 step: 250, loss is 0.003980375826358795\n",
      "epoch: 10 step: 251, loss is 0.02622731402516365\n",
      "epoch: 10 step: 252, loss is 4.532323146122508e-05\n",
      "epoch: 10 step: 253, loss is 0.00039158662548288703\n",
      "epoch: 10 step: 254, loss is 0.00011238070874242112\n",
      "epoch: 10 step: 255, loss is 0.0010924889938905835\n",
      "epoch: 10 step: 256, loss is 0.0034044156782329082\n",
      "epoch: 10 step: 257, loss is 0.005843061953783035\n",
      "epoch: 10 step: 258, loss is 0.00021641160128638148\n",
      "epoch: 10 step: 259, loss is 0.002227481920272112\n",
      "epoch: 10 step: 260, loss is 5.2929688536096364e-05\n",
      "epoch: 10 step: 261, loss is 3.5167399801139254e-06\n",
      "epoch: 10 step: 262, loss is 0.0021116603165864944\n",
      "epoch: 10 step: 263, loss is 0.012132873758673668\n",
      "epoch: 10 step: 264, loss is 0.3453241288661957\n",
      "epoch: 10 step: 265, loss is 5.8992289268644527e-05\n",
      "epoch: 10 step: 266, loss is 4.9942147597903386e-05\n",
      "epoch: 10 step: 267, loss is 0.12641403079032898\n",
      "epoch: 10 step: 268, loss is 0.00039864666177891195\n",
      "epoch: 10 step: 269, loss is 0.0005609403015114367\n",
      "epoch: 10 step: 270, loss is 0.03203045576810837\n",
      "epoch: 10 step: 271, loss is 0.002405081642791629\n",
      "epoch: 10 step: 272, loss is 1.1925284525204916e-05\n",
      "epoch: 10 step: 273, loss is 0.0007619308889843524\n",
      "epoch: 10 step: 274, loss is 4.365465429145843e-05\n",
      "epoch: 10 step: 275, loss is 0.00031707840389572084\n",
      "epoch: 10 step: 276, loss is 0.00016518763732165098\n",
      "epoch: 10 step: 277, loss is 8.276015432784334e-05\n",
      "epoch: 10 step: 278, loss is 2.6559948310023174e-05\n",
      "epoch: 10 step: 279, loss is 4.4790238462155685e-05\n",
      "epoch: 10 step: 280, loss is 0.0008525823941454291\n",
      "epoch: 10 step: 281, loss is 5.222584877628833e-05\n",
      "epoch: 10 step: 282, loss is 0.00016315434186253697\n",
      "epoch: 10 step: 283, loss is 0.0024158540181815624\n",
      "epoch: 10 step: 284, loss is 8.340781641891226e-05\n",
      "epoch: 10 step: 285, loss is 0.0009956310968846083\n",
      "epoch: 10 step: 286, loss is 0.030483288690447807\n",
      "epoch: 10 step: 287, loss is 0.0007673247600905597\n",
      "epoch: 10 step: 288, loss is 0.009107042104005814\n",
      "epoch: 10 step: 289, loss is 0.05344891548156738\n",
      "epoch: 10 step: 290, loss is 0.0035924899857491255\n",
      "epoch: 10 step: 291, loss is 0.015839962288737297\n",
      "epoch: 10 step: 292, loss is 0.0005549027700908482\n",
      "epoch: 10 step: 293, loss is 0.0001549968437757343\n",
      "epoch: 10 step: 294, loss is 0.12316237390041351\n",
      "epoch: 10 step: 295, loss is 0.007550026755779982\n",
      "epoch: 10 step: 296, loss is 0.00018145838112104684\n",
      "epoch: 10 step: 297, loss is 0.004459934309124947\n",
      "epoch: 10 step: 298, loss is 0.022133182734251022\n",
      "epoch: 10 step: 299, loss is 0.11979206651449203\n",
      "epoch: 10 step: 300, loss is 0.018016820773482323\n",
      "epoch: 10 step: 301, loss is 0.0005331109859980643\n",
      "epoch: 10 step: 302, loss is 0.0002832675236277282\n",
      "epoch: 10 step: 303, loss is 0.0006319019012153149\n",
      "epoch: 10 step: 304, loss is 0.0002069622278213501\n",
      "epoch: 10 step: 305, loss is 0.02143111824989319\n",
      "epoch: 10 step: 306, loss is 0.003860828699544072\n",
      "epoch: 10 step: 307, loss is 0.01701849326491356\n",
      "epoch: 10 step: 308, loss is 6.929596565896645e-05\n",
      "epoch: 10 step: 309, loss is 0.0013256171951070428\n",
      "epoch: 10 step: 310, loss is 0.00020086273434571922\n",
      "epoch: 10 step: 311, loss is 0.00571898790076375\n",
      "epoch: 10 step: 312, loss is 0.013241549022495747\n",
      "epoch: 10 step: 313, loss is 0.002889304654672742\n",
      "epoch: 10 step: 314, loss is 8.649305527796969e-05\n",
      "epoch: 10 step: 315, loss is 1.4097310668148566e-05\n",
      "epoch: 10 step: 316, loss is 0.002807190176099539\n",
      "epoch: 10 step: 317, loss is 0.00026644274475984275\n",
      "epoch: 10 step: 318, loss is 4.6877878048690036e-05\n",
      "epoch: 10 step: 319, loss is 0.024357957765460014\n",
      "epoch: 10 step: 320, loss is 0.0003471006057225168\n",
      "epoch: 10 step: 321, loss is 0.001638877554796636\n",
      "epoch: 10 step: 322, loss is 0.000462847005110234\n",
      "epoch: 10 step: 323, loss is 0.0007659581024199724\n",
      "epoch: 10 step: 324, loss is 0.002899979706853628\n",
      "epoch: 10 step: 325, loss is 0.006546363700181246\n",
      "epoch: 10 step: 326, loss is 0.00010355155245633796\n",
      "epoch: 10 step: 327, loss is 5.447032526717521e-05\n",
      "epoch: 10 step: 328, loss is 0.016592536121606827\n",
      "epoch: 10 step: 329, loss is 0.0005489991744980216\n",
      "epoch: 10 step: 330, loss is 0.0008667662623338401\n",
      "epoch: 10 step: 331, loss is 0.00011076920054620132\n",
      "epoch: 10 step: 332, loss is 5.8244939282303676e-05\n",
      "epoch: 10 step: 333, loss is 0.00484011135995388\n",
      "epoch: 10 step: 334, loss is 0.010811647400259972\n",
      "epoch: 10 step: 335, loss is 0.04518018290400505\n",
      "epoch: 10 step: 336, loss is 0.0004436391172930598\n",
      "epoch: 10 step: 337, loss is 0.01781364716589451\n",
      "epoch: 10 step: 338, loss is 0.005299714859575033\n",
      "epoch: 10 step: 339, loss is 7.191250188043341e-05\n",
      "epoch: 10 step: 340, loss is 5.6868288083933294e-05\n",
      "epoch: 10 step: 341, loss is 0.0015801785048097372\n",
      "epoch: 10 step: 342, loss is 0.00023550311743747443\n",
      "epoch: 10 step: 343, loss is 1.8661292415345088e-05\n",
      "epoch: 10 step: 344, loss is 0.11814574897289276\n",
      "epoch: 10 step: 345, loss is 0.00015482804155908525\n",
      "epoch: 10 step: 346, loss is 1.0241338713967707e-05\n",
      "epoch: 10 step: 347, loss is 0.000468944403110072\n",
      "epoch: 10 step: 348, loss is 0.10354185849428177\n",
      "epoch: 10 step: 349, loss is 0.0008255260181613266\n",
      "epoch: 10 step: 350, loss is 0.0006509784725494683\n",
      "epoch: 10 step: 351, loss is 0.010889559984207153\n",
      "epoch: 10 step: 352, loss is 0.0035874515306204557\n",
      "epoch: 10 step: 353, loss is 4.9173821025760844e-05\n",
      "epoch: 10 step: 354, loss is 0.00012344075366854668\n",
      "epoch: 10 step: 355, loss is 3.5572764318203554e-05\n",
      "epoch: 10 step: 356, loss is 0.0005370231810957193\n",
      "epoch: 10 step: 357, loss is 6.910698630235856e-06\n",
      "epoch: 10 step: 358, loss is 0.012939647771418095\n",
      "epoch: 10 step: 359, loss is 0.006177383009344339\n",
      "epoch: 10 step: 360, loss is 0.00044275823165662587\n",
      "epoch: 10 step: 361, loss is 0.00034864453482441604\n",
      "epoch: 10 step: 362, loss is 7.325250044232234e-05\n",
      "epoch: 10 step: 363, loss is 0.05184779316186905\n",
      "epoch: 10 step: 364, loss is 0.0017069359309971333\n",
      "epoch: 10 step: 365, loss is 0.00021150722750462592\n",
      "epoch: 10 step: 366, loss is 0.0028223295230418444\n",
      "epoch: 10 step: 367, loss is 0.0017479644156992435\n",
      "epoch: 10 step: 368, loss is 5.230468559602741e-06\n",
      "epoch: 10 step: 369, loss is 0.0013960294891148806\n",
      "epoch: 10 step: 370, loss is 0.02918553724884987\n",
      "epoch: 10 step: 371, loss is 3.306611688458361e-05\n",
      "epoch: 10 step: 372, loss is 0.003285988001152873\n",
      "epoch: 10 step: 373, loss is 0.00022029523097444326\n",
      "epoch: 10 step: 374, loss is 0.0003181631909683347\n",
      "epoch: 10 step: 375, loss is 2.9296252250787802e-05\n",
      "epoch: 10 step: 376, loss is 0.000147115410072729\n",
      "epoch: 10 step: 377, loss is 0.0004857705207541585\n",
      "epoch: 10 step: 378, loss is 0.001430546515621245\n",
      "epoch: 10 step: 379, loss is 0.000773817824665457\n",
      "epoch: 10 step: 380, loss is 0.00043771922355517745\n",
      "epoch: 10 step: 381, loss is 0.00540462601929903\n",
      "epoch: 10 step: 382, loss is 0.006790543906390667\n",
      "epoch: 10 step: 383, loss is 1.5161380360950716e-05\n",
      "epoch: 10 step: 384, loss is 0.0005414007464423776\n",
      "epoch: 10 step: 385, loss is 2.026123365794774e-05\n",
      "epoch: 10 step: 386, loss is 0.05990465730428696\n",
      "epoch: 10 step: 387, loss is 0.0007179487147368491\n",
      "epoch: 10 step: 388, loss is 0.03596477210521698\n",
      "epoch: 10 step: 389, loss is 0.00014336011372506618\n",
      "epoch: 10 step: 390, loss is 0.0005746417446061969\n",
      "epoch: 10 step: 391, loss is 1.3972925444249995e-05\n",
      "epoch: 10 step: 392, loss is 0.0004918348276987672\n",
      "epoch: 10 step: 393, loss is 0.0009600830380804837\n",
      "epoch: 10 step: 394, loss is 0.0002166354825021699\n",
      "epoch: 10 step: 395, loss is 0.022324785590171814\n",
      "epoch: 10 step: 396, loss is 0.0005330068524926901\n",
      "epoch: 10 step: 397, loss is 6.400179699994624e-05\n",
      "epoch: 10 step: 398, loss is 1.7484378986409865e-05\n",
      "epoch: 10 step: 399, loss is 0.00014362332876771688\n",
      "epoch: 10 step: 400, loss is 0.003745301393792033\n",
      "epoch: 10 step: 401, loss is 2.8367505365167744e-05\n",
      "epoch: 10 step: 402, loss is 0.2970852851867676\n",
      "epoch: 10 step: 403, loss is 0.00032056152122095227\n",
      "epoch: 10 step: 404, loss is 8.713669558346737e-06\n",
      "epoch: 10 step: 405, loss is 0.00057637644931674\n",
      "epoch: 10 step: 406, loss is 0.053997788578271866\n",
      "epoch: 10 step: 407, loss is 2.646268330863677e-05\n",
      "epoch: 10 step: 408, loss is 0.01869480311870575\n",
      "epoch: 10 step: 409, loss is 0.0031874310225248337\n",
      "epoch: 10 step: 410, loss is 0.021911947056651115\n",
      "epoch: 10 step: 411, loss is 0.0008782793884165585\n",
      "epoch: 10 step: 412, loss is 0.0011661003809422255\n",
      "epoch: 10 step: 413, loss is 0.011324102059006691\n",
      "epoch: 10 step: 414, loss is 0.0005596968112513423\n",
      "epoch: 10 step: 415, loss is 0.018753420561552048\n",
      "epoch: 10 step: 416, loss is 0.00031188104185275733\n",
      "epoch: 10 step: 417, loss is 0.000895208097063005\n",
      "epoch: 10 step: 418, loss is 0.0038311444222927094\n",
      "epoch: 10 step: 419, loss is 0.027214976027607918\n",
      "epoch: 10 step: 420, loss is 3.628688500612043e-05\n",
      "epoch: 10 step: 421, loss is 5.8253670431440696e-05\n",
      "epoch: 10 step: 422, loss is 0.0001382126793032512\n",
      "epoch: 10 step: 423, loss is 0.006050743628293276\n",
      "epoch: 10 step: 424, loss is 0.001552548143081367\n",
      "epoch: 10 step: 425, loss is 0.0032351871486753225\n",
      "epoch: 10 step: 426, loss is 0.023993168026208878\n",
      "epoch: 10 step: 427, loss is 5.080148548586294e-05\n",
      "epoch: 10 step: 428, loss is 0.0001043737938744016\n",
      "epoch: 10 step: 429, loss is 0.006467091850936413\n",
      "epoch: 10 step: 430, loss is 0.008239884860813618\n",
      "epoch: 10 step: 431, loss is 0.00038121588295325637\n",
      "epoch: 10 step: 432, loss is 0.006531682331115007\n",
      "epoch: 10 step: 433, loss is 0.05659187212586403\n",
      "epoch: 10 step: 434, loss is 0.0002758805640041828\n",
      "epoch: 10 step: 435, loss is 0.002263725968077779\n",
      "epoch: 10 step: 436, loss is 0.015526746399700642\n",
      "epoch: 10 step: 437, loss is 0.0034529203549027443\n",
      "epoch: 10 step: 438, loss is 0.001686743344180286\n",
      "epoch: 10 step: 439, loss is 0.00011598326091188937\n",
      "epoch: 10 step: 440, loss is 0.0019096678588539362\n",
      "epoch: 10 step: 441, loss is 0.005569694098085165\n",
      "epoch: 10 step: 442, loss is 0.00422170665115118\n",
      "epoch: 10 step: 443, loss is 1.44101495607174e-05\n",
      "epoch: 10 step: 444, loss is 5.270657129585743e-05\n",
      "epoch: 10 step: 445, loss is 0.03649338707327843\n",
      "epoch: 10 step: 446, loss is 9.625410166336223e-05\n",
      "epoch: 10 step: 447, loss is 0.00021052327065262944\n",
      "epoch: 10 step: 448, loss is 0.00024260309874080122\n",
      "epoch: 10 step: 449, loss is 0.00013721577124670148\n",
      "epoch: 10 step: 450, loss is 0.07534797489643097\n",
      "epoch: 10 step: 451, loss is 0.057222139090299606\n",
      "epoch: 10 step: 452, loss is 0.009016642346978188\n",
      "epoch: 10 step: 453, loss is 9.27943256101571e-05\n",
      "epoch: 10 step: 454, loss is 0.0005992257501929998\n",
      "epoch: 10 step: 455, loss is 0.3566470444202423\n",
      "epoch: 10 step: 456, loss is 0.00013470898556988686\n",
      "epoch: 10 step: 457, loss is 0.018275605514645576\n",
      "epoch: 10 step: 458, loss is 0.0042233592830598354\n",
      "epoch: 10 step: 459, loss is 0.05127904936671257\n",
      "epoch: 10 step: 460, loss is 0.004653052426874638\n",
      "epoch: 10 step: 461, loss is 0.011110184714198112\n",
      "epoch: 10 step: 462, loss is 0.00018914537213277072\n",
      "epoch: 10 step: 463, loss is 0.013359960168600082\n",
      "epoch: 10 step: 464, loss is 0.020649859681725502\n",
      "epoch: 10 step: 465, loss is 0.00013480419875122607\n",
      "epoch: 10 step: 466, loss is 0.004520176909863949\n",
      "epoch: 10 step: 467, loss is 0.10717114806175232\n",
      "epoch: 10 step: 468, loss is 6.260917143663391e-05\n",
      "epoch: 10 step: 469, loss is 0.004205957055091858\n",
      "epoch: 10 step: 470, loss is 0.005699761677533388\n",
      "epoch: 10 step: 471, loss is 0.2331405133008957\n",
      "epoch: 10 step: 472, loss is 0.0011147932382300496\n",
      "epoch: 10 step: 473, loss is 0.00017804036906454712\n",
      "epoch: 10 step: 474, loss is 0.00018014424131251872\n",
      "epoch: 10 step: 475, loss is 0.0005714212311431766\n",
      "epoch: 10 step: 476, loss is 0.0024069673381745815\n",
      "epoch: 10 step: 477, loss is 0.000626709486823529\n",
      "epoch: 10 step: 478, loss is 0.004675977863371372\n",
      "epoch: 10 step: 479, loss is 0.0005030597094446421\n",
      "epoch: 10 step: 480, loss is 0.00580670265480876\n",
      "epoch: 10 step: 481, loss is 0.002260536653921008\n",
      "epoch: 10 step: 482, loss is 0.00027419827529229224\n",
      "epoch: 10 step: 483, loss is 0.0006951573886908591\n",
      "epoch: 10 step: 484, loss is 0.00029772298876196146\n",
      "epoch: 10 step: 485, loss is 0.004108182620257139\n",
      "epoch: 10 step: 486, loss is 0.0997244119644165\n",
      "epoch: 10 step: 487, loss is 0.0013562919339165092\n",
      "epoch: 10 step: 488, loss is 0.05707491934299469\n",
      "epoch: 10 step: 489, loss is 0.00010462099453434348\n",
      "epoch: 10 step: 490, loss is 0.013628242537379265\n",
      "epoch: 10 step: 491, loss is 0.15421077609062195\n",
      "epoch: 10 step: 492, loss is 0.044566668570041656\n",
      "epoch: 10 step: 493, loss is 0.033028122037649155\n",
      "epoch: 10 step: 494, loss is 0.25126612186431885\n",
      "epoch: 10 step: 495, loss is 0.00026573354261927307\n",
      "epoch: 10 step: 496, loss is 0.000510517624206841\n",
      "epoch: 10 step: 497, loss is 0.0010025715455412865\n",
      "epoch: 10 step: 498, loss is 0.0005412448081187904\n",
      "epoch: 10 step: 499, loss is 0.0010531982406973839\n",
      "epoch: 10 step: 500, loss is 0.0002926869783550501\n",
      "epoch: 10 step: 501, loss is 0.00011110501509392634\n",
      "epoch: 10 step: 502, loss is 0.00033198384335264564\n",
      "epoch: 10 step: 503, loss is 0.0003300165699329227\n",
      "epoch: 10 step: 504, loss is 0.0020540431141853333\n",
      "epoch: 10 step: 505, loss is 0.002743369899690151\n",
      "epoch: 10 step: 506, loss is 0.0079107154160738\n",
      "epoch: 10 step: 507, loss is 0.0016396859427914023\n",
      "epoch: 10 step: 508, loss is 0.013620209880173206\n",
      "epoch: 10 step: 509, loss is 0.07524559646844864\n",
      "epoch: 10 step: 510, loss is 0.06447171419858932\n",
      "epoch: 10 step: 511, loss is 0.10384327173233032\n",
      "epoch: 10 step: 512, loss is 0.005552094429731369\n",
      "epoch: 10 step: 513, loss is 0.0009457666310481727\n",
      "epoch: 10 step: 514, loss is 0.05199551209807396\n",
      "epoch: 10 step: 515, loss is 0.0011212006211280823\n",
      "epoch: 10 step: 516, loss is 0.07796662300825119\n",
      "epoch: 10 step: 517, loss is 0.002161251148208976\n",
      "epoch: 10 step: 518, loss is 0.003973070997744799\n",
      "epoch: 10 step: 519, loss is 0.023097852244973183\n",
      "epoch: 10 step: 520, loss is 0.00042429473251104355\n",
      "epoch: 10 step: 521, loss is 0.003555583767592907\n",
      "epoch: 10 step: 522, loss is 0.0029170119669288397\n",
      "epoch: 10 step: 523, loss is 0.002367977751418948\n",
      "epoch: 10 step: 524, loss is 0.0015592846320942044\n",
      "epoch: 10 step: 525, loss is 0.00024155179562512785\n",
      "epoch: 10 step: 526, loss is 0.03109017387032509\n",
      "epoch: 10 step: 527, loss is 0.00018091054516844451\n",
      "epoch: 10 step: 528, loss is 0.005630631931126118\n",
      "epoch: 10 step: 529, loss is 0.00023824381059966981\n",
      "epoch: 10 step: 530, loss is 0.0007815147982910275\n",
      "epoch: 10 step: 531, loss is 0.0009459684952162206\n",
      "epoch: 10 step: 532, loss is 2.433526242384687e-05\n",
      "epoch: 10 step: 533, loss is 4.112912210985087e-05\n",
      "epoch: 10 step: 534, loss is 0.0010581016540527344\n",
      "epoch: 10 step: 535, loss is 0.015547416172921658\n",
      "epoch: 10 step: 536, loss is 0.34185755252838135\n",
      "epoch: 10 step: 537, loss is 0.08197949081659317\n",
      "epoch: 10 step: 538, loss is 0.0024835276417434216\n",
      "epoch: 10 step: 539, loss is 0.002625342458486557\n",
      "epoch: 10 step: 540, loss is 0.004141342826187611\n",
      "epoch: 10 step: 541, loss is 0.06319420039653778\n",
      "epoch: 10 step: 542, loss is 0.0006358875543810427\n",
      "epoch: 10 step: 543, loss is 0.010111985728144646\n",
      "epoch: 10 step: 544, loss is 0.03395975008606911\n",
      "epoch: 10 step: 545, loss is 0.05921265482902527\n",
      "epoch: 10 step: 546, loss is 1.8438058759784326e-05\n",
      "epoch: 10 step: 547, loss is 0.0008000002708286047\n",
      "epoch: 10 step: 548, loss is 0.0002251133555546403\n",
      "epoch: 10 step: 549, loss is 0.033570967614650726\n",
      "epoch: 10 step: 550, loss is 0.0025175288319587708\n",
      "epoch: 10 step: 551, loss is 0.0022229431197047234\n",
      "epoch: 10 step: 552, loss is 0.001774068339727819\n",
      "epoch: 10 step: 553, loss is 0.002929137786850333\n",
      "epoch: 10 step: 554, loss is 0.001981086563318968\n",
      "epoch: 10 step: 555, loss is 0.00025884906062856317\n",
      "epoch: 10 step: 556, loss is 0.004749451298266649\n",
      "epoch: 10 step: 557, loss is 0.00010349744115956128\n",
      "epoch: 10 step: 558, loss is 0.0001523252431070432\n",
      "epoch: 10 step: 559, loss is 0.009877040050923824\n",
      "epoch: 10 step: 560, loss is 0.1322779655456543\n",
      "epoch: 10 step: 561, loss is 0.007310378830879927\n",
      "epoch: 10 step: 562, loss is 0.00029458513017743826\n",
      "epoch: 10 step: 563, loss is 0.0006883788155391812\n",
      "epoch: 10 step: 564, loss is 0.0014655041741207242\n",
      "epoch: 10 step: 565, loss is 0.008809740655124187\n",
      "epoch: 10 step: 566, loss is 0.028491945937275887\n",
      "epoch: 10 step: 567, loss is 0.022214848548173904\n",
      "epoch: 10 step: 568, loss is 0.013204931281507015\n",
      "epoch: 10 step: 569, loss is 0.036021824926137924\n",
      "epoch: 10 step: 570, loss is 0.002194625325500965\n",
      "epoch: 10 step: 571, loss is 0.005206614267081022\n",
      "epoch: 10 step: 572, loss is 0.019930051639676094\n",
      "epoch: 10 step: 573, loss is 0.0003375441301614046\n",
      "epoch: 10 step: 574, loss is 0.0003348538593854755\n",
      "epoch: 10 step: 575, loss is 0.0626484677195549\n",
      "epoch: 10 step: 576, loss is 0.09638535231351852\n",
      "epoch: 10 step: 577, loss is 0.0326881930232048\n",
      "epoch: 10 step: 578, loss is 0.0006878547137603164\n",
      "epoch: 10 step: 579, loss is 0.01954631507396698\n",
      "epoch: 10 step: 580, loss is 0.001392688020132482\n",
      "epoch: 10 step: 581, loss is 0.1338994950056076\n",
      "epoch: 10 step: 582, loss is 0.004625566769391298\n",
      "epoch: 10 step: 583, loss is 0.00028791179647669196\n",
      "epoch: 10 step: 584, loss is 0.00031106159440241754\n",
      "epoch: 10 step: 585, loss is 0.00038739724550396204\n",
      "epoch: 10 step: 586, loss is 0.03291536122560501\n",
      "epoch: 10 step: 587, loss is 0.008739970624446869\n",
      "epoch: 10 step: 588, loss is 0.04993373528122902\n",
      "epoch: 10 step: 589, loss is 0.08856838196516037\n",
      "epoch: 10 step: 590, loss is 0.02687874622642994\n",
      "epoch: 10 step: 591, loss is 0.000776008062530309\n",
      "epoch: 10 step: 592, loss is 0.02244926616549492\n",
      "epoch: 10 step: 593, loss is 7.488515984732658e-05\n",
      "epoch: 10 step: 594, loss is 0.0002143632445950061\n",
      "epoch: 10 step: 595, loss is 3.1529849366052076e-05\n",
      "epoch: 10 step: 596, loss is 0.0009045577608048916\n",
      "epoch: 10 step: 597, loss is 2.0732417397084646e-05\n",
      "epoch: 10 step: 598, loss is 0.00014202955935616046\n",
      "epoch: 10 step: 599, loss is 9.949237573891878e-05\n",
      "epoch: 10 step: 600, loss is 1.5186904420261271e-05\n",
      "epoch: 10 step: 601, loss is 0.00030418881215155125\n",
      "epoch: 10 step: 602, loss is 0.00015960469318088144\n",
      "epoch: 10 step: 603, loss is 0.0009925590129569173\n",
      "epoch: 10 step: 604, loss is 0.001348692923784256\n",
      "epoch: 10 step: 605, loss is 0.004322913009673357\n",
      "epoch: 10 step: 606, loss is 0.0010908218100667\n",
      "epoch: 10 step: 607, loss is 0.005065918434411287\n",
      "epoch: 10 step: 608, loss is 5.312336725182831e-05\n",
      "epoch: 10 step: 609, loss is 0.000430562678957358\n",
      "epoch: 10 step: 610, loss is 0.0006862421287223697\n",
      "epoch: 10 step: 611, loss is 0.0024635668378323317\n",
      "epoch: 10 step: 612, loss is 0.03902930021286011\n",
      "epoch: 10 step: 613, loss is 0.00137432967312634\n",
      "epoch: 10 step: 614, loss is 0.006613184232264757\n",
      "epoch: 10 step: 615, loss is 0.0009195377351716161\n",
      "epoch: 10 step: 616, loss is 0.00997486338019371\n",
      "epoch: 10 step: 617, loss is 0.001107211341150105\n",
      "epoch: 10 step: 618, loss is 0.0003192079602740705\n",
      "epoch: 10 step: 619, loss is 0.0032944963313639164\n",
      "epoch: 10 step: 620, loss is 0.024893326684832573\n",
      "epoch: 10 step: 621, loss is 0.0261533260345459\n",
      "epoch: 10 step: 622, loss is 0.0953061506152153\n",
      "epoch: 10 step: 623, loss is 7.71827544667758e-05\n",
      "epoch: 10 step: 624, loss is 0.04728833585977554\n",
      "epoch: 10 step: 625, loss is 0.00039153388934209943\n",
      "epoch: 10 step: 626, loss is 0.0047054653987288475\n",
      "epoch: 10 step: 627, loss is 0.0010725811589509249\n",
      "epoch: 10 step: 628, loss is 0.0013184233102947474\n",
      "epoch: 10 step: 629, loss is 0.06024782359600067\n",
      "epoch: 10 step: 630, loss is 0.038163963705301285\n",
      "epoch: 10 step: 631, loss is 0.035855941474437714\n",
      "epoch: 10 step: 632, loss is 0.05911111831665039\n",
      "epoch: 10 step: 633, loss is 0.0018609716789796948\n",
      "epoch: 10 step: 634, loss is 0.00011683664342854172\n",
      "epoch: 10 step: 635, loss is 0.10341615974903107\n",
      "epoch: 10 step: 636, loss is 0.0008606350747868419\n",
      "epoch: 10 step: 637, loss is 0.000650289177428931\n",
      "epoch: 10 step: 638, loss is 0.0006221255171112716\n",
      "epoch: 10 step: 639, loss is 0.0010290067875757813\n",
      "epoch: 10 step: 640, loss is 0.00012215826427564025\n",
      "epoch: 10 step: 641, loss is 0.0006794428336434066\n",
      "epoch: 10 step: 642, loss is 0.00016337394481524825\n",
      "epoch: 10 step: 643, loss is 0.10042708367109299\n",
      "epoch: 10 step: 644, loss is 0.0008324459777213633\n",
      "epoch: 10 step: 645, loss is 0.00020412490994203836\n",
      "epoch: 10 step: 646, loss is 0.004395609721541405\n",
      "epoch: 10 step: 647, loss is 0.0005095655797049403\n",
      "epoch: 10 step: 648, loss is 0.00514205964282155\n",
      "epoch: 10 step: 649, loss is 0.0016081836074590683\n",
      "epoch: 10 step: 650, loss is 0.08457725495100021\n",
      "epoch: 10 step: 651, loss is 0.001467689871788025\n",
      "epoch: 10 step: 652, loss is 0.04022333025932312\n",
      "epoch: 10 step: 653, loss is 0.006597934290766716\n",
      "epoch: 10 step: 654, loss is 0.004410629626363516\n",
      "epoch: 10 step: 655, loss is 0.0005213578697293997\n",
      "epoch: 10 step: 656, loss is 0.025282200425863266\n",
      "epoch: 10 step: 657, loss is 0.000977658317424357\n",
      "epoch: 10 step: 658, loss is 0.001193688833154738\n",
      "epoch: 10 step: 659, loss is 0.001966237323358655\n",
      "epoch: 10 step: 660, loss is 0.031947243958711624\n",
      "epoch: 10 step: 661, loss is 0.0007589793531224132\n",
      "epoch: 10 step: 662, loss is 0.0006941785104572773\n",
      "epoch: 10 step: 663, loss is 1.3761686204816215e-05\n",
      "epoch: 10 step: 664, loss is 0.004441395401954651\n",
      "epoch: 10 step: 665, loss is 0.0019262265413999557\n",
      "epoch: 10 step: 666, loss is 0.1034356877207756\n",
      "epoch: 10 step: 667, loss is 0.0003298410738352686\n",
      "epoch: 10 step: 668, loss is 0.0004917931510135531\n",
      "epoch: 10 step: 669, loss is 0.0013876170851290226\n",
      "epoch: 10 step: 670, loss is 0.003206872148439288\n",
      "epoch: 10 step: 671, loss is 0.02625255472958088\n",
      "epoch: 10 step: 672, loss is 0.05326459929347038\n",
      "epoch: 10 step: 673, loss is 0.0007487778784707189\n",
      "epoch: 10 step: 674, loss is 0.00013063698133919388\n",
      "epoch: 10 step: 675, loss is 0.008241224102675915\n",
      "epoch: 10 step: 676, loss is 0.0002007084112847224\n",
      "epoch: 10 step: 677, loss is 0.005497995298355818\n",
      "epoch: 10 step: 678, loss is 0.0012510539963841438\n",
      "epoch: 10 step: 679, loss is 0.0050668613985180855\n",
      "epoch: 10 step: 680, loss is 0.14376920461654663\n",
      "epoch: 10 step: 681, loss is 0.02696719579398632\n",
      "epoch: 10 step: 682, loss is 0.0009433386730961502\n",
      "epoch: 10 step: 683, loss is 0.0018606516532599926\n",
      "epoch: 10 step: 684, loss is 0.034421443939208984\n",
      "epoch: 10 step: 685, loss is 0.0006613557343371212\n",
      "epoch: 10 step: 686, loss is 0.03589225932955742\n",
      "epoch: 10 step: 687, loss is 0.0009766746079549193\n",
      "epoch: 10 step: 688, loss is 0.0002801678783725947\n",
      "epoch: 10 step: 689, loss is 0.0032781849149614573\n",
      "epoch: 10 step: 690, loss is 0.004266634583473206\n",
      "epoch: 10 step: 691, loss is 0.000514413055498153\n",
      "epoch: 10 step: 692, loss is 0.010210363194346428\n",
      "epoch: 10 step: 693, loss is 0.0004871823766734451\n",
      "epoch: 10 step: 694, loss is 0.008501354604959488\n",
      "epoch: 10 step: 695, loss is 0.0012551494874060154\n",
      "epoch: 10 step: 696, loss is 0.008109324611723423\n",
      "epoch: 10 step: 697, loss is 0.0050578732043504715\n",
      "epoch: 10 step: 698, loss is 0.002516987267881632\n",
      "epoch: 10 step: 699, loss is 0.0007751169032417238\n",
      "epoch: 10 step: 700, loss is 0.00040993624133989215\n",
      "epoch: 10 step: 701, loss is 0.03005962446331978\n",
      "epoch: 10 step: 702, loss is 0.0005905855214223266\n",
      "epoch: 10 step: 703, loss is 0.0007616344373673201\n",
      "epoch: 10 step: 704, loss is 0.0035458095371723175\n",
      "epoch: 10 step: 705, loss is 0.11851923167705536\n",
      "epoch: 10 step: 706, loss is 0.001083266455680132\n",
      "epoch: 10 step: 707, loss is 0.024876045063138008\n",
      "epoch: 10 step: 708, loss is 0.000339013320626691\n",
      "epoch: 10 step: 709, loss is 0.0008848709403537214\n",
      "epoch: 10 step: 710, loss is 0.0007245276356115937\n",
      "epoch: 10 step: 711, loss is 0.025298669934272766\n",
      "epoch: 10 step: 712, loss is 0.0024848380126059055\n",
      "epoch: 10 step: 713, loss is 0.03132378309965134\n",
      "epoch: 10 step: 714, loss is 0.0016646794974803925\n",
      "epoch: 10 step: 715, loss is 0.00013404405035544187\n",
      "epoch: 10 step: 716, loss is 0.006331097800284624\n",
      "epoch: 10 step: 717, loss is 0.0004798934096470475\n",
      "epoch: 10 step: 718, loss is 0.00023201960721053183\n",
      "epoch: 10 step: 719, loss is 0.00040276465006172657\n",
      "epoch: 10 step: 720, loss is 0.0007339499425143003\n",
      "epoch: 10 step: 721, loss is 0.0051238504238426685\n",
      "epoch: 10 step: 722, loss is 0.005699668545275927\n",
      "epoch: 10 step: 723, loss is 0.0004249965422786772\n",
      "epoch: 10 step: 724, loss is 0.00019666683510877192\n",
      "epoch: 10 step: 725, loss is 0.001153147080913186\n",
      "epoch: 10 step: 726, loss is 4.065331813762896e-05\n",
      "epoch: 10 step: 727, loss is 8.814490865916014e-05\n",
      "epoch: 10 step: 728, loss is 0.00020267374929971993\n",
      "epoch: 10 step: 729, loss is 0.363023579120636\n",
      "epoch: 10 step: 730, loss is 0.004959062207490206\n",
      "epoch: 10 step: 731, loss is 0.0034675272181630135\n",
      "epoch: 10 step: 732, loss is 0.005959453526884317\n",
      "epoch: 10 step: 733, loss is 0.1135103777050972\n",
      "epoch: 10 step: 734, loss is 0.00256217154674232\n",
      "epoch: 10 step: 735, loss is 0.00048001459799706936\n",
      "epoch: 10 step: 736, loss is 3.30812472384423e-05\n",
      "epoch: 10 step: 737, loss is 0.0003134660073556006\n",
      "epoch: 10 step: 738, loss is 0.010242416523396969\n",
      "epoch: 10 step: 739, loss is 0.00277816504240036\n",
      "epoch: 10 step: 740, loss is 0.001689363969489932\n",
      "epoch: 10 step: 741, loss is 0.021280566230416298\n",
      "epoch: 10 step: 742, loss is 0.006023278459906578\n",
      "epoch: 10 step: 743, loss is 0.0054244124330580235\n",
      "epoch: 10 step: 744, loss is 0.016857225447893143\n",
      "epoch: 10 step: 745, loss is 0.00031734706135466695\n",
      "epoch: 10 step: 746, loss is 0.003443704219534993\n",
      "epoch: 10 step: 747, loss is 0.00014709128299728036\n",
      "epoch: 10 step: 748, loss is 0.002062281360849738\n",
      "epoch: 10 step: 749, loss is 0.00013662880519405007\n",
      "epoch: 10 step: 750, loss is 0.02226138487458229\n",
      "epoch: 10 step: 751, loss is 0.0012940074084326625\n",
      "epoch: 10 step: 752, loss is 0.0009308395092375576\n",
      "epoch: 10 step: 753, loss is 0.01741250604391098\n",
      "epoch: 10 step: 754, loss is 0.004304179921746254\n",
      "epoch: 10 step: 755, loss is 0.00016801686433609575\n",
      "epoch: 10 step: 756, loss is 0.027801522985100746\n",
      "epoch: 10 step: 757, loss is 0.0006258409703150392\n",
      "epoch: 10 step: 758, loss is 0.00023867336858529598\n",
      "epoch: 10 step: 759, loss is 0.021163564175367355\n",
      "epoch: 10 step: 760, loss is 0.04082997143268585\n",
      "epoch: 10 step: 761, loss is 0.01920130103826523\n",
      "epoch: 10 step: 762, loss is 0.007567526772618294\n",
      "epoch: 10 step: 763, loss is 0.0001454125886084512\n",
      "epoch: 10 step: 764, loss is 0.00456802686676383\n",
      "epoch: 10 step: 765, loss is 0.008546095341444016\n",
      "epoch: 10 step: 766, loss is 0.00011039560195058584\n",
      "epoch: 10 step: 767, loss is 0.02953517995774746\n",
      "epoch: 10 step: 768, loss is 0.037116680294275284\n",
      "epoch: 10 step: 769, loss is 0.00086445442866534\n",
      "epoch: 10 step: 770, loss is 0.04097335785627365\n",
      "epoch: 10 step: 771, loss is 0.0001411960256518796\n",
      "epoch: 10 step: 772, loss is 7.293989619938657e-05\n",
      "epoch: 10 step: 773, loss is 0.01225830428302288\n",
      "epoch: 10 step: 774, loss is 0.0018853205256164074\n",
      "epoch: 10 step: 775, loss is 0.0002891127369366586\n",
      "epoch: 10 step: 776, loss is 0.00018831127090379596\n",
      "epoch: 10 step: 777, loss is 0.0017487157601863146\n",
      "epoch: 10 step: 778, loss is 0.0014995451783761382\n",
      "epoch: 10 step: 779, loss is 9.20614693313837e-05\n",
      "epoch: 10 step: 780, loss is 0.014282348565757275\n",
      "epoch: 10 step: 781, loss is 0.00926183070987463\n",
      "epoch: 10 step: 782, loss is 0.01225297525525093\n",
      "epoch: 10 step: 783, loss is 0.014349309727549553\n",
      "epoch: 10 step: 784, loss is 0.00022650117170996964\n",
      "epoch: 10 step: 785, loss is 0.0278135035187006\n",
      "epoch: 10 step: 786, loss is 0.00047820896725170314\n",
      "epoch: 10 step: 787, loss is 0.001469784532673657\n",
      "epoch: 10 step: 788, loss is 0.003037555143237114\n",
      "epoch: 10 step: 789, loss is 0.0013008626410737634\n",
      "epoch: 10 step: 790, loss is 0.0009411803912371397\n",
      "epoch: 10 step: 791, loss is 0.0014360514469444752\n",
      "epoch: 10 step: 792, loss is 0.0018638253677636385\n",
      "epoch: 10 step: 793, loss is 0.0010934416204690933\n",
      "epoch: 10 step: 794, loss is 0.03822823986411095\n",
      "epoch: 10 step: 795, loss is 0.0005603765021078289\n",
      "epoch: 10 step: 796, loss is 0.06464914977550507\n",
      "epoch: 10 step: 797, loss is 0.0001924382522702217\n",
      "epoch: 10 step: 798, loss is 0.0004557235224638134\n",
      "epoch: 10 step: 799, loss is 0.03990616276860237\n",
      "epoch: 10 step: 800, loss is 0.006290268152952194\n",
      "epoch: 10 step: 801, loss is 0.016192641109228134\n",
      "epoch: 10 step: 802, loss is 0.0018071663798764348\n",
      "epoch: 10 step: 803, loss is 0.0030558703001588583\n",
      "epoch: 10 step: 804, loss is 0.001695489278063178\n",
      "epoch: 10 step: 805, loss is 1.0426419066789094e-05\n",
      "epoch: 10 step: 806, loss is 0.0002836409548763186\n",
      "epoch: 10 step: 807, loss is 3.9655777072766796e-05\n",
      "epoch: 10 step: 808, loss is 0.007160006556659937\n",
      "epoch: 10 step: 809, loss is 0.005076679866760969\n",
      "epoch: 10 step: 810, loss is 0.004225011914968491\n",
      "epoch: 10 step: 811, loss is 0.02257622219622135\n",
      "epoch: 10 step: 812, loss is 2.386536289122887e-05\n",
      "epoch: 10 step: 813, loss is 2.341896470170468e-05\n",
      "epoch: 10 step: 814, loss is 0.0038524095434695482\n",
      "epoch: 10 step: 815, loss is 1.6922509530559182e-05\n",
      "epoch: 10 step: 816, loss is 0.00410427013412118\n",
      "epoch: 10 step: 817, loss is 0.04866698756814003\n",
      "epoch: 10 step: 818, loss is 0.0070884651504457\n",
      "epoch: 10 step: 819, loss is 0.00047444243682548404\n",
      "epoch: 10 step: 820, loss is 0.0005734688602387905\n",
      "epoch: 10 step: 821, loss is 0.028059154748916626\n",
      "epoch: 10 step: 822, loss is 0.002981104888021946\n",
      "epoch: 10 step: 823, loss is 0.0008718193275853992\n",
      "epoch: 10 step: 824, loss is 0.000501997594255954\n",
      "epoch: 10 step: 825, loss is 0.014123630709946156\n",
      "epoch: 10 step: 826, loss is 9.067807695828378e-05\n",
      "epoch: 10 step: 827, loss is 4.751358210342005e-05\n",
      "epoch: 10 step: 828, loss is 0.011878582648932934\n",
      "epoch: 10 step: 829, loss is 1.971755045815371e-05\n",
      "epoch: 10 step: 830, loss is 0.012629599310457706\n",
      "epoch: 10 step: 831, loss is 3.896708676620619e-06\n",
      "epoch: 10 step: 832, loss is 1.929050085891504e-05\n",
      "epoch: 10 step: 833, loss is 6.871345976833254e-05\n",
      "epoch: 10 step: 834, loss is 0.0033580404706299305\n",
      "epoch: 10 step: 835, loss is 0.23916979134082794\n",
      "epoch: 10 step: 836, loss is 0.008120185695588589\n",
      "epoch: 10 step: 837, loss is 0.04812416061758995\n",
      "epoch: 10 step: 838, loss is 0.0006759423413313925\n",
      "epoch: 10 step: 839, loss is 5.765457171946764e-05\n",
      "epoch: 10 step: 840, loss is 0.002577387960627675\n",
      "epoch: 10 step: 841, loss is 1.6258773030131124e-05\n",
      "epoch: 10 step: 842, loss is 0.000241851870669052\n",
      "epoch: 10 step: 843, loss is 0.0011268884409219027\n",
      "epoch: 10 step: 844, loss is 0.0047360616736114025\n",
      "epoch: 10 step: 845, loss is 0.0006874041282571852\n",
      "epoch: 10 step: 846, loss is 0.0005086219171062112\n",
      "epoch: 10 step: 847, loss is 2.0549294276861474e-05\n",
      "epoch: 10 step: 848, loss is 0.014678091742098331\n",
      "epoch: 10 step: 849, loss is 0.0004042495565954596\n",
      "epoch: 10 step: 850, loss is 3.062645555473864e-05\n",
      "epoch: 10 step: 851, loss is 0.0014175517717376351\n",
      "epoch: 10 step: 852, loss is 0.00225622346624732\n",
      "epoch: 10 step: 853, loss is 3.973365528509021e-05\n",
      "epoch: 10 step: 854, loss is 9.99655676423572e-05\n",
      "epoch: 10 step: 855, loss is 0.04152977466583252\n",
      "epoch: 10 step: 856, loss is 0.05790730193257332\n",
      "epoch: 10 step: 857, loss is 0.009749962948262691\n",
      "epoch: 10 step: 858, loss is 0.024052103981375694\n",
      "epoch: 10 step: 859, loss is 0.0005625357734970748\n",
      "epoch: 10 step: 860, loss is 0.0023095044307410717\n",
      "epoch: 10 step: 861, loss is 0.0005475063226185739\n",
      "epoch: 10 step: 862, loss is 8.241098839789629e-05\n",
      "epoch: 10 step: 863, loss is 0.0015260432846844196\n",
      "epoch: 10 step: 864, loss is 0.0008658344158902764\n",
      "epoch: 10 step: 865, loss is 0.0014587689656764269\n",
      "epoch: 10 step: 866, loss is 1.701456494629383e-05\n",
      "epoch: 10 step: 867, loss is 4.4720425648847595e-05\n",
      "epoch: 10 step: 868, loss is 0.0014750318368896842\n",
      "epoch: 10 step: 869, loss is 8.085781882982701e-05\n",
      "epoch: 10 step: 870, loss is 0.0001114092519856058\n",
      "epoch: 10 step: 871, loss is 3.798993566306308e-05\n",
      "epoch: 10 step: 872, loss is 0.00019411410903558135\n",
      "epoch: 10 step: 873, loss is 0.025268137454986572\n",
      "epoch: 10 step: 874, loss is 0.000208916564588435\n",
      "epoch: 10 step: 875, loss is 0.0004942650557495654\n",
      "epoch: 10 step: 876, loss is 0.00030830485047772527\n",
      "epoch: 10 step: 877, loss is 0.002091125352308154\n",
      "epoch: 10 step: 878, loss is 0.0004564985283650458\n",
      "epoch: 10 step: 879, loss is 3.293028930784203e-05\n",
      "epoch: 10 step: 880, loss is 0.005860826466232538\n",
      "epoch: 10 step: 881, loss is 0.003896770067512989\n",
      "epoch: 10 step: 882, loss is 8.145300671458244e-05\n",
      "epoch: 10 step: 883, loss is 0.0046015516854822636\n",
      "epoch: 10 step: 884, loss is 0.001993288053199649\n",
      "epoch: 10 step: 885, loss is 0.0009314486524090171\n",
      "epoch: 10 step: 886, loss is 4.265106690581888e-05\n",
      "epoch: 10 step: 887, loss is 7.667778118047863e-05\n",
      "epoch: 10 step: 888, loss is 0.004161762539297342\n",
      "epoch: 10 step: 889, loss is 0.0002937417884822935\n",
      "epoch: 10 step: 890, loss is 0.00025920584448613226\n",
      "epoch: 10 step: 891, loss is 0.002961729420349002\n",
      "epoch: 10 step: 892, loss is 0.011431756429374218\n",
      "epoch: 10 step: 893, loss is 0.03695252165198326\n",
      "epoch: 10 step: 894, loss is 0.0001666843018028885\n",
      "epoch: 10 step: 895, loss is 8.0394784163218e-06\n",
      "epoch: 10 step: 896, loss is 5.707042510039173e-05\n",
      "epoch: 10 step: 897, loss is 0.00045829659211449325\n",
      "epoch: 10 step: 898, loss is 0.00036576035199686885\n",
      "epoch: 10 step: 899, loss is 0.0010579917579889297\n",
      "epoch: 10 step: 900, loss is 0.0008256316650658846\n",
      "epoch: 10 step: 901, loss is 0.003874946851283312\n",
      "epoch: 10 step: 902, loss is 7.613267371198162e-05\n",
      "epoch: 10 step: 903, loss is 0.002351309172809124\n",
      "epoch: 10 step: 904, loss is 0.03450871631503105\n",
      "epoch: 10 step: 905, loss is 0.011964976787567139\n",
      "epoch: 10 step: 906, loss is 0.00045089173363521695\n",
      "epoch: 10 step: 907, loss is 1.9047012756345794e-05\n",
      "epoch: 10 step: 908, loss is 0.003876123111695051\n",
      "epoch: 10 step: 909, loss is 0.0076534622348845005\n",
      "epoch: 10 step: 910, loss is 0.021477200090885162\n",
      "epoch: 10 step: 911, loss is 0.049375589936971664\n",
      "epoch: 10 step: 912, loss is 0.00011897063086507842\n",
      "epoch: 10 step: 913, loss is 0.0012277648784220219\n",
      "epoch: 10 step: 914, loss is 0.0007511635776609182\n",
      "epoch: 10 step: 915, loss is 0.005358580965548754\n",
      "epoch: 10 step: 916, loss is 0.00029063582769595087\n",
      "epoch: 10 step: 917, loss is 0.00015410268679261208\n",
      "epoch: 10 step: 918, loss is 0.02126769907772541\n",
      "epoch: 10 step: 919, loss is 0.0002362665836699307\n",
      "epoch: 10 step: 920, loss is 6.605421367567033e-05\n",
      "epoch: 10 step: 921, loss is 0.004259830340743065\n",
      "epoch: 10 step: 922, loss is 0.032506320625543594\n",
      "epoch: 10 step: 923, loss is 0.0003998807806055993\n",
      "epoch: 10 step: 924, loss is 0.00047011280548758805\n",
      "epoch: 10 step: 925, loss is 0.00041136849904432893\n",
      "epoch: 10 step: 926, loss is 0.00029012616141699255\n",
      "epoch: 10 step: 927, loss is 2.5670085960882716e-05\n",
      "epoch: 10 step: 928, loss is 0.00019815390987787396\n",
      "epoch: 10 step: 929, loss is 0.1268564909696579\n",
      "epoch: 10 step: 930, loss is 0.0066858697682619095\n",
      "epoch: 10 step: 931, loss is 0.006505526602268219\n",
      "epoch: 10 step: 932, loss is 4.9416070396546274e-05\n",
      "epoch: 10 step: 933, loss is 0.013444994576275349\n",
      "epoch: 10 step: 934, loss is 0.0007091218139976263\n",
      "epoch: 10 step: 935, loss is 0.07907918840646744\n",
      "epoch: 10 step: 936, loss is 4.224193617119454e-05\n",
      "epoch: 10 step: 937, loss is 1.1375898793630768e-05\n",
      "epoch: 10 step: 938, loss is 0.001856690738350153\n",
      "epoch: 10 step: 939, loss is 0.00029139971593394876\n",
      "epoch: 10 step: 940, loss is 0.0010126333218067884\n",
      "epoch: 10 step: 941, loss is 0.0003443244204390794\n",
      "epoch: 10 step: 942, loss is 1.7679754819255322e-05\n",
      "epoch: 10 step: 943, loss is 0.0005539343110285699\n",
      "epoch: 10 step: 944, loss is 0.0009577368618920445\n",
      "epoch: 10 step: 945, loss is 0.0002652053954079747\n",
      "epoch: 10 step: 946, loss is 0.00409314502030611\n",
      "epoch: 10 step: 947, loss is 0.0008228000369854271\n",
      "epoch: 10 step: 948, loss is 3.92123001802247e-05\n",
      "epoch: 10 step: 949, loss is 0.00022178029757924378\n",
      "epoch: 10 step: 950, loss is 0.013679590076208115\n",
      "epoch: 10 step: 951, loss is 4.1899133066181093e-05\n",
      "epoch: 10 step: 952, loss is 0.011560430750250816\n",
      "epoch: 10 step: 953, loss is 0.000924148946069181\n",
      "epoch: 10 step: 954, loss is 0.001350188278593123\n",
      "epoch: 10 step: 955, loss is 0.0012848852202296257\n",
      "epoch: 10 step: 956, loss is 0.00010627951269270852\n",
      "epoch: 10 step: 957, loss is 0.04281668737530708\n",
      "epoch: 10 step: 958, loss is 0.0005562629085034132\n",
      "epoch: 10 step: 959, loss is 0.0032389953266829252\n",
      "epoch: 10 step: 960, loss is 6.803436554037035e-05\n",
      "epoch: 10 step: 961, loss is 0.0006550442194566131\n",
      "epoch: 10 step: 962, loss is 0.0006177140166983008\n",
      "epoch: 10 step: 963, loss is 0.000251551391556859\n",
      "epoch: 10 step: 964, loss is 0.0012314049527049065\n",
      "epoch: 10 step: 965, loss is 1.8399197870166972e-05\n",
      "epoch: 10 step: 966, loss is 0.021291980519890785\n",
      "epoch: 10 step: 967, loss is 0.00010986928828060627\n",
      "epoch: 10 step: 968, loss is 0.009937595576047897\n",
      "epoch: 10 step: 969, loss is 0.010605055838823318\n",
      "epoch: 10 step: 970, loss is 0.0007779703009873629\n",
      "epoch: 10 step: 971, loss is 8.288319077109918e-05\n",
      "epoch: 10 step: 972, loss is 0.0020223085302859545\n",
      "epoch: 10 step: 973, loss is 0.037137970328330994\n",
      "epoch: 10 step: 974, loss is 0.00024207239039242268\n",
      "epoch: 10 step: 975, loss is 0.002255669329315424\n",
      "epoch: 10 step: 976, loss is 6.1617811297765e-06\n",
      "epoch: 10 step: 977, loss is 0.0004377625009510666\n",
      "epoch: 10 step: 978, loss is 0.0001114417245844379\n",
      "epoch: 10 step: 979, loss is 0.0049928296357393265\n",
      "epoch: 10 step: 980, loss is 7.806780922692269e-05\n",
      "epoch: 10 step: 981, loss is 0.0002087062457576394\n",
      "epoch: 10 step: 982, loss is 9.681915980763733e-05\n",
      "epoch: 10 step: 983, loss is 0.00022488689864985645\n",
      "epoch: 10 step: 984, loss is 0.000597310543525964\n",
      "epoch: 10 step: 985, loss is 0.0014112468343228102\n",
      "epoch: 10 step: 986, loss is 0.0009451835649088025\n",
      "epoch: 10 step: 987, loss is 0.0003236319753341377\n",
      "epoch: 10 step: 988, loss is 0.00025648329756222665\n",
      "epoch: 10 step: 989, loss is 6.816974928369746e-05\n",
      "epoch: 10 step: 990, loss is 0.0010709400521591306\n",
      "epoch: 10 step: 991, loss is 0.005615991540253162\n",
      "epoch: 10 step: 992, loss is 0.01498156413435936\n",
      "epoch: 10 step: 993, loss is 5.0458609621273354e-05\n",
      "epoch: 10 step: 994, loss is 0.004019438289105892\n",
      "epoch: 10 step: 995, loss is 0.00030417460948228836\n",
      "epoch: 10 step: 996, loss is 0.007450058590620756\n",
      "epoch: 10 step: 997, loss is 0.08127471804618835\n",
      "epoch: 10 step: 998, loss is 0.00020435539772734046\n",
      "epoch: 10 step: 999, loss is 0.0022610181476920843\n",
      "epoch: 10 step: 1000, loss is 0.0006671975716017187\n",
      "epoch: 10 step: 1001, loss is 0.0003237100609112531\n",
      "epoch: 10 step: 1002, loss is 0.001795101212337613\n",
      "epoch: 10 step: 1003, loss is 0.0032137997914105654\n",
      "epoch: 10 step: 1004, loss is 0.009036233648657799\n",
      "epoch: 10 step: 1005, loss is 0.0009563850471749902\n",
      "epoch: 10 step: 1006, loss is 0.0031533665023744106\n",
      "epoch: 10 step: 1007, loss is 0.006039454601705074\n",
      "epoch: 10 step: 1008, loss is 4.069876013090834e-05\n",
      "epoch: 10 step: 1009, loss is 8.67592025315389e-05\n",
      "epoch: 10 step: 1010, loss is 0.0007514646276831627\n",
      "epoch: 10 step: 1011, loss is 0.09388737380504608\n",
      "epoch: 10 step: 1012, loss is 0.014058385044336319\n",
      "epoch: 10 step: 1013, loss is 0.03581667318940163\n",
      "epoch: 10 step: 1014, loss is 0.0003333041677251458\n",
      "epoch: 10 step: 1015, loss is 0.00018281179654877633\n",
      "epoch: 10 step: 1016, loss is 0.00017102969286497682\n",
      "epoch: 10 step: 1017, loss is 0.00431081373244524\n",
      "epoch: 10 step: 1018, loss is 0.004806823562830687\n",
      "epoch: 10 step: 1019, loss is 0.00040400971192866564\n",
      "epoch: 10 step: 1020, loss is 0.00047297123819589615\n",
      "epoch: 10 step: 1021, loss is 0.0009343756828457117\n",
      "epoch: 10 step: 1022, loss is 0.005959875416010618\n",
      "epoch: 10 step: 1023, loss is 6.39280096947914e-06\n",
      "epoch: 10 step: 1024, loss is 1.0359381121816114e-05\n",
      "epoch: 10 step: 1025, loss is 0.0001658822293393314\n",
      "epoch: 10 step: 1026, loss is 0.0012587362434715033\n",
      "epoch: 10 step: 1027, loss is 0.028752902522683144\n",
      "epoch: 10 step: 1028, loss is 6.329983443720266e-05\n",
      "epoch: 10 step: 1029, loss is 0.00021632008429151028\n",
      "epoch: 10 step: 1030, loss is 0.0011832864256575704\n",
      "epoch: 10 step: 1031, loss is 9.031857916852459e-05\n",
      "epoch: 10 step: 1032, loss is 0.0013778945431113243\n",
      "epoch: 10 step: 1033, loss is 8.548201549274381e-06\n",
      "epoch: 10 step: 1034, loss is 0.016156302765011787\n",
      "epoch: 10 step: 1035, loss is 0.00020069924357812852\n",
      "epoch: 10 step: 1036, loss is 0.023150302469730377\n",
      "epoch: 10 step: 1037, loss is 0.0017555768135935068\n",
      "epoch: 10 step: 1038, loss is 0.00020510662579908967\n",
      "epoch: 10 step: 1039, loss is 0.0009689948055893183\n",
      "epoch: 10 step: 1040, loss is 0.0016799867153167725\n",
      "epoch: 10 step: 1041, loss is 0.00046852006926201284\n",
      "epoch: 10 step: 1042, loss is 0.00026704242918640375\n",
      "epoch: 10 step: 1043, loss is 7.465138332918286e-05\n",
      "epoch: 10 step: 1044, loss is 0.09596089273691177\n",
      "epoch: 10 step: 1045, loss is 0.0008789822459220886\n",
      "epoch: 10 step: 1046, loss is 0.0003395573585294187\n",
      "epoch: 10 step: 1047, loss is 0.002037458587437868\n",
      "epoch: 10 step: 1048, loss is 0.0023201839067041874\n",
      "epoch: 10 step: 1049, loss is 0.0006351578631438315\n",
      "epoch: 10 step: 1050, loss is 0.06452714651823044\n",
      "epoch: 10 step: 1051, loss is 3.602746801334433e-05\n",
      "epoch: 10 step: 1052, loss is 0.02697284147143364\n",
      "epoch: 10 step: 1053, loss is 0.0007298831478692591\n",
      "epoch: 10 step: 1054, loss is 0.0007356766727752984\n",
      "epoch: 10 step: 1055, loss is 2.144593236153014e-05\n",
      "epoch: 10 step: 1056, loss is 0.00020179404236841947\n",
      "epoch: 10 step: 1057, loss is 0.0001481998187955469\n",
      "epoch: 10 step: 1058, loss is 0.000984495971351862\n",
      "epoch: 10 step: 1059, loss is 0.000152338674524799\n",
      "epoch: 10 step: 1060, loss is 0.0033161176834255457\n",
      "epoch: 10 step: 1061, loss is 0.008301266469061375\n",
      "epoch: 10 step: 1062, loss is 0.006311892066150904\n",
      "epoch: 10 step: 1063, loss is 0.001215166412293911\n",
      "epoch: 10 step: 1064, loss is 0.0034161077346652746\n",
      "epoch: 10 step: 1065, loss is 1.3795297491014935e-05\n",
      "epoch: 10 step: 1066, loss is 0.0006151743582449853\n",
      "epoch: 10 step: 1067, loss is 0.017217086628079414\n",
      "epoch: 10 step: 1068, loss is 0.00739287817850709\n",
      "epoch: 10 step: 1069, loss is 0.10578197240829468\n",
      "epoch: 10 step: 1070, loss is 0.006983517669141293\n",
      "epoch: 10 step: 1071, loss is 0.0011211606906726956\n",
      "epoch: 10 step: 1072, loss is 0.0008850941667333245\n",
      "epoch: 10 step: 1073, loss is 0.0027222703211009502\n",
      "epoch: 10 step: 1074, loss is 0.009683520533144474\n",
      "epoch: 10 step: 1075, loss is 0.001548769068904221\n",
      "epoch: 10 step: 1076, loss is 0.0005152447847649455\n",
      "epoch: 10 step: 1077, loss is 0.01792864501476288\n",
      "epoch: 10 step: 1078, loss is 0.008295819163322449\n",
      "epoch: 10 step: 1079, loss is 0.00612166291102767\n",
      "epoch: 10 step: 1080, loss is 0.001327101606875658\n",
      "epoch: 10 step: 1081, loss is 0.00018170232942793518\n",
      "epoch: 10 step: 1082, loss is 0.014869419857859612\n",
      "epoch: 10 step: 1083, loss is 0.009544014930725098\n",
      "epoch: 10 step: 1084, loss is 0.10746248066425323\n",
      "epoch: 10 step: 1085, loss is 0.00021092862880323082\n",
      "epoch: 10 step: 1086, loss is 0.00028628885047510266\n",
      "epoch: 10 step: 1087, loss is 0.015710551291704178\n",
      "epoch: 10 step: 1088, loss is 0.00023504636192228645\n",
      "epoch: 10 step: 1089, loss is 1.4402687156689353e-05\n",
      "epoch: 10 step: 1090, loss is 0.00021602280321530998\n",
      "epoch: 10 step: 1091, loss is 0.00031705343280918896\n",
      "epoch: 10 step: 1092, loss is 0.016639672219753265\n",
      "epoch: 10 step: 1093, loss is 0.0007879755576141179\n",
      "epoch: 10 step: 1094, loss is 0.08032958209514618\n",
      "epoch: 10 step: 1095, loss is 0.0014220656594261527\n",
      "epoch: 10 step: 1096, loss is 0.0016854354180395603\n",
      "epoch: 10 step: 1097, loss is 0.004263145383447409\n",
      "epoch: 10 step: 1098, loss is 0.0010970642324537039\n",
      "epoch: 10 step: 1099, loss is 0.08184772729873657\n",
      "epoch: 10 step: 1100, loss is 0.0010728529887273908\n",
      "epoch: 10 step: 1101, loss is 0.03384694457054138\n",
      "epoch: 10 step: 1102, loss is 0.0003787155728787184\n",
      "epoch: 10 step: 1103, loss is 0.0001177755621029064\n",
      "epoch: 10 step: 1104, loss is 0.00042672400013543665\n",
      "epoch: 10 step: 1105, loss is 0.00017614713578950614\n",
      "epoch: 10 step: 1106, loss is 0.020509326830506325\n",
      "epoch: 10 step: 1107, loss is 0.00022860676108393818\n",
      "epoch: 10 step: 1108, loss is 0.0012862554285675287\n",
      "epoch: 10 step: 1109, loss is 0.002936396049335599\n",
      "epoch: 10 step: 1110, loss is 0.00023745076032355428\n",
      "epoch: 10 step: 1111, loss is 0.00035393674625083804\n",
      "epoch: 10 step: 1112, loss is 0.0009425391908735037\n",
      "epoch: 10 step: 1113, loss is 3.0717928893864155e-05\n",
      "epoch: 10 step: 1114, loss is 9.871966176433489e-05\n",
      "epoch: 10 step: 1115, loss is 0.21126219630241394\n",
      "epoch: 10 step: 1116, loss is 0.0020067235454916954\n",
      "epoch: 10 step: 1117, loss is 0.016149666160345078\n",
      "epoch: 10 step: 1118, loss is 5.890628381166607e-05\n",
      "epoch: 10 step: 1119, loss is 0.0035233087837696075\n",
      "epoch: 10 step: 1120, loss is 0.030698129907250404\n",
      "epoch: 10 step: 1121, loss is 0.00024919878342188895\n",
      "epoch: 10 step: 1122, loss is 0.011857024393975735\n",
      "epoch: 10 step: 1123, loss is 0.005271767266094685\n",
      "epoch: 10 step: 1124, loss is 0.0033587412908673286\n",
      "epoch: 10 step: 1125, loss is 0.0025385289918631315\n",
      "epoch: 10 step: 1126, loss is 0.0008581579313613474\n",
      "epoch: 10 step: 1127, loss is 0.08681678026914597\n",
      "epoch: 10 step: 1128, loss is 0.00024047937768045813\n",
      "epoch: 10 step: 1129, loss is 0.0006212674197740853\n",
      "epoch: 10 step: 1130, loss is 0.03302515670657158\n",
      "epoch: 10 step: 1131, loss is 7.856076990719885e-05\n",
      "epoch: 10 step: 1132, loss is 0.0009519733139313757\n",
      "epoch: 10 step: 1133, loss is 0.000188272402738221\n",
      "epoch: 10 step: 1134, loss is 0.00018521070887800306\n",
      "epoch: 10 step: 1135, loss is 0.0018477297853678465\n",
      "epoch: 10 step: 1136, loss is 0.011807899922132492\n",
      "epoch: 10 step: 1137, loss is 0.007133946754038334\n",
      "epoch: 10 step: 1138, loss is 0.015385959297418594\n",
      "epoch: 10 step: 1139, loss is 0.0013138661161065102\n",
      "epoch: 10 step: 1140, loss is 0.011174538172781467\n",
      "epoch: 10 step: 1141, loss is 0.00032071760506369174\n",
      "epoch: 10 step: 1142, loss is 0.0001649649057071656\n",
      "epoch: 10 step: 1143, loss is 0.002175490139052272\n",
      "epoch: 10 step: 1144, loss is 0.0017715360736474395\n",
      "epoch: 10 step: 1145, loss is 0.0001124414848163724\n",
      "epoch: 10 step: 1146, loss is 0.00040576691390015185\n",
      "epoch: 10 step: 1147, loss is 0.0031540736090391874\n",
      "epoch: 10 step: 1148, loss is 0.04409182071685791\n",
      "epoch: 10 step: 1149, loss is 2.562986992415972e-05\n",
      "epoch: 10 step: 1150, loss is 0.016188785433769226\n",
      "epoch: 10 step: 1151, loss is 0.00031465551001019776\n",
      "epoch: 10 step: 1152, loss is 8.594703831477091e-05\n",
      "epoch: 10 step: 1153, loss is 0.007051267195492983\n",
      "epoch: 10 step: 1154, loss is 0.005142160691320896\n",
      "epoch: 10 step: 1155, loss is 0.001148576964624226\n",
      "epoch: 10 step: 1156, loss is 0.002436154056340456\n",
      "epoch: 10 step: 1157, loss is 0.00019702249846886843\n",
      "epoch: 10 step: 1158, loss is 0.035728588700294495\n",
      "epoch: 10 step: 1159, loss is 0.0030712413135915995\n",
      "epoch: 10 step: 1160, loss is 0.00020181761647108942\n",
      "epoch: 10 step: 1161, loss is 0.0001248686166945845\n",
      "epoch: 10 step: 1162, loss is 3.985411603935063e-05\n",
      "epoch: 10 step: 1163, loss is 0.00013265643792692572\n",
      "epoch: 10 step: 1164, loss is 0.0011391157750040293\n",
      "epoch: 10 step: 1165, loss is 0.019784875214099884\n",
      "epoch: 10 step: 1166, loss is 0.028299059718847275\n",
      "epoch: 10 step: 1167, loss is 0.0002514783409424126\n",
      "epoch: 10 step: 1168, loss is 0.00014428877329919487\n",
      "epoch: 10 step: 1169, loss is 0.03067191317677498\n",
      "epoch: 10 step: 1170, loss is 2.5295259547419846e-05\n",
      "epoch: 10 step: 1171, loss is 0.0001081405280274339\n",
      "epoch: 10 step: 1172, loss is 0.0012320546666160226\n",
      "epoch: 10 step: 1173, loss is 0.0019058720208704472\n",
      "epoch: 10 step: 1174, loss is 0.00026286227512173355\n",
      "epoch: 10 step: 1175, loss is 1.276928924198728e-05\n",
      "epoch: 10 step: 1176, loss is 0.01752627082169056\n",
      "epoch: 10 step: 1177, loss is 0.0019414575072005391\n",
      "epoch: 10 step: 1178, loss is 0.00038382760249078274\n",
      "epoch: 10 step: 1179, loss is 0.005079890601336956\n",
      "epoch: 10 step: 1180, loss is 5.1369563152547926e-05\n",
      "epoch: 10 step: 1181, loss is 0.004027421120554209\n",
      "epoch: 10 step: 1182, loss is 0.00038762131589464843\n",
      "epoch: 10 step: 1183, loss is 0.0003209190326742828\n",
      "epoch: 10 step: 1184, loss is 0.046251330524683\n",
      "epoch: 10 step: 1185, loss is 8.813585736788809e-05\n",
      "epoch: 10 step: 1186, loss is 0.0008173773530870676\n",
      "epoch: 10 step: 1187, loss is 2.958422919618897e-05\n",
      "epoch: 10 step: 1188, loss is 0.0016391127137467265\n",
      "epoch: 10 step: 1189, loss is 0.022641126066446304\n",
      "epoch: 10 step: 1190, loss is 0.005796506069600582\n",
      "epoch: 10 step: 1191, loss is 0.0004039270570501685\n",
      "epoch: 10 step: 1192, loss is 0.0024263588711619377\n",
      "epoch: 10 step: 1193, loss is 0.0006296992069110274\n",
      "epoch: 10 step: 1194, loss is 0.005705890245735645\n",
      "epoch: 10 step: 1195, loss is 0.0004047433321829885\n",
      "epoch: 10 step: 1196, loss is 0.0017987764440476894\n",
      "epoch: 10 step: 1197, loss is 0.0014716896694153547\n",
      "epoch: 10 step: 1198, loss is 0.00035746334469877183\n",
      "epoch: 10 step: 1199, loss is 0.003419476095587015\n",
      "epoch: 10 step: 1200, loss is 0.00031845649937167764\n",
      "epoch: 10 step: 1201, loss is 0.0027010964695364237\n",
      "epoch: 10 step: 1202, loss is 0.0009415409294888377\n",
      "epoch: 10 step: 1203, loss is 7.967395504238084e-05\n",
      "epoch: 10 step: 1204, loss is 0.00025125034153461456\n",
      "epoch: 10 step: 1205, loss is 0.0007217219099402428\n",
      "epoch: 10 step: 1206, loss is 0.000320084800478071\n",
      "epoch: 10 step: 1207, loss is 0.00035699745058082044\n",
      "epoch: 10 step: 1208, loss is 0.00016558666538912803\n",
      "epoch: 10 step: 1209, loss is 0.016877561807632446\n",
      "epoch: 10 step: 1210, loss is 0.03893783688545227\n",
      "epoch: 10 step: 1211, loss is 0.04856204614043236\n",
      "epoch: 10 step: 1212, loss is 0.0002915934019256383\n",
      "epoch: 10 step: 1213, loss is 0.0029892749153077602\n",
      "epoch: 10 step: 1214, loss is 0.010993864387273788\n",
      "epoch: 10 step: 1215, loss is 0.001298955176025629\n",
      "epoch: 10 step: 1216, loss is 0.0015342861879616976\n",
      "epoch: 10 step: 1217, loss is 0.00013551175652537495\n",
      "epoch: 10 step: 1218, loss is 0.00014939894026611\n",
      "epoch: 10 step: 1219, loss is 0.0002233472332591191\n",
      "epoch: 10 step: 1220, loss is 1.0472245776327327e-05\n",
      "epoch: 10 step: 1221, loss is 0.004573405254632235\n",
      "epoch: 10 step: 1222, loss is 1.2958507795701735e-05\n",
      "epoch: 10 step: 1223, loss is 0.0012022644514217973\n",
      "epoch: 10 step: 1224, loss is 0.0029869009740650654\n",
      "epoch: 10 step: 1225, loss is 0.0030110040679574013\n",
      "epoch: 10 step: 1226, loss is 0.0032155911903828382\n",
      "epoch: 10 step: 1227, loss is 0.0007291482179425657\n",
      "epoch: 10 step: 1228, loss is 0.0029071555472910404\n",
      "epoch: 10 step: 1229, loss is 0.00019791278464253992\n",
      "epoch: 10 step: 1230, loss is 9.185335329675581e-06\n",
      "epoch: 10 step: 1231, loss is 0.04616253077983856\n",
      "epoch: 10 step: 1232, loss is 6.0016714087396394e-06\n",
      "epoch: 10 step: 1233, loss is 0.0011352469446137547\n",
      "epoch: 10 step: 1234, loss is 0.002608394483104348\n",
      "epoch: 10 step: 1235, loss is 0.0001407363888574764\n",
      "epoch: 10 step: 1236, loss is 2.0750062503793743e-06\n",
      "epoch: 10 step: 1237, loss is 0.009974239394068718\n",
      "epoch: 10 step: 1238, loss is 0.0033351753372699022\n",
      "epoch: 10 step: 1239, loss is 0.00587856862694025\n",
      "epoch: 10 step: 1240, loss is 0.0004283695889171213\n",
      "epoch: 10 step: 1241, loss is 0.010332027450203896\n",
      "epoch: 10 step: 1242, loss is 0.0004017266328446567\n",
      "epoch: 10 step: 1243, loss is 0.04068685695528984\n",
      "epoch: 10 step: 1244, loss is 0.07387024164199829\n",
      "epoch: 10 step: 1245, loss is 0.00469979178160429\n",
      "epoch: 10 step: 1246, loss is 0.0005071942578069866\n",
      "epoch: 10 step: 1247, loss is 0.00019697376410476863\n",
      "epoch: 10 step: 1248, loss is 0.0003938494483008981\n",
      "epoch: 10 step: 1249, loss is 0.0001357096480205655\n",
      "epoch: 10 step: 1250, loss is 0.00033810955937951803\n",
      "epoch: 10 step: 1251, loss is 0.007850756868720055\n",
      "epoch: 10 step: 1252, loss is 1.124186564993579e-05\n",
      "epoch: 10 step: 1253, loss is 0.0001973141625057906\n",
      "epoch: 10 step: 1254, loss is 9.160289482679218e-05\n",
      "epoch: 10 step: 1255, loss is 0.0004333546385169029\n",
      "epoch: 10 step: 1256, loss is 0.0012585273943841457\n",
      "epoch: 10 step: 1257, loss is 0.00042971293441951275\n",
      "epoch: 10 step: 1258, loss is 0.002729601925238967\n",
      "epoch: 10 step: 1259, loss is 1.6564939869567752e-05\n",
      "epoch: 10 step: 1260, loss is 7.811604882590473e-05\n",
      "epoch: 10 step: 1261, loss is 0.06365156173706055\n",
      "epoch: 10 step: 1262, loss is 1.5504419934586622e-05\n",
      "epoch: 10 step: 1263, loss is 0.0015725238481536508\n",
      "epoch: 10 step: 1264, loss is 8.054264071688522e-06\n",
      "epoch: 10 step: 1265, loss is 6.581692286999896e-05\n",
      "epoch: 10 step: 1266, loss is 5.4760253988206387e-05\n",
      "epoch: 10 step: 1267, loss is 0.017543118447065353\n",
      "epoch: 10 step: 1268, loss is 3.6913177609676495e-05\n",
      "epoch: 10 step: 1269, loss is 0.007391525898128748\n",
      "epoch: 10 step: 1270, loss is 0.022399891167879105\n",
      "epoch: 10 step: 1271, loss is 0.022299785166978836\n",
      "epoch: 10 step: 1272, loss is 0.0001730361400404945\n",
      "epoch: 10 step: 1273, loss is 0.005015137139707804\n",
      "epoch: 10 step: 1274, loss is 3.776091762119904e-05\n",
      "epoch: 10 step: 1275, loss is 1.3507141375157516e-05\n",
      "epoch: 10 step: 1276, loss is 0.0001749026559991762\n",
      "epoch: 10 step: 1277, loss is 0.00419871648773551\n",
      "epoch: 10 step: 1278, loss is 0.002636489225551486\n",
      "epoch: 10 step: 1279, loss is 0.03488987684249878\n",
      "epoch: 10 step: 1280, loss is 0.0006255691987462342\n",
      "epoch: 10 step: 1281, loss is 0.004781320691108704\n",
      "epoch: 10 step: 1282, loss is 0.0056578051298856735\n",
      "epoch: 10 step: 1283, loss is 0.01084903720766306\n",
      "epoch: 10 step: 1284, loss is 1.3783012036583386e-05\n",
      "epoch: 10 step: 1285, loss is 0.0001754175900714472\n",
      "epoch: 10 step: 1286, loss is 7.626992737641558e-05\n",
      "epoch: 10 step: 1287, loss is 1.5814524886081927e-05\n",
      "epoch: 10 step: 1288, loss is 0.0004193194617982954\n",
      "epoch: 10 step: 1289, loss is 0.00042560751899145544\n",
      "epoch: 10 step: 1290, loss is 0.07352550327777863\n",
      "epoch: 10 step: 1291, loss is 0.02333991602063179\n",
      "epoch: 10 step: 1292, loss is 0.006769954692572355\n",
      "epoch: 10 step: 1293, loss is 0.0016810068627819419\n",
      "epoch: 10 step: 1294, loss is 0.021218840032815933\n",
      "epoch: 10 step: 1295, loss is 0.0014827444683760405\n",
      "epoch: 10 step: 1296, loss is 2.6360725314589217e-05\n",
      "epoch: 10 step: 1297, loss is 5.394691106630489e-05\n",
      "epoch: 10 step: 1298, loss is 0.0009079346200451255\n",
      "epoch: 10 step: 1299, loss is 0.00883795041590929\n",
      "epoch: 10 step: 1300, loss is 6.439280696213245e-05\n",
      "epoch: 10 step: 1301, loss is 0.07435360550880432\n",
      "epoch: 10 step: 1302, loss is 7.475819438695908e-05\n",
      "epoch: 10 step: 1303, loss is 0.011671248823404312\n",
      "epoch: 10 step: 1304, loss is 0.003261190140619874\n",
      "epoch: 10 step: 1305, loss is 0.00019569581490941346\n",
      "epoch: 10 step: 1306, loss is 0.027644265443086624\n",
      "epoch: 10 step: 1307, loss is 0.0029659525025635958\n",
      "epoch: 10 step: 1308, loss is 0.001340770861133933\n",
      "epoch: 10 step: 1309, loss is 0.00019481059280224144\n",
      "epoch: 10 step: 1310, loss is 0.004669164773076773\n",
      "epoch: 10 step: 1311, loss is 0.0789993554353714\n",
      "epoch: 10 step: 1312, loss is 5.4004973208066076e-05\n",
      "epoch: 10 step: 1313, loss is 0.005481825675815344\n",
      "epoch: 10 step: 1314, loss is 0.0003268600848969072\n",
      "epoch: 10 step: 1315, loss is 0.000474371830932796\n",
      "epoch: 10 step: 1316, loss is 0.00048820232041180134\n",
      "epoch: 10 step: 1317, loss is 0.004221431445330381\n",
      "epoch: 10 step: 1318, loss is 6.99366646585986e-05\n",
      "epoch: 10 step: 1319, loss is 0.000916563905775547\n",
      "epoch: 10 step: 1320, loss is 0.0001784931810107082\n",
      "epoch: 10 step: 1321, loss is 0.00017474574269726872\n",
      "epoch: 10 step: 1322, loss is 0.007973184809088707\n",
      "epoch: 10 step: 1323, loss is 0.0003332649066578597\n",
      "epoch: 10 step: 1324, loss is 1.3726801626035012e-05\n",
      "epoch: 10 step: 1325, loss is 0.0017382274381816387\n",
      "epoch: 10 step: 1326, loss is 0.0007580591482110322\n",
      "epoch: 10 step: 1327, loss is 9.36621509026736e-05\n",
      "epoch: 10 step: 1328, loss is 0.0014204821782186627\n",
      "epoch: 10 step: 1329, loss is 0.05049550533294678\n",
      "epoch: 10 step: 1330, loss is 0.00025405740598216653\n",
      "epoch: 10 step: 1331, loss is 8.895988867152482e-05\n",
      "epoch: 10 step: 1332, loss is 0.0035363812930881977\n",
      "epoch: 10 step: 1333, loss is 0.03127022832632065\n",
      "epoch: 10 step: 1334, loss is 0.0005806609988212585\n",
      "epoch: 10 step: 1335, loss is 6.813713753217598e-06\n",
      "epoch: 10 step: 1336, loss is 0.0024334618356078863\n",
      "epoch: 10 step: 1337, loss is 0.0068357051350176334\n",
      "epoch: 10 step: 1338, loss is 3.530579851940274e-05\n",
      "epoch: 10 step: 1339, loss is 6.017196938046254e-05\n",
      "epoch: 10 step: 1340, loss is 0.001988545060157776\n",
      "epoch: 10 step: 1341, loss is 0.020927637815475464\n",
      "epoch: 10 step: 1342, loss is 0.004161876626312733\n",
      "epoch: 10 step: 1343, loss is 0.0006808803300373256\n",
      "epoch: 10 step: 1344, loss is 0.19617944955825806\n",
      "epoch: 10 step: 1345, loss is 0.013726058416068554\n",
      "epoch: 10 step: 1346, loss is 0.011761610396206379\n",
      "epoch: 10 step: 1347, loss is 0.0009996909648180008\n",
      "epoch: 10 step: 1348, loss is 8.203813194995746e-05\n",
      "epoch: 10 step: 1349, loss is 0.003788710106164217\n",
      "epoch: 10 step: 1350, loss is 0.21539229154586792\n",
      "epoch: 10 step: 1351, loss is 5.415571285993792e-05\n",
      "epoch: 10 step: 1352, loss is 0.00010841595940291882\n",
      "epoch: 10 step: 1353, loss is 0.0819978341460228\n",
      "epoch: 10 step: 1354, loss is 0.00029288019868545234\n",
      "epoch: 10 step: 1355, loss is 0.003013528883457184\n",
      "epoch: 10 step: 1356, loss is 0.0005524731241166592\n",
      "epoch: 10 step: 1357, loss is 0.0007519132341258228\n",
      "epoch: 10 step: 1358, loss is 0.017249852418899536\n",
      "epoch: 10 step: 1359, loss is 4.467980397748761e-05\n",
      "epoch: 10 step: 1360, loss is 0.0015242388471961021\n",
      "epoch: 10 step: 1361, loss is 0.0004681837453972548\n",
      "epoch: 10 step: 1362, loss is 0.0007867672247812152\n",
      "epoch: 10 step: 1363, loss is 0.0008723195642232895\n",
      "epoch: 10 step: 1364, loss is 0.02494521252810955\n",
      "epoch: 10 step: 1365, loss is 0.0015000656712800264\n",
      "epoch: 10 step: 1366, loss is 0.013991299085319042\n",
      "epoch: 10 step: 1367, loss is 0.007093448657542467\n",
      "epoch: 10 step: 1368, loss is 0.0004148783627897501\n",
      "epoch: 10 step: 1369, loss is 0.3374255299568176\n",
      "epoch: 10 step: 1370, loss is 0.0006815486121922731\n",
      "epoch: 10 step: 1371, loss is 0.0012035610852763057\n",
      "epoch: 10 step: 1372, loss is 0.06482428312301636\n",
      "epoch: 10 step: 1373, loss is 0.024552449584007263\n",
      "epoch: 10 step: 1374, loss is 0.0003888169885613024\n",
      "epoch: 10 step: 1375, loss is 4.995261406293139e-05\n",
      "epoch: 10 step: 1376, loss is 0.03524352237582207\n",
      "epoch: 10 step: 1377, loss is 0.0005399581859819591\n",
      "epoch: 10 step: 1378, loss is 0.0021957079879939556\n",
      "epoch: 10 step: 1379, loss is 0.0008818784262984991\n",
      "epoch: 10 step: 1380, loss is 0.04284914955496788\n",
      "epoch: 10 step: 1381, loss is 0.00162298243958503\n",
      "epoch: 10 step: 1382, loss is 0.0004713282978627831\n",
      "epoch: 10 step: 1383, loss is 0.007579609286040068\n",
      "epoch: 10 step: 1384, loss is 0.0003741896653082222\n",
      "epoch: 10 step: 1385, loss is 0.0016422959743067622\n",
      "epoch: 10 step: 1386, loss is 0.001505138585343957\n",
      "epoch: 10 step: 1387, loss is 0.0021308332215994596\n",
      "epoch: 10 step: 1388, loss is 0.0005797257763333619\n",
      "epoch: 10 step: 1389, loss is 0.008048259653151035\n",
      "epoch: 10 step: 1390, loss is 0.00021047597692813724\n",
      "epoch: 10 step: 1391, loss is 0.002510522026568651\n",
      "epoch: 10 step: 1392, loss is 0.0002229421224910766\n",
      "epoch: 10 step: 1393, loss is 0.0073431748896837234\n",
      "epoch: 10 step: 1394, loss is 0.0018824927974492311\n",
      "epoch: 10 step: 1395, loss is 0.0011607779888436198\n",
      "epoch: 10 step: 1396, loss is 0.0007528334390372038\n",
      "epoch: 10 step: 1397, loss is 0.0003126396331936121\n",
      "epoch: 10 step: 1398, loss is 0.004568804986774921\n",
      "epoch: 10 step: 1399, loss is 0.00022501341300085187\n",
      "epoch: 10 step: 1400, loss is 0.00639910576865077\n",
      "epoch: 10 step: 1401, loss is 0.006471913307905197\n",
      "epoch: 10 step: 1402, loss is 0.01873338222503662\n",
      "epoch: 10 step: 1403, loss is 0.004735920112580061\n",
      "epoch: 10 step: 1404, loss is 0.043708447366952896\n",
      "epoch: 10 step: 1405, loss is 0.0045105526223778725\n",
      "epoch: 10 step: 1406, loss is 0.041511014103889465\n",
      "epoch: 10 step: 1407, loss is 0.003974152263253927\n",
      "epoch: 10 step: 1408, loss is 0.0026911713648587465\n",
      "epoch: 10 step: 1409, loss is 0.0001583435368956998\n",
      "epoch: 10 step: 1410, loss is 0.0011201979359611869\n",
      "epoch: 10 step: 1411, loss is 0.009725525043904781\n",
      "epoch: 10 step: 1412, loss is 0.0004982684040442109\n",
      "epoch: 10 step: 1413, loss is 0.002925835084170103\n",
      "epoch: 10 step: 1414, loss is 0.0029750128742307425\n",
      "epoch: 10 step: 1415, loss is 0.0013208170421421528\n",
      "epoch: 10 step: 1416, loss is 0.004386294167488813\n",
      "epoch: 10 step: 1417, loss is 4.49256258434616e-05\n",
      "epoch: 10 step: 1418, loss is 3.6248111427994445e-06\n",
      "epoch: 10 step: 1419, loss is 0.020819149911403656\n",
      "epoch: 10 step: 1420, loss is 0.0009190828423015773\n",
      "epoch: 10 step: 1421, loss is 0.004364582244306803\n",
      "epoch: 10 step: 1422, loss is 2.3290780518436804e-05\n",
      "epoch: 10 step: 1423, loss is 0.0628853291273117\n",
      "epoch: 10 step: 1424, loss is 0.00045926921302452683\n",
      "epoch: 10 step: 1425, loss is 0.035571202635765076\n",
      "epoch: 10 step: 1426, loss is 0.00011968939361395314\n",
      "epoch: 10 step: 1427, loss is 0.0002639127487782389\n",
      "epoch: 10 step: 1428, loss is 3.0622260965174064e-05\n",
      "epoch: 10 step: 1429, loss is 0.00041924876859411597\n",
      "epoch: 10 step: 1430, loss is 0.05013969913125038\n",
      "epoch: 10 step: 1431, loss is 0.11304990947246552\n",
      "epoch: 10 step: 1432, loss is 0.001651212340220809\n",
      "epoch: 10 step: 1433, loss is 0.00039337799535132945\n",
      "epoch: 10 step: 1434, loss is 0.010340102948248386\n",
      "epoch: 10 step: 1435, loss is 7.682746945647523e-05\n",
      "epoch: 10 step: 1436, loss is 0.0020817925687879324\n",
      "epoch: 10 step: 1437, loss is 0.017880920320749283\n",
      "epoch: 10 step: 1438, loss is 0.18140757083892822\n",
      "epoch: 10 step: 1439, loss is 0.00016002802294678986\n",
      "epoch: 10 step: 1440, loss is 1.2345017239567824e-05\n",
      "epoch: 10 step: 1441, loss is 2.718223913689144e-05\n",
      "epoch: 10 step: 1442, loss is 0.0006231182487681508\n",
      "epoch: 10 step: 1443, loss is 0.0008839498041197658\n",
      "epoch: 10 step: 1444, loss is 0.000197834538994357\n",
      "epoch: 10 step: 1445, loss is 0.0724024623632431\n",
      "epoch: 10 step: 1446, loss is 0.00035907182609662414\n",
      "epoch: 10 step: 1447, loss is 0.13483723998069763\n",
      "epoch: 10 step: 1448, loss is 8.233196240325924e-06\n",
      "epoch: 10 step: 1449, loss is 0.0020521990954875946\n",
      "epoch: 10 step: 1450, loss is 0.0023032010067254305\n",
      "epoch: 10 step: 1451, loss is 0.005733733996748924\n",
      "epoch: 10 step: 1452, loss is 0.003921002149581909\n",
      "epoch: 10 step: 1453, loss is 0.00028257822850719094\n",
      "epoch: 10 step: 1454, loss is 5.960145790595561e-05\n",
      "epoch: 10 step: 1455, loss is 0.0006312431069090962\n",
      "epoch: 10 step: 1456, loss is 0.02447829581797123\n",
      "epoch: 10 step: 1457, loss is 0.0003466240887064487\n",
      "epoch: 10 step: 1458, loss is 0.00026717822765931487\n",
      "epoch: 10 step: 1459, loss is 0.009805625304579735\n",
      "epoch: 10 step: 1460, loss is 1.7990731066674925e-05\n",
      "epoch: 10 step: 1461, loss is 0.006991492584347725\n",
      "epoch: 10 step: 1462, loss is 0.0010905900271609426\n",
      "epoch: 10 step: 1463, loss is 0.032084330916404724\n",
      "epoch: 10 step: 1464, loss is 0.0007492388831451535\n",
      "epoch: 10 step: 1465, loss is 0.02811935544013977\n",
      "epoch: 10 step: 1466, loss is 6.756511720595881e-05\n",
      "epoch: 10 step: 1467, loss is 0.0019197238143533468\n",
      "epoch: 10 step: 1468, loss is 0.005795531440526247\n",
      "epoch: 10 step: 1469, loss is 6.213876076799352e-06\n",
      "epoch: 10 step: 1470, loss is 0.005384061951190233\n",
      "epoch: 10 step: 1471, loss is 0.002643236890435219\n",
      "epoch: 10 step: 1472, loss is 0.0009385919547639787\n",
      "epoch: 10 step: 1473, loss is 0.0014969289768487215\n",
      "epoch: 10 step: 1474, loss is 0.00433480367064476\n",
      "epoch: 10 step: 1475, loss is 0.004038772080093622\n",
      "epoch: 10 step: 1476, loss is 0.06585384905338287\n",
      "epoch: 10 step: 1477, loss is 0.00042436434887349606\n",
      "epoch: 10 step: 1478, loss is 0.006545498967170715\n",
      "epoch: 10 step: 1479, loss is 0.0020792505238205194\n",
      "epoch: 10 step: 1480, loss is 0.028754455968737602\n",
      "epoch: 10 step: 1481, loss is 0.0065557146444916725\n",
      "epoch: 10 step: 1482, loss is 0.006211399100720882\n",
      "epoch: 10 step: 1483, loss is 0.00013797804422210902\n",
      "epoch: 10 step: 1484, loss is 0.051125526428222656\n",
      "epoch: 10 step: 1485, loss is 0.00012926646741107106\n",
      "epoch: 10 step: 1486, loss is 0.0036793937906622887\n",
      "epoch: 10 step: 1487, loss is 0.00017165965982712805\n",
      "epoch: 10 step: 1488, loss is 0.00912713073194027\n",
      "epoch: 10 step: 1489, loss is 0.00034336099633947015\n",
      "epoch: 10 step: 1490, loss is 0.006474948022514582\n",
      "epoch: 10 step: 1491, loss is 0.0020823024678975344\n",
      "epoch: 10 step: 1492, loss is 0.00011031917529180646\n",
      "epoch: 10 step: 1493, loss is 0.0007549489382654428\n",
      "epoch: 10 step: 1494, loss is 0.00026585065643303096\n",
      "epoch: 10 step: 1495, loss is 0.0016260409029200673\n",
      "epoch: 10 step: 1496, loss is 0.0034002591855823994\n",
      "epoch: 10 step: 1497, loss is 0.027785221114754677\n",
      "epoch: 10 step: 1498, loss is 0.04323277622461319\n",
      "epoch: 10 step: 1499, loss is 0.03279261663556099\n",
      "epoch: 10 step: 1500, loss is 0.0001682710280874744\n",
      "epoch: 10 step: 1501, loss is 0.001020472845993936\n",
      "epoch: 10 step: 1502, loss is 0.02158377878367901\n",
      "epoch: 10 step: 1503, loss is 0.00019034469733014703\n",
      "epoch: 10 step: 1504, loss is 0.006516312249004841\n",
      "epoch: 10 step: 1505, loss is 0.009067293256521225\n",
      "epoch: 10 step: 1506, loss is 0.0012727577704936266\n",
      "epoch: 10 step: 1507, loss is 7.453031867044047e-05\n",
      "epoch: 10 step: 1508, loss is 0.08223526924848557\n",
      "epoch: 10 step: 1509, loss is 0.06186164170503616\n",
      "epoch: 10 step: 1510, loss is 0.00010246163583360612\n",
      "epoch: 10 step: 1511, loss is 0.11617112904787064\n",
      "epoch: 10 step: 1512, loss is 0.015903912484645844\n",
      "epoch: 10 step: 1513, loss is 0.000857991399243474\n",
      "epoch: 10 step: 1514, loss is 4.0318005630979314e-05\n",
      "epoch: 10 step: 1515, loss is 0.005350787192583084\n",
      "epoch: 10 step: 1516, loss is 0.011451485566794872\n",
      "epoch: 10 step: 1517, loss is 0.08218838274478912\n",
      "epoch: 10 step: 1518, loss is 0.04303102195262909\n",
      "epoch: 10 step: 1519, loss is 0.025219377130270004\n",
      "epoch: 10 step: 1520, loss is 5.223882180871442e-05\n",
      "epoch: 10 step: 1521, loss is 0.016035517677664757\n",
      "epoch: 10 step: 1522, loss is 0.00014446554996538907\n",
      "epoch: 10 step: 1523, loss is 0.001687686424702406\n",
      "epoch: 10 step: 1524, loss is 6.495146953966469e-05\n",
      "epoch: 10 step: 1525, loss is 0.0019484767690300941\n",
      "epoch: 10 step: 1526, loss is 0.003072006395086646\n",
      "epoch: 10 step: 1527, loss is 0.0004045590467285365\n",
      "epoch: 10 step: 1528, loss is 0.0033950514625757933\n",
      "epoch: 10 step: 1529, loss is 0.003618620801717043\n",
      "epoch: 10 step: 1530, loss is 0.0004093481693416834\n",
      "epoch: 10 step: 1531, loss is 0.017889203503727913\n",
      "epoch: 10 step: 1532, loss is 0.03009997308254242\n",
      "epoch: 10 step: 1533, loss is 0.004774885717779398\n",
      "epoch: 10 step: 1534, loss is 0.14785262942314148\n",
      "epoch: 10 step: 1535, loss is 0.0021376055665314198\n",
      "epoch: 10 step: 1536, loss is 0.0002557672851253301\n",
      "epoch: 10 step: 1537, loss is 0.007764365058392286\n",
      "epoch: 10 step: 1538, loss is 0.002466099802404642\n",
      "epoch: 10 step: 1539, loss is 0.030801523476839066\n",
      "epoch: 10 step: 1540, loss is 0.00017851963639259338\n",
      "epoch: 10 step: 1541, loss is 0.0006708695436827838\n",
      "epoch: 10 step: 1542, loss is 0.0006939677987247705\n",
      "epoch: 10 step: 1543, loss is 0.03248310834169388\n",
      "epoch: 10 step: 1544, loss is 0.000284148205537349\n",
      "epoch: 10 step: 1545, loss is 6.433791713789105e-05\n",
      "epoch: 10 step: 1546, loss is 0.010014466010034084\n",
      "epoch: 10 step: 1547, loss is 0.0007539313519373536\n",
      "epoch: 10 step: 1548, loss is 0.005528782028704882\n",
      "epoch: 10 step: 1549, loss is 0.000354702933691442\n",
      "epoch: 10 step: 1550, loss is 0.003191054565832019\n",
      "epoch: 10 step: 1551, loss is 0.03046916425228119\n",
      "epoch: 10 step: 1552, loss is 0.0034855317790061235\n",
      "epoch: 10 step: 1553, loss is 0.0003108021046500653\n",
      "epoch: 10 step: 1554, loss is 2.6333776986575685e-05\n",
      "epoch: 10 step: 1555, loss is 0.0063697705045342445\n",
      "epoch: 10 step: 1556, loss is 6.764541467418894e-05\n",
      "epoch: 10 step: 1557, loss is 0.19146212935447693\n",
      "epoch: 10 step: 1558, loss is 0.00013792187382932752\n",
      "epoch: 10 step: 1559, loss is 0.0025437765289098024\n",
      "epoch: 10 step: 1560, loss is 0.1387825459241867\n",
      "epoch: 10 step: 1561, loss is 0.0008364026434719563\n",
      "epoch: 10 step: 1562, loss is 0.04366365447640419\n",
      "epoch: 10 step: 1563, loss is 0.0001305229088757187\n",
      "epoch: 10 step: 1564, loss is 0.06410165876150131\n",
      "epoch: 10 step: 1565, loss is 0.08313368260860443\n",
      "epoch: 10 step: 1566, loss is 0.00014116130478214473\n",
      "epoch: 10 step: 1567, loss is 0.0018252929439768195\n",
      "epoch: 10 step: 1568, loss is 0.024209097027778625\n",
      "epoch: 10 step: 1569, loss is 0.00017339896294288337\n",
      "epoch: 10 step: 1570, loss is 0.013052255846560001\n",
      "epoch: 10 step: 1571, loss is 0.026592861860990524\n",
      "epoch: 10 step: 1572, loss is 0.006513989996165037\n",
      "epoch: 10 step: 1573, loss is 0.00013310463691595942\n",
      "epoch: 10 step: 1574, loss is 0.0003900717128999531\n",
      "epoch: 10 step: 1575, loss is 0.0003255006449762732\n",
      "epoch: 10 step: 1576, loss is 0.08937771618366241\n",
      "epoch: 10 step: 1577, loss is 0.01086963526904583\n",
      "epoch: 10 step: 1578, loss is 0.000995300360955298\n",
      "epoch: 10 step: 1579, loss is 0.011237945407629013\n",
      "epoch: 10 step: 1580, loss is 0.0032651061192154884\n",
      "epoch: 10 step: 1581, loss is 0.0028610529843717813\n",
      "epoch: 10 step: 1582, loss is 0.013349844142794609\n",
      "epoch: 10 step: 1583, loss is 0.00038039381615817547\n",
      "epoch: 10 step: 1584, loss is 0.00023809741833247244\n",
      "epoch: 10 step: 1585, loss is 7.802413892932236e-05\n",
      "epoch: 10 step: 1586, loss is 0.029489560052752495\n",
      "epoch: 10 step: 1587, loss is 2.8390091756591573e-05\n",
      "epoch: 10 step: 1588, loss is 0.00774018932133913\n",
      "epoch: 10 step: 1589, loss is 0.0004365470667835325\n",
      "epoch: 10 step: 1590, loss is 0.1156778633594513\n",
      "epoch: 10 step: 1591, loss is 7.021288911346346e-05\n",
      "epoch: 10 step: 1592, loss is 5.849142689839937e-05\n",
      "epoch: 10 step: 1593, loss is 0.002293885452672839\n",
      "epoch: 10 step: 1594, loss is 1.4621129594161175e-05\n",
      "epoch: 10 step: 1595, loss is 0.09239077568054199\n",
      "epoch: 10 step: 1596, loss is 0.0003304741403553635\n",
      "epoch: 10 step: 1597, loss is 0.0001595685025677085\n",
      "epoch: 10 step: 1598, loss is 1.0747837222879753e-05\n",
      "epoch: 10 step: 1599, loss is 0.002263014204800129\n",
      "epoch: 10 step: 1600, loss is 0.0009481798042543232\n",
      "epoch: 10 step: 1601, loss is 0.002280097221955657\n",
      "epoch: 10 step: 1602, loss is 0.0006920824525877833\n",
      "epoch: 10 step: 1603, loss is 0.00019457779126241803\n",
      "epoch: 10 step: 1604, loss is 0.0012670118594542146\n",
      "epoch: 10 step: 1605, loss is 0.00031041266629472375\n",
      "epoch: 10 step: 1606, loss is 0.0028113829903304577\n",
      "epoch: 10 step: 1607, loss is 0.0018072614911943674\n",
      "epoch: 10 step: 1608, loss is 0.0032589680049568415\n",
      "epoch: 10 step: 1609, loss is 0.005692154634743929\n",
      "epoch: 10 step: 1610, loss is 0.0007022136705927551\n",
      "epoch: 10 step: 1611, loss is 0.00015471162623725832\n",
      "epoch: 10 step: 1612, loss is 4.2870869947364554e-05\n",
      "epoch: 10 step: 1613, loss is 0.015051646158099174\n",
      "epoch: 10 step: 1614, loss is 9.838367986958474e-05\n",
      "epoch: 10 step: 1615, loss is 0.0027708939742296934\n",
      "epoch: 10 step: 1616, loss is 0.004767420701682568\n",
      "epoch: 10 step: 1617, loss is 0.00026672930107451975\n",
      "epoch: 10 step: 1618, loss is 6.622180808335543e-05\n",
      "epoch: 10 step: 1619, loss is 4.857061503571458e-05\n",
      "epoch: 10 step: 1620, loss is 0.0007485372480005026\n",
      "epoch: 10 step: 1621, loss is 0.0005841496167704463\n",
      "epoch: 10 step: 1622, loss is 0.0007152545731514692\n",
      "epoch: 10 step: 1623, loss is 0.000945228326600045\n",
      "epoch: 10 step: 1624, loss is 0.00026437375345267355\n",
      "epoch: 10 step: 1625, loss is 0.2592623829841614\n",
      "epoch: 10 step: 1626, loss is 0.007381365168839693\n",
      "epoch: 10 step: 1627, loss is 0.004211601801216602\n",
      "epoch: 10 step: 1628, loss is 0.009163236245512962\n",
      "epoch: 10 step: 1629, loss is 0.0031377351842820644\n",
      "epoch: 10 step: 1630, loss is 0.00015261225053109229\n",
      "epoch: 10 step: 1631, loss is 0.0013298438861966133\n",
      "epoch: 10 step: 1632, loss is 0.001071735518053174\n",
      "epoch: 10 step: 1633, loss is 0.0017197391716763377\n",
      "epoch: 10 step: 1634, loss is 0.21888130903244019\n",
      "epoch: 10 step: 1635, loss is 0.002153163542971015\n",
      "epoch: 10 step: 1636, loss is 5.920085095567629e-05\n",
      "epoch: 10 step: 1637, loss is 0.0001606215228093788\n",
      "epoch: 10 step: 1638, loss is 0.003725433722138405\n",
      "epoch: 10 step: 1639, loss is 0.0007527905399911106\n",
      "epoch: 10 step: 1640, loss is 0.000821715104393661\n",
      "epoch: 10 step: 1641, loss is 0.004474540241062641\n",
      "epoch: 10 step: 1642, loss is 0.015957139432430267\n",
      "epoch: 10 step: 1643, loss is 0.0003610481508076191\n",
      "epoch: 10 step: 1644, loss is 0.002077451441437006\n",
      "epoch: 10 step: 1645, loss is 0.0006783892749808729\n",
      "epoch: 10 step: 1646, loss is 0.011721254326403141\n",
      "epoch: 10 step: 1647, loss is 0.0006539586465805769\n",
      "epoch: 10 step: 1648, loss is 0.002143440768122673\n",
      "epoch: 10 step: 1649, loss is 0.001997921383008361\n",
      "epoch: 10 step: 1650, loss is 0.0001842362980823964\n",
      "epoch: 10 step: 1651, loss is 0.0574839748442173\n",
      "epoch: 10 step: 1652, loss is 0.011125982739031315\n",
      "epoch: 10 step: 1653, loss is 0.0003010057844221592\n",
      "epoch: 10 step: 1654, loss is 0.015077780932188034\n",
      "epoch: 10 step: 1655, loss is 0.000989368767477572\n",
      "epoch: 10 step: 1656, loss is 3.584679143386893e-05\n",
      "epoch: 10 step: 1657, loss is 4.2745767132146284e-05\n",
      "epoch: 10 step: 1658, loss is 0.00031598855275660753\n",
      "epoch: 10 step: 1659, loss is 0.0007414293359033763\n",
      "epoch: 10 step: 1660, loss is 0.0006872376543469727\n",
      "epoch: 10 step: 1661, loss is 0.00030533320386894047\n",
      "epoch: 10 step: 1662, loss is 0.00037244369741529226\n",
      "epoch: 10 step: 1663, loss is 0.00010446693340782076\n",
      "epoch: 10 step: 1664, loss is 0.03836233913898468\n",
      "epoch: 10 step: 1665, loss is 0.0006832196959294379\n",
      "epoch: 10 step: 1666, loss is 3.854045280604623e-05\n",
      "epoch: 10 step: 1667, loss is 0.005650172475725412\n",
      "epoch: 10 step: 1668, loss is 0.0001246091560460627\n",
      "epoch: 10 step: 1669, loss is 0.0029049646109342575\n",
      "epoch: 10 step: 1670, loss is 5.826486449223012e-05\n",
      "epoch: 10 step: 1671, loss is 0.0013345008483156562\n",
      "epoch: 10 step: 1672, loss is 1.665044328547083e-05\n",
      "epoch: 10 step: 1673, loss is 0.0011410219594836235\n",
      "epoch: 10 step: 1674, loss is 0.0019353271927684546\n",
      "epoch: 10 step: 1675, loss is 0.00076373788760975\n",
      "epoch: 10 step: 1676, loss is 0.0011672392720356584\n",
      "epoch: 10 step: 1677, loss is 0.000221371665247716\n",
      "epoch: 10 step: 1678, loss is 0.0001597940718056634\n",
      "epoch: 10 step: 1679, loss is 6.588540418306366e-05\n",
      "epoch: 10 step: 1680, loss is 0.000550007214769721\n",
      "epoch: 10 step: 1681, loss is 1.8113863916369155e-05\n",
      "epoch: 10 step: 1682, loss is 0.014287934638559818\n",
      "epoch: 10 step: 1683, loss is 0.08725391328334808\n",
      "epoch: 10 step: 1684, loss is 1.4327084500109777e-05\n",
      "epoch: 10 step: 1685, loss is 0.000974716036580503\n",
      "epoch: 10 step: 1686, loss is 0.00023208076891023666\n",
      "epoch: 10 step: 1687, loss is 0.11203721165657043\n",
      "epoch: 10 step: 1688, loss is 0.0002258252934552729\n",
      "epoch: 10 step: 1689, loss is 0.0005336053436622024\n",
      "epoch: 10 step: 1690, loss is 0.0005762734217569232\n",
      "epoch: 10 step: 1691, loss is 0.00018909224309027195\n",
      "epoch: 10 step: 1692, loss is 1.7285625290242024e-05\n",
      "epoch: 10 step: 1693, loss is 2.4958835638244636e-05\n",
      "epoch: 10 step: 1694, loss is 0.00014772056601941586\n",
      "epoch: 10 step: 1695, loss is 0.04374690726399422\n",
      "epoch: 10 step: 1696, loss is 6.359926192089915e-05\n",
      "epoch: 10 step: 1697, loss is 0.002065991749987006\n",
      "epoch: 10 step: 1698, loss is 0.0005663817864842713\n",
      "epoch: 10 step: 1699, loss is 7.741350600554142e-06\n",
      "epoch: 10 step: 1700, loss is 0.00013057154137641191\n",
      "epoch: 10 step: 1701, loss is 0.00022734550293534994\n",
      "epoch: 10 step: 1702, loss is 2.5919813197106123e-05\n",
      "epoch: 10 step: 1703, loss is 9.182243229588494e-05\n",
      "epoch: 10 step: 1704, loss is 1.52892607729882e-05\n",
      "epoch: 10 step: 1705, loss is 0.005907840561121702\n",
      "epoch: 10 step: 1706, loss is 0.00021271024888847023\n",
      "epoch: 10 step: 1707, loss is 0.11667720228433609\n",
      "epoch: 10 step: 1708, loss is 0.010649101808667183\n",
      "epoch: 10 step: 1709, loss is 0.02598484419286251\n",
      "epoch: 10 step: 1710, loss is 0.0021656767930835485\n",
      "epoch: 10 step: 1711, loss is 0.029447998851537704\n",
      "epoch: 10 step: 1712, loss is 0.010028094984591007\n",
      "epoch: 10 step: 1713, loss is 0.013213714584708214\n",
      "epoch: 10 step: 1714, loss is 0.00015282156527973711\n",
      "epoch: 10 step: 1715, loss is 0.022129341959953308\n",
      "epoch: 10 step: 1716, loss is 0.0007568742730654776\n",
      "epoch: 10 step: 1717, loss is 0.0002810115402098745\n",
      "epoch: 10 step: 1718, loss is 0.0002043142740149051\n",
      "epoch: 10 step: 1719, loss is 0.00020966326701454818\n",
      "epoch: 10 step: 1720, loss is 1.9557714040274732e-05\n",
      "epoch: 10 step: 1721, loss is 0.00245282263495028\n",
      "epoch: 10 step: 1722, loss is 0.037341129034757614\n",
      "epoch: 10 step: 1723, loss is 0.0001728349889162928\n",
      "epoch: 10 step: 1724, loss is 0.0006630515563301742\n",
      "epoch: 10 step: 1725, loss is 0.004050342831760645\n",
      "epoch: 10 step: 1726, loss is 0.020386651158332825\n",
      "epoch: 10 step: 1727, loss is 0.0014766603708267212\n",
      "epoch: 10 step: 1728, loss is 0.0001835682342061773\n",
      "epoch: 10 step: 1729, loss is 0.01152514573186636\n",
      "epoch: 10 step: 1730, loss is 0.0021184145007282495\n",
      "epoch: 10 step: 1731, loss is 0.00040772423380985856\n",
      "epoch: 10 step: 1732, loss is 4.425045335665345e-05\n",
      "epoch: 10 step: 1733, loss is 0.016688451170921326\n",
      "epoch: 10 step: 1734, loss is 8.114035881590098e-05\n",
      "epoch: 10 step: 1735, loss is 0.006154485512524843\n",
      "epoch: 10 step: 1736, loss is 0.005167675204575062\n",
      "epoch: 10 step: 1737, loss is 0.00552013237029314\n",
      "epoch: 10 step: 1738, loss is 8.635472113383003e-06\n",
      "epoch: 10 step: 1739, loss is 1.5183870345936157e-05\n",
      "epoch: 10 step: 1740, loss is 0.0002309279516339302\n",
      "epoch: 10 step: 1741, loss is 0.005151429213583469\n",
      "epoch: 10 step: 1742, loss is 9.620626224204898e-05\n",
      "epoch: 10 step: 1743, loss is 0.005148432217538357\n",
      "epoch: 10 step: 1744, loss is 0.022874053567647934\n",
      "epoch: 10 step: 1745, loss is 0.05085992068052292\n",
      "epoch: 10 step: 1746, loss is 0.059721872210502625\n",
      "epoch: 10 step: 1747, loss is 6.922513421159238e-05\n",
      "epoch: 10 step: 1748, loss is 0.00040808500489220023\n",
      "epoch: 10 step: 1749, loss is 0.015266290865838528\n",
      "epoch: 10 step: 1750, loss is 0.000193783562281169\n",
      "epoch: 10 step: 1751, loss is 7.946286132209934e-06\n",
      "epoch: 10 step: 1752, loss is 0.0026414189487695694\n",
      "epoch: 10 step: 1753, loss is 0.016585947945713997\n",
      "epoch: 10 step: 1754, loss is 0.0002470149483997375\n",
      "epoch: 10 step: 1755, loss is 0.003105859737843275\n",
      "epoch: 10 step: 1756, loss is 0.0011194059625267982\n",
      "epoch: 10 step: 1757, loss is 0.00013152381870895624\n",
      "epoch: 10 step: 1758, loss is 0.004336318001151085\n",
      "epoch: 10 step: 1759, loss is 0.0021014364901930094\n",
      "epoch: 10 step: 1760, loss is 9.569376743456814e-06\n",
      "epoch: 10 step: 1761, loss is 0.00020535290241241455\n",
      "epoch: 10 step: 1762, loss is 9.131063961831387e-06\n",
      "epoch: 10 step: 1763, loss is 0.0006273096660152078\n",
      "epoch: 10 step: 1764, loss is 3.2273874239763245e-05\n",
      "epoch: 10 step: 1765, loss is 9.525969289825298e-06\n",
      "epoch: 10 step: 1766, loss is 1.5150147191889118e-05\n",
      "epoch: 10 step: 1767, loss is 0.07938028872013092\n",
      "epoch: 10 step: 1768, loss is 0.00011749676923500374\n",
      "epoch: 10 step: 1769, loss is 1.8859416741179302e-05\n",
      "epoch: 10 step: 1770, loss is 0.00013209384633228183\n",
      "epoch: 10 step: 1771, loss is 0.006342457141727209\n",
      "epoch: 10 step: 1772, loss is 0.0001745789631968364\n",
      "epoch: 10 step: 1773, loss is 0.002588714240118861\n",
      "epoch: 10 step: 1774, loss is 0.0005807362613268197\n",
      "epoch: 10 step: 1775, loss is 0.016565358266234398\n",
      "epoch: 10 step: 1776, loss is 9.667537960922346e-05\n",
      "epoch: 10 step: 1777, loss is 0.00038029244751669466\n",
      "epoch: 10 step: 1778, loss is 0.00023619203420821577\n",
      "epoch: 10 step: 1779, loss is 0.0004846501396968961\n",
      "epoch: 10 step: 1780, loss is 4.589629952533869e-06\n",
      "epoch: 10 step: 1781, loss is 0.0005476733786053956\n",
      "epoch: 10 step: 1782, loss is 0.00022524238738697022\n",
      "epoch: 10 step: 1783, loss is 0.0035519450902938843\n",
      "epoch: 10 step: 1784, loss is 0.00256281322799623\n",
      "epoch: 10 step: 1785, loss is 0.028960872441530228\n",
      "epoch: 10 step: 1786, loss is 0.0010694890515878797\n",
      "epoch: 10 step: 1787, loss is 8.635729318484664e-05\n",
      "epoch: 10 step: 1788, loss is 0.00020358248730190098\n",
      "epoch: 10 step: 1789, loss is 0.0016152148600667715\n",
      "epoch: 10 step: 1790, loss is 0.02739122323691845\n",
      "epoch: 10 step: 1791, loss is 0.0002748231927398592\n",
      "epoch: 10 step: 1792, loss is 0.0005759808700531721\n",
      "epoch: 10 step: 1793, loss is 0.0014504267601296306\n",
      "epoch: 10 step: 1794, loss is 0.0006908152136020362\n",
      "epoch: 10 step: 1795, loss is 0.019784262403845787\n",
      "epoch: 10 step: 1796, loss is 0.000841420900542289\n",
      "epoch: 10 step: 1797, loss is 0.00022916031593922526\n",
      "epoch: 10 step: 1798, loss is 4.9829905037768185e-05\n",
      "epoch: 10 step: 1799, loss is 0.002001948654651642\n",
      "epoch: 10 step: 1800, loss is 4.78632609883789e-05\n",
      "epoch: 10 step: 1801, loss is 0.0015776018844917417\n",
      "epoch: 10 step: 1802, loss is 0.03820992261171341\n",
      "epoch: 10 step: 1803, loss is 0.0016152671305462718\n",
      "epoch: 10 step: 1804, loss is 0.03460891544818878\n",
      "epoch: 10 step: 1805, loss is 9.946456702891737e-05\n",
      "epoch: 10 step: 1806, loss is 0.003882726887241006\n",
      "epoch: 10 step: 1807, loss is 0.00023654841061215848\n",
      "epoch: 10 step: 1808, loss is 0.0020738313905894756\n",
      "epoch: 10 step: 1809, loss is 0.0001099613873520866\n",
      "epoch: 10 step: 1810, loss is 0.00038088971632532775\n",
      "epoch: 10 step: 1811, loss is 0.01975259929895401\n",
      "epoch: 10 step: 1812, loss is 0.006385720334947109\n",
      "epoch: 10 step: 1813, loss is 0.0015953010879456997\n",
      "epoch: 10 step: 1814, loss is 0.0010191923938691616\n",
      "epoch: 10 step: 1815, loss is 0.00027139161829836667\n",
      "epoch: 10 step: 1816, loss is 0.00040963306673802435\n",
      "epoch: 10 step: 1817, loss is 0.0038935572374612093\n",
      "epoch: 10 step: 1818, loss is 0.17651039361953735\n",
      "epoch: 10 step: 1819, loss is 3.773798016482033e-05\n",
      "epoch: 10 step: 1820, loss is 3.596996612031944e-05\n",
      "epoch: 10 step: 1821, loss is 0.0007571701426059008\n",
      "epoch: 10 step: 1822, loss is 0.00033494317904114723\n",
      "epoch: 10 step: 1823, loss is 9.216665785061195e-05\n",
      "epoch: 10 step: 1824, loss is 0.0003808697802014649\n",
      "epoch: 10 step: 1825, loss is 2.6337917006458156e-05\n",
      "epoch: 10 step: 1826, loss is 0.0001622447744011879\n",
      "epoch: 10 step: 1827, loss is 0.0011645109625533223\n",
      "epoch: 10 step: 1828, loss is 0.0014614536194130778\n",
      "epoch: 10 step: 1829, loss is 0.014485898427665234\n",
      "epoch: 10 step: 1830, loss is 1.4242466022551525e-05\n",
      "epoch: 10 step: 1831, loss is 0.004591634962707758\n",
      "epoch: 10 step: 1832, loss is 0.0016472036950290203\n",
      "epoch: 10 step: 1833, loss is 0.08832226693630219\n",
      "epoch: 10 step: 1834, loss is 0.02091502957046032\n",
      "epoch: 10 step: 1835, loss is 4.337880091043189e-05\n",
      "epoch: 10 step: 1836, loss is 0.000284828944131732\n",
      "epoch: 10 step: 1837, loss is 0.0075554135255515575\n",
      "epoch: 10 step: 1838, loss is 0.039831943809986115\n",
      "epoch: 10 step: 1839, loss is 0.0002316878380952403\n",
      "epoch: 10 step: 1840, loss is 0.0012207187246531248\n",
      "epoch: 10 step: 1841, loss is 0.0016517334152013063\n",
      "epoch: 10 step: 1842, loss is 2.5160728910122998e-05\n",
      "epoch: 10 step: 1843, loss is 0.010688829235732555\n",
      "epoch: 10 step: 1844, loss is 0.001097327098250389\n",
      "epoch: 10 step: 1845, loss is 0.006108501926064491\n",
      "epoch: 10 step: 1846, loss is 0.023982470855116844\n",
      "epoch: 10 step: 1847, loss is 0.000784310803283006\n",
      "epoch: 10 step: 1848, loss is 2.994941132783424e-05\n",
      "epoch: 10 step: 1849, loss is 7.520088547607884e-05\n",
      "epoch: 10 step: 1850, loss is 0.0005422454560175538\n",
      "epoch: 10 step: 1851, loss is 0.0062058051116764545\n",
      "epoch: 10 step: 1852, loss is 0.00023000834335107356\n",
      "epoch: 10 step: 1853, loss is 0.0008767921244725585\n",
      "epoch: 10 step: 1854, loss is 7.843760977266356e-05\n",
      "epoch: 10 step: 1855, loss is 0.0003487526555545628\n",
      "epoch: 10 step: 1856, loss is 0.0002068183821393177\n",
      "epoch: 10 step: 1857, loss is 0.005595947150141001\n",
      "epoch: 10 step: 1858, loss is 0.000458930095192045\n",
      "epoch: 10 step: 1859, loss is 0.000954595219809562\n",
      "epoch: 10 step: 1860, loss is 0.06302103400230408\n",
      "epoch: 10 step: 1861, loss is 0.0034571168944239616\n",
      "epoch: 10 step: 1862, loss is 4.4780681491829455e-05\n",
      "epoch: 10 step: 1863, loss is 0.0007932630833238363\n",
      "epoch: 10 step: 1864, loss is 0.028293278068304062\n",
      "epoch: 10 step: 1865, loss is 0.008608710952103138\n",
      "epoch: 10 step: 1866, loss is 0.029594385996460915\n",
      "epoch: 10 step: 1867, loss is 0.0006970257381908596\n",
      "epoch: 10 step: 1868, loss is 5.156216138857417e-05\n",
      "epoch: 10 step: 1869, loss is 6.955931166885421e-05\n",
      "epoch: 10 step: 1870, loss is 0.01569572277367115\n",
      "epoch: 10 step: 1871, loss is 7.94593506725505e-05\n",
      "epoch: 10 step: 1872, loss is 0.07043986767530441\n",
      "epoch: 10 step: 1873, loss is 0.0005653567495755851\n",
      "epoch: 10 step: 1874, loss is 0.0002665483916644007\n",
      "epoch: 10 step: 1875, loss is 0.0008536462555639446\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "id": "4f9a2ab3e4faa63f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T07:44:03.468596Z",
     "start_time": "2025-04-07T07:44:03.384929Z"
    }
   },
   "source": [
    "steps = step_loss[\"step\"]\n",
    "loss_value = step_loss[\"loss_value\"]\n",
    "steps = list(map(int, steps))\n",
    "loss_value = list(map(float, loss_value))\n",
    "plt.plot(steps, loss_value, color=\"red\")\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"Loss_value\")\n",
    "plt.title(\"Loss function value change chart\")\n",
    "plt.show()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "id": "2a1e5a42b213c5a2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T07:44:08.789954Z",
     "start_time": "2025-04-07T07:44:08.216043Z"
    }
   },
   "source": [
    "def test_net(network, model, mnist_path):\n",
    "    \"\"\"Define the evaluation method.\"\"\"\n",
    "    print(\"============== Starting Testing ==============\")\n",
    "    # load the saved model for evaluation\n",
    "    param_dict = load_checkpoint(\"./checkpoints/checkpoint_lenet-10_1875.ckpt\")\n",
    "    # load parameter to the network\n",
    "    load_param_into_net(network, param_dict)\n",
    "    # load testing dataset\n",
    "    ds_eval = create_dataset(os.path.join(mnist_path, \"test\"))\n",
    "    acc = model.eval(ds_eval, dataset_sink_mode=True)\n",
    "    print(\"============== Accuracy:{} ==============\".format(acc))\n",
    "\n",
    "\n",
    "test_net(network, model, mnist_path)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] ME(7418:129632434239040,MainProcess):2025-04-07-15:44:08.221.000 [mindspore/dataset/core/validator_helpers.py:744] 'Resize' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Resize' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(7418:129632434239040,MainProcess):2025-04-07-15:44:08.222.000 [mindspore/dataset/core/validator_helpers.py:744] 'Rescale' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Rescale' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(7418:129632434239040,MainProcess):2025-04-07-15:44:08.222.000 [mindspore/dataset/core/validator_helpers.py:744] 'Rescale' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Rescale' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(7418:129632434239040,MainProcess):2025-04-07-15:44:08.222.000 [mindspore/dataset/core/validator_helpers.py:744] 'HWC2CHW' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'HWC2CHW' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(7418:129632434239040,MainProcess):2025-04-07-15:44:08.222.000 [mindspore/dataset/core/validator_helpers.py:744] 'TypeCast' from mindspore.dataset.transforms.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'TypeCast' from mindspore.dataset.transforms instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== Starting Testing ==============\n",
      "============== Accuracy:{'Accuracy': np.float64(0.9899839743589743)} ==============\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "id": "7eabb7aefb2571dd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T07:44:19.205740Z",
     "start_time": "2025-04-07T07:44:10.652005Z"
    }
   },
   "source": [
    "def acc_model_info(network, model, mnist_path, model_numbers, epoch_size):\n",
    "    \"\"\"Define the plot info method\"\"\"\n",
    "    step_list = []\n",
    "    acc_list = []\n",
    "    for i in range(1, epoch_size + 1):\n",
    "        # load the saved model for evaluation\n",
    "        #加载同一个模型得到的模型训练步数变化，精度随之变化\n",
    "        param_dict = load_checkpoint(\"./checkpoints/checkpoint_lenet-10_1875.ckpt\")\n",
    "        #加载不同一个模型得到的模型训练步数变化，精度随之变化\n",
    "        #param_dict = load_checkpoint(\"checkpoint_lenet-{}_1875.ckpt\".format(str(i)))\n",
    "\n",
    "        # load parameter to the network\n",
    "        load_param_into_net(network, param_dict)\n",
    "        # load testing dataset\n",
    "    for i in range(1, model_numbers + 1):\n",
    "        ds_eval = create_dataset(os.path.join(mnist_path, \"test\"))\n",
    "        acc = model.eval(ds_eval, dataset_sink_mode=True)\n",
    "        acc_list.append(acc['Accuracy'])\n",
    "        step_list.append(i * 125)\n",
    "    return step_list, acc_list\n",
    "\n",
    "\n",
    "# Draw line chart according to training steps and model accuracy\n",
    "l1, l2 = acc_model_info(network, model, mnist_path, 15, 10)\n",
    "plt.xlabel(\"Model of Steps\")\n",
    "plt.ylabel(\"Model accuracy\")\n",
    "plt.title(\"Model accuracy variation chart\")\n",
    "plt.plot(l1, l2, 'red')\n",
    "plt.show()"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] ME(7418:129632434239040,MainProcess):2025-04-07-15:44:10.684.000 [mindspore/dataset/core/validator_helpers.py:744] 'Resize' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Resize' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(7418:129632434239040,MainProcess):2025-04-07-15:44:10.685.000 [mindspore/dataset/core/validator_helpers.py:744] 'Rescale' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Rescale' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(7418:129632434239040,MainProcess):2025-04-07-15:44:10.685.000 [mindspore/dataset/core/validator_helpers.py:744] 'Rescale' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Rescale' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(7418:129632434239040,MainProcess):2025-04-07-15:44:10.685.000 [mindspore/dataset/core/validator_helpers.py:744] 'HWC2CHW' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'HWC2CHW' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(7418:129632434239040,MainProcess):2025-04-07-15:44:10.686.000 [mindspore/dataset/core/validator_helpers.py:744] 'TypeCast' from mindspore.dataset.transforms.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'TypeCast' from mindspore.dataset.transforms instead.\n",
      "[WARNING] ME(7418:129632434239040,MainProcess):2025-04-07-15:44:11.490.000 [mindspore/dataset/core/validator_helpers.py:744] 'Resize' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Resize' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(7418:129632434239040,MainProcess):2025-04-07-15:44:11.491.000 [mindspore/dataset/core/validator_helpers.py:744] 'Rescale' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Rescale' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(7418:129632434239040,MainProcess):2025-04-07-15:44:11.491.000 [mindspore/dataset/core/validator_helpers.py:744] 'Rescale' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Rescale' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(7418:129632434239040,MainProcess):2025-04-07-15:44:11.491.000 [mindspore/dataset/core/validator_helpers.py:744] 'HWC2CHW' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'HWC2CHW' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(7418:129632434239040,MainProcess):2025-04-07-15:44:11.492.000 [mindspore/dataset/core/validator_helpers.py:744] 'TypeCast' from mindspore.dataset.transforms.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'TypeCast' from mindspore.dataset.transforms instead.\n",
      "[WARNING] ME(7418:129632434239040,MainProcess):2025-04-07-15:44:12.430.00 [mindspore/dataset/core/validator_helpers.py:744] 'Resize' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Resize' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(7418:129632434239040,MainProcess):2025-04-07-15:44:12.430.00 [mindspore/dataset/core/validator_helpers.py:744] 'Rescale' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Rescale' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(7418:129632434239040,MainProcess):2025-04-07-15:44:12.430.00 [mindspore/dataset/core/validator_helpers.py:744] 'Rescale' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Rescale' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(7418:129632434239040,MainProcess):2025-04-07-15:44:12.440.00 [mindspore/dataset/core/validator_helpers.py:744] 'HWC2CHW' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'HWC2CHW' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(7418:129632434239040,MainProcess):2025-04-07-15:44:12.440.00 [mindspore/dataset/core/validator_helpers.py:744] 'TypeCast' from mindspore.dataset.transforms.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'TypeCast' from mindspore.dataset.transforms instead.\n",
      "[WARNING] ME(7418:129632434239040,MainProcess):2025-04-07-15:44:12.630.000 [mindspore/dataset/core/validator_helpers.py:744] 'Resize' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Resize' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(7418:129632434239040,MainProcess):2025-04-07-15:44:12.631.000 [mindspore/dataset/core/validator_helpers.py:744] 'Rescale' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Rescale' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(7418:129632434239040,MainProcess):2025-04-07-15:44:12.631.000 [mindspore/dataset/core/validator_helpers.py:744] 'Rescale' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Rescale' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(7418:129632434239040,MainProcess):2025-04-07-15:44:12.632.000 [mindspore/dataset/core/validator_helpers.py:744] 'HWC2CHW' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'HWC2CHW' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(7418:129632434239040,MainProcess):2025-04-07-15:44:12.632.000 [mindspore/dataset/core/validator_helpers.py:744] 'TypeCast' from mindspore.dataset.transforms.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'TypeCast' from mindspore.dataset.transforms instead.\n",
      "[WARNING] ME(7418:129632434239040,MainProcess):2025-04-07-15:44:13.141.000 [mindspore/dataset/core/validator_helpers.py:744] 'Resize' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Resize' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(7418:129632434239040,MainProcess):2025-04-07-15:44:13.142.000 [mindspore/dataset/core/validator_helpers.py:744] 'Rescale' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Rescale' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(7418:129632434239040,MainProcess):2025-04-07-15:44:13.143.000 [mindspore/dataset/core/validator_helpers.py:744] 'Rescale' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Rescale' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(7418:129632434239040,MainProcess):2025-04-07-15:44:13.143.000 [mindspore/dataset/core/validator_helpers.py:744] 'HWC2CHW' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'HWC2CHW' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(7418:129632434239040,MainProcess):2025-04-07-15:44:13.143.000 [mindspore/dataset/core/validator_helpers.py:744] 'TypeCast' from mindspore.dataset.transforms.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'TypeCast' from mindspore.dataset.transforms instead.\n",
      "[WARNING] ME(7418:129632434239040,MainProcess):2025-04-07-15:44:13.658.000 [mindspore/dataset/core/validator_helpers.py:744] 'Resize' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Resize' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(7418:129632434239040,MainProcess):2025-04-07-15:44:13.659.000 [mindspore/dataset/core/validator_helpers.py:744] 'Rescale' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Rescale' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(7418:129632434239040,MainProcess):2025-04-07-15:44:13.659.000 [mindspore/dataset/core/validator_helpers.py:744] 'Rescale' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Rescale' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(7418:129632434239040,MainProcess):2025-04-07-15:44:13.660.000 [mindspore/dataset/core/validator_helpers.py:744] 'HWC2CHW' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'HWC2CHW' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(7418:129632434239040,MainProcess):2025-04-07-15:44:13.660.000 [mindspore/dataset/core/validator_helpers.py:744] 'TypeCast' from mindspore.dataset.transforms.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'TypeCast' from mindspore.dataset.transforms instead.\n",
      "[WARNING] ME(7418:129632434239040,MainProcess):2025-04-07-15:44:14.179.000 [mindspore/dataset/core/validator_helpers.py:744] 'Resize' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Resize' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(7418:129632434239040,MainProcess):2025-04-07-15:44:14.179.000 [mindspore/dataset/core/validator_helpers.py:744] 'Rescale' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Rescale' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(7418:129632434239040,MainProcess):2025-04-07-15:44:14.180.000 [mindspore/dataset/core/validator_helpers.py:744] 'Rescale' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Rescale' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(7418:129632434239040,MainProcess):2025-04-07-15:44:14.180.000 [mindspore/dataset/core/validator_helpers.py:744] 'HWC2CHW' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'HWC2CHW' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(7418:129632434239040,MainProcess):2025-04-07-15:44:14.181.000 [mindspore/dataset/core/validator_helpers.py:744] 'TypeCast' from mindspore.dataset.transforms.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'TypeCast' from mindspore.dataset.transforms instead.\n",
      "[WARNING] ME(7418:129632434239040,MainProcess):2025-04-07-15:44:14.756.000 [mindspore/dataset/core/validator_helpers.py:744] 'Resize' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Resize' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(7418:129632434239040,MainProcess):2025-04-07-15:44:14.757.000 [mindspore/dataset/core/validator_helpers.py:744] 'Rescale' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Rescale' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(7418:129632434239040,MainProcess):2025-04-07-15:44:14.757.000 [mindspore/dataset/core/validator_helpers.py:744] 'Rescale' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Rescale' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(7418:129632434239040,MainProcess):2025-04-07-15:44:14.757.000 [mindspore/dataset/core/validator_helpers.py:744] 'HWC2CHW' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'HWC2CHW' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(7418:129632434239040,MainProcess):2025-04-07-15:44:14.758.000 [mindspore/dataset/core/validator_helpers.py:744] 'TypeCast' from mindspore.dataset.transforms.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'TypeCast' from mindspore.dataset.transforms instead.\n",
      "[WARNING] ME(7418:129632434239040,MainProcess):2025-04-07-15:44:15.326.000 [mindspore/dataset/core/validator_helpers.py:744] 'Resize' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Resize' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(7418:129632434239040,MainProcess):2025-04-07-15:44:15.327.000 [mindspore/dataset/core/validator_helpers.py:744] 'Rescale' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Rescale' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(7418:129632434239040,MainProcess):2025-04-07-15:44:15.327.000 [mindspore/dataset/core/validator_helpers.py:744] 'Rescale' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Rescale' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(7418:129632434239040,MainProcess):2025-04-07-15:44:15.327.000 [mindspore/dataset/core/validator_helpers.py:744] 'HWC2CHW' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'HWC2CHW' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(7418:129632434239040,MainProcess):2025-04-07-15:44:15.328.000 [mindspore/dataset/core/validator_helpers.py:744] 'TypeCast' from mindspore.dataset.transforms.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'TypeCast' from mindspore.dataset.transforms instead.\n",
      "[WARNING] ME(7418:129632434239040,MainProcess):2025-04-07-15:44:15.852.000 [mindspore/dataset/core/validator_helpers.py:744] 'Resize' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Resize' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(7418:129632434239040,MainProcess):2025-04-07-15:44:15.853.000 [mindspore/dataset/core/validator_helpers.py:744] 'Rescale' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Rescale' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(7418:129632434239040,MainProcess):2025-04-07-15:44:15.853.000 [mindspore/dataset/core/validator_helpers.py:744] 'Rescale' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Rescale' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(7418:129632434239040,MainProcess):2025-04-07-15:44:15.853.000 [mindspore/dataset/core/validator_helpers.py:744] 'HWC2CHW' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'HWC2CHW' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(7418:129632434239040,MainProcess):2025-04-07-15:44:15.854.000 [mindspore/dataset/core/validator_helpers.py:744] 'TypeCast' from mindspore.dataset.transforms.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'TypeCast' from mindspore.dataset.transforms instead.\n",
      "[WARNING] ME(7418:129632434239040,MainProcess):2025-04-07-15:44:16.387.000 [mindspore/dataset/core/validator_helpers.py:744] 'Resize' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Resize' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(7418:129632434239040,MainProcess):2025-04-07-15:44:16.388.000 [mindspore/dataset/core/validator_helpers.py:744] 'Rescale' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Rescale' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(7418:129632434239040,MainProcess):2025-04-07-15:44:16.388.000 [mindspore/dataset/core/validator_helpers.py:744] 'Rescale' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Rescale' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(7418:129632434239040,MainProcess):2025-04-07-15:44:16.388.000 [mindspore/dataset/core/validator_helpers.py:744] 'HWC2CHW' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'HWC2CHW' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(7418:129632434239040,MainProcess):2025-04-07-15:44:16.389.000 [mindspore/dataset/core/validator_helpers.py:744] 'TypeCast' from mindspore.dataset.transforms.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'TypeCast' from mindspore.dataset.transforms instead.\n",
      "[WARNING] ME(7418:129632434239040,MainProcess):2025-04-07-15:44:16.927.000 [mindspore/dataset/core/validator_helpers.py:744] 'Resize' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Resize' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(7418:129632434239040,MainProcess):2025-04-07-15:44:16.928.000 [mindspore/dataset/core/validator_helpers.py:744] 'Rescale' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Rescale' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(7418:129632434239040,MainProcess):2025-04-07-15:44:16.928.000 [mindspore/dataset/core/validator_helpers.py:744] 'Rescale' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Rescale' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(7418:129632434239040,MainProcess):2025-04-07-15:44:16.928.000 [mindspore/dataset/core/validator_helpers.py:744] 'HWC2CHW' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'HWC2CHW' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(7418:129632434239040,MainProcess):2025-04-07-15:44:16.928.000 [mindspore/dataset/core/validator_helpers.py:744] 'TypeCast' from mindspore.dataset.transforms.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'TypeCast' from mindspore.dataset.transforms instead.\n",
      "[WARNING] ME(7418:129632434239040,MainProcess):2025-04-07-15:44:17.529.000 [mindspore/dataset/core/validator_helpers.py:744] 'Resize' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Resize' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(7418:129632434239040,MainProcess):2025-04-07-15:44:17.530.000 [mindspore/dataset/core/validator_helpers.py:744] 'Rescale' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Rescale' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(7418:129632434239040,MainProcess):2025-04-07-15:44:17.530.000 [mindspore/dataset/core/validator_helpers.py:744] 'Rescale' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Rescale' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(7418:129632434239040,MainProcess):2025-04-07-15:44:17.530.000 [mindspore/dataset/core/validator_helpers.py:744] 'HWC2CHW' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'HWC2CHW' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(7418:129632434239040,MainProcess):2025-04-07-15:44:17.530.000 [mindspore/dataset/core/validator_helpers.py:744] 'TypeCast' from mindspore.dataset.transforms.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'TypeCast' from mindspore.dataset.transforms instead.\n",
      "[WARNING] ME(7418:129632434239040,MainProcess):2025-04-07-15:44:18.880.00 [mindspore/dataset/core/validator_helpers.py:744] 'Resize' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Resize' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(7418:129632434239040,MainProcess):2025-04-07-15:44:18.890.00 [mindspore/dataset/core/validator_helpers.py:744] 'Rescale' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Rescale' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(7418:129632434239040,MainProcess):2025-04-07-15:44:18.890.00 [mindspore/dataset/core/validator_helpers.py:744] 'Rescale' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Rescale' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(7418:129632434239040,MainProcess):2025-04-07-15:44:18.890.00 [mindspore/dataset/core/validator_helpers.py:744] 'HWC2CHW' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'HWC2CHW' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(7418:129632434239040,MainProcess):2025-04-07-15:44:18.890.00 [mindspore/dataset/core/validator_helpers.py:744] 'TypeCast' from mindspore.dataset.transforms.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'TypeCast' from mindspore.dataset.transforms instead.\n",
      "[WARNING] ME(7418:129632434239040,MainProcess):2025-04-07-15:44:18.619.000 [mindspore/dataset/core/validator_helpers.py:744] 'Resize' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Resize' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(7418:129632434239040,MainProcess):2025-04-07-15:44:18.620.000 [mindspore/dataset/core/validator_helpers.py:744] 'Rescale' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Rescale' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(7418:129632434239040,MainProcess):2025-04-07-15:44:18.620.000 [mindspore/dataset/core/validator_helpers.py:744] 'Rescale' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Rescale' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(7418:129632434239040,MainProcess):2025-04-07-15:44:18.620.000 [mindspore/dataset/core/validator_helpers.py:744] 'HWC2CHW' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'HWC2CHW' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(7418:129632434239040,MainProcess):2025-04-07-15:44:18.620.000 [mindspore/dataset/core/validator_helpers.py:744] 'TypeCast' from mindspore.dataset.transforms.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'TypeCast' from mindspore.dataset.transforms instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "id": "1b171859-4307-4d8f-9151-762b2429c002",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T07:44:22.650544Z",
     "start_time": "2025-04-07T07:44:22.601946Z"
    }
   },
   "source": [
    "def acc_model_info(network, model, mnist_path, model_numbers, epoch_size):\n",
    "    \"\"\"Define the plot info method\"\"\"\n",
    "    step_list = []\n",
    "    acc_list = []\n",
    "    for i in range(1, epoch_size + 1):\n",
    "        # load the saved model for evaluation\n",
    "        #加载同一个模型得到的模型训练步数变化，精度随之变化\n",
    "        # param_dict = load_checkpoint(\"./checkpoints/checkpoint_lenet-10_1875.ckpt\")\n",
    "        #加载不同一个模型得到的模型训练步数变化，精度随之变化\n",
    "        param_dict = load_checkpoint(\"./checkpoints/checkpoint_lenet-{}_1875.ckpt\".format(str(i)))\n",
    "\n",
    "        # load parameter to the network\n",
    "        load_param_into_net(network, param_dict)\n",
    "        # load testing dataset\n",
    "    for i in range(1, model_numbers + 1):\n",
    "        ds_eval = create_dataset(os.path.join(mnist_path, \"test\"))\n",
    "        acc = model.eval(ds_eval, dataset_sink_mode=True)\n",
    "        acc_list.append(acc['Accuracy'])\n",
    "        step_list.append(i * 125)\n",
    "    return step_list, acc_list\n",
    "\n",
    "\n",
    "# Draw line chart according to training steps and model accuracy\n",
    "l1, l2 = acc_model_info(network, model, mnist_path, 15, 10)\n",
    "plt.xlabel(\"Model of Steps\")\n",
    "plt.ylabel(\"Model accuracy\")\n",
    "plt.title(\"Model accuracy variation chart\")\n",
    "plt.plot(l1, l2, 'red')\n",
    "plt.show()"
   ],
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "For 'load_checkpoint', the checkpoint file: /home/grtsinry43/PycharmProjects/JupyterProject/checkpoints/checkpoint_lenet-1_1875.ckpt does not exist, please check whether the 'ckpt_file_name' is correct.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[25], line 24\u001B[0m\n\u001B[1;32m     20\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m step_list, acc_list\n\u001B[1;32m     23\u001B[0m \u001B[38;5;66;03m# Draw line chart according to training steps and model accuracy\u001B[39;00m\n\u001B[0;32m---> 24\u001B[0m l1, l2 \u001B[38;5;241m=\u001B[39m \u001B[43macc_model_info\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnetwork\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmnist_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m15\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m10\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     25\u001B[0m plt\u001B[38;5;241m.\u001B[39mxlabel(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mModel of Steps\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     26\u001B[0m plt\u001B[38;5;241m.\u001B[39mylabel(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mModel accuracy\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "Cell \u001B[0;32mIn[25], line 10\u001B[0m, in \u001B[0;36macc_model_info\u001B[0;34m(network, model, mnist_path, model_numbers, epoch_size)\u001B[0m\n\u001B[1;32m      4\u001B[0m acc_list \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m1\u001B[39m, epoch_size \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m):\n\u001B[1;32m      6\u001B[0m     \u001B[38;5;66;03m# load the saved model for evaluation\u001B[39;00m\n\u001B[1;32m      7\u001B[0m     \u001B[38;5;66;03m#加载同一个模型得到的模型训练步数变化，精度随之变化\u001B[39;00m\n\u001B[1;32m      8\u001B[0m     \u001B[38;5;66;03m# param_dict = load_checkpoint(\"./checkpoints/checkpoint_lenet-10_1875.ckpt\")\u001B[39;00m\n\u001B[1;32m      9\u001B[0m     \u001B[38;5;66;03m#加载不同一个模型得到的模型训练步数变化，精度随之变化\u001B[39;00m\n\u001B[0;32m---> 10\u001B[0m     param_dict \u001B[38;5;241m=\u001B[39m \u001B[43mload_checkpoint\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m./checkpoints/checkpoint_lenet-\u001B[39;49m\u001B[38;5;132;43;01m{}\u001B[39;49;00m\u001B[38;5;124;43m_1875.ckpt\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mformat\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mstr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mi\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     12\u001B[0m     \u001B[38;5;66;03m# load parameter to the network\u001B[39;00m\n\u001B[1;32m     13\u001B[0m     load_param_into_net(network, param_dict)\n",
      "File \u001B[0;32m~/.conda/envs/homework/lib/python3.10/site-packages/mindspore/train/serialization.py:1523\u001B[0m, in \u001B[0;36mload_checkpoint\u001B[0;34m(ckpt_file_name, net, strict_load, filter_prefix, dec_key, dec_mode, specify_prefix, choice_func, crc_check, remove_redundancy, format)\u001B[0m\n\u001B[1;32m   1521\u001B[0m             parameter_dict[key] \u001B[38;5;241m=\u001B[39m Parameter(Tensor(value[\u001B[38;5;241m0\u001B[39m]), name\u001B[38;5;241m=\u001B[39mkey)\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1523\u001B[0m     \u001B[43m_load_into_param_dict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mckpt_file_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparameter_dict\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mspecify_prefix\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfilter_prefix\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mchoice_func\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdec_key\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1524\u001B[0m \u001B[43m                          \u001B[49m\u001B[43mdec_mode\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcrc_check\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mformat\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1526\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m parameter_dict:\n\u001B[1;32m   1527\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe loaded parameter dict is empty after filter or specify, please check whether \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1528\u001B[0m                      \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfilter_prefix\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m or \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mspecify_prefix\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m are set correctly.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/.conda/envs/homework/lib/python3.10/site-packages/mindspore/train/serialization.py:1322\u001B[0m, in \u001B[0;36m_load_into_param_dict\u001B[0;34m(ckpt_file_name, parameter_dict, specify_prefix, filter_prefix, choice_func, dec_key, dec_mode, crc_check, format)\u001B[0m\n\u001B[1;32m   1319\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_load_into_param_dict\u001B[39m(ckpt_file_name, parameter_dict, specify_prefix, filter_prefix, choice_func, dec_key,\n\u001B[1;32m   1320\u001B[0m                           dec_mode, crc_check, \u001B[38;5;28mformat\u001B[39m):\n\u001B[1;32m   1321\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"load parameter into parameter_dict\"\"\"\u001B[39;00m\n\u001B[0;32m-> 1322\u001B[0m     ckpt_file_name \u001B[38;5;241m=\u001B[39m \u001B[43m_check_ckpt_file_name\u001B[49m\u001B[43m(\u001B[49m\u001B[43mckpt_file_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mformat\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1323\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mformat\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msafetensors\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m   1324\u001B[0m         \u001B[38;5;28;01mwith\u001B[39;00m safe_open(ckpt_file_name, framework\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mnp\u001B[39m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m f:\n",
      "File \u001B[0;32m~/.conda/envs/homework/lib/python3.10/site-packages/mindspore/train/serialization.py:1667\u001B[0m, in \u001B[0;36m_check_ckpt_file_name\u001B[0;34m(ckpt_file_name, format)\u001B[0m\n\u001B[1;32m   1665\u001B[0m ckpt_file_name \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mrealpath(ckpt_file_name)\n\u001B[1;32m   1666\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mexists(ckpt_file_name):\n\u001B[0;32m-> 1667\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFor \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mload_checkpoint\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, the checkpoint file: \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m does not exist, please check \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1668\u001B[0m                      \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mwhether the \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mckpt_file_name\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m is correct.\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(ckpt_file_name))\n\u001B[1;32m   1670\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m ckpt_file_name\n",
      "\u001B[0;31mValueError\u001B[0m: For 'load_checkpoint', the checkpoint file: /home/grtsinry43/PycharmProjects/JupyterProject/checkpoints/checkpoint_lenet-1_1875.ckpt does not exist, please check whether the 'ckpt_file_name' is correct."
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "id": "d849a4e84cf68362",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T07:44:37.728803Z",
     "start_time": "2025-04-07T07:44:36.907403Z"
    }
   },
   "source": [
    "ds_test = create_dataset(test_data_path).create_dict_iterator()\n",
    "data = ds_test.__next__()\n",
    "images = data[\"image\"].asnumpy()\n",
    "labels = data[\n",
    "    \"label\"].asnumpy()  # The subscript of data picture is the standard for us to judge whether it is correct or not\n",
    "\n",
    "output = model.predict(Tensor(data['image']))\n",
    "# The predict function returns the probability of 0-9 numbers corresponding to each picture\n",
    "prb = output.asnumpy()\n",
    "pred = np.argmax(output.asnumpy(), axis=1)\n",
    "err_num = []\n",
    "index = 1\n",
    "for i in range(len(labels)):\n",
    "    plt.subplot(4, 8, i + 1)\n",
    "    color = 'blue' if pred[i] == labels[i] else 'red'\n",
    "    plt.title(\"pre:{}\".format(pred[i]), color=color)\n",
    "    plt.imshow(np.squeeze(images[i]))\n",
    "    plt.axis(\"off\")\n",
    "    if color == 'red':\n",
    "        index = 0\n",
    "        # Print out the wrong data identified by the current group\n",
    "        print(\"Row {}, column {} is incorrectly identified as {}, the correct value should be {}\".format(int(i / 8) + 1,\n",
    "                                                                                                         i % 8 + 1,\n",
    "                                                                                                         pred[i],\n",
    "                                                                                                         labels[i]),\n",
    "              '\\n')\n",
    "if index:\n",
    "    print(\"All the figures in this group are predicted correctly！\")\n",
    "print(pred, \"<--Predicted figures\")  # Print the numbers recognized by each group of pictures\n",
    "print(labels, \"<--The right number\")  # Print the subscript corresponding to each group of pictures\n",
    "plt.show()"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] ME(7418:129632434239040,MainProcess):2025-04-07-15:44:36.910.000 [mindspore/dataset/core/validator_helpers.py:744] 'Resize' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Resize' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(7418:129632434239040,MainProcess):2025-04-07-15:44:36.911.000 [mindspore/dataset/core/validator_helpers.py:744] 'Rescale' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Rescale' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(7418:129632434239040,MainProcess):2025-04-07-15:44:36.911.000 [mindspore/dataset/core/validator_helpers.py:744] 'Rescale' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Rescale' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(7418:129632434239040,MainProcess):2025-04-07-15:44:36.911.000 [mindspore/dataset/core/validator_helpers.py:744] 'HWC2CHW' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'HWC2CHW' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(7418:129632434239040,MainProcess):2025-04-07-15:44:36.911.000 [mindspore/dataset/core/validator_helpers.py:744] 'TypeCast' from mindspore.dataset.transforms.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'TypeCast' from mindspore.dataset.transforms instead.\n",
      "[ERROR] CORE(7418,75e66731b640,python):2025-04-07-15:44:37.105.468 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_7418/2100403872.py]\n",
      "[WARNING] CORE(7418,75e66731b640,python):2025-04-07-15:44:37.105.482 [mindspore/core/utils/info.cc:121] ToString] The file '/tmp/ipykernel_7418/2100403872.py' may not exists.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All the figures in this group are predicted correctly！\n",
      "[9 6 2 7 8 4 4 0 7 1 9 8 0 8 3 9 7 0 6 1 1 0 9 8 6 7 6 3 2 0 0 5] <--Predicted figures\n",
      "[9 6 2 7 8 4 4 0 7 1 9 8 0 8 3 9 7 0 6 1 1 0 9 8 6 7 6 3 2 0 0 5] <--The right number\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 32 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "id": "db6fb49e2fb93fbd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T07:44:44.175094Z",
     "start_time": "2025-04-07T07:44:44.073703Z"
    }
   },
   "source": [
    "# define the pie drawing function of probability analysis\n",
    "def plot_pie(prbs):\n",
    "    dict1 = {}\n",
    "    # Remove the negative number and build the dictionary dict1. The key is the number and the value is the probability value\n",
    "    for i in range(10):\n",
    "        if prbs[i] > 0:\n",
    "            dict1[str(i)] = prbs[i]\n",
    "\n",
    "    label_list = dict1.keys()  # Label of each part\n",
    "    size = dict1.values()  # Size of each part\n",
    "    colors = [\"red\", \"green\", \"pink\", \"blue\", \"purple\", \"orange\", \"gray\"]  # Building a round cake pigment Library\n",
    "    color = colors[: len(size)]  # Color of each part\n",
    "    plt.pie(size, colors=color, labels=label_list, labeldistance=1.1, autopct=\"%1.1f%%\", shadow=False, startangle=90,\n",
    "            pctdistance=0.6)\n",
    "    plt.axis(\"equal\")  # Set the scale size of x-axis and y-axis to be equal\n",
    "    plt.legend()\n",
    "    plt.title(\"Image classification\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "for i in range(2):\n",
    "    print(\"Figure {} probability of corresponding numbers [0-9]:\\n\".format(i + 1), prb[i])\n",
    "    plot_pie(prb[i])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Figure 1 probability of corresponding numbers [0-9]:\n",
      " [ -6.763879  -10.332399   -7.613531   -2.8446858  12.078036   -2.3978648\n",
      " -19.046787    4.1482215   1.3607234  30.924307 ]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Figure 2 probability of corresponding numbers [0-9]:\n",
      " [  4.257677   -1.9731985   1.6863294 -10.233752    2.5221639   3.7849915\n",
      "  26.514875  -14.962747   -2.6158319  -7.3505726]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c136c66d393efb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d7e8c29c4bf42f49",
   "metadata": {},
   "source": [
    "ii. 将上述模型稍加修改，迁移到美国邮政编码手写数字集的数字识别上，检测识别率并进行改进。\n",
    "iii. 试试用前期遗传算法生成的手写数字图像，看看效果如何。"
   ]
  },
  {
   "cell_type": "code",
   "id": "95fcd5a6445b7370",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T07:48:22.478818Z",
     "start_time": "2025-04-07T07:48:22.472828Z"
    }
   },
   "source": [
    "import h5py\n",
    "import os\n",
    "import mindspore.dataset as ds\n",
    "import numpy as np\n",
    "import mindspore.dataset.vision.c_transforms as CV\n",
    "from mindspore.dataset.vision import Inter\n",
    "import mindspore.dataset.transforms.c_transforms as C\n",
    "from mindspore.common import dtype as mstype\n",
    "from mindspore import Tensor\n",
    "\n",
    "class USPSDataset:\n",
    "    def __init__(self, data_path, is_train=True):\n",
    "        with h5py.File(data_path, 'r') as hf:\n",
    "            if is_train:\n",
    "                train = hf.get('train')\n",
    "                self.X = train.get('data')[:]\n",
    "                self.y = train.get('target')[:]\n",
    "            else:\n",
    "                test = hf.get('test')\n",
    "                self.X = test.get('data')[:]\n",
    "                self.y = test.get('target')[:]\n",
    "\n",
    "        # USPS original size is 16x16, needs to be converted to the appropriate format\n",
    "        self.X = self.X.reshape((-1, 16, 16, 1)).astype(np.float32)\n",
    "        self.y = self.y.astype(np.int32)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return np.asarray(self.X[idx]), np.asarray(self.y[idx])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "\n",
    "import mindspore.dataset as ds\n",
    "import mindspore.dataset.vision.c_transforms as CV\n",
    "from mindspore.dataset.vision import Inter\n",
    "import mindspore.dataset.transforms.c_transforms as C\n",
    "from mindspore.common import dtype as mstype\n",
    "\n",
    "def create_usps_dataset(data_path, batch_size=32, repeat_size=1, num_parallel_workers=1, is_train=True):\n",
    "    # 创建USPS数据集\n",
    "    usps_dataset = USPSDataset(data_path, is_train)\n",
    "    dataset_generator = ds.GeneratorDataset(\n",
    "        usps_dataset, [\"image\", \"label\"], shuffle=is_train)\n",
    "\n",
    "    # 数据预处理操作\n",
    "    resize_height, resize_width = 32, 32\n",
    "\n",
    "    # 定义与MNIST相同的操作\n",
    "    resize_op = CV.Resize((resize_height, resize_width), interpolation=Inter.LINEAR)\n",
    "    rescale_op = CV.Rescale(1.0 / 255.0, 0.0)\n",
    "    rescale_nml_op = CV.Rescale(1 / 0.3081, -1 * 0.1307 / 0.3081)\n",
    "    hwc2chw_op = CV.HWC2CHW()\n",
    "    type_cast_op = C.TypeCast(mstype.int32)\n",
    "\n",
    "    # 应用操作\n",
    "    dataset_generator = dataset_generator.map(operations=type_cast_op, input_columns=[\"label\"])\n",
    "    dataset_generator = dataset_generator.map(operations=resize_op, input_columns=[\"image\"])\n",
    "    dataset_generator = dataset_generator.map(operations=rescale_op, input_columns=[\"image\"])\n",
    "    dataset_generator = dataset_generator.map(operations=rescale_nml_op, input_columns=[\"image\"])\n",
    "    dataset_generator = dataset_generator.map(operations=hwc2chw_op, input_columns=[\"image\"])\n",
    "\n",
    "    # 批处理\n",
    "    buffer_size = 10000\n",
    "    if is_train:\n",
    "        dataset_generator = dataset_generator.shuffle(buffer_size=buffer_size)\n",
    "    dataset_generator = dataset_generator.batch(batch_size, drop_remainder=True)\n",
    "    dataset_generator = dataset_generator.repeat(repeat_size)\n",
    "\n",
    "    return dataset_generator"
   ],
   "outputs": [],
   "execution_count": 38
  },
  {
   "cell_type": "code",
   "id": "d4a4b29a0ef089e0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T07:48:25.159993Z",
     "start_time": "2025-04-07T07:48:25.155347Z"
    }
   },
   "source": [
    "from mindspore.train.serialization import load_checkpoint, load_param_into_net\n",
    "from mindspore.train.callback import ModelCheckpoint, CheckpointConfig, LossMonitor, Callback\n",
    "from mindspore.train import Model\n",
    "from mindspore.nn.metrics import Accuracy\n",
    "from mindspore.nn.loss import SoftmaxCrossEntropyWithLogits\n",
    "import mindspore.nn as nn\n",
    "\n",
    "\n",
    "def transfer_learning_usps(network, pretrained_ckpt_path, usps_data_path, epoch_size=5):\n",
    "    # 加载预训练的LeNet5网络参数\n",
    "    param_dict = load_checkpoint(pretrained_ckpt_path)\n",
    "    load_param_into_net(network, param_dict)\n",
    "\n",
    "    # 微调设置 - 可以选择冻结部分层\n",
    "    # 仅训练全连接层\n",
    "    for param in network.trainable_params():\n",
    "        if \"fc3\" not in param.name:\n",
    "            param.requires_grad = False\n",
    "\n",
    "    # 定义优化器（使用较小的学习率进行微调）\n",
    "    lr = 0.001\n",
    "    net_opt = nn.Momentum(network.trainable_params(), lr, momentum=0.9)\n",
    "\n",
    "    # 定义损失函数和模型\n",
    "    net_loss = SoftmaxCrossEntropyWithLogits(sparse=True, reduction='mean')\n",
    "    model = Model(network, net_loss, net_opt, metrics={\"Accuracy\": Accuracy()})\n",
    "\n",
    "    # 定义回调函数\n",
    "    step_loss = {\"step\": [], \"loss_value\": []}\n",
    "\n",
    "    class Step_loss_info(Callback):\n",
    "        def step_end(self, run_context):\n",
    "            cb_params = run_context.original_args()\n",
    "            step_loss[\"loss_value\"].append(str(cb_params.net_outputs))\n",
    "            step_loss[\"step\"].append(str(cb_params.cur_step_num))\n",
    "\n",
    "    config_ck = CheckpointConfig(save_checkpoint_steps=50, keep_checkpoint_max=10)\n",
    "    ckpoint_cb = ModelCheckpoint(prefix=\"usps_lenet\", config=config_ck, directory=\"./usps_checkpoints\")\n",
    "\n",
    "    # 加载USPS数据集\n",
    "    ds_train = create_usps_dataset(usps_data_path, batch_size=32, is_train=True)\n",
    "\n",
    "    # 训练模型\n",
    "    print(\"============== Starting USPS Transfer Learning ==============\")\n",
    "    model.train(epoch_size, ds_train, callbacks=[ckpoint_cb, LossMonitor(), Step_loss_info()], dataset_sink_mode=True)\n",
    "\n",
    "    # 评估USPS测试集\n",
    "    ds_eval = create_usps_dataset(usps_data_path, batch_size=32, is_train=False)\n",
    "    acc = model.eval(ds_eval, dataset_sink_mode=True)\n",
    "    print(\"============== USPS Accuracy: {} ==============\".format(acc))\n",
    "\n",
    "    return model"
   ],
   "outputs": [],
   "execution_count": 39
  },
  {
   "cell_type": "code",
   "id": "6ef549f4cc03d49b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T07:48:27.209441Z",
     "start_time": "2025-04-07T07:48:27.204666Z"
    }
   },
   "source": [
    "class ImprovedLeNet5(nn.Cell):\n",
    "    \"\"\"添加批归一化和dropout的改进LeNet模型\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(ImprovedLeNet5, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=5, pad_mode=\"valid\")\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=5, pad_mode=\"valid\")\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.fc1 = nn.Dense(64 * 5 * 5, 512)\n",
    "        self.bn3 = nn.BatchNorm1d(512)\n",
    "        self.fc2 = nn.Dense(512, 128)\n",
    "        self.bn4 = nn.BatchNorm1d(128)\n",
    "        self.fc3 = nn.Dense(128, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.max_pool2d = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def construct(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.max_pool2d(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.max_pool2d(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.bn4(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ],
   "outputs": [],
   "execution_count": 40
  },
  {
   "cell_type": "code",
   "id": "8e61ab8459ea7638",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T07:48:29.219686Z",
     "start_time": "2025-04-07T07:48:29.211008Z"
    }
   },
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def visualize_results(model, usps_data_path):\n",
    "    # 加载测试数据\n",
    "    ds_test = create_usps_dataset(usps_data_path, batch_size=32, is_train=False)\n",
    "    data = next(ds_test.create_dict_iterator())\n",
    "\n",
    "    # 预测\n",
    "    images = data[\"image\"].asnumpy()\n",
    "    labels = data[\"label\"].asnumpy()\n",
    "    output = model.predict(Tensor(data['image']))\n",
    "    pred = np.argmax(output.asnumpy(), axis=1)\n",
    "\n",
    "    # 可视化\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    for i in range(min(32, len(labels))):\n",
    "        plt.subplot(4, 8, i + 1)\n",
    "        color = 'blue' if pred[i] == labels[i] else 'red'\n",
    "        plt.title(\"pre:{}\".format(pred[i]), color=color)\n",
    "        plt.imshow(np.squeeze(images[i]))\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # 计算准确率\n",
    "    correct = (pred == labels).sum()\n",
    "    total = len(labels)\n",
    "    print(f\"Accuracy: {correct / total * 100:.2f}%\")\n",
    "\n",
    "\n",
    "def test_genetic_algo_images(model, image_folder):\n",
    "    \"\"\"测试遗传算法生成的图像\"\"\"\n",
    "    from PIL import Image\n",
    "    import os\n",
    "\n",
    "    # 加载图像\n",
    "    images = []\n",
    "    labels = []\n",
    "\n",
    "    for filename in os.listdir(image_folder):\n",
    "        if filename.endswith(\".png\") or filename.endswith(\".jpg\"):\n",
    "            # 从文件名中提取标签（假设格式为\"digit_5.png\"）\n",
    "            try:\n",
    "                label = int(filename.split(\"_\")[1].split(\".\")[0])\n",
    "                # 加载并预处理图像\n",
    "                img_path = os.path.join(image_folder, filename)\n",
    "                img = Image.open(img_path).convert('L')  # 转为灰度图\n",
    "                img = img.resize((32, 32))  # 调整大小匹配模型输入\n",
    "                img_array = np.array(img).reshape(1, 32, 32, 1).astype(np.float32)\n",
    "\n",
    "                # 应用与训练数据相同的预处理\n",
    "                img_array = img_array / 255.0\n",
    "                img_array = (img_array - 0.1307) / 0.3081\n",
    "                img_array = np.transpose(img_array, (0, 3, 1, 2))  # NHWC到NCHW\n",
    "\n",
    "                images.append(img_array)\n",
    "                labels.append(label)\n",
    "            except:\n",
    "                print(f\"跳过文件 {filename} - 格式不正确\")\n",
    "\n",
    "    # 合并所有图像\n",
    "    if images:\n",
    "        all_images = np.vstack(images)\n",
    "        all_labels = np.array(labels)\n",
    "\n",
    "        # 预测\n",
    "        input_tensor = Tensor(all_images)\n",
    "        output = model.predict(input_tensor)\n",
    "        pred = np.argmax(output.asnumpy(), axis=1)\n",
    "\n",
    "        # 显示结果\n",
    "        fig, axes = plt.subplots(len(images), 2, figsize=(8, 4 * len(images)))\n",
    "\n",
    "        for i in range(len(images)):\n",
    "            if len(images) == 1:  # 处理只有一张图片的特殊情况\n",
    "                ax1, ax2 = axes\n",
    "            else:\n",
    "                ax1, ax2 = axes[i]\n",
    "\n",
    "            ax1.imshow(np.squeeze(images[i]), cmap='gray')\n",
    "            ax1.set_title(f\"真实标签: {labels[i]}\")\n",
    "            ax1.axis('off')\n",
    "\n",
    "            # 绘制预测概率\n",
    "            probs = output.asnumpy()[i]\n",
    "            ax2.bar(range(10), probs)\n",
    "            ax2.set_title(f\"预测: {pred[i]} (概率: {probs[pred[i]]:.2f})\")\n",
    "            ax2.set_xticks(range(10))\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # 计算准确率\n",
    "        correct = (pred == all_labels).sum()\n",
    "        total = len(all_labels)\n",
    "        print(f\"遗传算法图像准确率: {correct / total * 100:.2f}%\")\n",
    "    else:\n",
    "        print(\"在指定文件夹中没有找到有效图像\")"
   ],
   "outputs": [],
   "execution_count": 41
  },
  {
   "cell_type": "code",
   "id": "49bb0c83e660628c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T07:48:37.180066Z",
     "start_time": "2025-04-07T07:48:32.296160Z"
    }
   },
   "source": [
    "# 主函数 - 运行整个迁移学习过程\n",
    "def main():\n",
    "    # 初始化LeNet5网络\n",
    "    network = LeNet5()\n",
    "\n",
    "    # 设定路径\n",
    "    pretrained_model_path = \"./checkpoints/checkpoint_lenet-10_1875.ckpt\"  # MNIST预训练模型\n",
    "    usps_data_path = \"./data/usps/usps.h5\"  # USPS数据集路径\n",
    "    genetic_images_path = \"./genetic_algorithm_images/\"  # 遗传算法生成图像的文件夹\n",
    "\n",
    "    # 执行迁移学习\n",
    "    model = transfer_learning_usps(network, pretrained_model_path, usps_data_path, epoch_size=5)\n",
    "\n",
    "    # 可视化USPS数据集上的结果\n",
    "    visualize_results(model, usps_data_path)\n",
    "\n",
    "    # 使用改进的模型（可选）\n",
    "    improved_network = ImprovedLeNet5()\n",
    "    improved_model = transfer_learning_usps(improved_network, pretrained_model_path, usps_data_path, epoch_size=5)\n",
    "\n",
    "    # 测试遗传算法生成的图像\n",
    "    test_genetic_algo_images(model, genetic_images_path)\n",
    "\n",
    "    # 比较原始模型和改进模型的性能（可选）\n",
    "    ds_eval = create_usps_dataset(usps_data_path, batch_size=32, is_train=False)\n",
    "    original_acc = model.eval(ds_eval, dataset_sink_mode=True)\n",
    "    improved_acc = improved_model.eval(ds_eval, dataset_sink_mode=True)\n",
    "\n",
    "    print(f\"原始LeNet5在USPS上的准确率: {original_acc['Accuracy'] * 100:.2f}%\")\n",
    "    print(f\"改进LeNet5在USPS上的准确率: {improved_acc['Accuracy'] * 100:.2f}%\")\n",
    "\n",
    "\n",
    "# 执行主函数\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] ME(7418:129632434239040,MainProcess):2025-04-07-15:48:32.336.000 [mindspore/dataset/core/validator_helpers.py:744] 'Resize' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Resize' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(7418:129632434239040,MainProcess):2025-04-07-15:48:32.337.000 [mindspore/dataset/core/validator_helpers.py:744] 'Rescale' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Rescale' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(7418:129632434239040,MainProcess):2025-04-07-15:48:32.337.000 [mindspore/dataset/core/validator_helpers.py:744] 'Rescale' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Rescale' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(7418:129632434239040,MainProcess):2025-04-07-15:48:32.337.000 [mindspore/dataset/core/validator_helpers.py:744] 'HWC2CHW' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'HWC2CHW' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(7418:129632434239040,MainProcess):2025-04-07-15:48:32.337.000 [mindspore/dataset/core/validator_helpers.py:744] 'TypeCast' from mindspore.dataset.transforms.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'TypeCast' from mindspore.dataset.transforms instead.\n",
      "[WARNING] ME(7418:129632434239040,MainProcess):2025-04-07-15:48:32.339.000 [mindspore/train/model.py:1419] For Step_loss_info callback, {'step_end'} methods may not be supported in later version, Use methods prefixed with 'on_train' or 'on_eval' instead when using customized callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== Starting USPS Transfer Learning ==============\n",
      "epoch: 1 step: 1, loss is 2.3268587589263916\n",
      "epoch: 1 step: 2, loss is 2.340838670730591\n",
      "epoch: 1 step: 3, loss is 2.2978248596191406\n",
      "epoch: 1 step: 4, loss is 2.2920548915863037\n",
      "epoch: 1 step: 5, loss is 2.298778533935547\n",
      "epoch: 1 step: 6, loss is 2.3431034088134766\n",
      "epoch: 1 step: 7, loss is 2.3236377239227295\n",
      "epoch: 1 step: 8, loss is 2.3582680225372314\n",
      "epoch: 1 step: 9, loss is 2.310331106185913\n",
      "epoch: 1 step: 10, loss is 2.295938491821289\n",
      "epoch: 1 step: 11, loss is 2.273242712020874\n",
      "epoch: 1 step: 12, loss is 2.3156514167785645\n",
      "epoch: 1 step: 13, loss is 2.296466588973999\n",
      "epoch: 1 step: 14, loss is 2.326814889907837\n",
      "epoch: 1 step: 15, loss is 2.319678783416748\n",
      "epoch: 1 step: 16, loss is 2.3130016326904297\n",
      "epoch: 1 step: 17, loss is 2.28810715675354\n",
      "epoch: 1 step: 18, loss is 2.3414955139160156\n",
      "epoch: 1 step: 19, loss is 2.3910508155822754\n",
      "epoch: 1 step: 20, loss is 2.297266960144043\n",
      "epoch: 1 step: 21, loss is 2.2890710830688477\n",
      "epoch: 1 step: 22, loss is 2.4421212673187256\n",
      "epoch: 1 step: 23, loss is 2.382871150970459\n",
      "epoch: 1 step: 24, loss is 2.2751922607421875\n",
      "epoch: 1 step: 25, loss is 2.2903189659118652\n",
      "epoch: 1 step: 26, loss is 2.3529138565063477\n",
      "epoch: 1 step: 27, loss is 2.2278616428375244\n",
      "epoch: 1 step: 28, loss is 2.29303240776062\n",
      "epoch: 1 step: 29, loss is 2.2989771366119385\n",
      "epoch: 1 step: 30, loss is 2.3221657276153564\n",
      "epoch: 1 step: 31, loss is 2.340425729751587\n",
      "epoch: 1 step: 32, loss is 2.265432834625244\n",
      "epoch: 1 step: 33, loss is 2.3262319564819336\n",
      "epoch: 1 step: 34, loss is 2.3366715908050537\n",
      "epoch: 1 step: 35, loss is 2.339308261871338\n",
      "epoch: 1 step: 36, loss is 2.324394941329956\n",
      "epoch: 1 step: 37, loss is 2.3413150310516357\n",
      "epoch: 1 step: 38, loss is 2.327194929122925\n",
      "epoch: 1 step: 39, loss is 2.3532745838165283\n",
      "epoch: 1 step: 40, loss is 2.3298380374908447\n",
      "epoch: 1 step: 41, loss is 2.2924964427948\n",
      "epoch: 1 step: 42, loss is 2.3496785163879395\n",
      "epoch: 1 step: 43, loss is 2.3165082931518555\n",
      "epoch: 1 step: 44, loss is 2.3136935234069824\n",
      "epoch: 1 step: 45, loss is 2.3290021419525146\n",
      "epoch: 1 step: 46, loss is 2.275383710861206\n",
      "epoch: 1 step: 47, loss is 2.3307652473449707\n",
      "epoch: 1 step: 48, loss is 2.3205597400665283\n",
      "epoch: 1 step: 49, loss is 2.3345439434051514\n",
      "epoch: 1 step: 50, loss is 2.3616983890533447\n",
      "epoch: 1 step: 51, loss is 2.3091161251068115\n",
      "epoch: 1 step: 52, loss is 2.3577992916107178\n",
      "epoch: 1 step: 53, loss is 2.299945592880249\n",
      "epoch: 1 step: 54, loss is 2.3121187686920166\n",
      "epoch: 1 step: 55, loss is 2.2699573040008545\n",
      "epoch: 1 step: 56, loss is 2.356421947479248\n",
      "epoch: 1 step: 57, loss is 2.266927480697632\n",
      "epoch: 1 step: 58, loss is 2.396909236907959\n",
      "epoch: 1 step: 59, loss is 2.3474555015563965\n",
      "epoch: 1 step: 60, loss is 2.4037251472473145\n",
      "epoch: 1 step: 61, loss is 2.2989516258239746\n",
      "epoch: 1 step: 62, loss is 2.3288443088531494\n",
      "epoch: 1 step: 63, loss is 2.34916615486145\n",
      "epoch: 1 step: 64, loss is 2.367709159851074\n",
      "epoch: 1 step: 65, loss is 2.3688557147979736\n",
      "epoch: 1 step: 66, loss is 2.357285261154175\n",
      "epoch: 1 step: 67, loss is 2.3011069297790527\n",
      "epoch: 1 step: 68, loss is 2.3551156520843506\n",
      "epoch: 1 step: 69, loss is 2.4083266258239746\n",
      "epoch: 1 step: 70, loss is 2.3253777027130127\n",
      "epoch: 1 step: 71, loss is 2.316481590270996\n",
      "epoch: 1 step: 72, loss is 2.3185698986053467\n",
      "epoch: 1 step: 73, loss is 2.3465847969055176\n",
      "epoch: 1 step: 74, loss is 2.32790470123291\n",
      "epoch: 1 step: 75, loss is 2.308349609375\n",
      "epoch: 1 step: 76, loss is 2.3141655921936035\n",
      "epoch: 1 step: 77, loss is 2.3645403385162354\n",
      "epoch: 1 step: 78, loss is 2.335146903991699\n",
      "epoch: 1 step: 79, loss is 2.328542709350586\n",
      "epoch: 1 step: 80, loss is 2.239745616912842\n",
      "epoch: 1 step: 81, loss is 2.2732858657836914\n",
      "epoch: 1 step: 82, loss is 2.3513436317443848\n",
      "epoch: 1 step: 83, loss is 2.262225389480591\n",
      "epoch: 1 step: 84, loss is 2.2863316535949707\n",
      "epoch: 1 step: 85, loss is 2.2810380458831787\n",
      "epoch: 1 step: 86, loss is 2.2859227657318115\n",
      "epoch: 1 step: 87, loss is 2.300678014755249\n",
      "epoch: 1 step: 88, loss is 2.3402154445648193\n",
      "epoch: 1 step: 89, loss is 2.3047728538513184\n",
      "epoch: 1 step: 90, loss is 2.346799850463867\n",
      "epoch: 1 step: 91, loss is 2.279904842376709\n",
      "epoch: 1 step: 92, loss is 2.3415558338165283\n",
      "epoch: 1 step: 93, loss is 2.3228237628936768\n",
      "epoch: 1 step: 94, loss is 2.3328559398651123\n",
      "epoch: 1 step: 95, loss is 2.3242781162261963\n",
      "epoch: 1 step: 96, loss is 2.3393468856811523\n",
      "epoch: 1 step: 97, loss is 2.354233980178833\n",
      "epoch: 1 step: 98, loss is 2.3051645755767822\n",
      "epoch: 1 step: 99, loss is 2.27443265914917\n",
      "epoch: 1 step: 100, loss is 2.318631649017334\n",
      "epoch: 1 step: 101, loss is 2.3652122020721436\n",
      "epoch: 1 step: 102, loss is 2.273618221282959\n",
      "epoch: 1 step: 103, loss is 2.332176685333252\n",
      "epoch: 1 step: 104, loss is 2.3192741870880127\n",
      "epoch: 1 step: 105, loss is 2.3532936573028564\n",
      "epoch: 1 step: 106, loss is 2.3013906478881836\n",
      "epoch: 1 step: 107, loss is 2.3603038787841797\n",
      "epoch: 1 step: 108, loss is 2.2953197956085205\n",
      "epoch: 1 step: 109, loss is 2.2915873527526855\n",
      "epoch: 1 step: 110, loss is 2.3163375854492188\n",
      "epoch: 1 step: 111, loss is 2.356079578399658\n",
      "epoch: 1 step: 112, loss is 2.341104030609131\n",
      "epoch: 1 step: 113, loss is 2.2833619117736816\n",
      "epoch: 1 step: 114, loss is 2.3417913913726807\n",
      "epoch: 1 step: 115, loss is 2.345249652862549\n",
      "epoch: 1 step: 116, loss is 2.33569598197937\n",
      "epoch: 1 step: 117, loss is 2.351425886154175\n",
      "epoch: 1 step: 118, loss is 2.280510187149048\n",
      "epoch: 1 step: 119, loss is 2.269136667251587\n",
      "epoch: 1 step: 120, loss is 2.3142552375793457\n",
      "epoch: 1 step: 121, loss is 2.3008086681365967\n",
      "epoch: 1 step: 122, loss is 2.312523126602173\n",
      "epoch: 1 step: 123, loss is 2.2748799324035645\n",
      "epoch: 1 step: 124, loss is 2.321354389190674\n",
      "epoch: 1 step: 125, loss is 2.377032995223999\n",
      "epoch: 1 step: 126, loss is 2.350550889968872\n",
      "epoch: 1 step: 127, loss is 2.368776321411133\n",
      "epoch: 1 step: 128, loss is 2.3162050247192383\n",
      "epoch: 1 step: 129, loss is 2.353797197341919\n",
      "epoch: 1 step: 130, loss is 2.2939257621765137\n",
      "epoch: 1 step: 131, loss is 2.303769588470459\n",
      "epoch: 1 step: 132, loss is 2.3218820095062256\n",
      "epoch: 1 step: 133, loss is 2.3463032245635986\n",
      "epoch: 1 step: 134, loss is 2.3170831203460693\n",
      "epoch: 1 step: 135, loss is 2.306757926940918\n",
      "epoch: 1 step: 136, loss is 2.358011484146118\n",
      "epoch: 1 step: 137, loss is 2.3394978046417236\n",
      "epoch: 1 step: 138, loss is 2.303797960281372\n",
      "epoch: 1 step: 139, loss is 2.361241579055786\n",
      "epoch: 1 step: 140, loss is 2.3310177326202393\n",
      "epoch: 1 step: 141, loss is 2.350252628326416\n",
      "epoch: 1 step: 142, loss is 2.307041645050049\n",
      "epoch: 1 step: 143, loss is 2.327805519104004\n",
      "epoch: 1 step: 144, loss is 2.3460581302642822\n",
      "epoch: 1 step: 145, loss is 2.3715386390686035\n",
      "epoch: 1 step: 146, loss is 2.3306376934051514\n",
      "epoch: 1 step: 147, loss is 2.3185019493103027\n",
      "epoch: 1 step: 148, loss is 2.3563613891601562\n",
      "epoch: 1 step: 149, loss is 2.2855372428894043\n",
      "epoch: 1 step: 150, loss is 2.345046043395996\n",
      "epoch: 1 step: 151, loss is 2.3443901538848877\n",
      "epoch: 1 step: 152, loss is 2.2891390323638916\n",
      "epoch: 1 step: 153, loss is 2.268004894256592\n",
      "epoch: 1 step: 154, loss is 2.264420747756958\n",
      "epoch: 1 step: 155, loss is 2.3679890632629395\n",
      "epoch: 1 step: 156, loss is 2.3342785835266113\n",
      "epoch: 1 step: 157, loss is 2.3012547492980957\n",
      "epoch: 1 step: 158, loss is 2.305375814437866\n",
      "epoch: 1 step: 159, loss is 2.313244104385376\n",
      "epoch: 1 step: 160, loss is 2.315929889678955\n",
      "epoch: 1 step: 161, loss is 2.30720591545105\n",
      "epoch: 1 step: 162, loss is 2.292135000228882\n",
      "epoch: 1 step: 163, loss is 2.30427622795105\n",
      "epoch: 1 step: 164, loss is 2.3267934322357178\n",
      "epoch: 1 step: 165, loss is 2.2668259143829346\n",
      "epoch: 1 step: 166, loss is 2.3188719749450684\n",
      "epoch: 1 step: 167, loss is 2.3256893157958984\n",
      "epoch: 1 step: 168, loss is 2.269131898880005\n",
      "epoch: 1 step: 169, loss is 2.2558467388153076\n",
      "epoch: 1 step: 170, loss is 2.3134422302246094\n",
      "epoch: 1 step: 171, loss is 2.3284645080566406\n",
      "epoch: 1 step: 172, loss is 2.3524246215820312\n",
      "epoch: 1 step: 173, loss is 2.3150923252105713\n",
      "epoch: 1 step: 174, loss is 2.3360390663146973\n",
      "epoch: 1 step: 175, loss is 2.346991777420044\n",
      "epoch: 1 step: 176, loss is 2.3091139793395996\n",
      "epoch: 1 step: 177, loss is 2.2665507793426514\n",
      "epoch: 1 step: 178, loss is 2.320469856262207\n",
      "epoch: 1 step: 179, loss is 2.368443250656128\n",
      "epoch: 1 step: 180, loss is 2.291167736053467\n",
      "epoch: 1 step: 181, loss is 2.2769525051116943\n",
      "epoch: 1 step: 182, loss is 2.3051950931549072\n",
      "epoch: 1 step: 183, loss is 2.2940433025360107\n",
      "epoch: 1 step: 184, loss is 2.350541353225708\n",
      "epoch: 1 step: 185, loss is 2.2802798748016357\n",
      "epoch: 1 step: 186, loss is 2.3384039402008057\n",
      "epoch: 1 step: 187, loss is 2.3563477993011475\n",
      "epoch: 1 step: 188, loss is 2.237006664276123\n",
      "epoch: 1 step: 189, loss is 2.2907474040985107\n",
      "epoch: 1 step: 190, loss is 2.309908866882324\n",
      "epoch: 1 step: 191, loss is 2.334144353866577\n",
      "epoch: 1 step: 192, loss is 2.2853565216064453\n",
      "epoch: 1 step: 193, loss is 2.3255772590637207\n",
      "epoch: 1 step: 194, loss is 2.335866689682007\n",
      "epoch: 1 step: 195, loss is 2.2937402725219727\n",
      "epoch: 1 step: 196, loss is 2.3431153297424316\n",
      "epoch: 1 step: 197, loss is 2.3209292888641357\n",
      "epoch: 1 step: 198, loss is 2.2836904525756836\n",
      "epoch: 1 step: 199, loss is 2.3081202507019043\n",
      "epoch: 1 step: 200, loss is 2.274273157119751\n",
      "epoch: 1 step: 201, loss is 2.308969497680664\n",
      "epoch: 1 step: 202, loss is 2.2658374309539795\n",
      "epoch: 1 step: 203, loss is 2.297700881958008\n",
      "epoch: 1 step: 204, loss is 2.3038835525512695\n",
      "epoch: 1 step: 205, loss is 2.297422409057617\n",
      "epoch: 1 step: 206, loss is 2.283503293991089\n",
      "epoch: 1 step: 207, loss is 2.2685189247131348\n",
      "epoch: 1 step: 208, loss is 2.3242592811584473\n",
      "epoch: 1 step: 209, loss is 2.3095529079437256\n",
      "epoch: 1 step: 210, loss is 2.3099236488342285\n",
      "epoch: 1 step: 211, loss is 2.3807027339935303\n",
      "epoch: 1 step: 212, loss is 2.292264461517334\n",
      "epoch: 1 step: 213, loss is 2.34570050239563\n",
      "epoch: 1 step: 214, loss is 2.344257354736328\n",
      "epoch: 1 step: 215, loss is 2.3219664096832275\n",
      "epoch: 1 step: 216, loss is 2.2599260807037354\n",
      "epoch: 1 step: 217, loss is 2.276212692260742\n",
      "epoch: 1 step: 218, loss is 2.3622143268585205\n",
      "epoch: 1 step: 219, loss is 2.3146872520446777\n",
      "epoch: 1 step: 220, loss is 2.277797222137451\n",
      "epoch: 1 step: 221, loss is 2.2934961318969727\n",
      "epoch: 1 step: 222, loss is 2.343310832977295\n",
      "epoch: 1 step: 223, loss is 2.2485485076904297\n",
      "epoch: 1 step: 224, loss is 2.2982749938964844\n",
      "epoch: 1 step: 225, loss is 2.275623083114624\n",
      "epoch: 1 step: 226, loss is 2.361921787261963\n",
      "epoch: 1 step: 227, loss is 2.3214950561523438\n",
      "epoch: 2 step: 1, loss is 2.285991668701172\n",
      "epoch: 2 step: 2, loss is 2.3213155269622803\n",
      "epoch: 2 step: 3, loss is 2.2827515602111816\n",
      "epoch: 2 step: 4, loss is 2.3698790073394775\n",
      "epoch: 2 step: 5, loss is 2.313387632369995\n",
      "epoch: 2 step: 6, loss is 2.3184525966644287\n",
      "epoch: 2 step: 7, loss is 2.30568790435791\n",
      "epoch: 2 step: 8, loss is 2.294950008392334\n",
      "epoch: 2 step: 9, loss is 2.348893642425537\n",
      "epoch: 2 step: 10, loss is 2.289567470550537\n",
      "epoch: 2 step: 11, loss is 2.365288257598877\n",
      "epoch: 2 step: 12, loss is 2.3654544353485107\n",
      "epoch: 2 step: 13, loss is 2.319028377532959\n",
      "epoch: 2 step: 14, loss is 2.3292598724365234\n",
      "epoch: 2 step: 15, loss is 2.300853967666626\n",
      "epoch: 2 step: 16, loss is 2.3266916275024414\n",
      "epoch: 2 step: 17, loss is 2.2715914249420166\n",
      "epoch: 2 step: 18, loss is 2.3015027046203613\n",
      "epoch: 2 step: 19, loss is 2.295588493347168\n",
      "epoch: 2 step: 20, loss is 2.314772844314575\n",
      "epoch: 2 step: 21, loss is 2.3001651763916016\n",
      "epoch: 2 step: 22, loss is 2.2983059883117676\n",
      "epoch: 2 step: 23, loss is 2.3088090419769287\n",
      "epoch: 2 step: 24, loss is 2.2741124629974365\n",
      "epoch: 2 step: 25, loss is 2.2542202472686768\n",
      "epoch: 2 step: 26, loss is 2.2491323947906494\n",
      "epoch: 2 step: 27, loss is 2.3452277183532715\n",
      "epoch: 2 step: 28, loss is 2.312690019607544\n",
      "epoch: 2 step: 29, loss is 2.3244781494140625\n",
      "epoch: 2 step: 30, loss is 2.2812321186065674\n",
      "epoch: 2 step: 31, loss is 2.2908668518066406\n",
      "epoch: 2 step: 32, loss is 2.36306095123291\n",
      "epoch: 2 step: 33, loss is 2.2977659702301025\n",
      "epoch: 2 step: 34, loss is 2.2858972549438477\n",
      "epoch: 2 step: 35, loss is 2.288825273513794\n",
      "epoch: 2 step: 36, loss is 2.3429598808288574\n",
      "epoch: 2 step: 37, loss is 2.2949271202087402\n",
      "epoch: 2 step: 38, loss is 2.337946891784668\n",
      "epoch: 2 step: 39, loss is 2.297654628753662\n",
      "epoch: 2 step: 40, loss is 2.2578094005584717\n",
      "epoch: 2 step: 41, loss is 2.3344204425811768\n",
      "epoch: 2 step: 42, loss is 2.2802717685699463\n",
      "epoch: 2 step: 43, loss is 2.273577928543091\n",
      "epoch: 2 step: 44, loss is 2.35031795501709\n",
      "epoch: 2 step: 45, loss is 2.280573844909668\n",
      "epoch: 2 step: 46, loss is 2.292525291442871\n",
      "epoch: 2 step: 47, loss is 2.278550624847412\n",
      "epoch: 2 step: 48, loss is 2.3554131984710693\n",
      "epoch: 2 step: 49, loss is 2.360466480255127\n",
      "epoch: 2 step: 50, loss is 2.3090004920959473\n",
      "epoch: 2 step: 51, loss is 2.267296075820923\n",
      "epoch: 2 step: 52, loss is 2.2571933269500732\n",
      "epoch: 2 step: 53, loss is 2.3371741771698\n",
      "epoch: 2 step: 54, loss is 2.294320583343506\n",
      "epoch: 2 step: 55, loss is 2.314924716949463\n",
      "epoch: 2 step: 56, loss is 2.3134496212005615\n",
      "epoch: 2 step: 57, loss is 2.256226062774658\n",
      "epoch: 2 step: 58, loss is 2.3269941806793213\n",
      "epoch: 2 step: 59, loss is 2.3259799480438232\n",
      "epoch: 2 step: 60, loss is 2.2842769622802734\n",
      "epoch: 2 step: 61, loss is 2.291961669921875\n",
      "epoch: 2 step: 62, loss is 2.310441493988037\n",
      "epoch: 2 step: 63, loss is 2.260810613632202\n",
      "epoch: 2 step: 64, loss is 2.2982494831085205\n",
      "epoch: 2 step: 65, loss is 2.282353639602661\n",
      "epoch: 2 step: 66, loss is 2.3158724308013916\n",
      "epoch: 2 step: 67, loss is 2.290755033493042\n",
      "epoch: 2 step: 68, loss is 2.260815143585205\n",
      "epoch: 2 step: 69, loss is 2.3563430309295654\n",
      "epoch: 2 step: 70, loss is 2.252046823501587\n",
      "epoch: 2 step: 71, loss is 2.306447744369507\n",
      "epoch: 2 step: 72, loss is 2.2837510108947754\n",
      "epoch: 2 step: 73, loss is 2.2731451988220215\n",
      "epoch: 2 step: 74, loss is 2.30723237991333\n",
      "epoch: 2 step: 75, loss is 2.2815449237823486\n",
      "epoch: 2 step: 76, loss is 2.341571569442749\n",
      "epoch: 2 step: 77, loss is 2.2921648025512695\n",
      "epoch: 2 step: 78, loss is 2.3055739402770996\n",
      "epoch: 2 step: 79, loss is 2.319007158279419\n",
      "epoch: 2 step: 80, loss is 2.2987818717956543\n",
      "epoch: 2 step: 81, loss is 2.3314743041992188\n",
      "epoch: 2 step: 82, loss is 2.2931976318359375\n",
      "epoch: 2 step: 83, loss is 2.284698009490967\n",
      "epoch: 2 step: 84, loss is 2.3062758445739746\n",
      "epoch: 2 step: 85, loss is 2.2717208862304688\n",
      "epoch: 2 step: 86, loss is 2.283714532852173\n",
      "epoch: 2 step: 87, loss is 2.308349132537842\n",
      "epoch: 2 step: 88, loss is 2.2729671001434326\n",
      "epoch: 2 step: 89, loss is 2.2949981689453125\n",
      "epoch: 2 step: 90, loss is 2.2295548915863037\n",
      "epoch: 2 step: 91, loss is 2.3209733963012695\n",
      "epoch: 2 step: 92, loss is 2.2947824001312256\n",
      "epoch: 2 step: 93, loss is 2.2993123531341553\n",
      "epoch: 2 step: 94, loss is 2.318844795227051\n",
      "epoch: 2 step: 95, loss is 2.255680561065674\n",
      "epoch: 2 step: 96, loss is 2.3698208332061768\n",
      "epoch: 2 step: 97, loss is 2.2655255794525146\n",
      "epoch: 2 step: 98, loss is 2.3265254497528076\n",
      "epoch: 2 step: 99, loss is 2.3218204975128174\n",
      "epoch: 2 step: 100, loss is 2.2767388820648193\n",
      "epoch: 2 step: 101, loss is 2.244156837463379\n",
      "epoch: 2 step: 102, loss is 2.320892810821533\n",
      "epoch: 2 step: 103, loss is 2.2779102325439453\n",
      "epoch: 2 step: 104, loss is 2.256098508834839\n",
      "epoch: 2 step: 105, loss is 2.3355178833007812\n",
      "epoch: 2 step: 106, loss is 2.2486519813537598\n",
      "epoch: 2 step: 107, loss is 2.3264236450195312\n",
      "epoch: 2 step: 108, loss is 2.339745283126831\n",
      "epoch: 2 step: 109, loss is 2.311945676803589\n",
      "epoch: 2 step: 110, loss is 2.2978601455688477\n",
      "epoch: 2 step: 111, loss is 2.275627613067627\n",
      "epoch: 2 step: 112, loss is 2.250511884689331\n",
      "epoch: 2 step: 113, loss is 2.3386282920837402\n",
      "epoch: 2 step: 114, loss is 2.3075790405273438\n",
      "epoch: 2 step: 115, loss is 2.3100876808166504\n",
      "epoch: 2 step: 116, loss is 2.3004672527313232\n",
      "epoch: 2 step: 117, loss is 2.292402505874634\n",
      "epoch: 2 step: 118, loss is 2.319047212600708\n",
      "epoch: 2 step: 119, loss is 2.2763915061950684\n",
      "epoch: 2 step: 120, loss is 2.2602713108062744\n",
      "epoch: 2 step: 121, loss is 2.307373285293579\n",
      "epoch: 2 step: 122, loss is 2.3312909603118896\n",
      "epoch: 2 step: 123, loss is 2.3408143520355225\n",
      "epoch: 2 step: 124, loss is 2.308196783065796\n",
      "epoch: 2 step: 125, loss is 2.3365118503570557\n",
      "epoch: 2 step: 126, loss is 2.299767017364502\n",
      "epoch: 2 step: 127, loss is 2.248756170272827\n",
      "epoch: 2 step: 128, loss is 2.290637731552124\n",
      "epoch: 2 step: 129, loss is 2.283393621444702\n",
      "epoch: 2 step: 130, loss is 2.279572010040283\n",
      "epoch: 2 step: 131, loss is 2.308025598526001\n",
      "epoch: 2 step: 132, loss is 2.2738559246063232\n",
      "epoch: 2 step: 133, loss is 2.3135743141174316\n",
      "epoch: 2 step: 134, loss is 2.322622776031494\n",
      "epoch: 2 step: 135, loss is 2.3217239379882812\n",
      "epoch: 2 step: 136, loss is 2.289557456970215\n",
      "epoch: 2 step: 137, loss is 2.290379762649536\n",
      "epoch: 2 step: 138, loss is 2.2669546604156494\n",
      "epoch: 2 step: 139, loss is 2.3551199436187744\n",
      "epoch: 2 step: 140, loss is 2.3243227005004883\n",
      "epoch: 2 step: 141, loss is 2.312201976776123\n",
      "epoch: 2 step: 142, loss is 2.2657225131988525\n",
      "epoch: 2 step: 143, loss is 2.3503589630126953\n",
      "epoch: 2 step: 144, loss is 2.2642979621887207\n",
      "epoch: 2 step: 145, loss is 2.3131673336029053\n",
      "epoch: 2 step: 146, loss is 2.2596051692962646\n",
      "epoch: 2 step: 147, loss is 2.3045339584350586\n",
      "epoch: 2 step: 148, loss is 2.2934584617614746\n",
      "epoch: 2 step: 149, loss is 2.2675161361694336\n",
      "epoch: 2 step: 150, loss is 2.2710068225860596\n",
      "epoch: 2 step: 151, loss is 2.3226869106292725\n",
      "epoch: 2 step: 152, loss is 2.244997978210449\n",
      "epoch: 2 step: 153, loss is 2.3108620643615723\n",
      "epoch: 2 step: 154, loss is 2.2931103706359863\n",
      "epoch: 2 step: 155, loss is 2.340132474899292\n",
      "epoch: 2 step: 156, loss is 2.339280128479004\n",
      "epoch: 2 step: 157, loss is 2.247728109359741\n",
      "epoch: 2 step: 158, loss is 2.2994675636291504\n",
      "epoch: 2 step: 159, loss is 2.2990899085998535\n",
      "epoch: 2 step: 160, loss is 2.244637966156006\n",
      "epoch: 2 step: 161, loss is 2.294316291809082\n",
      "epoch: 2 step: 162, loss is 2.325108766555786\n",
      "epoch: 2 step: 163, loss is 2.266155958175659\n",
      "epoch: 2 step: 164, loss is 2.2400121688842773\n",
      "epoch: 2 step: 165, loss is 2.275448799133301\n",
      "epoch: 2 step: 166, loss is 2.328096389770508\n",
      "epoch: 2 step: 167, loss is 2.2819230556488037\n",
      "epoch: 2 step: 168, loss is 2.33380389213562\n",
      "epoch: 2 step: 169, loss is 2.2838354110717773\n",
      "epoch: 2 step: 170, loss is 2.291738271713257\n",
      "epoch: 2 step: 171, loss is 2.2972631454467773\n",
      "epoch: 2 step: 172, loss is 2.2874503135681152\n",
      "epoch: 2 step: 173, loss is 2.300051689147949\n",
      "epoch: 2 step: 174, loss is 2.3254573345184326\n",
      "epoch: 2 step: 175, loss is 2.3117516040802\n",
      "epoch: 2 step: 176, loss is 2.2681620121002197\n",
      "epoch: 2 step: 177, loss is 2.2999179363250732\n",
      "epoch: 2 step: 178, loss is 2.2745070457458496\n",
      "epoch: 2 step: 179, loss is 2.3119397163391113\n",
      "epoch: 2 step: 180, loss is 2.3319251537323\n",
      "epoch: 2 step: 181, loss is 2.2443621158599854\n",
      "epoch: 2 step: 182, loss is 2.26776385307312\n",
      "epoch: 2 step: 183, loss is 2.2547764778137207\n",
      "epoch: 2 step: 184, loss is 2.2924227714538574\n",
      "epoch: 2 step: 185, loss is 2.298279047012329\n",
      "epoch: 2 step: 186, loss is 2.2878315448760986\n",
      "epoch: 2 step: 187, loss is 2.26090145111084\n",
      "epoch: 2 step: 188, loss is 2.327970504760742\n",
      "epoch: 2 step: 189, loss is 2.295713424682617\n",
      "epoch: 2 step: 190, loss is 2.310002565383911\n",
      "epoch: 2 step: 191, loss is 2.287773847579956\n",
      "epoch: 2 step: 192, loss is 2.238711357116699\n",
      "epoch: 2 step: 193, loss is 2.3044536113739014\n",
      "epoch: 2 step: 194, loss is 2.2890713214874268\n",
      "epoch: 2 step: 195, loss is 2.2936720848083496\n",
      "epoch: 2 step: 196, loss is 2.267479419708252\n",
      "epoch: 2 step: 197, loss is 2.2831239700317383\n",
      "epoch: 2 step: 198, loss is 2.319997787475586\n",
      "epoch: 2 step: 199, loss is 2.261394500732422\n",
      "epoch: 2 step: 200, loss is 2.2772252559661865\n",
      "epoch: 2 step: 201, loss is 2.269624710083008\n",
      "epoch: 2 step: 202, loss is 2.296319007873535\n",
      "epoch: 2 step: 203, loss is 2.3109214305877686\n",
      "epoch: 2 step: 204, loss is 2.280125856399536\n",
      "epoch: 2 step: 205, loss is 2.301020383834839\n",
      "epoch: 2 step: 206, loss is 2.2662110328674316\n",
      "epoch: 2 step: 207, loss is 2.3263397216796875\n",
      "epoch: 2 step: 208, loss is 2.2839431762695312\n",
      "epoch: 2 step: 209, loss is 2.274895191192627\n",
      "epoch: 2 step: 210, loss is 2.3264875411987305\n",
      "epoch: 2 step: 211, loss is 2.3240678310394287\n",
      "epoch: 2 step: 212, loss is 2.2827308177948\n",
      "epoch: 2 step: 213, loss is 2.285799980163574\n",
      "epoch: 2 step: 214, loss is 2.269676923751831\n",
      "epoch: 2 step: 215, loss is 2.315208911895752\n",
      "epoch: 2 step: 216, loss is 2.310772657394409\n",
      "epoch: 2 step: 217, loss is 2.311962127685547\n",
      "epoch: 2 step: 218, loss is 2.3340463638305664\n",
      "epoch: 2 step: 219, loss is 2.240483283996582\n",
      "epoch: 2 step: 220, loss is 2.2855873107910156\n",
      "epoch: 2 step: 221, loss is 2.2428290843963623\n",
      "epoch: 2 step: 222, loss is 2.30206561088562\n",
      "epoch: 2 step: 223, loss is 2.296421527862549\n",
      "epoch: 2 step: 224, loss is 2.307810068130493\n",
      "epoch: 2 step: 225, loss is 2.3299784660339355\n",
      "epoch: 2 step: 226, loss is 2.2716622352600098\n",
      "epoch: 2 step: 227, loss is 2.2155566215515137\n",
      "epoch: 3 step: 1, loss is 2.253422975540161\n",
      "epoch: 3 step: 2, loss is 2.3365540504455566\n",
      "epoch: 3 step: 3, loss is 2.367048501968384\n",
      "epoch: 3 step: 4, loss is 2.329476833343506\n",
      "epoch: 3 step: 5, loss is 2.2958860397338867\n",
      "epoch: 3 step: 6, loss is 2.280428886413574\n",
      "epoch: 3 step: 7, loss is 2.281622886657715\n",
      "epoch: 3 step: 8, loss is 2.2515883445739746\n",
      "epoch: 3 step: 9, loss is 2.3195643424987793\n",
      "epoch: 3 step: 10, loss is 2.3259057998657227\n",
      "epoch: 3 step: 11, loss is 2.3220906257629395\n",
      "epoch: 3 step: 12, loss is 2.3162412643432617\n",
      "epoch: 3 step: 13, loss is 2.278900384902954\n",
      "epoch: 3 step: 14, loss is 2.27162504196167\n",
      "epoch: 3 step: 15, loss is 2.2990753650665283\n",
      "epoch: 3 step: 16, loss is 2.3000428676605225\n",
      "epoch: 3 step: 17, loss is 2.3081321716308594\n",
      "epoch: 3 step: 18, loss is 2.297708749771118\n",
      "epoch: 3 step: 19, loss is 2.283170700073242\n",
      "epoch: 3 step: 20, loss is 2.2960684299468994\n",
      "epoch: 3 step: 21, loss is 2.2735910415649414\n",
      "epoch: 3 step: 22, loss is 2.2854597568511963\n",
      "epoch: 3 step: 23, loss is 2.2932169437408447\n",
      "epoch: 3 step: 24, loss is 2.3330583572387695\n",
      "epoch: 3 step: 25, loss is 2.300854206085205\n",
      "epoch: 3 step: 26, loss is 2.2627012729644775\n",
      "epoch: 3 step: 27, loss is 2.308478593826294\n",
      "epoch: 3 step: 28, loss is 2.2336232662200928\n",
      "epoch: 3 step: 29, loss is 2.3072807788848877\n",
      "epoch: 3 step: 30, loss is 2.2825262546539307\n",
      "epoch: 3 step: 31, loss is 2.266413450241089\n",
      "epoch: 3 step: 32, loss is 2.279620409011841\n",
      "epoch: 3 step: 33, loss is 2.2825539112091064\n",
      "epoch: 3 step: 34, loss is 2.290645122528076\n",
      "epoch: 3 step: 35, loss is 2.299124002456665\n",
      "epoch: 3 step: 36, loss is 2.3079864978790283\n",
      "epoch: 3 step: 37, loss is 2.2836225032806396\n",
      "epoch: 3 step: 38, loss is 2.283903121948242\n",
      "epoch: 3 step: 39, loss is 2.2353761196136475\n",
      "epoch: 3 step: 40, loss is 2.311357021331787\n",
      "epoch: 3 step: 41, loss is 2.2407710552215576\n",
      "epoch: 3 step: 42, loss is 2.233842611312866\n",
      "epoch: 3 step: 43, loss is 2.313215970993042\n",
      "epoch: 3 step: 44, loss is 2.3073434829711914\n",
      "epoch: 3 step: 45, loss is 2.346224546432495\n",
      "epoch: 3 step: 46, loss is 2.2491748332977295\n",
      "epoch: 3 step: 47, loss is 2.2417795658111572\n",
      "epoch: 3 step: 48, loss is 2.2939348220825195\n",
      "epoch: 3 step: 49, loss is 2.2738091945648193\n",
      "epoch: 3 step: 50, loss is 2.265089511871338\n",
      "epoch: 3 step: 51, loss is 2.288125991821289\n",
      "epoch: 3 step: 52, loss is 2.2454330921173096\n",
      "epoch: 3 step: 53, loss is 2.226590394973755\n",
      "epoch: 3 step: 54, loss is 2.230741024017334\n",
      "epoch: 3 step: 55, loss is 2.264129400253296\n",
      "epoch: 3 step: 56, loss is 2.28444242477417\n",
      "epoch: 3 step: 57, loss is 2.324086904525757\n",
      "epoch: 3 step: 58, loss is 2.2885677814483643\n",
      "epoch: 3 step: 59, loss is 2.292389392852783\n",
      "epoch: 3 step: 60, loss is 2.3016693592071533\n",
      "epoch: 3 step: 61, loss is 2.2661337852478027\n",
      "epoch: 3 step: 62, loss is 2.2866275310516357\n",
      "epoch: 3 step: 63, loss is 2.264957904815674\n",
      "epoch: 3 step: 64, loss is 2.2846245765686035\n",
      "epoch: 3 step: 65, loss is 2.3151051998138428\n",
      "epoch: 3 step: 66, loss is 2.2569079399108887\n",
      "epoch: 3 step: 67, loss is 2.298248291015625\n",
      "epoch: 3 step: 68, loss is 2.2411975860595703\n",
      "epoch: 3 step: 69, loss is 2.2485759258270264\n",
      "epoch: 3 step: 70, loss is 2.2946584224700928\n",
      "epoch: 3 step: 71, loss is 2.232309103012085\n",
      "epoch: 3 step: 72, loss is 2.2602875232696533\n",
      "epoch: 3 step: 73, loss is 2.308765172958374\n",
      "epoch: 3 step: 74, loss is 2.3175506591796875\n",
      "epoch: 3 step: 75, loss is 2.3358888626098633\n",
      "epoch: 3 step: 76, loss is 2.2549595832824707\n",
      "epoch: 3 step: 77, loss is 2.28444504737854\n",
      "epoch: 3 step: 78, loss is 2.2811484336853027\n",
      "epoch: 3 step: 79, loss is 2.29433536529541\n",
      "epoch: 3 step: 80, loss is 2.2885797023773193\n",
      "epoch: 3 step: 81, loss is 2.3043434619903564\n",
      "epoch: 3 step: 82, loss is 2.3250982761383057\n",
      "epoch: 3 step: 83, loss is 2.3266913890838623\n",
      "epoch: 3 step: 84, loss is 2.3058276176452637\n",
      "epoch: 3 step: 85, loss is 2.296238422393799\n",
      "epoch: 3 step: 86, loss is 2.3116648197174072\n",
      "epoch: 3 step: 87, loss is 2.2194361686706543\n",
      "epoch: 3 step: 88, loss is 2.2529456615448\n",
      "epoch: 3 step: 89, loss is 2.2818117141723633\n",
      "epoch: 3 step: 90, loss is 2.2954983711242676\n",
      "epoch: 3 step: 91, loss is 2.30897855758667\n",
      "epoch: 3 step: 92, loss is 2.260862350463867\n",
      "epoch: 3 step: 93, loss is 2.275641441345215\n",
      "epoch: 3 step: 94, loss is 2.27121639251709\n",
      "epoch: 3 step: 95, loss is 2.2650294303894043\n",
      "epoch: 3 step: 96, loss is 2.305962085723877\n",
      "epoch: 3 step: 97, loss is 2.2571158409118652\n",
      "epoch: 3 step: 98, loss is 2.259737014770508\n",
      "epoch: 3 step: 99, loss is 2.265124797821045\n",
      "epoch: 3 step: 100, loss is 2.316225528717041\n",
      "epoch: 3 step: 101, loss is 2.325310707092285\n",
      "epoch: 3 step: 102, loss is 2.3264477252960205\n",
      "epoch: 3 step: 103, loss is 2.2792022228240967\n",
      "epoch: 3 step: 104, loss is 2.3043317794799805\n",
      "epoch: 3 step: 105, loss is 2.260988712310791\n",
      "epoch: 3 step: 106, loss is 2.2801530361175537\n",
      "epoch: 3 step: 107, loss is 2.311344861984253\n",
      "epoch: 3 step: 108, loss is 2.2919206619262695\n",
      "epoch: 3 step: 109, loss is 2.297402858734131\n",
      "epoch: 3 step: 110, loss is 2.302079439163208\n",
      "epoch: 3 step: 111, loss is 2.241924285888672\n",
      "epoch: 3 step: 112, loss is 2.2818706035614014\n",
      "epoch: 3 step: 113, loss is 2.2844078540802\n",
      "epoch: 3 step: 114, loss is 2.3446788787841797\n",
      "epoch: 3 step: 115, loss is 2.281764507293701\n",
      "epoch: 3 step: 116, loss is 2.2582449913024902\n",
      "epoch: 3 step: 117, loss is 2.2782480716705322\n",
      "epoch: 3 step: 118, loss is 2.2961695194244385\n",
      "epoch: 3 step: 119, loss is 2.27451491355896\n",
      "epoch: 3 step: 120, loss is 2.296893835067749\n",
      "epoch: 3 step: 121, loss is 2.2907965183258057\n",
      "epoch: 3 step: 122, loss is 2.3423330783843994\n",
      "epoch: 3 step: 123, loss is 2.2830183506011963\n",
      "epoch: 3 step: 124, loss is 2.2708349227905273\n",
      "epoch: 3 step: 125, loss is 2.303478479385376\n",
      "epoch: 3 step: 126, loss is 2.284437894821167\n",
      "epoch: 3 step: 127, loss is 2.3117713928222656\n",
      "epoch: 3 step: 128, loss is 2.2604291439056396\n",
      "epoch: 3 step: 129, loss is 2.263880729675293\n",
      "epoch: 3 step: 130, loss is 2.2761640548706055\n",
      "epoch: 3 step: 131, loss is 2.234482765197754\n",
      "epoch: 3 step: 132, loss is 2.232276678085327\n",
      "epoch: 3 step: 133, loss is 2.360379934310913\n",
      "epoch: 3 step: 134, loss is 2.2717769145965576\n",
      "epoch: 3 step: 135, loss is 2.2873260974884033\n",
      "epoch: 3 step: 136, loss is 2.251955986022949\n",
      "epoch: 3 step: 137, loss is 2.3292860984802246\n",
      "epoch: 3 step: 138, loss is 2.3124101161956787\n",
      "epoch: 3 step: 139, loss is 2.3120579719543457\n",
      "epoch: 3 step: 140, loss is 2.306244134902954\n",
      "epoch: 3 step: 141, loss is 2.2621426582336426\n",
      "epoch: 3 step: 142, loss is 2.3105432987213135\n",
      "epoch: 3 step: 143, loss is 2.289947986602783\n",
      "epoch: 3 step: 144, loss is 2.298100471496582\n",
      "epoch: 3 step: 145, loss is 2.273129463195801\n",
      "epoch: 3 step: 146, loss is 2.296257495880127\n",
      "epoch: 3 step: 147, loss is 2.2485697269439697\n",
      "epoch: 3 step: 148, loss is 2.3073253631591797\n",
      "epoch: 3 step: 149, loss is 2.272986650466919\n",
      "epoch: 3 step: 150, loss is 2.3126680850982666\n",
      "epoch: 3 step: 151, loss is 2.270921230316162\n",
      "epoch: 3 step: 152, loss is 2.2980809211730957\n",
      "epoch: 3 step: 153, loss is 2.3714988231658936\n",
      "epoch: 3 step: 154, loss is 2.28645658493042\n",
      "epoch: 3 step: 155, loss is 2.2968688011169434\n",
      "epoch: 3 step: 156, loss is 2.257211923599243\n",
      "epoch: 3 step: 157, loss is 2.2769126892089844\n",
      "epoch: 3 step: 158, loss is 2.303027391433716\n",
      "epoch: 3 step: 159, loss is 2.3146114349365234\n",
      "epoch: 3 step: 160, loss is 2.261874198913574\n",
      "epoch: 3 step: 161, loss is 2.2679686546325684\n",
      "epoch: 3 step: 162, loss is 2.2836570739746094\n",
      "epoch: 3 step: 163, loss is 2.2689099311828613\n",
      "epoch: 3 step: 164, loss is 2.3032853603363037\n",
      "epoch: 3 step: 165, loss is 2.2925679683685303\n",
      "epoch: 3 step: 166, loss is 2.2841224670410156\n",
      "epoch: 3 step: 167, loss is 2.2640843391418457\n",
      "epoch: 3 step: 168, loss is 2.2684550285339355\n",
      "epoch: 3 step: 169, loss is 2.2453911304473877\n",
      "epoch: 3 step: 170, loss is 2.286402702331543\n",
      "epoch: 3 step: 171, loss is 2.261049747467041\n",
      "epoch: 3 step: 172, loss is 2.2422380447387695\n",
      "epoch: 3 step: 173, loss is 2.2754409313201904\n",
      "epoch: 3 step: 174, loss is 2.286287307739258\n",
      "epoch: 3 step: 175, loss is 2.245354413986206\n",
      "epoch: 3 step: 176, loss is 2.2906529903411865\n",
      "epoch: 3 step: 177, loss is 2.2463607788085938\n",
      "epoch: 3 step: 178, loss is 2.324812889099121\n",
      "epoch: 3 step: 179, loss is 2.2799835205078125\n",
      "epoch: 3 step: 180, loss is 2.3171589374542236\n",
      "epoch: 3 step: 181, loss is 2.301625967025757\n",
      "epoch: 3 step: 182, loss is 2.289731740951538\n",
      "epoch: 3 step: 183, loss is 2.2302777767181396\n",
      "epoch: 3 step: 184, loss is 2.3033735752105713\n",
      "epoch: 3 step: 185, loss is 2.279733419418335\n",
      "epoch: 3 step: 186, loss is 2.335557460784912\n",
      "epoch: 3 step: 187, loss is 2.2862401008605957\n",
      "epoch: 3 step: 188, loss is 2.2728421688079834\n",
      "epoch: 3 step: 189, loss is 2.2231109142303467\n",
      "epoch: 3 step: 190, loss is 2.268108367919922\n",
      "epoch: 3 step: 191, loss is 2.282308578491211\n",
      "epoch: 3 step: 192, loss is 2.2551109790802\n",
      "epoch: 3 step: 193, loss is 2.341360092163086\n",
      "epoch: 3 step: 194, loss is 2.2811498641967773\n",
      "epoch: 3 step: 195, loss is 2.2697691917419434\n",
      "epoch: 3 step: 196, loss is 2.2451884746551514\n",
      "epoch: 3 step: 197, loss is 2.291116714477539\n",
      "epoch: 3 step: 198, loss is 2.2766027450561523\n",
      "epoch: 3 step: 199, loss is 2.30202579498291\n",
      "epoch: 3 step: 200, loss is 2.257939100265503\n",
      "epoch: 3 step: 201, loss is 2.3018510341644287\n",
      "epoch: 3 step: 202, loss is 2.275808811187744\n",
      "epoch: 3 step: 203, loss is 2.3268966674804688\n",
      "epoch: 3 step: 204, loss is 2.283724308013916\n",
      "epoch: 3 step: 205, loss is 2.2801263332366943\n",
      "epoch: 3 step: 206, loss is 2.2613630294799805\n",
      "epoch: 3 step: 207, loss is 2.2696025371551514\n",
      "epoch: 3 step: 208, loss is 2.2775206565856934\n",
      "epoch: 3 step: 209, loss is 2.2792365550994873\n",
      "epoch: 3 step: 210, loss is 2.3026235103607178\n",
      "epoch: 3 step: 211, loss is 2.2896876335144043\n",
      "epoch: 3 step: 212, loss is 2.2337207794189453\n",
      "epoch: 3 step: 213, loss is 2.3007686138153076\n",
      "epoch: 3 step: 214, loss is 2.306825876235962\n",
      "epoch: 3 step: 215, loss is 2.283200263977051\n",
      "epoch: 3 step: 216, loss is 2.2987215518951416\n",
      "epoch: 3 step: 217, loss is 2.2120468616485596\n",
      "epoch: 3 step: 218, loss is 2.2947657108306885\n",
      "epoch: 3 step: 219, loss is 2.3232274055480957\n",
      "epoch: 3 step: 220, loss is 2.3151237964630127\n",
      "epoch: 3 step: 221, loss is 2.3080191612243652\n",
      "epoch: 3 step: 222, loss is 2.3022897243499756\n",
      "epoch: 3 step: 223, loss is 2.2716047763824463\n",
      "epoch: 3 step: 224, loss is 2.3017396926879883\n",
      "epoch: 3 step: 225, loss is 2.261396646499634\n",
      "epoch: 3 step: 226, loss is 2.2584385871887207\n",
      "epoch: 3 step: 227, loss is 2.2912001609802246\n",
      "epoch: 4 step: 1, loss is 2.2974584102630615\n",
      "epoch: 4 step: 2, loss is 2.351489782333374\n",
      "epoch: 4 step: 3, loss is 2.287350654602051\n",
      "epoch: 4 step: 4, loss is 2.3008573055267334\n",
      "epoch: 4 step: 5, loss is 2.2482893466949463\n",
      "epoch: 4 step: 6, loss is 2.3091983795166016\n",
      "epoch: 4 step: 7, loss is 2.2943525314331055\n",
      "epoch: 4 step: 8, loss is 2.3201355934143066\n",
      "epoch: 4 step: 9, loss is 2.2935829162597656\n",
      "epoch: 4 step: 10, loss is 2.3094840049743652\n",
      "epoch: 4 step: 11, loss is 2.3024706840515137\n",
      "epoch: 4 step: 12, loss is 2.315886974334717\n",
      "epoch: 4 step: 13, loss is 2.286115884780884\n",
      "epoch: 4 step: 14, loss is 2.267056703567505\n",
      "epoch: 4 step: 15, loss is 2.2214393615722656\n",
      "epoch: 4 step: 16, loss is 2.2922468185424805\n",
      "epoch: 4 step: 17, loss is 2.27121639251709\n",
      "epoch: 4 step: 18, loss is 2.350330114364624\n",
      "epoch: 4 step: 19, loss is 2.325833320617676\n",
      "epoch: 4 step: 20, loss is 2.314465284347534\n",
      "epoch: 4 step: 21, loss is 2.307814359664917\n",
      "epoch: 4 step: 22, loss is 2.1919009685516357\n",
      "epoch: 4 step: 23, loss is 2.28104567527771\n",
      "epoch: 4 step: 24, loss is 2.3055479526519775\n",
      "epoch: 4 step: 25, loss is 2.3213398456573486\n",
      "epoch: 4 step: 26, loss is 2.2847445011138916\n",
      "epoch: 4 step: 27, loss is 2.2690553665161133\n",
      "epoch: 4 step: 28, loss is 2.283643960952759\n",
      "epoch: 4 step: 29, loss is 2.263772487640381\n",
      "epoch: 4 step: 30, loss is 2.2647242546081543\n",
      "epoch: 4 step: 31, loss is 2.2836644649505615\n",
      "epoch: 4 step: 32, loss is 2.273984432220459\n",
      "epoch: 4 step: 33, loss is 2.300586700439453\n",
      "epoch: 4 step: 34, loss is 2.296703577041626\n",
      "epoch: 4 step: 35, loss is 2.263331174850464\n",
      "epoch: 4 step: 36, loss is 2.2577366828918457\n",
      "epoch: 4 step: 37, loss is 2.2637782096862793\n",
      "epoch: 4 step: 38, loss is 2.210327625274658\n",
      "epoch: 4 step: 39, loss is 2.227271795272827\n",
      "epoch: 4 step: 40, loss is 2.2599122524261475\n",
      "epoch: 4 step: 41, loss is 2.3077776432037354\n",
      "epoch: 4 step: 42, loss is 2.264401435852051\n",
      "epoch: 4 step: 43, loss is 2.300865888595581\n",
      "epoch: 4 step: 44, loss is 2.2774152755737305\n",
      "epoch: 4 step: 45, loss is 2.263582706451416\n",
      "epoch: 4 step: 46, loss is 2.271836757659912\n",
      "epoch: 4 step: 47, loss is 2.2984659671783447\n",
      "epoch: 4 step: 48, loss is 2.254210948944092\n",
      "epoch: 4 step: 49, loss is 2.248842716217041\n",
      "epoch: 4 step: 50, loss is 2.308791399002075\n",
      "epoch: 4 step: 51, loss is 2.322235345840454\n",
      "epoch: 4 step: 52, loss is 2.2618613243103027\n",
      "epoch: 4 step: 53, loss is 2.275207996368408\n",
      "epoch: 4 step: 54, loss is 2.3154709339141846\n",
      "epoch: 4 step: 55, loss is 2.3102686405181885\n",
      "epoch: 4 step: 56, loss is 2.256011962890625\n",
      "epoch: 4 step: 57, loss is 2.25384259223938\n",
      "epoch: 4 step: 58, loss is 2.2867302894592285\n",
      "epoch: 4 step: 59, loss is 2.2897441387176514\n",
      "epoch: 4 step: 60, loss is 2.2868752479553223\n",
      "epoch: 4 step: 61, loss is 2.3077216148376465\n",
      "epoch: 4 step: 62, loss is 2.2713255882263184\n",
      "epoch: 4 step: 63, loss is 2.3182759284973145\n",
      "epoch: 4 step: 64, loss is 2.3118858337402344\n",
      "epoch: 4 step: 65, loss is 2.262892246246338\n",
      "epoch: 4 step: 66, loss is 2.264155387878418\n",
      "epoch: 4 step: 67, loss is 2.2605323791503906\n",
      "epoch: 4 step: 68, loss is 2.2662081718444824\n",
      "epoch: 4 step: 69, loss is 2.2692909240722656\n",
      "epoch: 4 step: 70, loss is 2.295955181121826\n",
      "epoch: 4 step: 71, loss is 2.250763416290283\n",
      "epoch: 4 step: 72, loss is 2.3148043155670166\n",
      "epoch: 4 step: 73, loss is 2.268867254257202\n",
      "epoch: 4 step: 74, loss is 2.2437942028045654\n",
      "epoch: 4 step: 75, loss is 2.301295757293701\n",
      "epoch: 4 step: 76, loss is 2.284921169281006\n",
      "epoch: 4 step: 77, loss is 2.305884599685669\n",
      "epoch: 4 step: 78, loss is 2.231231451034546\n",
      "epoch: 4 step: 79, loss is 2.283250331878662\n",
      "epoch: 4 step: 80, loss is 2.2536299228668213\n",
      "epoch: 4 step: 81, loss is 2.311403274536133\n",
      "epoch: 4 step: 82, loss is 2.223418712615967\n",
      "epoch: 4 step: 83, loss is 2.3226585388183594\n",
      "epoch: 4 step: 84, loss is 2.310263156890869\n",
      "epoch: 4 step: 85, loss is 2.2721686363220215\n",
      "epoch: 4 step: 86, loss is 2.2792370319366455\n",
      "epoch: 4 step: 87, loss is 2.250128746032715\n",
      "epoch: 4 step: 88, loss is 2.235039234161377\n",
      "epoch: 4 step: 89, loss is 2.2469255924224854\n",
      "epoch: 4 step: 90, loss is 2.2980141639709473\n",
      "epoch: 4 step: 91, loss is 2.276344060897827\n",
      "epoch: 4 step: 92, loss is 2.25536847114563\n",
      "epoch: 4 step: 93, loss is 2.3410208225250244\n",
      "epoch: 4 step: 94, loss is 2.241875410079956\n",
      "epoch: 4 step: 95, loss is 2.223827838897705\n",
      "epoch: 4 step: 96, loss is 2.2570736408233643\n",
      "epoch: 4 step: 97, loss is 2.276684045791626\n",
      "epoch: 4 step: 98, loss is 2.2817821502685547\n",
      "epoch: 4 step: 99, loss is 2.34619402885437\n",
      "epoch: 4 step: 100, loss is 2.2666237354278564\n",
      "epoch: 4 step: 101, loss is 2.30370831489563\n",
      "epoch: 4 step: 102, loss is 2.263592481613159\n",
      "epoch: 4 step: 103, loss is 2.2556469440460205\n",
      "epoch: 4 step: 104, loss is 2.2709009647369385\n",
      "epoch: 4 step: 105, loss is 2.2328524589538574\n",
      "epoch: 4 step: 106, loss is 2.2734737396240234\n",
      "epoch: 4 step: 107, loss is 2.3355398178100586\n",
      "epoch: 4 step: 108, loss is 2.273519515991211\n",
      "epoch: 4 step: 109, loss is 2.2936530113220215\n",
      "epoch: 4 step: 110, loss is 2.2618865966796875\n",
      "epoch: 4 step: 111, loss is 2.2921371459960938\n",
      "epoch: 4 step: 112, loss is 2.2417304515838623\n",
      "epoch: 4 step: 113, loss is 2.2819299697875977\n",
      "epoch: 4 step: 114, loss is 2.2928476333618164\n",
      "epoch: 4 step: 115, loss is 2.262126922607422\n",
      "epoch: 4 step: 116, loss is 2.303452968597412\n",
      "epoch: 4 step: 117, loss is 2.274470090866089\n",
      "epoch: 4 step: 118, loss is 2.308014154434204\n",
      "epoch: 4 step: 119, loss is 2.251829147338867\n",
      "epoch: 4 step: 120, loss is 2.2823030948638916\n",
      "epoch: 4 step: 121, loss is 2.2605931758880615\n",
      "epoch: 4 step: 122, loss is 2.2755277156829834\n",
      "epoch: 4 step: 123, loss is 2.277517080307007\n",
      "epoch: 4 step: 124, loss is 2.264037847518921\n",
      "epoch: 4 step: 125, loss is 2.3117775917053223\n",
      "epoch: 4 step: 126, loss is 2.2653350830078125\n",
      "epoch: 4 step: 127, loss is 2.193441390991211\n",
      "epoch: 4 step: 128, loss is 2.343364715576172\n",
      "epoch: 4 step: 129, loss is 2.299644708633423\n",
      "epoch: 4 step: 130, loss is 2.2903451919555664\n",
      "epoch: 4 step: 131, loss is 2.2886314392089844\n",
      "epoch: 4 step: 132, loss is 2.2453110218048096\n",
      "epoch: 4 step: 133, loss is 2.2829065322875977\n",
      "epoch: 4 step: 134, loss is 2.255250930786133\n",
      "epoch: 4 step: 135, loss is 2.285282850265503\n",
      "epoch: 4 step: 136, loss is 2.252372980117798\n",
      "epoch: 4 step: 137, loss is 2.2052581310272217\n",
      "epoch: 4 step: 138, loss is 2.325697660446167\n",
      "epoch: 4 step: 139, loss is 2.2982983589172363\n",
      "epoch: 4 step: 140, loss is 2.2699546813964844\n",
      "epoch: 4 step: 141, loss is 2.2974088191986084\n",
      "epoch: 4 step: 142, loss is 2.20847487449646\n",
      "epoch: 4 step: 143, loss is 2.2954044342041016\n",
      "epoch: 4 step: 144, loss is 2.2854886054992676\n",
      "epoch: 4 step: 145, loss is 2.21718692779541\n",
      "epoch: 4 step: 146, loss is 2.3122212886810303\n",
      "epoch: 4 step: 147, loss is 2.3480076789855957\n",
      "epoch: 4 step: 148, loss is 2.3114442825317383\n",
      "epoch: 4 step: 149, loss is 2.3032755851745605\n",
      "epoch: 4 step: 150, loss is 2.2222626209259033\n",
      "epoch: 4 step: 151, loss is 2.332902669906616\n",
      "epoch: 4 step: 152, loss is 2.274165153503418\n",
      "epoch: 4 step: 153, loss is 2.2206900119781494\n",
      "epoch: 4 step: 154, loss is 2.296480178833008\n",
      "epoch: 4 step: 155, loss is 2.3173811435699463\n",
      "epoch: 4 step: 156, loss is 2.2381443977355957\n",
      "epoch: 4 step: 157, loss is 2.2121806144714355\n",
      "epoch: 4 step: 158, loss is 2.2170450687408447\n",
      "epoch: 4 step: 159, loss is 2.3112833499908447\n",
      "epoch: 4 step: 160, loss is 2.2702512741088867\n",
      "epoch: 4 step: 161, loss is 2.298748254776001\n",
      "epoch: 4 step: 162, loss is 2.2365174293518066\n",
      "epoch: 4 step: 163, loss is 2.2368075847625732\n",
      "epoch: 4 step: 164, loss is 2.29410982131958\n",
      "epoch: 4 step: 165, loss is 2.323836088180542\n",
      "epoch: 4 step: 166, loss is 2.2491800785064697\n",
      "epoch: 4 step: 167, loss is 2.2944083213806152\n",
      "epoch: 4 step: 168, loss is 2.274653911590576\n",
      "epoch: 4 step: 169, loss is 2.286702871322632\n",
      "epoch: 4 step: 170, loss is 2.2370076179504395\n",
      "epoch: 4 step: 171, loss is 2.263469696044922\n",
      "epoch: 4 step: 172, loss is 2.272888660430908\n",
      "epoch: 4 step: 173, loss is 2.2734487056732178\n",
      "epoch: 4 step: 174, loss is 2.309025526046753\n",
      "epoch: 4 step: 175, loss is 2.317439079284668\n",
      "epoch: 4 step: 176, loss is 2.275303840637207\n",
      "epoch: 4 step: 177, loss is 2.207301139831543\n",
      "epoch: 4 step: 178, loss is 2.2323570251464844\n",
      "epoch: 4 step: 179, loss is 2.233651876449585\n",
      "epoch: 4 step: 180, loss is 2.2883851528167725\n",
      "epoch: 4 step: 181, loss is 2.2964928150177\n",
      "epoch: 4 step: 182, loss is 2.2726101875305176\n",
      "epoch: 4 step: 183, loss is 2.274489164352417\n",
      "epoch: 4 step: 184, loss is 2.276916265487671\n",
      "epoch: 4 step: 185, loss is 2.326124906539917\n",
      "epoch: 4 step: 186, loss is 2.25852108001709\n",
      "epoch: 4 step: 187, loss is 2.250305414199829\n",
      "epoch: 4 step: 188, loss is 2.2592058181762695\n",
      "epoch: 4 step: 189, loss is 2.2253150939941406\n",
      "epoch: 4 step: 190, loss is 2.323573350906372\n",
      "epoch: 4 step: 191, loss is 2.3287923336029053\n",
      "epoch: 4 step: 192, loss is 2.2986397743225098\n",
      "epoch: 4 step: 193, loss is 2.259622097015381\n",
      "epoch: 4 step: 194, loss is 2.2976503372192383\n",
      "epoch: 4 step: 195, loss is 2.3545143604278564\n",
      "epoch: 4 step: 196, loss is 2.186091184616089\n",
      "epoch: 4 step: 197, loss is 2.2933225631713867\n",
      "epoch: 4 step: 198, loss is 2.2617225646972656\n",
      "epoch: 4 step: 199, loss is 2.2477688789367676\n",
      "epoch: 4 step: 200, loss is 2.303192615509033\n",
      "epoch: 4 step: 201, loss is 2.2961342334747314\n",
      "epoch: 4 step: 202, loss is 2.318171977996826\n",
      "epoch: 4 step: 203, loss is 2.2556066513061523\n",
      "epoch: 4 step: 204, loss is 2.2622647285461426\n",
      "epoch: 4 step: 205, loss is 2.2964203357696533\n",
      "epoch: 4 step: 206, loss is 2.295192241668701\n",
      "epoch: 4 step: 207, loss is 2.275829792022705\n",
      "epoch: 4 step: 208, loss is 2.327310562133789\n",
      "epoch: 4 step: 209, loss is 2.323798656463623\n",
      "epoch: 4 step: 210, loss is 2.2557201385498047\n",
      "epoch: 4 step: 211, loss is 2.2463130950927734\n",
      "epoch: 4 step: 212, loss is 2.286557197570801\n",
      "epoch: 4 step: 213, loss is 2.3285322189331055\n",
      "epoch: 4 step: 214, loss is 2.255136489868164\n",
      "epoch: 4 step: 215, loss is 2.274779796600342\n",
      "epoch: 4 step: 216, loss is 2.3032867908477783\n",
      "epoch: 4 step: 217, loss is 2.2836875915527344\n",
      "epoch: 4 step: 218, loss is 2.2813899517059326\n",
      "epoch: 4 step: 219, loss is 2.279719352722168\n",
      "epoch: 4 step: 220, loss is 2.3161396980285645\n",
      "epoch: 4 step: 221, loss is 2.2865240573883057\n",
      "epoch: 4 step: 222, loss is 2.265852451324463\n",
      "epoch: 4 step: 223, loss is 2.2431700229644775\n",
      "epoch: 4 step: 224, loss is 2.290449619293213\n",
      "epoch: 4 step: 225, loss is 2.2137107849121094\n",
      "epoch: 4 step: 226, loss is 2.3004086017608643\n",
      "epoch: 4 step: 227, loss is 2.2234597206115723\n",
      "epoch: 5 step: 1, loss is 2.2798500061035156\n",
      "epoch: 5 step: 2, loss is 2.2830700874328613\n",
      "epoch: 5 step: 3, loss is 2.2333498001098633\n",
      "epoch: 5 step: 4, loss is 2.2509374618530273\n",
      "epoch: 5 step: 5, loss is 2.2723703384399414\n",
      "epoch: 5 step: 6, loss is 2.254256010055542\n",
      "epoch: 5 step: 7, loss is 2.269493341445923\n",
      "epoch: 5 step: 8, loss is 2.1738264560699463\n",
      "epoch: 5 step: 9, loss is 2.284209966659546\n",
      "epoch: 5 step: 10, loss is 2.264173984527588\n",
      "epoch: 5 step: 11, loss is 2.20166015625\n",
      "epoch: 5 step: 12, loss is 2.2922301292419434\n",
      "epoch: 5 step: 13, loss is 2.288060426712036\n",
      "epoch: 5 step: 14, loss is 2.271721839904785\n",
      "epoch: 5 step: 15, loss is 2.2838194370269775\n",
      "epoch: 5 step: 16, loss is 2.270331382751465\n",
      "epoch: 5 step: 17, loss is 2.3043277263641357\n",
      "epoch: 5 step: 18, loss is 2.2538583278656006\n",
      "epoch: 5 step: 19, loss is 2.297199010848999\n",
      "epoch: 5 step: 20, loss is 2.2501978874206543\n",
      "epoch: 5 step: 21, loss is 2.1976122856140137\n",
      "epoch: 5 step: 22, loss is 2.2922563552856445\n",
      "epoch: 5 step: 23, loss is 2.2812891006469727\n",
      "epoch: 5 step: 24, loss is 2.279204845428467\n",
      "epoch: 5 step: 25, loss is 2.212928533554077\n",
      "epoch: 5 step: 26, loss is 2.313185214996338\n",
      "epoch: 5 step: 27, loss is 2.313462257385254\n",
      "epoch: 5 step: 28, loss is 2.312105178833008\n",
      "epoch: 5 step: 29, loss is 2.2378880977630615\n",
      "epoch: 5 step: 30, loss is 2.274979591369629\n",
      "epoch: 5 step: 31, loss is 2.211977958679199\n",
      "epoch: 5 step: 32, loss is 2.2499897480010986\n",
      "epoch: 5 step: 33, loss is 2.30710506439209\n",
      "epoch: 5 step: 34, loss is 2.247408390045166\n",
      "epoch: 5 step: 35, loss is 2.2888948917388916\n",
      "epoch: 5 step: 36, loss is 2.26126766204834\n",
      "epoch: 5 step: 37, loss is 2.2915375232696533\n",
      "epoch: 5 step: 38, loss is 2.231463670730591\n",
      "epoch: 5 step: 39, loss is 2.2455055713653564\n",
      "epoch: 5 step: 40, loss is 2.2457733154296875\n",
      "epoch: 5 step: 41, loss is 2.2889623641967773\n",
      "epoch: 5 step: 42, loss is 2.256073236465454\n",
      "epoch: 5 step: 43, loss is 2.229454278945923\n",
      "epoch: 5 step: 44, loss is 2.24908185005188\n",
      "epoch: 5 step: 45, loss is 2.337366819381714\n",
      "epoch: 5 step: 46, loss is 2.3142647743225098\n",
      "epoch: 5 step: 47, loss is 2.2614965438842773\n",
      "epoch: 5 step: 48, loss is 2.26928448677063\n",
      "epoch: 5 step: 49, loss is 2.253535509109497\n",
      "epoch: 5 step: 50, loss is 2.251152992248535\n",
      "epoch: 5 step: 51, loss is 2.30745267868042\n",
      "epoch: 5 step: 52, loss is 2.3201704025268555\n",
      "epoch: 5 step: 53, loss is 2.232363700866699\n",
      "epoch: 5 step: 54, loss is 2.2865822315216064\n",
      "epoch: 5 step: 55, loss is 2.3415567874908447\n",
      "epoch: 5 step: 56, loss is 2.2819457054138184\n",
      "epoch: 5 step: 57, loss is 2.288461923599243\n",
      "epoch: 5 step: 58, loss is 2.3044464588165283\n",
      "epoch: 5 step: 59, loss is 2.26035213470459\n",
      "epoch: 5 step: 60, loss is 2.274308681488037\n",
      "epoch: 5 step: 61, loss is 2.279541492462158\n",
      "epoch: 5 step: 62, loss is 2.207315683364868\n",
      "epoch: 5 step: 63, loss is 2.271329641342163\n",
      "epoch: 5 step: 64, loss is 2.2468795776367188\n",
      "epoch: 5 step: 65, loss is 2.257256507873535\n",
      "epoch: 5 step: 66, loss is 2.328169822692871\n",
      "epoch: 5 step: 67, loss is 2.2713303565979004\n",
      "epoch: 5 step: 68, loss is 2.2832155227661133\n",
      "epoch: 5 step: 69, loss is 2.258756399154663\n",
      "epoch: 5 step: 70, loss is 2.2532694339752197\n",
      "epoch: 5 step: 71, loss is 2.3034470081329346\n",
      "epoch: 5 step: 72, loss is 2.22700572013855\n",
      "epoch: 5 step: 73, loss is 2.3170862197875977\n",
      "epoch: 5 step: 74, loss is 2.245767116546631\n",
      "epoch: 5 step: 75, loss is 2.285456895828247\n",
      "epoch: 5 step: 76, loss is 2.2636165618896484\n",
      "epoch: 5 step: 77, loss is 2.224952220916748\n",
      "epoch: 5 step: 78, loss is 2.2885959148406982\n",
      "epoch: 5 step: 79, loss is 2.2848119735717773\n",
      "epoch: 5 step: 80, loss is 2.271179676055908\n",
      "epoch: 5 step: 81, loss is 2.286301374435425\n",
      "epoch: 5 step: 82, loss is 2.3817028999328613\n",
      "epoch: 5 step: 83, loss is 2.297719955444336\n",
      "epoch: 5 step: 84, loss is 2.3086318969726562\n",
      "epoch: 5 step: 85, loss is 2.288517475128174\n",
      "epoch: 5 step: 86, loss is 2.2767066955566406\n",
      "epoch: 5 step: 87, loss is 2.2468647956848145\n",
      "epoch: 5 step: 88, loss is 2.279378652572632\n",
      "epoch: 5 step: 89, loss is 2.261155605316162\n",
      "epoch: 5 step: 90, loss is 2.274545907974243\n",
      "epoch: 5 step: 91, loss is 2.2520580291748047\n",
      "epoch: 5 step: 92, loss is 2.261204957962036\n",
      "epoch: 5 step: 93, loss is 2.2291901111602783\n",
      "epoch: 5 step: 94, loss is 2.269613027572632\n",
      "epoch: 5 step: 95, loss is 2.289334535598755\n",
      "epoch: 5 step: 96, loss is 2.2901010513305664\n",
      "epoch: 5 step: 97, loss is 2.302626609802246\n",
      "epoch: 5 step: 98, loss is 2.325563669204712\n",
      "epoch: 5 step: 99, loss is 2.23582124710083\n",
      "epoch: 5 step: 100, loss is 2.2832608222961426\n",
      "epoch: 5 step: 101, loss is 2.324685573577881\n",
      "epoch: 5 step: 102, loss is 2.302945613861084\n",
      "epoch: 5 step: 103, loss is 2.2812423706054688\n",
      "epoch: 5 step: 104, loss is 2.31864857673645\n",
      "epoch: 5 step: 105, loss is 2.267186164855957\n",
      "epoch: 5 step: 106, loss is 2.2850003242492676\n",
      "epoch: 5 step: 107, loss is 2.286497116088867\n",
      "epoch: 5 step: 108, loss is 2.2478065490722656\n",
      "epoch: 5 step: 109, loss is 2.2421393394470215\n",
      "epoch: 5 step: 110, loss is 2.266864061355591\n",
      "epoch: 5 step: 111, loss is 2.340583562850952\n",
      "epoch: 5 step: 112, loss is 2.2314891815185547\n",
      "epoch: 5 step: 113, loss is 2.224538564682007\n",
      "epoch: 5 step: 114, loss is 2.2504332065582275\n",
      "epoch: 5 step: 115, loss is 2.239535331726074\n",
      "epoch: 5 step: 116, loss is 2.2672133445739746\n",
      "epoch: 5 step: 117, loss is 2.259080648422241\n",
      "epoch: 5 step: 118, loss is 2.2746734619140625\n",
      "epoch: 5 step: 119, loss is 2.2776529788970947\n",
      "epoch: 5 step: 120, loss is 2.240163564682007\n",
      "epoch: 5 step: 121, loss is 2.2779126167297363\n",
      "epoch: 5 step: 122, loss is 2.27939772605896\n",
      "epoch: 5 step: 123, loss is 2.19812273979187\n",
      "epoch: 5 step: 124, loss is 2.2983906269073486\n",
      "epoch: 5 step: 125, loss is 2.261742115020752\n",
      "epoch: 5 step: 126, loss is 2.3101847171783447\n",
      "epoch: 5 step: 127, loss is 2.22037935256958\n",
      "epoch: 5 step: 128, loss is 2.2676658630371094\n",
      "epoch: 5 step: 129, loss is 2.2630977630615234\n",
      "epoch: 5 step: 130, loss is 2.2648890018463135\n",
      "epoch: 5 step: 131, loss is 2.284236431121826\n",
      "epoch: 5 step: 132, loss is 2.2935094833374023\n",
      "epoch: 5 step: 133, loss is 2.3291430473327637\n",
      "epoch: 5 step: 134, loss is 2.2309012413024902\n",
      "epoch: 5 step: 135, loss is 2.2378251552581787\n",
      "epoch: 5 step: 136, loss is 2.2759995460510254\n",
      "epoch: 5 step: 137, loss is 2.27536678314209\n",
      "epoch: 5 step: 138, loss is 2.2695624828338623\n",
      "epoch: 5 step: 139, loss is 2.2500855922698975\n",
      "epoch: 5 step: 140, loss is 2.301734209060669\n",
      "epoch: 5 step: 141, loss is 2.2422096729278564\n",
      "epoch: 5 step: 142, loss is 2.182288408279419\n",
      "epoch: 5 step: 143, loss is 2.286099672317505\n",
      "epoch: 5 step: 144, loss is 2.251925230026245\n",
      "epoch: 5 step: 145, loss is 2.3184731006622314\n",
      "epoch: 5 step: 146, loss is 2.21046781539917\n",
      "epoch: 5 step: 147, loss is 2.3291475772857666\n",
      "epoch: 5 step: 148, loss is 2.279778480529785\n",
      "epoch: 5 step: 149, loss is 2.321021556854248\n",
      "epoch: 5 step: 150, loss is 2.2898612022399902\n",
      "epoch: 5 step: 151, loss is 2.2445366382598877\n",
      "epoch: 5 step: 152, loss is 2.285111427307129\n",
      "epoch: 5 step: 153, loss is 2.2884087562561035\n",
      "epoch: 5 step: 154, loss is 2.2998228073120117\n",
      "epoch: 5 step: 155, loss is 2.3531711101531982\n",
      "epoch: 5 step: 156, loss is 2.27056884765625\n",
      "epoch: 5 step: 157, loss is 2.3112270832061768\n",
      "epoch: 5 step: 158, loss is 2.2484281063079834\n",
      "epoch: 5 step: 159, loss is 2.2190780639648438\n",
      "epoch: 5 step: 160, loss is 2.3188393115997314\n",
      "epoch: 5 step: 161, loss is 2.262422800064087\n",
      "epoch: 5 step: 162, loss is 2.263742685317993\n",
      "epoch: 5 step: 163, loss is 2.2628133296966553\n",
      "epoch: 5 step: 164, loss is 2.251818895339966\n",
      "epoch: 5 step: 165, loss is 2.264657735824585\n",
      "epoch: 5 step: 166, loss is 2.2570276260375977\n",
      "epoch: 5 step: 167, loss is 2.2424378395080566\n",
      "epoch: 5 step: 168, loss is 2.256765127182007\n",
      "epoch: 5 step: 169, loss is 2.217146396636963\n",
      "epoch: 5 step: 170, loss is 2.275430679321289\n",
      "epoch: 5 step: 171, loss is 2.3098058700561523\n",
      "epoch: 5 step: 172, loss is 2.2819066047668457\n",
      "epoch: 5 step: 173, loss is 2.3015201091766357\n",
      "epoch: 5 step: 174, loss is 2.1797986030578613\n",
      "epoch: 5 step: 175, loss is 2.301924705505371\n",
      "epoch: 5 step: 176, loss is 2.281036615371704\n",
      "epoch: 5 step: 177, loss is 2.2532620429992676\n",
      "epoch: 5 step: 178, loss is 2.316854476928711\n",
      "epoch: 5 step: 179, loss is 2.203234910964966\n",
      "epoch: 5 step: 180, loss is 2.2711892127990723\n",
      "epoch: 5 step: 181, loss is 2.2180581092834473\n",
      "epoch: 5 step: 182, loss is 2.3270232677459717\n",
      "epoch: 5 step: 183, loss is 2.2830312252044678\n",
      "epoch: 5 step: 184, loss is 2.3272147178649902\n",
      "epoch: 5 step: 185, loss is 2.2567622661590576\n",
      "epoch: 5 step: 186, loss is 2.299126148223877\n",
      "epoch: 5 step: 187, loss is 2.325939178466797\n",
      "epoch: 5 step: 188, loss is 2.3081471920013428\n",
      "epoch: 5 step: 189, loss is 2.2904083728790283\n",
      "epoch: 5 step: 190, loss is 2.248420476913452\n",
      "epoch: 5 step: 191, loss is 2.28918719291687\n",
      "epoch: 5 step: 192, loss is 2.280961036682129\n",
      "epoch: 5 step: 193, loss is 2.299994707107544\n",
      "epoch: 5 step: 194, loss is 2.3512511253356934\n",
      "epoch: 5 step: 195, loss is 2.234452962875366\n",
      "epoch: 5 step: 196, loss is 2.251976728439331\n",
      "epoch: 5 step: 197, loss is 2.2977235317230225\n",
      "epoch: 5 step: 198, loss is 2.2912685871124268\n",
      "epoch: 5 step: 199, loss is 2.329319715499878\n",
      "epoch: 5 step: 200, loss is 2.3133106231689453\n",
      "epoch: 5 step: 201, loss is 2.3631174564361572\n",
      "epoch: 5 step: 202, loss is 2.3183701038360596\n",
      "epoch: 5 step: 203, loss is 2.283191680908203\n",
      "epoch: 5 step: 204, loss is 2.3164093494415283\n",
      "epoch: 5 step: 205, loss is 2.234220027923584\n",
      "epoch: 5 step: 206, loss is 2.2267162799835205\n",
      "epoch: 5 step: 207, loss is 2.2512693405151367\n",
      "epoch: 5 step: 208, loss is 2.231722354888916\n",
      "epoch: 5 step: 209, loss is 2.3419687747955322\n",
      "epoch: 5 step: 210, loss is 2.2838475704193115\n",
      "epoch: 5 step: 211, loss is 2.327868700027466\n",
      "epoch: 5 step: 212, loss is 2.3378772735595703\n",
      "epoch: 5 step: 213, loss is 2.2848451137542725\n",
      "epoch: 5 step: 214, loss is 2.3156371116638184\n",
      "epoch: 5 step: 215, loss is 2.250188112258911\n",
      "epoch: 5 step: 216, loss is 2.335857391357422\n",
      "epoch: 5 step: 217, loss is 2.306882619857788\n",
      "epoch: 5 step: 218, loss is 2.283817768096924\n",
      "epoch: 5 step: 219, loss is 2.246276378631592\n",
      "epoch: 5 step: 220, loss is 2.2985057830810547\n",
      "epoch: 5 step: 221, loss is 2.304457426071167\n",
      "epoch: 5 step: 222, loss is 2.25978422164917\n",
      "epoch: 5 step: 223, loss is 2.2184641361236572\n",
      "epoch: 5 step: 224, loss is 2.3045880794525146\n",
      "epoch: 5 step: 225, loss is 2.3388075828552246\n",
      "epoch: 5 step: 226, loss is 2.2550718784332275\n",
      "epoch: 5 step: 227, loss is 2.292391061782837\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] ME(7418:129632434239040,MainProcess):2025-04-07-15:48:35.922.000 [mindspore/dataset/core/validator_helpers.py:744] 'Resize' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Resize' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(7418:129632434239040,MainProcess):2025-04-07-15:48:35.923.000 [mindspore/dataset/core/validator_helpers.py:744] 'Rescale' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Rescale' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(7418:129632434239040,MainProcess):2025-04-07-15:48:35.923.000 [mindspore/dataset/core/validator_helpers.py:744] 'Rescale' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Rescale' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(7418:129632434239040,MainProcess):2025-04-07-15:48:35.923.000 [mindspore/dataset/core/validator_helpers.py:744] 'HWC2CHW' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'HWC2CHW' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(7418:129632434239040,MainProcess):2025-04-07-15:48:35.924.000 [mindspore/dataset/core/validator_helpers.py:744] 'TypeCast' from mindspore.dataset.transforms.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'TypeCast' from mindspore.dataset.transforms instead.\n",
      "[WARNING] ME(7418:129632434239040,MainProcess):2025-04-07-15:48:36.143.000 [mindspore/dataset/core/validator_helpers.py:744] 'Resize' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Resize' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(7418:129632434239040,MainProcess):2025-04-07-15:48:36.144.000 [mindspore/dataset/core/validator_helpers.py:744] 'Rescale' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Rescale' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(7418:129632434239040,MainProcess):2025-04-07-15:48:36.144.000 [mindspore/dataset/core/validator_helpers.py:744] 'Rescale' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Rescale' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(7418:129632434239040,MainProcess):2025-04-07-15:48:36.144.000 [mindspore/dataset/core/validator_helpers.py:744] 'HWC2CHW' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'HWC2CHW' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(7418:129632434239040,MainProcess):2025-04-07-15:48:36.144.000 [mindspore/dataset/core/validator_helpers.py:744] 'TypeCast' from mindspore.dataset.transforms.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'TypeCast' from mindspore.dataset.transforms instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== USPS Accuracy: {'Accuracy': np.float64(0.17842741935483872)} ==============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ERROR] CORE(7418,75e66731b640,python):2025-04-07-15:48:36.213.747 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_7418/2100403872.py]\n",
      "[WARNING] CORE(7418,75e66731b640,python):2025-04-07-15:48:36.213.759 [mindspore/core/utils/info.cc:121] ToString] The file '/tmp/ipykernel_7418/2100403872.py' may not exists.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 32 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] ME(7418:129632434239040,MainProcess):2025-04-07-15:48:37.125.000 [mindspore/nn/layer/basic.py:176] For Dropout, this parameter `keep_prob` will be deprecated, please use `p` instead.\n",
      "[CRITICAL] ME(7418:129632434239040,MainProcess):2025-04-07-15:48:37.128.000 [mindspore/train/serialization.py:257] Failed to combine the net and the parameters for param conv1.weight.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 18.75%\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "For 'load_param_into_net', conv1.weight in the argument 'net' should have the same shape as conv1.weight in the argument 'parameter_dict'. But got its shape (32, 1, 5, 5) in the argument 'net' and shape (6, 1, 5, 5) in the argument 'parameter_dict'.May you need to check whether the checkpoint you loaded is correct or the batch size and so on in the 'net' and 'parameter_dict' are same.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[42], line 35\u001B[0m\n\u001B[1;32m     33\u001B[0m \u001B[38;5;66;03m# 执行主函数\u001B[39;00m\n\u001B[1;32m     34\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;18m__name__\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__main__\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m---> 35\u001B[0m     \u001B[43mmain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[42], line 19\u001B[0m, in \u001B[0;36mmain\u001B[0;34m()\u001B[0m\n\u001B[1;32m     17\u001B[0m \u001B[38;5;66;03m# 使用改进的模型（可选）\u001B[39;00m\n\u001B[1;32m     18\u001B[0m improved_network \u001B[38;5;241m=\u001B[39m ImprovedLeNet5()\n\u001B[0;32m---> 19\u001B[0m improved_model \u001B[38;5;241m=\u001B[39m \u001B[43mtransfer_learning_usps\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimproved_network\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpretrained_model_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43musps_data_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepoch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m5\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     21\u001B[0m \u001B[38;5;66;03m# 测试遗传算法生成的图像\u001B[39;00m\n\u001B[1;32m     22\u001B[0m test_genetic_algo_images(model, genetic_images_path)\n",
      "Cell \u001B[0;32mIn[39], line 12\u001B[0m, in \u001B[0;36mtransfer_learning_usps\u001B[0;34m(network, pretrained_ckpt_path, usps_data_path, epoch_size)\u001B[0m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mtransfer_learning_usps\u001B[39m(network, pretrained_ckpt_path, usps_data_path, epoch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m5\u001B[39m):\n\u001B[1;32m     10\u001B[0m     \u001B[38;5;66;03m# 加载预训练的LeNet5网络参数\u001B[39;00m\n\u001B[1;32m     11\u001B[0m     param_dict \u001B[38;5;241m=\u001B[39m load_checkpoint(pretrained_ckpt_path)\n\u001B[0;32m---> 12\u001B[0m     \u001B[43mload_param_into_net\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnetwork\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparam_dict\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     14\u001B[0m     \u001B[38;5;66;03m# 微调设置 - 可以选择冻结部分层\u001B[39;00m\n\u001B[1;32m     15\u001B[0m     \u001B[38;5;66;03m# 仅训练全连接层\u001B[39;00m\n\u001B[1;32m     16\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m param \u001B[38;5;129;01min\u001B[39;00m network\u001B[38;5;241m.\u001B[39mtrainable_params():\n",
      "File \u001B[0;32m~/.conda/envs/homework/lib/python3.10/site-packages/mindspore/train/serialization.py:1836\u001B[0m, in \u001B[0;36mload_param_into_net\u001B[0;34m(net, parameter_dict, strict_load, remove_redundancy)\u001B[0m\n\u001B[1;32m   1834\u001B[0m     \u001B[38;5;28;01mcontinue\u001B[39;00m\n\u001B[1;32m   1835\u001B[0m new_param \u001B[38;5;241m=\u001B[39m parameter_dict[param\u001B[38;5;241m.\u001B[39mname]\n\u001B[0;32m-> 1836\u001B[0m \u001B[43m_update_param\u001B[49m\u001B[43m(\u001B[49m\u001B[43mparam\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnew_param\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstrict_load\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1837\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(param, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minit_param\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m param\u001B[38;5;241m.\u001B[39minit_param:\n\u001B[1;32m   1838\u001B[0m     param\u001B[38;5;241m.\u001B[39minit_param \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "File \u001B[0;32m~/.conda/envs/homework/lib/python3.10/site-packages/mindspore/train/serialization.py:263\u001B[0m, in \u001B[0;36m_update_param\u001B[0;34m(param, new_param, strict_load)\u001B[0m\n\u001B[1;32m    257\u001B[0m         logger\u001B[38;5;241m.\u001B[39mcritical(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFailed to combine the net and the parameters for param \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, param\u001B[38;5;241m.\u001B[39mname)\n\u001B[1;32m    258\u001B[0m         msg \u001B[38;5;241m=\u001B[39m (\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFor \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mload_param_into_net\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mparam\u001B[38;5;241m.\u001B[39mname\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m in the argument \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mnet\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m should have the same shape \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    259\u001B[0m                \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mas \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mparam\u001B[38;5;241m.\u001B[39mname\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m in the argument \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mparameter_dict\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m. But got its shape \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mparam\u001B[38;5;241m.\u001B[39mdata\u001B[38;5;241m.\u001B[39mshape\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m in\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    260\u001B[0m                \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m the argument \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mnet\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m and shape \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mnew_param\u001B[38;5;241m.\u001B[39mdata\u001B[38;5;241m.\u001B[39mshape\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m in the argument \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mparameter_dict\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    261\u001B[0m                \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMay you need to check whether the checkpoint you loaded is correct or the batch size and \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    262\u001B[0m                \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mso on in the \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mnet\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m and \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mparameter_dict\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m are same.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m--> 263\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(msg)\n\u001B[1;32m    265\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m param\u001B[38;5;241m.\u001B[39mdata\u001B[38;5;241m.\u001B[39mdtype \u001B[38;5;241m!=\u001B[39m new_param\u001B[38;5;241m.\u001B[39mdata\u001B[38;5;241m.\u001B[39mdtype:\n\u001B[1;32m    266\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m _type_convert(param, new_param, strict_load):\n",
      "\u001B[0;31mRuntimeError\u001B[0m: For 'load_param_into_net', conv1.weight in the argument 'net' should have the same shape as conv1.weight in the argument 'parameter_dict'. But got its shape (32, 1, 5, 5) in the argument 'net' and shape (6, 1, 5, 5) in the argument 'parameter_dict'.May you need to check whether the checkpoint you loaded is correct or the batch size and so on in the 'net' and 'parameter_dict' are same."
     ]
    }
   ],
   "execution_count": 42
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
