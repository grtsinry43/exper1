{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-03T06:41:19.559908Z",
     "start_time": "2025-04-03T06:41:19.147542Z"
    }
   },
   "source": [
    "from mindspore import context\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import mindspore.dataset as ds\n",
    "\n",
    "context.set_context(mode=context.GRAPH_MODE,\n",
    "                    device_target=\"CPU\")  # Windows version, set to use CPU for graph calculation\n",
    "train_data_path = \"./data/train\"\n",
    "test_data_path = \"./data/test\"\n",
    "mnist_ds = ds.MnistDataset(train_data_path)  # Load training dataset\n",
    "print('The type of mnist_ds:', type(mnist_ds))\n",
    "print(\"Number of pictures contained in the mnist_ds：\", mnist_ds.get_dataset_size())  # 60000 pictures in total\n",
    "\n",
    "dic_ds = mnist_ds.create_dict_iterator()  # Convert dataset to dictionary type\n",
    "item = dic_ds.__next__()\n",
    "img = item[\"image\"].asnumpy()\n",
    "label = item[\"label\"].asnumpy()\n",
    "\n",
    "print(\"The item of mnist_ds:\",\n",
    "      item.keys())  # Take a single data to view the data structure, including two keys, image and label\n",
    "print(\"Tensor of image in item:\", img.shape)  # View the tensor of image (28,28,1)\n",
    "print(\"The label of item:\", label)\n",
    "\n",
    "plt.imshow(np.squeeze(img))\n",
    "plt.title(\"number:%s\" % item[\"label\"])\n",
    "plt.show()"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] ME(30262:132595847530048,MainProcess):2025-04-03-14:41:19.151.000 [mindspore/context.py:1335] For 'context.set_context', the parameter 'device_target' will be deprecated and removed in a future version. Please use the api mindspore.set_device() instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The type of mnist_ds: <class 'mindspore.dataset.engine.datasets_vision.MnistDataset'>\n",
      "Number of pictures contained in the mnist_ds： 60000\n",
      "The item of mnist_ds: dict_keys(['image', 'label'])\n",
      "Tensor of image in item: (28, 28, 1)\n",
      "The label of item: 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T06:43:33.907219Z",
     "start_time": "2025-04-03T06:43:33.897770Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import mindspore.dataset.vision.c_transforms as CV\n",
    "import mindspore.dataset.transforms.c_transforms as C\n",
    "from mindspore.dataset.vision import Inter\n",
    "from mindspore.common import dtype as mstype\n",
    "\n",
    "\n",
    "def create_dataset(data_path, batch_size=32, repeat_size=1,\n",
    "                   num_parallel_workers=1):\n",
    "    \"\"\" create dataset for train or test\n",
    "    Args:\n",
    "        data_path: Data path\n",
    "        batch_size: The number of data records in each group\n",
    "        repeat_size: The number of replicated data records\n",
    "        num_parallel_workers: The number of parallel workers\n",
    "    \"\"\"\n",
    "    # define dataset\n",
    "    mnist_ds = ds.MnistDataset(data_path)\n",
    "\n",
    "    # Define some parameters needed for data enhancement and rough justification\n",
    "    resize_height, resize_width = 32, 32\n",
    "    rescale = 1.0 / 255.0\n",
    "    shift = 0.0\n",
    "    rescale_nml = 1 / 0.3081\n",
    "    shift_nml = -1 * 0.1307 / 0.3081\n",
    "\n",
    "    # According to the parameters, generate the corresponding data enhancement method\n",
    "    resize_op = CV.Resize((resize_height, resize_width),\n",
    "                          interpolation=Inter.LINEAR)  # Resize images to (32, 32) by bilinear interpolation\n",
    "    rescale_nml_op = CV.Rescale(rescale_nml, shift_nml)  # normalize images\n",
    "    rescale_op = CV.Rescale(rescale, shift)  # rescale images\n",
    "    hwc2chw_op = CV.HWC2CHW()  # change shape from (height, width, channel) to (channel, height, width) to fit network.\n",
    "    type_cast_op = C.TypeCast(mstype.int32)  # change data type of label to int32 to fit network\n",
    "\n",
    "    # Using map () to apply operations to a dataset\n",
    "    mnist_ds = mnist_ds.map(input_columns=\"label\", operations=type_cast_op, num_parallel_workers=num_parallel_workers)\n",
    "    mnist_ds = mnist_ds.map(input_columns=\"image\", operations=resize_op, num_parallel_workers=num_parallel_workers)\n",
    "    mnist_ds = mnist_ds.map(input_columns=\"image\", operations=rescale_op, num_parallel_workers=num_parallel_workers)\n",
    "    mnist_ds = mnist_ds.map(input_columns=\"image\", operations=rescale_nml_op, num_parallel_workers=num_parallel_workers)\n",
    "    mnist_ds = mnist_ds.map(input_columns=\"image\", operations=hwc2chw_op, num_parallel_workers=num_parallel_workers)\n",
    "    # Process the generated dataset\n",
    "    buffer_size = 10000\n",
    "    mnist_ds = mnist_ds.shuffle(buffer_size=buffer_size)  # 10000 as in LeNet train script\n",
    "    mnist_ds = mnist_ds.batch(batch_size, drop_remainder=True)\n",
    "    mnist_ds = mnist_ds.repeat(repeat_size)\n",
    "\n",
    "    return mnist_ds"
   ],
   "id": "ae5c545dc5326151",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T06:43:47.270319Z",
     "start_time": "2025-04-03T06:43:47.260752Z"
    }
   },
   "cell_type": "code",
   "source": [
    "datas = create_dataset(train_data_path)  # Process the train dataset\n",
    "print('Number of groups in the dataset:', datas.get_dataset_size())  # Number of query dataset groups"
   ],
   "id": "fac95f1215bf418a",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] ME(30262:132595847530048,MainProcess):2025-04-03-14:43:47.262.000 [mindspore/dataset/core/validator_helpers.py:744] 'Resize' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Resize' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(30262:132595847530048,MainProcess):2025-04-03-14:43:47.263.000 [mindspore/dataset/core/validator_helpers.py:744] 'Rescale' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Rescale' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(30262:132595847530048,MainProcess):2025-04-03-14:43:47.264.000 [mindspore/dataset/core/validator_helpers.py:744] 'Rescale' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Rescale' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(30262:132595847530048,MainProcess):2025-04-03-14:43:47.265.000 [mindspore/dataset/core/validator_helpers.py:744] 'HWC2CHW' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'HWC2CHW' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(30262:132595847530048,MainProcess):2025-04-03-14:43:47.265.000 [mindspore/dataset/core/validator_helpers.py:744] 'TypeCast' from mindspore.dataset.transforms.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'TypeCast' from mindspore.dataset.transforms instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of groups in the dataset: 1875\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T06:44:20.935563Z",
     "start_time": "2025-04-03T06:44:20.401702Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data = datas.create_dict_iterator().__next__()  # Take a set of datasets\n",
    "print(data.keys())\n",
    "images = data[\"image\"].asnumpy()  # Take out the image data in this dataset\n",
    "labels = data[\"label\"].asnumpy()  # Take out the label (subscript) of this data set\n",
    "print('Tensor of image:', images.shape)  # Query the tensor of images in each dataset (32,1,32,32)\n",
    "print('labels:', labels)"
   ],
   "id": "10ccb9f7c27abf33",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['image', 'label'])\n",
      "Tensor of image: (32, 1, 32, 32)\n",
      "labels: [3 9 2 0 7 0 7 4 6 4 7 3 7 3 8 1 9 9 0 7 5 4 1 2 6 7 0 3 2 9 6 4]\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T06:44:39.918789Z",
     "start_time": "2025-04-03T06:44:38.554914Z"
    }
   },
   "cell_type": "code",
   "source": [
    "count = 1\n",
    "for i in images:\n",
    "    plt.subplot(4, 8, count)\n",
    "    plt.imshow(np.squeeze(i))\n",
    "    plt.title('num:%s' % labels[count - 1])\n",
    "    plt.xticks([])\n",
    "    count += 1\n",
    "    plt.axis(\"off\")\n",
    "plt.show()  # Print a total of 32 pictures in the group"
   ],
   "id": "305ec7daaacd26d4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 32 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T06:45:52.254927Z",
     "start_time": "2025-04-03T06:45:52.248158Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import mindspore.nn as nn\n",
    "from mindspore.common.initializer import TruncatedNormal\n",
    "\n",
    "\n",
    "# Initialize 2D convolution function\n",
    "def conv(in_channels, out_channels, kernel_size, stride=1, padding=0):\n",
    "    \"\"\"Conv layer weight initial.\"\"\"\n",
    "    weight = weight_variable()\n",
    "    return nn.Conv2d(in_channels, out_channels,\n",
    "                     kernel_size=kernel_size, stride=stride, padding=padding,\n",
    "                     weight_init=weight, has_bias=False, pad_mode=\"valid\")\n",
    "\n",
    "\n",
    "# Initialize full connection layer\n",
    "def fc_with_initialize(input_channels, out_channels):\n",
    "    \"\"\"Fc layer weight initial.\"\"\"\n",
    "    weight = weight_variable()\n",
    "    bias = weight_variable()\n",
    "    return nn.Dense(input_channels, out_channels, weight, bias)\n",
    "\n",
    "\n",
    "# Set truncated normal distribution\n",
    "def weight_variable():\n",
    "    \"\"\"Weight initial.\"\"\"\n",
    "    return TruncatedNormal(0.02)"
   ],
   "id": "eeceb68c50aade08",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T06:47:27.614370Z",
     "start_time": "2025-04-03T06:47:27.602931Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class LeNet5(nn.Cell):\n",
    "    \"\"\"Lenet network structure.\"\"\"\n",
    "\n",
    "    # define the operator required\n",
    "    def __init__(self):\n",
    "        super(LeNet5, self).__init__()\n",
    "        self.batch_size = 32  # 32 pictures in each group\n",
    "        self.conv1 = conv(1, 6,\n",
    "                          5)  # Convolution layer 1, 1 channel input (1 Figure), 6 channel output (6 figures), convolution core 5 * 5\n",
    "        self.conv2 = conv(6, 16, 5)  # Convolution layer 2,6-channel input, 16 channel output, convolution kernel 5 * 5\n",
    "        self.fc1 = fc_with_initialize(16 * 5 * 5, 120)\n",
    "        self.fc2 = fc_with_initialize(120, 84)\n",
    "        self.fc3 = fc_with_initialize(84, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.max_pool2d = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "    # use the preceding operators to construct networks\n",
    "    def construct(self, x):\n",
    "        x = self.conv1(x)  # 1*32*32-->6*28*28\n",
    "        x = self.relu(x)  # 6*28*28-->6*14*14\n",
    "        x = self.max_pool2d(x)  # Pool layer\n",
    "        x = self.conv2(x)  # Convolution layer\n",
    "        x = self.relu(x)  # Function excitation layer\n",
    "        x = self.max_pool2d(x)  # Pool layer\n",
    "        x = self.flatten(x)  # Dimensionality reduction\n",
    "        x = self.fc1(x)  # Full connection\n",
    "        x = self.relu(x)  # Function excitation layer\n",
    "        x = self.fc2(x)  # Full connection\n",
    "        x = self.relu(x)  # Function excitation layer\n",
    "        x = self.fc3(x)  # Full connection\n",
    "        return x"
   ],
   "id": "ab061c42419732bb",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T06:48:52.144940Z",
     "start_time": "2025-04-03T06:48:52.126357Z"
    }
   },
   "cell_type": "code",
   "source": [
    "network = LeNet5()\n",
    "print(network)"
   ],
   "id": "6ebe0e11e661b22d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LeNet5<\n",
      "  (conv1): Conv2d<input_channels=1, output_channels=6, kernel_size=(5, 5), stride=(1, 1), pad_mode=valid, padding=0, dilation=(1, 1), group=1, has_bias=False, weight_init=<mindspore.common.initializer.TruncatedNormal object at 0x7897c4321ea0>, bias_init=None, format=NCHW>\n",
      "  (conv2): Conv2d<input_channels=6, output_channels=16, kernel_size=(5, 5), stride=(1, 1), pad_mode=valid, padding=0, dilation=(1, 1), group=1, has_bias=False, weight_init=<mindspore.common.initializer.TruncatedNormal object at 0x7897d7a174f0>, bias_init=None, format=NCHW>\n",
      "  (fc1): Dense<input_channels=400, output_channels=120, has_bias=True>\n",
      "  (fc2): Dense<input_channels=120, output_channels=84, has_bias=True>\n",
      "  (fc3): Dense<input_channels=84, output_channels=10, has_bias=True>\n",
      "  (relu): ReLU<>\n",
      "  (max_pool2d): MaxPool2d<kernel_size=2, stride=2, pad_mode=VALID>\n",
      "  (flatten): Flatten<>\n",
      "  >\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T06:49:10.443705Z",
     "start_time": "2025-04-03T06:49:10.435883Z"
    }
   },
   "cell_type": "code",
   "source": [
    "param = network.trainable_params()\n",
    "param"
   ],
   "id": "9019e96f6bd58040",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter (name=conv1.weight, shape=(6, 1, 5, 5), dtype=Float32, requires_grad=True),\n",
       " Parameter (name=conv2.weight, shape=(16, 6, 5, 5), dtype=Float32, requires_grad=True),\n",
       " Parameter (name=fc1.weight, shape=(120, 400), dtype=Float32, requires_grad=True),\n",
       " Parameter (name=fc1.bias, shape=(120,), dtype=Float32, requires_grad=True),\n",
       " Parameter (name=fc2.weight, shape=(84, 120), dtype=Float32, requires_grad=True),\n",
       " Parameter (name=fc2.bias, shape=(84,), dtype=Float32, requires_grad=True),\n",
       " Parameter (name=fc3.weight, shape=(10, 84), dtype=Float32, requires_grad=True),\n",
       " Parameter (name=fc3.bias, shape=(10,), dtype=Float32, requires_grad=True)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T06:50:15.832675Z",
     "start_time": "2025-04-03T06:50:15.825960Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Training and testing related modules\n",
    "import argparse\n",
    "import os\n",
    "from mindspore import Tensor\n",
    "from mindspore.train.serialization import load_checkpoint, load_param_into_net\n",
    "from mindspore.train.callback import ModelCheckpoint, CheckpointConfig, LossMonitor,Callback\n",
    "from mindspore.train import Model\n",
    "from mindspore.nn.metrics import Accuracy\n",
    "from mindspore.nn.loss import SoftmaxCrossEntropyWithLogits\n",
    "\n",
    "def train_net(model, epoch_size, mnist_path, repeat_size, ckpoint_cb, step_loss_info):\n",
    "    \"\"\"Define the training method.\"\"\"\n",
    "    print(\"============== Starting Training ==============\")\n",
    "    # load training dataset\n",
    "    ds_train = create_dataset(os.path.join(mnist_path, \"train\"), 32, repeat_size)\n",
    "    model.train(epoch_size, ds_train, callbacks=[ckpoint_cb, LossMonitor(), step_loss_info], dataset_sink_mode=True)"
   ],
   "id": "de680f79303dcb0f",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T06:51:16.565717Z",
     "start_time": "2025-04-03T06:51:16.562056Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Custom callback function\n",
    "class Step_loss_info(Callback):\n",
    "    def step_end(self, run_context):\n",
    "        cb_params = run_context.original_args()\n",
    "        # step_ Loss dictionary for saving loss value and step number information\n",
    "        step_loss[\"loss_value\"].append(str(cb_params.net_outputs))\n",
    "        step_loss[\"step\"].append(str(cb_params.cur_step_num))"
   ],
   "id": "70efac08893fd649",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T06:53:07.116220Z",
     "start_time": "2025-04-03T06:51:44.513024Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "if os.name == \"nt\":\n",
    "    os.system('del/f/s/q *.ckpt *.meta')# Clean up old run files before in Windows\n",
    "else:\n",
    "    os.system('rm -f *.ckpt *.meta *.pb')# Clean up old run files before in Linux\n",
    "\n",
    "lr = 0.01 # learning rate\n",
    "momentum = 0.9 #\n",
    "\n",
    "# create the network\n",
    "network = LeNet5()\n",
    "\n",
    "# define the optimizer\n",
    "net_opt = nn.Momentum(network.trainable_params(), lr, momentum)\n",
    "\n",
    "\n",
    "# define the loss function\n",
    "net_loss = SoftmaxCrossEntropyWithLogits(sparse=True, reduction='mean')\n",
    "# define the model\n",
    "model = Model(network, net_loss, net_opt, metrics={\"Accuracy\": Accuracy()} )\n",
    "epoch_size = 10\n",
    "mnist_path = \"./data\"\n",
    "\n",
    "config_ck = CheckpointConfig(save_checkpoint_steps=125, keep_checkpoint_max=16)\n",
    "# save the network model and parameters for subsequence fine-tuning\n",
    "\n",
    "ckpoint_cb = ModelCheckpoint(prefix=\"checkpoint_lenet\", config=config_ck)\n",
    "# group layers into an object with training and evaluation features\n",
    "step_loss = {\"step\": [], \"loss_value\": []}\n",
    "# step_ Loss dictionary for saving loss value and step number information\n",
    "step_loss_info = Step_loss_info()\n",
    "# save the steps and loss value\n",
    "repeat_size = 1\n",
    "train_net(model, epoch_size, mnist_path, repeat_size, ckpoint_cb, step_loss_info)"
   ],
   "id": "78f652e02c2bd6d9",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] ME(30262:132595847530048,MainProcess):2025-04-03-14:51:44.544.000 [mindspore/dataset/core/validator_helpers.py:744] 'Resize' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Resize' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(30262:132595847530048,MainProcess):2025-04-03-14:51:44.544.000 [mindspore/dataset/core/validator_helpers.py:744] 'Rescale' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Rescale' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(30262:132595847530048,MainProcess):2025-04-03-14:51:44.545.000 [mindspore/dataset/core/validator_helpers.py:744] 'Rescale' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Rescale' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(30262:132595847530048,MainProcess):2025-04-03-14:51:44.545.000 [mindspore/dataset/core/validator_helpers.py:744] 'HWC2CHW' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'HWC2CHW' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(30262:132595847530048,MainProcess):2025-04-03-14:51:44.546.000 [mindspore/dataset/core/validator_helpers.py:744] 'TypeCast' from mindspore.dataset.transforms.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'TypeCast' from mindspore.dataset.transforms instead.\n",
      "[WARNING] ME(30262:132595847530048,MainProcess):2025-04-03-14:51:44.549.000 [mindspore/train/model.py:1419] For Step_loss_info callback, {'step_end'} methods may not be supported in later version, Use methods prefixed with 'on_train' or 'on_eval' instead when using customized callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== Starting Training ==============\n",
      "epoch: 1 step: 1, loss is 2.3007149696350098\n",
      "epoch: 1 step: 2, loss is 2.3027236461639404\n",
      "epoch: 1 step: 3, loss is 2.301825523376465\n",
      "epoch: 1 step: 4, loss is 2.2992844581604004\n",
      "epoch: 1 step: 5, loss is 2.3038063049316406\n",
      "epoch: 1 step: 6, loss is 2.3020694255828857\n",
      "epoch: 1 step: 7, loss is 2.3010506629943848\n",
      "epoch: 1 step: 8, loss is 2.3047430515289307\n",
      "epoch: 1 step: 9, loss is 2.300079107284546\n",
      "epoch: 1 step: 10, loss is 2.3057174682617188\n",
      "epoch: 1 step: 11, loss is 2.300433397293091\n",
      "epoch: 1 step: 12, loss is 2.302960157394409\n",
      "epoch: 1 step: 13, loss is 2.3048901557922363\n",
      "epoch: 1 step: 14, loss is 2.2989909648895264\n",
      "epoch: 1 step: 15, loss is 2.3006248474121094\n",
      "epoch: 1 step: 16, loss is 2.3031868934631348\n",
      "epoch: 1 step: 17, loss is 2.3056387901306152\n",
      "epoch: 1 step: 18, loss is 2.2969870567321777\n",
      "epoch: 1 step: 19, loss is 2.295560121536255\n",
      "epoch: 1 step: 20, loss is 2.3088691234588623\n",
      "epoch: 1 step: 21, loss is 2.3032989501953125\n",
      "epoch: 1 step: 22, loss is 2.2997515201568604\n",
      "epoch: 1 step: 23, loss is 2.3048293590545654\n",
      "epoch: 1 step: 24, loss is 2.302180528640747\n",
      "epoch: 1 step: 25, loss is 2.301426887512207\n",
      "epoch: 1 step: 26, loss is 2.309541702270508\n",
      "epoch: 1 step: 27, loss is 2.3025031089782715\n",
      "epoch: 1 step: 28, loss is 2.31230092048645\n",
      "epoch: 1 step: 29, loss is 2.2973272800445557\n",
      "epoch: 1 step: 30, loss is 2.3033618927001953\n",
      "epoch: 1 step: 31, loss is 2.29988694190979\n",
      "epoch: 1 step: 32, loss is 2.3019602298736572\n",
      "epoch: 1 step: 33, loss is 2.300175428390503\n",
      "epoch: 1 step: 34, loss is 2.3008713722229004\n",
      "epoch: 1 step: 35, loss is 2.301351547241211\n",
      "epoch: 1 step: 36, loss is 2.3101882934570312\n",
      "epoch: 1 step: 37, loss is 2.3047046661376953\n",
      "epoch: 1 step: 38, loss is 2.298532247543335\n",
      "epoch: 1 step: 39, loss is 2.3086159229278564\n",
      "epoch: 1 step: 40, loss is 2.306276321411133\n",
      "epoch: 1 step: 41, loss is 2.302971124649048\n",
      "epoch: 1 step: 42, loss is 2.30582857131958\n",
      "epoch: 1 step: 43, loss is 2.301586389541626\n",
      "epoch: 1 step: 44, loss is 2.289501905441284\n",
      "epoch: 1 step: 45, loss is 2.295666217803955\n",
      "epoch: 1 step: 46, loss is 2.2989282608032227\n",
      "epoch: 1 step: 47, loss is 2.296391010284424\n",
      "epoch: 1 step: 48, loss is 2.3038740158081055\n",
      "epoch: 1 step: 49, loss is 2.306776285171509\n",
      "epoch: 1 step: 50, loss is 2.301574945449829\n",
      "epoch: 1 step: 51, loss is 2.2893264293670654\n",
      "epoch: 1 step: 52, loss is 2.3084158897399902\n",
      "epoch: 1 step: 53, loss is 2.286543607711792\n",
      "epoch: 1 step: 54, loss is 2.291485071182251\n",
      "epoch: 1 step: 55, loss is 2.2977688312530518\n",
      "epoch: 1 step: 56, loss is 2.3016107082366943\n",
      "epoch: 1 step: 57, loss is 2.294218063354492\n",
      "epoch: 1 step: 58, loss is 2.3128552436828613\n",
      "epoch: 1 step: 59, loss is 2.3118343353271484\n",
      "epoch: 1 step: 60, loss is 2.2957184314727783\n",
      "epoch: 1 step: 61, loss is 2.3003125190734863\n",
      "epoch: 1 step: 62, loss is 2.308758020401001\n",
      "epoch: 1 step: 63, loss is 2.3162436485290527\n",
      "epoch: 1 step: 64, loss is 2.3014726638793945\n",
      "epoch: 1 step: 65, loss is 2.3034627437591553\n",
      "epoch: 1 step: 66, loss is 2.2993154525756836\n",
      "epoch: 1 step: 67, loss is 2.306027412414551\n",
      "epoch: 1 step: 68, loss is 2.301332950592041\n",
      "epoch: 1 step: 69, loss is 2.3165221214294434\n",
      "epoch: 1 step: 70, loss is 2.3152413368225098\n",
      "epoch: 1 step: 71, loss is 2.285081386566162\n",
      "epoch: 1 step: 72, loss is 2.291923999786377\n",
      "epoch: 1 step: 73, loss is 2.3061797618865967\n",
      "epoch: 1 step: 74, loss is 2.299337387084961\n",
      "epoch: 1 step: 75, loss is 2.3118631839752197\n",
      "epoch: 1 step: 76, loss is 2.3042283058166504\n",
      "epoch: 1 step: 77, loss is 2.3011860847473145\n",
      "epoch: 1 step: 78, loss is 2.3097338676452637\n",
      "epoch: 1 step: 79, loss is 2.3021702766418457\n",
      "epoch: 1 step: 80, loss is 2.306429624557495\n",
      "epoch: 1 step: 81, loss is 2.3074326515197754\n",
      "epoch: 1 step: 82, loss is 2.3046011924743652\n",
      "epoch: 1 step: 83, loss is 2.293910503387451\n",
      "epoch: 1 step: 84, loss is 2.313694715499878\n",
      "epoch: 1 step: 85, loss is 2.2952723503112793\n",
      "epoch: 1 step: 86, loss is 2.3162152767181396\n",
      "epoch: 1 step: 87, loss is 2.298474073410034\n",
      "epoch: 1 step: 88, loss is 2.303126573562622\n",
      "epoch: 1 step: 89, loss is 2.301614761352539\n",
      "epoch: 1 step: 90, loss is 2.302503824234009\n",
      "epoch: 1 step: 91, loss is 2.3071460723876953\n",
      "epoch: 1 step: 92, loss is 2.3075480461120605\n",
      "epoch: 1 step: 93, loss is 2.3065614700317383\n",
      "epoch: 1 step: 94, loss is 2.3100080490112305\n",
      "epoch: 1 step: 95, loss is 2.3048787117004395\n",
      "epoch: 1 step: 96, loss is 2.298611879348755\n",
      "epoch: 1 step: 97, loss is 2.307015895843506\n",
      "epoch: 1 step: 98, loss is 2.3218977451324463\n",
      "epoch: 1 step: 99, loss is 2.3147504329681396\n",
      "epoch: 1 step: 100, loss is 2.2914469242095947\n",
      "epoch: 1 step: 101, loss is 2.2967405319213867\n",
      "epoch: 1 step: 102, loss is 2.3055272102355957\n",
      "epoch: 1 step: 103, loss is 2.3012712001800537\n",
      "epoch: 1 step: 104, loss is 2.301518201828003\n",
      "epoch: 1 step: 105, loss is 2.2975399494171143\n",
      "epoch: 1 step: 106, loss is 2.316192388534546\n",
      "epoch: 1 step: 107, loss is 2.296041965484619\n",
      "epoch: 1 step: 108, loss is 2.3003532886505127\n",
      "epoch: 1 step: 109, loss is 2.302870750427246\n",
      "epoch: 1 step: 110, loss is 2.295081615447998\n",
      "epoch: 1 step: 111, loss is 2.302358865737915\n",
      "epoch: 1 step: 112, loss is 2.3005900382995605\n",
      "epoch: 1 step: 113, loss is 2.300724506378174\n",
      "epoch: 1 step: 114, loss is 2.291738271713257\n",
      "epoch: 1 step: 115, loss is 2.2996904850006104\n",
      "epoch: 1 step: 116, loss is 2.308441162109375\n",
      "epoch: 1 step: 117, loss is 2.309525489807129\n",
      "epoch: 1 step: 118, loss is 2.30277156829834\n",
      "epoch: 1 step: 119, loss is 2.2989981174468994\n",
      "epoch: 1 step: 120, loss is 2.2896475791931152\n",
      "epoch: 1 step: 121, loss is 2.2909765243530273\n",
      "epoch: 1 step: 122, loss is 2.3045036792755127\n",
      "epoch: 1 step: 123, loss is 2.3037757873535156\n",
      "epoch: 1 step: 124, loss is 2.311274766921997\n",
      "epoch: 1 step: 125, loss is 2.2915658950805664\n",
      "epoch: 1 step: 126, loss is 2.3031909465789795\n",
      "epoch: 1 step: 127, loss is 2.3011250495910645\n",
      "epoch: 1 step: 128, loss is 2.29247784614563\n",
      "epoch: 1 step: 129, loss is 2.3033998012542725\n",
      "epoch: 1 step: 130, loss is 2.3034768104553223\n",
      "epoch: 1 step: 131, loss is 2.302006721496582\n",
      "epoch: 1 step: 132, loss is 2.2966196537017822\n",
      "epoch: 1 step: 133, loss is 2.289525032043457\n",
      "epoch: 1 step: 134, loss is 2.304107904434204\n",
      "epoch: 1 step: 135, loss is 2.3083465099334717\n",
      "epoch: 1 step: 136, loss is 2.326261043548584\n",
      "epoch: 1 step: 137, loss is 2.3160743713378906\n",
      "epoch: 1 step: 138, loss is 2.3066203594207764\n",
      "epoch: 1 step: 139, loss is 2.3034143447875977\n",
      "epoch: 1 step: 140, loss is 2.290825366973877\n",
      "epoch: 1 step: 141, loss is 2.2976951599121094\n",
      "epoch: 1 step: 142, loss is 2.2912843227386475\n",
      "epoch: 1 step: 143, loss is 2.3183932304382324\n",
      "epoch: 1 step: 144, loss is 2.2997381687164307\n",
      "epoch: 1 step: 145, loss is 2.31142520904541\n",
      "epoch: 1 step: 146, loss is 2.306079387664795\n",
      "epoch: 1 step: 147, loss is 2.2806713581085205\n",
      "epoch: 1 step: 148, loss is 2.309575080871582\n",
      "epoch: 1 step: 149, loss is 2.293966054916382\n",
      "epoch: 1 step: 150, loss is 2.3121094703674316\n",
      "epoch: 1 step: 151, loss is 2.3061683177948\n",
      "epoch: 1 step: 152, loss is 2.2909860610961914\n",
      "epoch: 1 step: 153, loss is 2.2879841327667236\n",
      "epoch: 1 step: 154, loss is 2.288878917694092\n",
      "epoch: 1 step: 155, loss is 2.299933433532715\n",
      "epoch: 1 step: 156, loss is 2.3109166622161865\n",
      "epoch: 1 step: 157, loss is 2.295961380004883\n",
      "epoch: 1 step: 158, loss is 2.297483444213867\n",
      "epoch: 1 step: 159, loss is 2.303616523742676\n",
      "epoch: 1 step: 160, loss is 2.3138933181762695\n",
      "epoch: 1 step: 161, loss is 2.290067672729492\n",
      "epoch: 1 step: 162, loss is 2.294917106628418\n",
      "epoch: 1 step: 163, loss is 2.297441244125366\n",
      "epoch: 1 step: 164, loss is 2.305194139480591\n",
      "epoch: 1 step: 165, loss is 2.2969532012939453\n",
      "epoch: 1 step: 166, loss is 2.3143928050994873\n",
      "epoch: 1 step: 167, loss is 2.286308526992798\n",
      "epoch: 1 step: 168, loss is 2.2934815883636475\n",
      "epoch: 1 step: 169, loss is 2.284362316131592\n",
      "epoch: 1 step: 170, loss is 2.2993180751800537\n",
      "epoch: 1 step: 171, loss is 2.3226609230041504\n",
      "epoch: 1 step: 172, loss is 2.296264410018921\n",
      "epoch: 1 step: 173, loss is 2.288907766342163\n",
      "epoch: 1 step: 174, loss is 2.302051305770874\n",
      "epoch: 1 step: 175, loss is 2.2997918128967285\n",
      "epoch: 1 step: 176, loss is 2.3077425956726074\n",
      "epoch: 1 step: 177, loss is 2.2909910678863525\n",
      "epoch: 1 step: 178, loss is 2.301401138305664\n",
      "epoch: 1 step: 179, loss is 2.306682586669922\n",
      "epoch: 1 step: 180, loss is 2.33074688911438\n",
      "epoch: 1 step: 181, loss is 2.3002758026123047\n",
      "epoch: 1 step: 182, loss is 2.300804615020752\n",
      "epoch: 1 step: 183, loss is 2.2918701171875\n",
      "epoch: 1 step: 184, loss is 2.3047597408294678\n",
      "epoch: 1 step: 185, loss is 2.2953920364379883\n",
      "epoch: 1 step: 186, loss is 2.3140273094177246\n",
      "epoch: 1 step: 187, loss is 2.2841708660125732\n",
      "epoch: 1 step: 188, loss is 2.3120980262756348\n",
      "epoch: 1 step: 189, loss is 2.3058955669403076\n",
      "epoch: 1 step: 190, loss is 2.3167977333068848\n",
      "epoch: 1 step: 191, loss is 2.3148367404937744\n",
      "epoch: 1 step: 192, loss is 2.298274040222168\n",
      "epoch: 1 step: 193, loss is 2.3102049827575684\n",
      "epoch: 1 step: 194, loss is 2.295020818710327\n",
      "epoch: 1 step: 195, loss is 2.3032689094543457\n",
      "epoch: 1 step: 196, loss is 2.317643404006958\n",
      "epoch: 1 step: 197, loss is 2.317286491394043\n",
      "epoch: 1 step: 198, loss is 2.2951955795288086\n",
      "epoch: 1 step: 199, loss is 2.2808971405029297\n",
      "epoch: 1 step: 200, loss is 2.2963027954101562\n",
      "epoch: 1 step: 201, loss is 2.2842414379119873\n",
      "epoch: 1 step: 202, loss is 2.295431613922119\n",
      "epoch: 1 step: 203, loss is 2.3090031147003174\n",
      "epoch: 1 step: 204, loss is 2.2873735427856445\n",
      "epoch: 1 step: 205, loss is 2.315765857696533\n",
      "epoch: 1 step: 206, loss is 2.29769229888916\n",
      "epoch: 1 step: 207, loss is 2.29996657371521\n",
      "epoch: 1 step: 208, loss is 2.311231851577759\n",
      "epoch: 1 step: 209, loss is 2.29813289642334\n",
      "epoch: 1 step: 210, loss is 2.3147776126861572\n",
      "epoch: 1 step: 211, loss is 2.318469762802124\n",
      "epoch: 1 step: 212, loss is 2.307460069656372\n",
      "epoch: 1 step: 213, loss is 2.3133554458618164\n",
      "epoch: 1 step: 214, loss is 2.3016889095306396\n",
      "epoch: 1 step: 215, loss is 2.29959774017334\n",
      "epoch: 1 step: 216, loss is 2.318894147872925\n",
      "epoch: 1 step: 217, loss is 2.274831533432007\n",
      "epoch: 1 step: 218, loss is 2.325258255004883\n",
      "epoch: 1 step: 219, loss is 2.290066957473755\n",
      "epoch: 1 step: 220, loss is 2.303283214569092\n",
      "epoch: 1 step: 221, loss is 2.299696445465088\n",
      "epoch: 1 step: 222, loss is 2.3261160850524902\n",
      "epoch: 1 step: 223, loss is 2.2993481159210205\n",
      "epoch: 1 step: 224, loss is 2.3038933277130127\n",
      "epoch: 1 step: 225, loss is 2.2928662300109863\n",
      "epoch: 1 step: 226, loss is 2.3068530559539795\n",
      "epoch: 1 step: 227, loss is 2.283328056335449\n",
      "epoch: 1 step: 228, loss is 2.2946014404296875\n",
      "epoch: 1 step: 229, loss is 2.3051185607910156\n",
      "epoch: 1 step: 230, loss is 2.312055826187134\n",
      "epoch: 1 step: 231, loss is 2.3075177669525146\n",
      "epoch: 1 step: 232, loss is 2.292630434036255\n",
      "epoch: 1 step: 233, loss is 2.313403606414795\n",
      "epoch: 1 step: 234, loss is 2.3051369190216064\n",
      "epoch: 1 step: 235, loss is 2.306227922439575\n",
      "epoch: 1 step: 236, loss is 2.304135322570801\n",
      "epoch: 1 step: 237, loss is 2.309201717376709\n",
      "epoch: 1 step: 238, loss is 2.3142929077148438\n",
      "epoch: 1 step: 239, loss is 2.302326202392578\n",
      "epoch: 1 step: 240, loss is 2.29054856300354\n",
      "epoch: 1 step: 241, loss is 2.3008816242218018\n",
      "epoch: 1 step: 242, loss is 2.314791440963745\n",
      "epoch: 1 step: 243, loss is 2.2873992919921875\n",
      "epoch: 1 step: 244, loss is 2.2893729209899902\n",
      "epoch: 1 step: 245, loss is 2.2963473796844482\n",
      "epoch: 1 step: 246, loss is 2.2988357543945312\n",
      "epoch: 1 step: 247, loss is 2.3109166622161865\n",
      "epoch: 1 step: 248, loss is 2.300589084625244\n",
      "epoch: 1 step: 249, loss is 2.3037190437316895\n",
      "epoch: 1 step: 250, loss is 2.3125429153442383\n",
      "epoch: 1 step: 251, loss is 2.3199644088745117\n",
      "epoch: 1 step: 252, loss is 2.307720422744751\n",
      "epoch: 1 step: 253, loss is 2.3077714443206787\n",
      "epoch: 1 step: 254, loss is 2.317579507827759\n",
      "epoch: 1 step: 255, loss is 2.305392265319824\n",
      "epoch: 1 step: 256, loss is 2.2996675968170166\n",
      "epoch: 1 step: 257, loss is 2.3062920570373535\n",
      "epoch: 1 step: 258, loss is 2.2994112968444824\n",
      "epoch: 1 step: 259, loss is 2.301828145980835\n",
      "epoch: 1 step: 260, loss is 2.313275098800659\n",
      "epoch: 1 step: 261, loss is 2.297358751296997\n",
      "epoch: 1 step: 262, loss is 2.296499252319336\n",
      "epoch: 1 step: 263, loss is 2.2994301319122314\n",
      "epoch: 1 step: 264, loss is 2.310549259185791\n",
      "epoch: 1 step: 265, loss is 2.3051207065582275\n",
      "epoch: 1 step: 266, loss is 2.3152735233306885\n",
      "epoch: 1 step: 267, loss is 2.303129196166992\n",
      "epoch: 1 step: 268, loss is 2.3080086708068848\n",
      "epoch: 1 step: 269, loss is 2.310636281967163\n",
      "epoch: 1 step: 270, loss is 2.3073318004608154\n",
      "epoch: 1 step: 271, loss is 2.3030714988708496\n",
      "epoch: 1 step: 272, loss is 2.296107053756714\n",
      "epoch: 1 step: 273, loss is 2.3142037391662598\n",
      "epoch: 1 step: 274, loss is 2.2952895164489746\n",
      "epoch: 1 step: 275, loss is 2.307021141052246\n",
      "epoch: 1 step: 276, loss is 2.3077893257141113\n",
      "epoch: 1 step: 277, loss is 2.3018341064453125\n",
      "epoch: 1 step: 278, loss is 2.2995126247406006\n",
      "epoch: 1 step: 279, loss is 2.3084516525268555\n",
      "epoch: 1 step: 280, loss is 2.2920844554901123\n",
      "epoch: 1 step: 281, loss is 2.3033227920532227\n",
      "epoch: 1 step: 282, loss is 2.299907684326172\n",
      "epoch: 1 step: 283, loss is 2.2975783348083496\n",
      "epoch: 1 step: 284, loss is 2.309929370880127\n",
      "epoch: 1 step: 285, loss is 2.298431873321533\n",
      "epoch: 1 step: 286, loss is 2.3014445304870605\n",
      "epoch: 1 step: 287, loss is 2.3010473251342773\n",
      "epoch: 1 step: 288, loss is 2.3061723709106445\n",
      "epoch: 1 step: 289, loss is 2.29818058013916\n",
      "epoch: 1 step: 290, loss is 2.302729845046997\n",
      "epoch: 1 step: 291, loss is 2.2983388900756836\n",
      "epoch: 1 step: 292, loss is 2.3002891540527344\n",
      "epoch: 1 step: 293, loss is 2.3070952892303467\n",
      "epoch: 1 step: 294, loss is 2.3028881549835205\n",
      "epoch: 1 step: 295, loss is 2.3138182163238525\n",
      "epoch: 1 step: 296, loss is 2.2988109588623047\n",
      "epoch: 1 step: 297, loss is 2.2967264652252197\n",
      "epoch: 1 step: 298, loss is 2.2915198802948\n",
      "epoch: 1 step: 299, loss is 2.2938458919525146\n",
      "epoch: 1 step: 300, loss is 2.2860543727874756\n",
      "epoch: 1 step: 301, loss is 2.2952682971954346\n",
      "epoch: 1 step: 302, loss is 2.30096173286438\n",
      "epoch: 1 step: 303, loss is 2.3012430667877197\n",
      "epoch: 1 step: 304, loss is 2.2909860610961914\n",
      "epoch: 1 step: 305, loss is 2.3080482482910156\n",
      "epoch: 1 step: 306, loss is 2.3046867847442627\n",
      "epoch: 1 step: 307, loss is 2.308532238006592\n",
      "epoch: 1 step: 308, loss is 2.30072283744812\n",
      "epoch: 1 step: 309, loss is 2.307424783706665\n",
      "epoch: 1 step: 310, loss is 2.2962355613708496\n",
      "epoch: 1 step: 311, loss is 2.3000478744506836\n",
      "epoch: 1 step: 312, loss is 2.296745777130127\n",
      "epoch: 1 step: 313, loss is 2.295405626296997\n",
      "epoch: 1 step: 314, loss is 2.29609751701355\n",
      "epoch: 1 step: 315, loss is 2.3074822425842285\n",
      "epoch: 1 step: 316, loss is 2.303483486175537\n",
      "epoch: 1 step: 317, loss is 2.3052124977111816\n",
      "epoch: 1 step: 318, loss is 2.284173011779785\n",
      "epoch: 1 step: 319, loss is 2.305485248565674\n",
      "epoch: 1 step: 320, loss is 2.2965543270111084\n",
      "epoch: 1 step: 321, loss is 2.2954294681549072\n",
      "epoch: 1 step: 322, loss is 2.2941761016845703\n",
      "epoch: 1 step: 323, loss is 2.284395694732666\n",
      "epoch: 1 step: 324, loss is 2.3215575218200684\n",
      "epoch: 1 step: 325, loss is 2.292613983154297\n",
      "epoch: 1 step: 326, loss is 2.2892110347747803\n",
      "epoch: 1 step: 327, loss is 2.2900025844573975\n",
      "epoch: 1 step: 328, loss is 2.301013469696045\n",
      "epoch: 1 step: 329, loss is 2.3012614250183105\n",
      "epoch: 1 step: 330, loss is 2.3029792308807373\n",
      "epoch: 1 step: 331, loss is 2.31152606010437\n",
      "epoch: 1 step: 332, loss is 2.286699056625366\n",
      "epoch: 1 step: 333, loss is 2.299227476119995\n",
      "epoch: 1 step: 334, loss is 2.3111863136291504\n",
      "epoch: 1 step: 335, loss is 2.305163860321045\n",
      "epoch: 1 step: 336, loss is 2.2869088649749756\n",
      "epoch: 1 step: 337, loss is 2.3081276416778564\n",
      "epoch: 1 step: 338, loss is 2.2969627380371094\n",
      "epoch: 1 step: 339, loss is 2.297571897506714\n",
      "epoch: 1 step: 340, loss is 2.2890148162841797\n",
      "epoch: 1 step: 341, loss is 2.3010787963867188\n",
      "epoch: 1 step: 342, loss is 2.295091152191162\n",
      "epoch: 1 step: 343, loss is 2.3033320903778076\n",
      "epoch: 1 step: 344, loss is 2.3004202842712402\n",
      "epoch: 1 step: 345, loss is 2.3164987564086914\n",
      "epoch: 1 step: 346, loss is 2.3015623092651367\n",
      "epoch: 1 step: 347, loss is 2.2922534942626953\n",
      "epoch: 1 step: 348, loss is 2.322038173675537\n",
      "epoch: 1 step: 349, loss is 2.3050427436828613\n",
      "epoch: 1 step: 350, loss is 2.3120172023773193\n",
      "epoch: 1 step: 351, loss is 2.31584095954895\n",
      "epoch: 1 step: 352, loss is 2.30025315284729\n",
      "epoch: 1 step: 353, loss is 2.3014955520629883\n",
      "epoch: 1 step: 354, loss is 2.311795473098755\n",
      "epoch: 1 step: 355, loss is 2.2983295917510986\n",
      "epoch: 1 step: 356, loss is 2.2935361862182617\n",
      "epoch: 1 step: 357, loss is 2.299201011657715\n",
      "epoch: 1 step: 358, loss is 2.309305429458618\n",
      "epoch: 1 step: 359, loss is 2.286905527114868\n",
      "epoch: 1 step: 360, loss is 2.3030917644500732\n",
      "epoch: 1 step: 361, loss is 2.30183482170105\n",
      "epoch: 1 step: 362, loss is 2.3064582347869873\n",
      "epoch: 1 step: 363, loss is 2.3014137744903564\n",
      "epoch: 1 step: 364, loss is 2.3011491298675537\n",
      "epoch: 1 step: 365, loss is 2.273906707763672\n",
      "epoch: 1 step: 366, loss is 2.2983286380767822\n",
      "epoch: 1 step: 367, loss is 2.310999870300293\n",
      "epoch: 1 step: 368, loss is 2.295213460922241\n",
      "epoch: 1 step: 369, loss is 2.314990520477295\n",
      "epoch: 1 step: 370, loss is 2.3115310668945312\n",
      "epoch: 1 step: 371, loss is 2.3013432025909424\n",
      "epoch: 1 step: 372, loss is 2.3142380714416504\n",
      "epoch: 1 step: 373, loss is 2.3071866035461426\n",
      "epoch: 1 step: 374, loss is 2.289818286895752\n",
      "epoch: 1 step: 375, loss is 2.301201343536377\n",
      "epoch: 1 step: 376, loss is 2.2878124713897705\n",
      "epoch: 1 step: 377, loss is 2.2762720584869385\n",
      "epoch: 1 step: 378, loss is 2.2900948524475098\n",
      "epoch: 1 step: 379, loss is 2.3141908645629883\n",
      "epoch: 1 step: 380, loss is 2.3211398124694824\n",
      "epoch: 1 step: 381, loss is 2.3029794692993164\n",
      "epoch: 1 step: 382, loss is 2.3041539192199707\n",
      "epoch: 1 step: 383, loss is 2.321399450302124\n",
      "epoch: 1 step: 384, loss is 2.299866199493408\n",
      "epoch: 1 step: 385, loss is 2.279698371887207\n",
      "epoch: 1 step: 386, loss is 2.316211700439453\n",
      "epoch: 1 step: 387, loss is 2.3093836307525635\n",
      "epoch: 1 step: 388, loss is 2.2974588871002197\n",
      "epoch: 1 step: 389, loss is 2.31917405128479\n",
      "epoch: 1 step: 390, loss is 2.318636655807495\n",
      "epoch: 1 step: 391, loss is 2.3203699588775635\n",
      "epoch: 1 step: 392, loss is 2.30179500579834\n",
      "epoch: 1 step: 393, loss is 2.284306764602661\n",
      "epoch: 1 step: 394, loss is 2.290802478790283\n",
      "epoch: 1 step: 395, loss is 2.3225290775299072\n",
      "epoch: 1 step: 396, loss is 2.2991230487823486\n",
      "epoch: 1 step: 397, loss is 2.295241117477417\n",
      "epoch: 1 step: 398, loss is 2.3049535751342773\n",
      "epoch: 1 step: 399, loss is 2.2785913944244385\n",
      "epoch: 1 step: 400, loss is 2.2940514087677\n",
      "epoch: 1 step: 401, loss is 2.298733949661255\n",
      "epoch: 1 step: 402, loss is 2.3096468448638916\n",
      "epoch: 1 step: 403, loss is 2.307130813598633\n",
      "epoch: 1 step: 404, loss is 2.2814910411834717\n",
      "epoch: 1 step: 405, loss is 2.2926933765411377\n",
      "epoch: 1 step: 406, loss is 2.3124711513519287\n",
      "epoch: 1 step: 407, loss is 2.317230701446533\n",
      "epoch: 1 step: 408, loss is 2.291761636734009\n",
      "epoch: 1 step: 409, loss is 2.3080172538757324\n",
      "epoch: 1 step: 410, loss is 2.3068554401397705\n",
      "epoch: 1 step: 411, loss is 2.318352222442627\n",
      "epoch: 1 step: 412, loss is 2.3052687644958496\n",
      "epoch: 1 step: 413, loss is 2.307690382003784\n",
      "epoch: 1 step: 414, loss is 2.299616813659668\n",
      "epoch: 1 step: 415, loss is 2.318504810333252\n",
      "epoch: 1 step: 416, loss is 2.298211097717285\n",
      "epoch: 1 step: 417, loss is 2.2967543601989746\n",
      "epoch: 1 step: 418, loss is 2.305854320526123\n",
      "epoch: 1 step: 419, loss is 2.309769868850708\n",
      "epoch: 1 step: 420, loss is 2.3004536628723145\n",
      "epoch: 1 step: 421, loss is 2.3137924671173096\n",
      "epoch: 1 step: 422, loss is 2.3094406127929688\n",
      "epoch: 1 step: 423, loss is 2.3030402660369873\n",
      "epoch: 1 step: 424, loss is 2.3030998706817627\n",
      "epoch: 1 step: 425, loss is 2.294157028198242\n",
      "epoch: 1 step: 426, loss is 2.2969024181365967\n",
      "epoch: 1 step: 427, loss is 2.2996926307678223\n",
      "epoch: 1 step: 428, loss is 2.317941665649414\n",
      "epoch: 1 step: 429, loss is 2.2996177673339844\n",
      "epoch: 1 step: 430, loss is 2.2823169231414795\n",
      "epoch: 1 step: 431, loss is 2.2917404174804688\n",
      "epoch: 1 step: 432, loss is 2.285440444946289\n",
      "epoch: 1 step: 433, loss is 2.3184738159179688\n",
      "epoch: 1 step: 434, loss is 2.3046605587005615\n",
      "epoch: 1 step: 435, loss is 2.297808885574341\n",
      "epoch: 1 step: 436, loss is 2.3126072883605957\n",
      "epoch: 1 step: 437, loss is 2.317091941833496\n",
      "epoch: 1 step: 438, loss is 2.2975637912750244\n",
      "epoch: 1 step: 439, loss is 2.3013265132904053\n",
      "epoch: 1 step: 440, loss is 2.305992603302002\n",
      "epoch: 1 step: 441, loss is 2.3093302249908447\n",
      "epoch: 1 step: 442, loss is 2.316710948944092\n",
      "epoch: 1 step: 443, loss is 2.300546646118164\n",
      "epoch: 1 step: 444, loss is 2.2993569374084473\n",
      "epoch: 1 step: 445, loss is 2.2815918922424316\n",
      "epoch: 1 step: 446, loss is 2.300516128540039\n",
      "epoch: 1 step: 447, loss is 2.2924437522888184\n",
      "epoch: 1 step: 448, loss is 2.280646800994873\n",
      "epoch: 1 step: 449, loss is 2.2983529567718506\n",
      "epoch: 1 step: 450, loss is 2.298044443130493\n",
      "epoch: 1 step: 451, loss is 2.30070161819458\n",
      "epoch: 1 step: 452, loss is 2.2988741397857666\n",
      "epoch: 1 step: 453, loss is 2.307647943496704\n",
      "epoch: 1 step: 454, loss is 2.2939651012420654\n",
      "epoch: 1 step: 455, loss is 2.301551580429077\n",
      "epoch: 1 step: 456, loss is 2.303483486175537\n",
      "epoch: 1 step: 457, loss is 2.3087806701660156\n",
      "epoch: 1 step: 458, loss is 2.3019332885742188\n",
      "epoch: 1 step: 459, loss is 2.2929868698120117\n",
      "epoch: 1 step: 460, loss is 2.30267333984375\n",
      "epoch: 1 step: 461, loss is 2.302889108657837\n",
      "epoch: 1 step: 462, loss is 2.3138787746429443\n",
      "epoch: 1 step: 463, loss is 2.3074159622192383\n",
      "epoch: 1 step: 464, loss is 2.3016109466552734\n",
      "epoch: 1 step: 465, loss is 2.3040761947631836\n",
      "epoch: 1 step: 466, loss is 2.3100743293762207\n",
      "epoch: 1 step: 467, loss is 2.2979822158813477\n",
      "epoch: 1 step: 468, loss is 2.309048652648926\n",
      "epoch: 1 step: 469, loss is 2.307396173477173\n",
      "epoch: 1 step: 470, loss is 2.312761068344116\n",
      "epoch: 1 step: 471, loss is 2.300224542617798\n",
      "epoch: 1 step: 472, loss is 2.291654109954834\n",
      "epoch: 1 step: 473, loss is 2.2974750995635986\n",
      "epoch: 1 step: 474, loss is 2.2910423278808594\n",
      "epoch: 1 step: 475, loss is 2.3080296516418457\n",
      "epoch: 1 step: 476, loss is 2.3024990558624268\n",
      "epoch: 1 step: 477, loss is 2.299304485321045\n",
      "epoch: 1 step: 478, loss is 2.305330991744995\n",
      "epoch: 1 step: 479, loss is 2.305838108062744\n",
      "epoch: 1 step: 480, loss is 2.3048760890960693\n",
      "epoch: 1 step: 481, loss is 2.2940311431884766\n",
      "epoch: 1 step: 482, loss is 2.2963197231292725\n",
      "epoch: 1 step: 483, loss is 2.316763401031494\n",
      "epoch: 1 step: 484, loss is 2.300194263458252\n",
      "epoch: 1 step: 485, loss is 2.293071985244751\n",
      "epoch: 1 step: 486, loss is 2.2955856323242188\n",
      "epoch: 1 step: 487, loss is 2.3041157722473145\n",
      "epoch: 1 step: 488, loss is 2.313488721847534\n",
      "epoch: 1 step: 489, loss is 2.3035354614257812\n",
      "epoch: 1 step: 490, loss is 2.3020262718200684\n",
      "epoch: 1 step: 491, loss is 2.3242530822753906\n",
      "epoch: 1 step: 492, loss is 2.3003058433532715\n",
      "epoch: 1 step: 493, loss is 2.2901363372802734\n",
      "epoch: 1 step: 494, loss is 2.3002448081970215\n",
      "epoch: 1 step: 495, loss is 2.2991061210632324\n",
      "epoch: 1 step: 496, loss is 2.2962472438812256\n",
      "epoch: 1 step: 497, loss is 2.3052914142608643\n",
      "epoch: 1 step: 498, loss is 2.3141424655914307\n",
      "epoch: 1 step: 499, loss is 2.280301809310913\n",
      "epoch: 1 step: 500, loss is 2.2907612323760986\n",
      "epoch: 1 step: 501, loss is 2.292752265930176\n",
      "epoch: 1 step: 502, loss is 2.2917544841766357\n",
      "epoch: 1 step: 503, loss is 2.2990798950195312\n",
      "epoch: 1 step: 504, loss is 2.3025224208831787\n",
      "epoch: 1 step: 505, loss is 2.287086009979248\n",
      "epoch: 1 step: 506, loss is 2.3112330436706543\n",
      "epoch: 1 step: 507, loss is 2.2880825996398926\n",
      "epoch: 1 step: 508, loss is 2.2902233600616455\n",
      "epoch: 1 step: 509, loss is 2.2964696884155273\n",
      "epoch: 1 step: 510, loss is 2.3054585456848145\n",
      "epoch: 1 step: 511, loss is 2.3070340156555176\n",
      "epoch: 1 step: 512, loss is 2.314751148223877\n",
      "epoch: 1 step: 513, loss is 2.291023015975952\n",
      "epoch: 1 step: 514, loss is 2.292562246322632\n",
      "epoch: 1 step: 515, loss is 2.291691303253174\n",
      "epoch: 1 step: 516, loss is 2.3135452270507812\n",
      "epoch: 1 step: 517, loss is 2.2936758995056152\n",
      "epoch: 1 step: 518, loss is 2.2750535011291504\n",
      "epoch: 1 step: 519, loss is 2.310363531112671\n",
      "epoch: 1 step: 520, loss is 2.2918145656585693\n",
      "epoch: 1 step: 521, loss is 2.314157724380493\n",
      "epoch: 1 step: 522, loss is 2.3074307441711426\n",
      "epoch: 1 step: 523, loss is 2.280735731124878\n",
      "epoch: 1 step: 524, loss is 2.2776575088500977\n",
      "epoch: 1 step: 525, loss is 2.2988133430480957\n",
      "epoch: 1 step: 526, loss is 2.337869167327881\n",
      "epoch: 1 step: 527, loss is 2.315329074859619\n",
      "epoch: 1 step: 528, loss is 2.3233041763305664\n",
      "epoch: 1 step: 529, loss is 2.3099026679992676\n",
      "epoch: 1 step: 530, loss is 2.300673484802246\n",
      "epoch: 1 step: 531, loss is 2.300473690032959\n",
      "epoch: 1 step: 532, loss is 2.291325569152832\n",
      "epoch: 1 step: 533, loss is 2.2961177825927734\n",
      "epoch: 1 step: 534, loss is 2.315774440765381\n",
      "epoch: 1 step: 535, loss is 2.2892184257507324\n",
      "epoch: 1 step: 536, loss is 2.305917739868164\n",
      "epoch: 1 step: 537, loss is 2.3021390438079834\n",
      "epoch: 1 step: 538, loss is 2.3085005283355713\n",
      "epoch: 1 step: 539, loss is 2.285001277923584\n",
      "epoch: 1 step: 540, loss is 2.3037731647491455\n",
      "epoch: 1 step: 541, loss is 2.3019821643829346\n",
      "epoch: 1 step: 542, loss is 2.3164780139923096\n",
      "epoch: 1 step: 543, loss is 2.2898201942443848\n",
      "epoch: 1 step: 544, loss is 2.3055617809295654\n",
      "epoch: 1 step: 545, loss is 2.2911198139190674\n",
      "epoch: 1 step: 546, loss is 2.2969067096710205\n",
      "epoch: 1 step: 547, loss is 2.305582284927368\n",
      "epoch: 1 step: 548, loss is 2.2802491188049316\n",
      "epoch: 1 step: 549, loss is 2.2903695106506348\n",
      "epoch: 1 step: 550, loss is 2.2933285236358643\n",
      "epoch: 1 step: 551, loss is 2.278007984161377\n",
      "epoch: 1 step: 552, loss is 2.2971277236938477\n",
      "epoch: 1 step: 553, loss is 2.2850935459136963\n",
      "epoch: 1 step: 554, loss is 2.279184341430664\n",
      "epoch: 1 step: 555, loss is 2.317497491836548\n",
      "epoch: 1 step: 556, loss is 2.3054678440093994\n",
      "epoch: 1 step: 557, loss is 2.2887444496154785\n",
      "epoch: 1 step: 558, loss is 2.2883434295654297\n",
      "epoch: 1 step: 559, loss is 2.2948813438415527\n",
      "epoch: 1 step: 560, loss is 2.2872824668884277\n",
      "epoch: 1 step: 561, loss is 2.3098888397216797\n",
      "epoch: 1 step: 562, loss is 2.286447525024414\n",
      "epoch: 1 step: 563, loss is 2.2924914360046387\n",
      "epoch: 1 step: 564, loss is 2.2971534729003906\n",
      "epoch: 1 step: 565, loss is 2.2723300457000732\n",
      "epoch: 1 step: 566, loss is 2.2931714057922363\n",
      "epoch: 1 step: 567, loss is 2.3081023693084717\n",
      "epoch: 1 step: 568, loss is 2.3138844966888428\n",
      "epoch: 1 step: 569, loss is 2.298067092895508\n",
      "epoch: 1 step: 570, loss is 2.275815963745117\n",
      "epoch: 1 step: 571, loss is 2.293207883834839\n",
      "epoch: 1 step: 572, loss is 2.2944819927215576\n",
      "epoch: 1 step: 573, loss is 2.3007829189300537\n",
      "epoch: 1 step: 574, loss is 2.295140027999878\n",
      "epoch: 1 step: 575, loss is 2.29732084274292\n",
      "epoch: 1 step: 576, loss is 2.2986602783203125\n",
      "epoch: 1 step: 577, loss is 2.2989211082458496\n",
      "epoch: 1 step: 578, loss is 2.2822251319885254\n",
      "epoch: 1 step: 579, loss is 2.302520513534546\n",
      "epoch: 1 step: 580, loss is 2.3025600910186768\n",
      "epoch: 1 step: 581, loss is 2.2935662269592285\n",
      "epoch: 1 step: 582, loss is 2.3146464824676514\n",
      "epoch: 1 step: 583, loss is 2.3190736770629883\n",
      "epoch: 1 step: 584, loss is 2.2759368419647217\n",
      "epoch: 1 step: 585, loss is 2.2618863582611084\n",
      "epoch: 1 step: 586, loss is 2.2722091674804688\n",
      "epoch: 1 step: 587, loss is 2.2707738876342773\n",
      "epoch: 1 step: 588, loss is 2.284569501876831\n",
      "epoch: 1 step: 589, loss is 2.2979586124420166\n",
      "epoch: 1 step: 590, loss is 2.299849033355713\n",
      "epoch: 1 step: 591, loss is 2.2717809677124023\n",
      "epoch: 1 step: 592, loss is 2.308871269226074\n",
      "epoch: 1 step: 593, loss is 2.3116753101348877\n",
      "epoch: 1 step: 594, loss is 2.3273696899414062\n",
      "epoch: 1 step: 595, loss is 2.2583107948303223\n",
      "epoch: 1 step: 596, loss is 2.3045055866241455\n",
      "epoch: 1 step: 597, loss is 2.302527666091919\n",
      "epoch: 1 step: 598, loss is 2.2822368144989014\n",
      "epoch: 1 step: 599, loss is 2.309584379196167\n",
      "epoch: 1 step: 600, loss is 2.288339614868164\n",
      "epoch: 1 step: 601, loss is 2.323134183883667\n",
      "epoch: 1 step: 602, loss is 2.3143274784088135\n",
      "epoch: 1 step: 603, loss is 2.3156962394714355\n",
      "epoch: 1 step: 604, loss is 2.270282030105591\n",
      "epoch: 1 step: 605, loss is 2.2929039001464844\n",
      "epoch: 1 step: 606, loss is 2.2812979221343994\n",
      "epoch: 1 step: 607, loss is 2.321277618408203\n",
      "epoch: 1 step: 608, loss is 2.2879788875579834\n",
      "epoch: 1 step: 609, loss is 2.3094077110290527\n",
      "epoch: 1 step: 610, loss is 2.3142480850219727\n",
      "epoch: 1 step: 611, loss is 2.276813268661499\n",
      "epoch: 1 step: 612, loss is 2.2779619693756104\n",
      "epoch: 1 step: 613, loss is 2.2995920181274414\n",
      "epoch: 1 step: 614, loss is 2.2903263568878174\n",
      "epoch: 1 step: 615, loss is 2.2823169231414795\n",
      "epoch: 1 step: 616, loss is 2.292814016342163\n",
      "epoch: 1 step: 617, loss is 2.2788965702056885\n",
      "epoch: 1 step: 618, loss is 2.3148458003997803\n",
      "epoch: 1 step: 619, loss is 2.2681922912597656\n",
      "epoch: 1 step: 620, loss is 2.291158437728882\n",
      "epoch: 1 step: 621, loss is 2.2911171913146973\n",
      "epoch: 1 step: 622, loss is 2.280832290649414\n",
      "epoch: 1 step: 623, loss is 2.286372184753418\n",
      "epoch: 1 step: 624, loss is 2.296571969985962\n",
      "epoch: 1 step: 625, loss is 2.302262544631958\n",
      "epoch: 1 step: 626, loss is 2.281254291534424\n",
      "epoch: 1 step: 627, loss is 2.2798683643341064\n",
      "epoch: 1 step: 628, loss is 2.287015676498413\n",
      "epoch: 1 step: 629, loss is 2.2820212841033936\n",
      "epoch: 1 step: 630, loss is 2.271043300628662\n",
      "epoch: 1 step: 631, loss is 2.317030191421509\n",
      "epoch: 1 step: 632, loss is 2.2647335529327393\n",
      "epoch: 1 step: 633, loss is 2.3113107681274414\n",
      "epoch: 1 step: 634, loss is 2.3262205123901367\n",
      "epoch: 1 step: 635, loss is 2.320671319961548\n",
      "epoch: 1 step: 636, loss is 2.298924684524536\n",
      "epoch: 1 step: 637, loss is 2.2805399894714355\n",
      "epoch: 1 step: 638, loss is 2.311208486557007\n",
      "epoch: 1 step: 639, loss is 2.2704946994781494\n",
      "epoch: 1 step: 640, loss is 2.266665458679199\n",
      "epoch: 1 step: 641, loss is 2.2856698036193848\n",
      "epoch: 1 step: 642, loss is 2.264153480529785\n",
      "epoch: 1 step: 643, loss is 2.2871031761169434\n",
      "epoch: 1 step: 644, loss is 2.2841854095458984\n",
      "epoch: 1 step: 645, loss is 2.260035753250122\n",
      "epoch: 1 step: 646, loss is 2.2916476726531982\n",
      "epoch: 1 step: 647, loss is 2.2548303604125977\n",
      "epoch: 1 step: 648, loss is 2.2508137226104736\n",
      "epoch: 1 step: 649, loss is 2.2576191425323486\n",
      "epoch: 1 step: 650, loss is 2.2649552822113037\n",
      "epoch: 1 step: 651, loss is 2.2469863891601562\n",
      "epoch: 1 step: 652, loss is 2.271364688873291\n",
      "epoch: 1 step: 653, loss is 2.261946439743042\n",
      "epoch: 1 step: 654, loss is 2.2788891792297363\n",
      "epoch: 1 step: 655, loss is 2.2427268028259277\n",
      "epoch: 1 step: 656, loss is 2.2362935543060303\n",
      "epoch: 1 step: 657, loss is 2.271285057067871\n",
      "epoch: 1 step: 658, loss is 2.2502856254577637\n",
      "epoch: 1 step: 659, loss is 2.275452136993408\n",
      "epoch: 1 step: 660, loss is 2.213796854019165\n",
      "epoch: 1 step: 661, loss is 2.2070810794830322\n",
      "epoch: 1 step: 662, loss is 2.243852138519287\n",
      "epoch: 1 step: 663, loss is 2.2378170490264893\n",
      "epoch: 1 step: 664, loss is 2.1996734142303467\n",
      "epoch: 1 step: 665, loss is 2.255824089050293\n",
      "epoch: 1 step: 666, loss is 2.2475805282592773\n",
      "epoch: 1 step: 667, loss is 2.1544835567474365\n",
      "epoch: 1 step: 668, loss is 2.1810455322265625\n",
      "epoch: 1 step: 669, loss is 2.161004066467285\n",
      "epoch: 1 step: 670, loss is 2.0915398597717285\n",
      "epoch: 1 step: 671, loss is 2.2151973247528076\n",
      "epoch: 1 step: 672, loss is 2.0681676864624023\n",
      "epoch: 1 step: 673, loss is 2.2073113918304443\n",
      "epoch: 1 step: 674, loss is 2.120776891708374\n",
      "epoch: 1 step: 675, loss is 2.0436277389526367\n",
      "epoch: 1 step: 676, loss is 2.0113906860351562\n",
      "epoch: 1 step: 677, loss is 1.9500834941864014\n",
      "epoch: 1 step: 678, loss is 1.906778335571289\n",
      "epoch: 1 step: 679, loss is 1.8848215341567993\n",
      "epoch: 1 step: 680, loss is 1.759877324104309\n",
      "epoch: 1 step: 681, loss is 1.8148069381713867\n",
      "epoch: 1 step: 682, loss is 1.7154182195663452\n",
      "epoch: 1 step: 683, loss is 1.8239972591400146\n",
      "epoch: 1 step: 684, loss is 1.8391649723052979\n",
      "epoch: 1 step: 685, loss is 1.6809126138687134\n",
      "epoch: 1 step: 686, loss is 1.7490592002868652\n",
      "epoch: 1 step: 687, loss is 1.8883131742477417\n",
      "epoch: 1 step: 688, loss is 1.729860782623291\n",
      "epoch: 1 step: 689, loss is 1.729864478111267\n",
      "epoch: 1 step: 690, loss is 1.5546830892562866\n",
      "epoch: 1 step: 691, loss is 1.544969916343689\n",
      "epoch: 1 step: 692, loss is 1.6210592985153198\n",
      "epoch: 1 step: 693, loss is 1.587701678276062\n",
      "epoch: 1 step: 694, loss is 1.3649734258651733\n",
      "epoch: 1 step: 695, loss is 1.451503872871399\n",
      "epoch: 1 step: 696, loss is 1.5077595710754395\n",
      "epoch: 1 step: 697, loss is 1.332666039466858\n",
      "epoch: 1 step: 698, loss is 1.266936182975769\n",
      "epoch: 1 step: 699, loss is 1.5531091690063477\n",
      "epoch: 1 step: 700, loss is 1.4623513221740723\n",
      "epoch: 1 step: 701, loss is 1.3124980926513672\n",
      "epoch: 1 step: 702, loss is 1.300161361694336\n",
      "epoch: 1 step: 703, loss is 1.3665732145309448\n",
      "epoch: 1 step: 704, loss is 1.3071143627166748\n",
      "epoch: 1 step: 705, loss is 1.1449689865112305\n",
      "epoch: 1 step: 706, loss is 1.877659797668457\n",
      "epoch: 1 step: 707, loss is 1.3868581056594849\n",
      "epoch: 1 step: 708, loss is 1.3952114582061768\n",
      "epoch: 1 step: 709, loss is 1.4501514434814453\n",
      "epoch: 1 step: 710, loss is 1.3455588817596436\n",
      "epoch: 1 step: 711, loss is 1.3988511562347412\n",
      "epoch: 1 step: 712, loss is 1.1686885356903076\n",
      "epoch: 1 step: 713, loss is 0.9294410943984985\n",
      "epoch: 1 step: 714, loss is 1.1547634601593018\n",
      "epoch: 1 step: 715, loss is 1.5134923458099365\n",
      "epoch: 1 step: 716, loss is 0.8195813894271851\n",
      "epoch: 1 step: 717, loss is 0.8568456768989563\n",
      "epoch: 1 step: 718, loss is 1.2039109468460083\n",
      "epoch: 1 step: 719, loss is 0.9113140106201172\n",
      "epoch: 1 step: 720, loss is 0.8774216175079346\n",
      "epoch: 1 step: 721, loss is 0.8452203273773193\n",
      "epoch: 1 step: 722, loss is 1.0203981399536133\n",
      "epoch: 1 step: 723, loss is 0.7860174179077148\n",
      "epoch: 1 step: 724, loss is 1.4749382734298706\n",
      "epoch: 1 step: 725, loss is 1.4046915769577026\n",
      "epoch: 1 step: 726, loss is 1.1653268337249756\n",
      "epoch: 1 step: 727, loss is 0.9360783696174622\n",
      "epoch: 1 step: 728, loss is 1.3503773212432861\n",
      "epoch: 1 step: 729, loss is 1.225358486175537\n",
      "epoch: 1 step: 730, loss is 0.8539217114448547\n",
      "epoch: 1 step: 731, loss is 1.077396035194397\n",
      "epoch: 1 step: 732, loss is 0.8473322987556458\n",
      "epoch: 1 step: 733, loss is 0.9674288034439087\n",
      "epoch: 1 step: 734, loss is 0.9763595461845398\n",
      "epoch: 1 step: 735, loss is 0.8586110472679138\n",
      "epoch: 1 step: 736, loss is 0.8400944471359253\n",
      "epoch: 1 step: 737, loss is 0.8986652493476868\n",
      "epoch: 1 step: 738, loss is 0.8512929081916809\n",
      "epoch: 1 step: 739, loss is 1.1246633529663086\n",
      "epoch: 1 step: 740, loss is 0.7452517747879028\n",
      "epoch: 1 step: 741, loss is 0.7110865116119385\n",
      "epoch: 1 step: 742, loss is 1.439115047454834\n",
      "epoch: 1 step: 743, loss is 0.5372471809387207\n",
      "epoch: 1 step: 744, loss is 0.9096143841743469\n",
      "epoch: 1 step: 745, loss is 0.6739459037780762\n",
      "epoch: 1 step: 746, loss is 0.6521760821342468\n",
      "epoch: 1 step: 747, loss is 0.6277157664299011\n",
      "epoch: 1 step: 748, loss is 1.0718555450439453\n",
      "epoch: 1 step: 749, loss is 0.9246134161949158\n",
      "epoch: 1 step: 750, loss is 0.9089545607566833\n",
      "epoch: 1 step: 751, loss is 0.5845233798027039\n",
      "epoch: 1 step: 752, loss is 1.3304158449172974\n",
      "epoch: 1 step: 753, loss is 0.7255709767341614\n",
      "epoch: 1 step: 754, loss is 1.0388984680175781\n",
      "epoch: 1 step: 755, loss is 0.5872758626937866\n",
      "epoch: 1 step: 756, loss is 0.7697367072105408\n",
      "epoch: 1 step: 757, loss is 0.600357711315155\n",
      "epoch: 1 step: 758, loss is 0.509238600730896\n",
      "epoch: 1 step: 759, loss is 0.5133190155029297\n",
      "epoch: 1 step: 760, loss is 1.062881588935852\n",
      "epoch: 1 step: 761, loss is 0.7965860366821289\n",
      "epoch: 1 step: 762, loss is 0.801398515701294\n",
      "epoch: 1 step: 763, loss is 0.743507444858551\n",
      "epoch: 1 step: 764, loss is 0.45998165011405945\n",
      "epoch: 1 step: 765, loss is 0.44769349694252014\n",
      "epoch: 1 step: 766, loss is 0.5284425020217896\n",
      "epoch: 1 step: 767, loss is 0.5165871381759644\n",
      "epoch: 1 step: 768, loss is 0.7288275361061096\n",
      "epoch: 1 step: 769, loss is 0.4465746283531189\n",
      "epoch: 1 step: 770, loss is 0.5135889053344727\n",
      "epoch: 1 step: 771, loss is 0.817986011505127\n",
      "epoch: 1 step: 772, loss is 0.8381636142730713\n",
      "epoch: 1 step: 773, loss is 0.8842306733131409\n",
      "epoch: 1 step: 774, loss is 0.6103103756904602\n",
      "epoch: 1 step: 775, loss is 0.4141896367073059\n",
      "epoch: 1 step: 776, loss is 0.5435701012611389\n",
      "epoch: 1 step: 777, loss is 0.7119224667549133\n",
      "epoch: 1 step: 778, loss is 0.7831616997718811\n",
      "epoch: 1 step: 779, loss is 0.36762338876724243\n",
      "epoch: 1 step: 780, loss is 0.44190365076065063\n",
      "epoch: 1 step: 781, loss is 0.6154623031616211\n",
      "epoch: 1 step: 782, loss is 0.4630070924758911\n",
      "epoch: 1 step: 783, loss is 0.44177696108818054\n",
      "epoch: 1 step: 784, loss is 0.6802759170532227\n",
      "epoch: 1 step: 785, loss is 0.43232461810112\n",
      "epoch: 1 step: 786, loss is 0.2928311824798584\n",
      "epoch: 1 step: 787, loss is 0.8536776304244995\n",
      "epoch: 1 step: 788, loss is 0.2689833343029022\n",
      "epoch: 1 step: 789, loss is 0.5057840943336487\n",
      "epoch: 1 step: 790, loss is 0.3700820505619049\n",
      "epoch: 1 step: 791, loss is 0.33059558272361755\n",
      "epoch: 1 step: 792, loss is 0.5649292469024658\n",
      "epoch: 1 step: 793, loss is 1.0644240379333496\n",
      "epoch: 1 step: 794, loss is 0.39412960410118103\n",
      "epoch: 1 step: 795, loss is 0.5145886540412903\n",
      "epoch: 1 step: 796, loss is 0.6446248292922974\n",
      "epoch: 1 step: 797, loss is 0.49306154251098633\n",
      "epoch: 1 step: 798, loss is 0.5254046320915222\n",
      "epoch: 1 step: 799, loss is 0.3685392439365387\n",
      "epoch: 1 step: 800, loss is 0.3015400171279907\n",
      "epoch: 1 step: 801, loss is 0.4640863537788391\n",
      "epoch: 1 step: 802, loss is 0.4306942820549011\n",
      "epoch: 1 step: 803, loss is 0.7485241889953613\n",
      "epoch: 1 step: 804, loss is 0.8608142137527466\n",
      "epoch: 1 step: 805, loss is 0.6160344481468201\n",
      "epoch: 1 step: 806, loss is 0.5749852061271667\n",
      "epoch: 1 step: 807, loss is 0.44536495208740234\n",
      "epoch: 1 step: 808, loss is 0.3848787844181061\n",
      "epoch: 1 step: 809, loss is 0.5743324756622314\n",
      "epoch: 1 step: 810, loss is 0.34488070011138916\n",
      "epoch: 1 step: 811, loss is 0.7496699690818787\n",
      "epoch: 1 step: 812, loss is 0.5909755825996399\n",
      "epoch: 1 step: 813, loss is 0.5475651621818542\n",
      "epoch: 1 step: 814, loss is 0.34929752349853516\n",
      "epoch: 1 step: 815, loss is 0.3839181661605835\n",
      "epoch: 1 step: 816, loss is 0.4595431983470917\n",
      "epoch: 1 step: 817, loss is 0.606718122959137\n",
      "epoch: 1 step: 818, loss is 0.4483417272567749\n",
      "epoch: 1 step: 819, loss is 0.5290195941925049\n",
      "epoch: 1 step: 820, loss is 0.7931280732154846\n",
      "epoch: 1 step: 821, loss is 0.3611985743045807\n",
      "epoch: 1 step: 822, loss is 0.6721769571304321\n",
      "epoch: 1 step: 823, loss is 0.3371567726135254\n",
      "epoch: 1 step: 824, loss is 0.39156922698020935\n",
      "epoch: 1 step: 825, loss is 0.31490546464920044\n",
      "epoch: 1 step: 826, loss is 0.34461939334869385\n",
      "epoch: 1 step: 827, loss is 0.3382967710494995\n",
      "epoch: 1 step: 828, loss is 0.6161020398139954\n",
      "epoch: 1 step: 829, loss is 0.8743693232536316\n",
      "epoch: 1 step: 830, loss is 0.14664575457572937\n",
      "epoch: 1 step: 831, loss is 0.401141881942749\n",
      "epoch: 1 step: 832, loss is 0.2858509123325348\n",
      "epoch: 1 step: 833, loss is 0.30708909034729004\n",
      "epoch: 1 step: 834, loss is 0.6890337467193604\n",
      "epoch: 1 step: 835, loss is 0.511994481086731\n",
      "epoch: 1 step: 836, loss is 0.4196901321411133\n",
      "epoch: 1 step: 837, loss is 0.39060765504837036\n",
      "epoch: 1 step: 838, loss is 0.37757188081741333\n",
      "epoch: 1 step: 839, loss is 0.2818034291267395\n",
      "epoch: 1 step: 840, loss is 0.5457082390785217\n",
      "epoch: 1 step: 841, loss is 0.5668257474899292\n",
      "epoch: 1 step: 842, loss is 0.32246455550193787\n",
      "epoch: 1 step: 843, loss is 0.4150890111923218\n",
      "epoch: 1 step: 844, loss is 0.5300227999687195\n",
      "epoch: 1 step: 845, loss is 0.47340714931488037\n",
      "epoch: 1 step: 846, loss is 0.4384678602218628\n",
      "epoch: 1 step: 847, loss is 0.2105284482240677\n",
      "epoch: 1 step: 848, loss is 0.27846166491508484\n",
      "epoch: 1 step: 849, loss is 0.2756022810935974\n",
      "epoch: 1 step: 850, loss is 0.23142479360103607\n",
      "epoch: 1 step: 851, loss is 0.5046516060829163\n",
      "epoch: 1 step: 852, loss is 0.22694410383701324\n",
      "epoch: 1 step: 853, loss is 0.3236466348171234\n",
      "epoch: 1 step: 854, loss is 0.2676016092300415\n",
      "epoch: 1 step: 855, loss is 0.33159327507019043\n",
      "epoch: 1 step: 856, loss is 0.437234491109848\n",
      "epoch: 1 step: 857, loss is 0.2552134692668915\n",
      "epoch: 1 step: 858, loss is 0.7729613780975342\n",
      "epoch: 1 step: 859, loss is 0.17592990398406982\n",
      "epoch: 1 step: 860, loss is 0.09359847009181976\n",
      "epoch: 1 step: 861, loss is 0.22852230072021484\n",
      "epoch: 1 step: 862, loss is 0.2356429547071457\n",
      "epoch: 1 step: 863, loss is 0.7679260969161987\n",
      "epoch: 1 step: 864, loss is 0.05811759829521179\n",
      "epoch: 1 step: 865, loss is 0.4915693700313568\n",
      "epoch: 1 step: 866, loss is 0.23872791230678558\n",
      "epoch: 1 step: 867, loss is 0.31876078248023987\n",
      "epoch: 1 step: 868, loss is 0.4882121682167053\n",
      "epoch: 1 step: 869, loss is 0.3303145170211792\n",
      "epoch: 1 step: 870, loss is 0.42137569189071655\n",
      "epoch: 1 step: 871, loss is 0.3391035497188568\n",
      "epoch: 1 step: 872, loss is 0.27218523621559143\n",
      "epoch: 1 step: 873, loss is 0.25079646706581116\n",
      "epoch: 1 step: 874, loss is 0.25570887327194214\n",
      "epoch: 1 step: 875, loss is 0.7925834059715271\n",
      "epoch: 1 step: 876, loss is 0.5004285573959351\n",
      "epoch: 1 step: 877, loss is 0.2586771547794342\n",
      "epoch: 1 step: 878, loss is 0.3357852101325989\n",
      "epoch: 1 step: 879, loss is 0.3073437511920929\n",
      "epoch: 1 step: 880, loss is 0.34677019715309143\n",
      "epoch: 1 step: 881, loss is 0.2655453681945801\n",
      "epoch: 1 step: 882, loss is 0.33048927783966064\n",
      "epoch: 1 step: 883, loss is 0.5254188776016235\n",
      "epoch: 1 step: 884, loss is 0.2121339589357376\n",
      "epoch: 1 step: 885, loss is 0.30141082406044006\n",
      "epoch: 1 step: 886, loss is 0.3659857511520386\n",
      "epoch: 1 step: 887, loss is 0.7077432870864868\n",
      "epoch: 1 step: 888, loss is 0.3891683518886566\n",
      "epoch: 1 step: 889, loss is 0.3548426926136017\n",
      "epoch: 1 step: 890, loss is 0.2859625220298767\n",
      "epoch: 1 step: 891, loss is 0.5656256079673767\n",
      "epoch: 1 step: 892, loss is 0.38812264800071716\n",
      "epoch: 1 step: 893, loss is 0.22464235126972198\n",
      "epoch: 1 step: 894, loss is 0.6803387999534607\n",
      "epoch: 1 step: 895, loss is 0.3170353174209595\n",
      "epoch: 1 step: 896, loss is 0.794771671295166\n",
      "epoch: 1 step: 897, loss is 0.3813760280609131\n",
      "epoch: 1 step: 898, loss is 0.21458356082439423\n",
      "epoch: 1 step: 899, loss is 0.2773813009262085\n",
      "epoch: 1 step: 900, loss is 0.1209799274802208\n",
      "epoch: 1 step: 901, loss is 0.178055077791214\n",
      "epoch: 1 step: 902, loss is 0.23290368914604187\n",
      "epoch: 1 step: 903, loss is 0.13047178089618683\n",
      "epoch: 1 step: 904, loss is 0.1095137670636177\n",
      "epoch: 1 step: 905, loss is 0.23166070878505707\n",
      "epoch: 1 step: 906, loss is 0.1834343820810318\n",
      "epoch: 1 step: 907, loss is 0.15202747285366058\n",
      "epoch: 1 step: 908, loss is 0.46507731080055237\n",
      "epoch: 1 step: 909, loss is 0.1639498919248581\n",
      "epoch: 1 step: 910, loss is 0.2093433290719986\n",
      "epoch: 1 step: 911, loss is 0.23520876467227936\n",
      "epoch: 1 step: 912, loss is 0.5020503401756287\n",
      "epoch: 1 step: 913, loss is 0.36067238450050354\n",
      "epoch: 1 step: 914, loss is 0.08162976801395416\n",
      "epoch: 1 step: 915, loss is 0.10143546015024185\n",
      "epoch: 1 step: 916, loss is 0.5655164122581482\n",
      "epoch: 1 step: 917, loss is 0.39885273575782776\n",
      "epoch: 1 step: 918, loss is 0.39914143085479736\n",
      "epoch: 1 step: 919, loss is 0.14688634872436523\n",
      "epoch: 1 step: 920, loss is 0.133500337600708\n",
      "epoch: 1 step: 921, loss is 0.23780779540538788\n",
      "epoch: 1 step: 922, loss is 0.43310829997062683\n",
      "epoch: 1 step: 923, loss is 0.4143301546573639\n",
      "epoch: 1 step: 924, loss is 0.4662550687789917\n",
      "epoch: 1 step: 925, loss is 0.127506822347641\n",
      "epoch: 1 step: 926, loss is 0.17282244563102722\n",
      "epoch: 1 step: 927, loss is 0.2942500114440918\n",
      "epoch: 1 step: 928, loss is 0.29453301429748535\n",
      "epoch: 1 step: 929, loss is 0.11697273701429367\n",
      "epoch: 1 step: 930, loss is 0.6303799152374268\n",
      "epoch: 1 step: 931, loss is 0.16132773458957672\n",
      "epoch: 1 step: 932, loss is 0.17646124958992004\n",
      "epoch: 1 step: 933, loss is 0.4651935398578644\n",
      "epoch: 1 step: 934, loss is 0.17784731090068817\n",
      "epoch: 1 step: 935, loss is 0.31167253851890564\n",
      "epoch: 1 step: 936, loss is 0.28931912779808044\n",
      "epoch: 1 step: 937, loss is 0.5653125047683716\n",
      "epoch: 1 step: 938, loss is 0.20612716674804688\n",
      "epoch: 1 step: 939, loss is 0.2685600221157074\n",
      "epoch: 1 step: 940, loss is 0.13467825949192047\n",
      "epoch: 1 step: 941, loss is 0.20657606422901154\n",
      "epoch: 1 step: 942, loss is 0.047137755900621414\n",
      "epoch: 1 step: 943, loss is 0.32396259903907776\n",
      "epoch: 1 step: 944, loss is 0.0557825043797493\n",
      "epoch: 1 step: 945, loss is 0.08912798017263412\n",
      "epoch: 1 step: 946, loss is 0.19449731707572937\n",
      "epoch: 1 step: 947, loss is 0.2188444882631302\n",
      "epoch: 1 step: 948, loss is 0.2708515524864197\n",
      "epoch: 1 step: 949, loss is 0.29074570536613464\n",
      "epoch: 1 step: 950, loss is 0.1949114352464676\n",
      "epoch: 1 step: 951, loss is 0.3823513686656952\n",
      "epoch: 1 step: 952, loss is 0.2927308976650238\n",
      "epoch: 1 step: 953, loss is 0.49161648750305176\n",
      "epoch: 1 step: 954, loss is 0.16948308050632477\n",
      "epoch: 1 step: 955, loss is 0.14694955945014954\n",
      "epoch: 1 step: 956, loss is 0.22808116674423218\n",
      "epoch: 1 step: 957, loss is 0.26243239641189575\n",
      "epoch: 1 step: 958, loss is 0.414737343788147\n",
      "epoch: 1 step: 959, loss is 0.16531424224376678\n",
      "epoch: 1 step: 960, loss is 0.0758194774389267\n",
      "epoch: 1 step: 961, loss is 0.24890126287937164\n",
      "epoch: 1 step: 962, loss is 0.05331936478614807\n",
      "epoch: 1 step: 963, loss is 0.2972557544708252\n",
      "epoch: 1 step: 964, loss is 0.29173028469085693\n",
      "epoch: 1 step: 965, loss is 0.049920521676540375\n",
      "epoch: 1 step: 966, loss is 0.26599887013435364\n",
      "epoch: 1 step: 967, loss is 0.13179254531860352\n",
      "epoch: 1 step: 968, loss is 0.11702112853527069\n",
      "epoch: 1 step: 969, loss is 0.19254349172115326\n",
      "epoch: 1 step: 970, loss is 0.2655348479747772\n",
      "epoch: 1 step: 971, loss is 0.27859798073768616\n",
      "epoch: 1 step: 972, loss is 0.17310647666454315\n",
      "epoch: 1 step: 973, loss is 0.06098861247301102\n",
      "epoch: 1 step: 974, loss is 0.18024645745754242\n",
      "epoch: 1 step: 975, loss is 0.06705595552921295\n",
      "epoch: 1 step: 976, loss is 0.49142539501190186\n",
      "epoch: 1 step: 977, loss is 0.07830528169870377\n",
      "epoch: 1 step: 978, loss is 0.035730428993701935\n",
      "epoch: 1 step: 979, loss is 0.3523896038532257\n",
      "epoch: 1 step: 980, loss is 0.1262192577123642\n",
      "epoch: 1 step: 981, loss is 0.04793030396103859\n",
      "epoch: 1 step: 982, loss is 0.486185222864151\n",
      "epoch: 1 step: 983, loss is 0.2878473401069641\n",
      "epoch: 1 step: 984, loss is 0.18261471390724182\n",
      "epoch: 1 step: 985, loss is 0.16539359092712402\n",
      "epoch: 1 step: 986, loss is 0.21712365746498108\n",
      "epoch: 1 step: 987, loss is 0.14219194650650024\n",
      "epoch: 1 step: 988, loss is 0.19484679400920868\n",
      "epoch: 1 step: 989, loss is 0.056005947291851044\n",
      "epoch: 1 step: 990, loss is 0.07597009837627411\n",
      "epoch: 1 step: 991, loss is 0.1566135138273239\n",
      "epoch: 1 step: 992, loss is 0.18257854878902435\n",
      "epoch: 1 step: 993, loss is 0.31937769055366516\n",
      "epoch: 1 step: 994, loss is 0.47283416986465454\n",
      "epoch: 1 step: 995, loss is 0.17621922492980957\n",
      "epoch: 1 step: 996, loss is 0.5605990886688232\n",
      "epoch: 1 step: 997, loss is 0.29273468255996704\n",
      "epoch: 1 step: 998, loss is 0.22215449810028076\n",
      "epoch: 1 step: 999, loss is 0.26488998532295227\n",
      "epoch: 1 step: 1000, loss is 0.221559539437294\n",
      "epoch: 1 step: 1001, loss is 0.09119228273630142\n",
      "epoch: 1 step: 1002, loss is 0.1358513981103897\n",
      "epoch: 1 step: 1003, loss is 0.1595420390367508\n",
      "epoch: 1 step: 1004, loss is 0.11046168208122253\n",
      "epoch: 1 step: 1005, loss is 0.26674726605415344\n",
      "epoch: 1 step: 1006, loss is 0.0635964646935463\n",
      "epoch: 1 step: 1007, loss is 0.22402773797512054\n",
      "epoch: 1 step: 1008, loss is 0.24657878279685974\n",
      "epoch: 1 step: 1009, loss is 0.3250371217727661\n",
      "epoch: 1 step: 1010, loss is 0.10433557629585266\n",
      "epoch: 1 step: 1011, loss is 0.13444972038269043\n",
      "epoch: 1 step: 1012, loss is 0.23154474794864655\n",
      "epoch: 1 step: 1013, loss is 0.08986858278512955\n",
      "epoch: 1 step: 1014, loss is 0.16325707733631134\n",
      "epoch: 1 step: 1015, loss is 0.12754730880260468\n",
      "epoch: 1 step: 1016, loss is 0.15959717333316803\n",
      "epoch: 1 step: 1017, loss is 0.10453403741121292\n",
      "epoch: 1 step: 1018, loss is 0.06282807141542435\n",
      "epoch: 1 step: 1019, loss is 0.3027274012565613\n",
      "epoch: 1 step: 1020, loss is 0.23371827602386475\n",
      "epoch: 1 step: 1021, loss is 0.14427316188812256\n",
      "epoch: 1 step: 1022, loss is 0.43506136536598206\n",
      "epoch: 1 step: 1023, loss is 0.14879721403121948\n",
      "epoch: 1 step: 1024, loss is 0.2484402358531952\n",
      "epoch: 1 step: 1025, loss is 0.28396743535995483\n",
      "epoch: 1 step: 1026, loss is 0.15838876366615295\n",
      "epoch: 1 step: 1027, loss is 0.08898777514696121\n",
      "epoch: 1 step: 1028, loss is 0.038543570786714554\n",
      "epoch: 1 step: 1029, loss is 0.2520211338996887\n",
      "epoch: 1 step: 1030, loss is 0.061478693038225174\n",
      "epoch: 1 step: 1031, loss is 0.062162674963474274\n",
      "epoch: 1 step: 1032, loss is 0.10076835751533508\n",
      "epoch: 1 step: 1033, loss is 0.22981992363929749\n",
      "epoch: 1 step: 1034, loss is 0.19127248227596283\n",
      "epoch: 1 step: 1035, loss is 0.10122236609458923\n",
      "epoch: 1 step: 1036, loss is 0.22828352451324463\n",
      "epoch: 1 step: 1037, loss is 0.16712310910224915\n",
      "epoch: 1 step: 1038, loss is 0.09219446033239365\n",
      "epoch: 1 step: 1039, loss is 0.07384244352579117\n",
      "epoch: 1 step: 1040, loss is 0.19557297229766846\n",
      "epoch: 1 step: 1041, loss is 0.09032342582941055\n",
      "epoch: 1 step: 1042, loss is 0.010047266259789467\n",
      "epoch: 1 step: 1043, loss is 0.04369286820292473\n",
      "epoch: 1 step: 1044, loss is 0.3173683285713196\n",
      "epoch: 1 step: 1045, loss is 0.1668943613767624\n",
      "epoch: 1 step: 1046, loss is 0.2513115406036377\n",
      "epoch: 1 step: 1047, loss is 0.09660735726356506\n",
      "epoch: 1 step: 1048, loss is 0.14372199773788452\n",
      "epoch: 1 step: 1049, loss is 0.06389210373163223\n",
      "epoch: 1 step: 1050, loss is 0.19616825878620148\n",
      "epoch: 1 step: 1051, loss is 0.43617957830429077\n",
      "epoch: 1 step: 1052, loss is 0.01707618683576584\n",
      "epoch: 1 step: 1053, loss is 0.17687606811523438\n",
      "epoch: 1 step: 1054, loss is 0.01473997626453638\n",
      "epoch: 1 step: 1055, loss is 0.2370799481868744\n",
      "epoch: 1 step: 1056, loss is 0.06824447214603424\n",
      "epoch: 1 step: 1057, loss is 0.2944164574146271\n",
      "epoch: 1 step: 1058, loss is 0.21241648495197296\n",
      "epoch: 1 step: 1059, loss is 0.3021658658981323\n",
      "epoch: 1 step: 1060, loss is 0.27382075786590576\n",
      "epoch: 1 step: 1061, loss is 0.02549614943563938\n",
      "epoch: 1 step: 1062, loss is 0.15958209335803986\n",
      "epoch: 1 step: 1063, loss is 0.18983161449432373\n",
      "epoch: 1 step: 1064, loss is 0.4501221179962158\n",
      "epoch: 1 step: 1065, loss is 0.13436682522296906\n",
      "epoch: 1 step: 1066, loss is 0.11581918597221375\n",
      "epoch: 1 step: 1067, loss is 0.055309657007455826\n",
      "epoch: 1 step: 1068, loss is 0.14793674647808075\n",
      "epoch: 1 step: 1069, loss is 0.21922259032726288\n",
      "epoch: 1 step: 1070, loss is 0.05894182622432709\n",
      "epoch: 1 step: 1071, loss is 0.5677720308303833\n",
      "epoch: 1 step: 1072, loss is 0.3370979428291321\n",
      "epoch: 1 step: 1073, loss is 0.09834006428718567\n",
      "epoch: 1 step: 1074, loss is 0.2095015048980713\n",
      "epoch: 1 step: 1075, loss is 0.5028504729270935\n",
      "epoch: 1 step: 1076, loss is 0.26864373683929443\n",
      "epoch: 1 step: 1077, loss is 0.21426385641098022\n",
      "epoch: 1 step: 1078, loss is 0.12963616847991943\n",
      "epoch: 1 step: 1079, loss is 0.2432151585817337\n",
      "epoch: 1 step: 1080, loss is 0.2584565579891205\n",
      "epoch: 1 step: 1081, loss is 0.04321635141968727\n",
      "epoch: 1 step: 1082, loss is 0.14493438601493835\n",
      "epoch: 1 step: 1083, loss is 0.21265006065368652\n",
      "epoch: 1 step: 1084, loss is 0.1596021056175232\n",
      "epoch: 1 step: 1085, loss is 0.3044322431087494\n",
      "epoch: 1 step: 1086, loss is 0.1598869115114212\n",
      "epoch: 1 step: 1087, loss is 0.1449197679758072\n",
      "epoch: 1 step: 1088, loss is 0.16226588189601898\n",
      "epoch: 1 step: 1089, loss is 0.11522360891103745\n",
      "epoch: 1 step: 1090, loss is 0.20336998999118805\n",
      "epoch: 1 step: 1091, loss is 0.07890193164348602\n",
      "epoch: 1 step: 1092, loss is 0.143443301320076\n",
      "epoch: 1 step: 1093, loss is 0.03161678463220596\n",
      "epoch: 1 step: 1094, loss is 0.09625329077243805\n",
      "epoch: 1 step: 1095, loss is 0.28799110651016235\n",
      "epoch: 1 step: 1096, loss is 0.2258068472146988\n",
      "epoch: 1 step: 1097, loss is 0.35640206933021545\n",
      "epoch: 1 step: 1098, loss is 0.04963360354304314\n",
      "epoch: 1 step: 1099, loss is 0.39417457580566406\n",
      "epoch: 1 step: 1100, loss is 0.10167524218559265\n",
      "epoch: 1 step: 1101, loss is 0.18071907758712769\n",
      "epoch: 1 step: 1102, loss is 0.11870604753494263\n",
      "epoch: 1 step: 1103, loss is 0.07750387489795685\n",
      "epoch: 1 step: 1104, loss is 0.11694252490997314\n",
      "epoch: 1 step: 1105, loss is 0.30381205677986145\n",
      "epoch: 1 step: 1106, loss is 0.11001650243997574\n",
      "epoch: 1 step: 1107, loss is 0.1732090562582016\n",
      "epoch: 1 step: 1108, loss is 0.199860081076622\n",
      "epoch: 1 step: 1109, loss is 0.1411968618631363\n",
      "epoch: 1 step: 1110, loss is 0.059139590710401535\n",
      "epoch: 1 step: 1111, loss is 0.05387282371520996\n",
      "epoch: 1 step: 1112, loss is 0.3814752995967865\n",
      "epoch: 1 step: 1113, loss is 0.11358766257762909\n",
      "epoch: 1 step: 1114, loss is 0.4518970847129822\n",
      "epoch: 1 step: 1115, loss is 0.17559275031089783\n",
      "epoch: 1 step: 1116, loss is 0.11989115178585052\n",
      "epoch: 1 step: 1117, loss is 0.05313052982091904\n",
      "epoch: 1 step: 1118, loss is 0.14807075262069702\n",
      "epoch: 1 step: 1119, loss is 0.2416233867406845\n",
      "epoch: 1 step: 1120, loss is 0.4346759021282196\n",
      "epoch: 1 step: 1121, loss is 0.38804757595062256\n",
      "epoch: 1 step: 1122, loss is 0.17869554460048676\n",
      "epoch: 1 step: 1123, loss is 0.10163751244544983\n",
      "epoch: 1 step: 1124, loss is 0.2030329704284668\n",
      "epoch: 1 step: 1125, loss is 0.31081444025039673\n",
      "epoch: 1 step: 1126, loss is 0.005754808895289898\n",
      "epoch: 1 step: 1127, loss is 0.2968977093696594\n",
      "epoch: 1 step: 1128, loss is 0.07419965416193008\n",
      "epoch: 1 step: 1129, loss is 0.1727411299943924\n",
      "epoch: 1 step: 1130, loss is 0.2607368230819702\n",
      "epoch: 1 step: 1131, loss is 0.3322349786758423\n",
      "epoch: 1 step: 1132, loss is 0.19476652145385742\n",
      "epoch: 1 step: 1133, loss is 0.5576346516609192\n",
      "epoch: 1 step: 1134, loss is 0.27701371908187866\n",
      "epoch: 1 step: 1135, loss is 0.09510796517133713\n",
      "epoch: 1 step: 1136, loss is 0.09447668492794037\n",
      "epoch: 1 step: 1137, loss is 0.19361311197280884\n",
      "epoch: 1 step: 1138, loss is 0.051690079271793365\n",
      "epoch: 1 step: 1139, loss is 0.1558239459991455\n",
      "epoch: 1 step: 1140, loss is 0.2238369584083557\n",
      "epoch: 1 step: 1141, loss is 0.15382976830005646\n",
      "epoch: 1 step: 1142, loss is 0.12755730748176575\n",
      "epoch: 1 step: 1143, loss is 0.37110012769699097\n",
      "epoch: 1 step: 1144, loss is 0.5385813117027283\n",
      "epoch: 1 step: 1145, loss is 0.16329488158226013\n",
      "epoch: 1 step: 1146, loss is 0.1685350239276886\n",
      "epoch: 1 step: 1147, loss is 0.08201318979263306\n",
      "epoch: 1 step: 1148, loss is 0.2941350042819977\n",
      "epoch: 1 step: 1149, loss is 0.09532869607210159\n",
      "epoch: 1 step: 1150, loss is 0.051434971392154694\n",
      "epoch: 1 step: 1151, loss is 0.2475966215133667\n",
      "epoch: 1 step: 1152, loss is 0.1468697041273117\n",
      "epoch: 1 step: 1153, loss is 0.12005492299795151\n",
      "epoch: 1 step: 1154, loss is 0.17529548704624176\n",
      "epoch: 1 step: 1155, loss is 0.19395457208156586\n",
      "epoch: 1 step: 1156, loss is 0.1525527387857437\n",
      "epoch: 1 step: 1157, loss is 0.09134063869714737\n",
      "epoch: 1 step: 1158, loss is 0.2535334527492523\n",
      "epoch: 1 step: 1159, loss is 0.3383716940879822\n",
      "epoch: 1 step: 1160, loss is 0.48152974247932434\n",
      "epoch: 1 step: 1161, loss is 0.03977722302079201\n",
      "epoch: 1 step: 1162, loss is 0.08100517839193344\n",
      "epoch: 1 step: 1163, loss is 0.06541787832975388\n",
      "epoch: 1 step: 1164, loss is 0.06910008937120438\n",
      "epoch: 1 step: 1165, loss is 0.06955268234014511\n",
      "epoch: 1 step: 1166, loss is 0.07803916931152344\n",
      "epoch: 1 step: 1167, loss is 0.09028416872024536\n",
      "epoch: 1 step: 1168, loss is 0.0919075459241867\n",
      "epoch: 1 step: 1169, loss is 0.11622228473424911\n",
      "epoch: 1 step: 1170, loss is 0.03488651663064957\n",
      "epoch: 1 step: 1171, loss is 0.03355158865451813\n",
      "epoch: 1 step: 1172, loss is 0.016541406512260437\n",
      "epoch: 1 step: 1173, loss is 0.29356157779693604\n",
      "epoch: 1 step: 1174, loss is 0.3791044056415558\n",
      "epoch: 1 step: 1175, loss is 0.2347092181444168\n",
      "epoch: 1 step: 1176, loss is 0.25929394364356995\n",
      "epoch: 1 step: 1177, loss is 0.4455251395702362\n",
      "epoch: 1 step: 1178, loss is 0.20028820633888245\n",
      "epoch: 1 step: 1179, loss is 0.16249357163906097\n",
      "epoch: 1 step: 1180, loss is 0.12750424444675446\n",
      "epoch: 1 step: 1181, loss is 0.05496975779533386\n",
      "epoch: 1 step: 1182, loss is 0.17739909887313843\n",
      "epoch: 1 step: 1183, loss is 0.04693957418203354\n",
      "epoch: 1 step: 1184, loss is 0.16288742423057556\n",
      "epoch: 1 step: 1185, loss is 0.036069355905056\n",
      "epoch: 1 step: 1186, loss is 0.17524704337120056\n",
      "epoch: 1 step: 1187, loss is 0.014716081321239471\n",
      "epoch: 1 step: 1188, loss is 0.4402260184288025\n",
      "epoch: 1 step: 1189, loss is 0.18986178934574127\n",
      "epoch: 1 step: 1190, loss is 0.2631506323814392\n",
      "epoch: 1 step: 1191, loss is 0.0985022783279419\n",
      "epoch: 1 step: 1192, loss is 0.049404896795749664\n",
      "epoch: 1 step: 1193, loss is 0.17576858401298523\n",
      "epoch: 1 step: 1194, loss is 0.2106504589319229\n",
      "epoch: 1 step: 1195, loss is 0.09360016882419586\n",
      "epoch: 1 step: 1196, loss is 0.24525178968906403\n",
      "epoch: 1 step: 1197, loss is 0.23540730774402618\n",
      "epoch: 1 step: 1198, loss is 0.05846592038869858\n",
      "epoch: 1 step: 1199, loss is 0.20691615343093872\n",
      "epoch: 1 step: 1200, loss is 0.39711135625839233\n",
      "epoch: 1 step: 1201, loss is 0.10770723968744278\n",
      "epoch: 1 step: 1202, loss is 0.09575685858726501\n",
      "epoch: 1 step: 1203, loss is 0.20908690989017487\n",
      "epoch: 1 step: 1204, loss is 0.11468101292848587\n",
      "epoch: 1 step: 1205, loss is 0.23277701437473297\n",
      "epoch: 1 step: 1206, loss is 0.26976948976516724\n",
      "epoch: 1 step: 1207, loss is 0.02749140001833439\n",
      "epoch: 1 step: 1208, loss is 0.09162245690822601\n",
      "epoch: 1 step: 1209, loss is 0.1348646879196167\n",
      "epoch: 1 step: 1210, loss is 0.2287105768918991\n",
      "epoch: 1 step: 1211, loss is 0.1484532356262207\n",
      "epoch: 1 step: 1212, loss is 0.3999752998352051\n",
      "epoch: 1 step: 1213, loss is 0.15757855772972107\n",
      "epoch: 1 step: 1214, loss is 0.08805375546216965\n",
      "epoch: 1 step: 1215, loss is 0.22034233808517456\n",
      "epoch: 1 step: 1216, loss is 0.16228604316711426\n",
      "epoch: 1 step: 1217, loss is 0.009586176835000515\n",
      "epoch: 1 step: 1218, loss is 0.15515387058258057\n",
      "epoch: 1 step: 1219, loss is 0.15990960597991943\n",
      "epoch: 1 step: 1220, loss is 0.34699714183807373\n",
      "epoch: 1 step: 1221, loss is 0.16071416437625885\n",
      "epoch: 1 step: 1222, loss is 0.09827475994825363\n",
      "epoch: 1 step: 1223, loss is 0.12666328251361847\n",
      "epoch: 1 step: 1224, loss is 0.1830003708600998\n",
      "epoch: 1 step: 1225, loss is 0.4811426103115082\n",
      "epoch: 1 step: 1226, loss is 0.04746713116765022\n",
      "epoch: 1 step: 1227, loss is 0.2876097857952118\n",
      "epoch: 1 step: 1228, loss is 0.09837514162063599\n",
      "epoch: 1 step: 1229, loss is 0.019707150757312775\n",
      "epoch: 1 step: 1230, loss is 0.21726427972316742\n",
      "epoch: 1 step: 1231, loss is 0.06557848304510117\n",
      "epoch: 1 step: 1232, loss is 0.07941630482673645\n",
      "epoch: 1 step: 1233, loss is 0.052012257277965546\n",
      "epoch: 1 step: 1234, loss is 0.21371221542358398\n",
      "epoch: 1 step: 1235, loss is 0.30264419317245483\n",
      "epoch: 1 step: 1236, loss is 0.22372128069400787\n",
      "epoch: 1 step: 1237, loss is 0.039483919739723206\n",
      "epoch: 1 step: 1238, loss is 0.4410218894481659\n",
      "epoch: 1 step: 1239, loss is 0.23074418306350708\n",
      "epoch: 1 step: 1240, loss is 0.13762353360652924\n",
      "epoch: 1 step: 1241, loss is 0.1298053115606308\n",
      "epoch: 1 step: 1242, loss is 0.29087185859680176\n",
      "epoch: 1 step: 1243, loss is 0.08970847725868225\n",
      "epoch: 1 step: 1244, loss is 0.10434938967227936\n",
      "epoch: 1 step: 1245, loss is 0.18926608562469482\n",
      "epoch: 1 step: 1246, loss is 0.5032597780227661\n",
      "epoch: 1 step: 1247, loss is 0.1863315999507904\n",
      "epoch: 1 step: 1248, loss is 0.22507663071155548\n",
      "epoch: 1 step: 1249, loss is 0.07500948011875153\n",
      "epoch: 1 step: 1250, loss is 0.061261389404535294\n",
      "epoch: 1 step: 1251, loss is 0.03642372786998749\n",
      "epoch: 1 step: 1252, loss is 0.19430838525295258\n",
      "epoch: 1 step: 1253, loss is 0.28537651896476746\n",
      "epoch: 1 step: 1254, loss is 0.06936585158109665\n",
      "epoch: 1 step: 1255, loss is 0.0857149288058281\n",
      "epoch: 1 step: 1256, loss is 0.20211078226566315\n",
      "epoch: 1 step: 1257, loss is 0.04500045254826546\n",
      "epoch: 1 step: 1258, loss is 0.16918009519577026\n",
      "epoch: 1 step: 1259, loss is 0.27893301844596863\n",
      "epoch: 1 step: 1260, loss is 0.12250544875860214\n",
      "epoch: 1 step: 1261, loss is 0.25243884325027466\n",
      "epoch: 1 step: 1262, loss is 0.15806204080581665\n",
      "epoch: 1 step: 1263, loss is 0.09576763957738876\n",
      "epoch: 1 step: 1264, loss is 0.17501933872699738\n",
      "epoch: 1 step: 1265, loss is 0.1981787383556366\n",
      "epoch: 1 step: 1266, loss is 0.12273995578289032\n",
      "epoch: 1 step: 1267, loss is 0.14687956869602203\n",
      "epoch: 1 step: 1268, loss is 0.22472596168518066\n",
      "epoch: 1 step: 1269, loss is 0.05366943031549454\n",
      "epoch: 1 step: 1270, loss is 0.13605546951293945\n",
      "epoch: 1 step: 1271, loss is 0.21827317774295807\n",
      "epoch: 1 step: 1272, loss is 0.3212417960166931\n",
      "epoch: 1 step: 1273, loss is 0.3874649405479431\n",
      "epoch: 1 step: 1274, loss is 0.1518736332654953\n",
      "epoch: 1 step: 1275, loss is 0.07241371273994446\n",
      "epoch: 1 step: 1276, loss is 0.02880379743874073\n",
      "epoch: 1 step: 1277, loss is 0.07996433973312378\n",
      "epoch: 1 step: 1278, loss is 0.3905412554740906\n",
      "epoch: 1 step: 1279, loss is 0.1126074269413948\n",
      "epoch: 1 step: 1280, loss is 0.024442605674266815\n",
      "epoch: 1 step: 1281, loss is 0.11385676264762878\n",
      "epoch: 1 step: 1282, loss is 0.23185193538665771\n",
      "epoch: 1 step: 1283, loss is 0.022081121802330017\n",
      "epoch: 1 step: 1284, loss is 0.17889925837516785\n",
      "epoch: 1 step: 1285, loss is 0.0706586167216301\n",
      "epoch: 1 step: 1286, loss is 0.055894725024700165\n",
      "epoch: 1 step: 1287, loss is 0.1609707474708557\n",
      "epoch: 1 step: 1288, loss is 0.04037340357899666\n",
      "epoch: 1 step: 1289, loss is 0.05784601718187332\n",
      "epoch: 1 step: 1290, loss is 0.23386065661907196\n",
      "epoch: 1 step: 1291, loss is 0.16670720279216766\n",
      "epoch: 1 step: 1292, loss is 0.057976480573415756\n",
      "epoch: 1 step: 1293, loss is 0.3463383615016937\n",
      "epoch: 1 step: 1294, loss is 0.06369388103485107\n",
      "epoch: 1 step: 1295, loss is 0.19459232687950134\n",
      "epoch: 1 step: 1296, loss is 0.06509951502084732\n",
      "epoch: 1 step: 1297, loss is 0.12898245453834534\n",
      "epoch: 1 step: 1298, loss is 0.6011378765106201\n",
      "epoch: 1 step: 1299, loss is 0.10361787676811218\n",
      "epoch: 1 step: 1300, loss is 0.07279905676841736\n",
      "epoch: 1 step: 1301, loss is 0.06041165813803673\n",
      "epoch: 1 step: 1302, loss is 0.175031378865242\n",
      "epoch: 1 step: 1303, loss is 0.4294033944606781\n",
      "epoch: 1 step: 1304, loss is 0.17646265029907227\n",
      "epoch: 1 step: 1305, loss is 0.2609892189502716\n",
      "epoch: 1 step: 1306, loss is 0.18412329256534576\n",
      "epoch: 1 step: 1307, loss is 0.14264576137065887\n",
      "epoch: 1 step: 1308, loss is 0.20940575003623962\n",
      "epoch: 1 step: 1309, loss is 0.09945346415042877\n",
      "epoch: 1 step: 1310, loss is 0.15469393134117126\n",
      "epoch: 1 step: 1311, loss is 0.23924820125102997\n",
      "epoch: 1 step: 1312, loss is 0.11802760511636734\n",
      "epoch: 1 step: 1313, loss is 0.17264220118522644\n",
      "epoch: 1 step: 1314, loss is 0.15069591999053955\n",
      "epoch: 1 step: 1315, loss is 0.039299555122852325\n",
      "epoch: 1 step: 1316, loss is 0.15611866116523743\n",
      "epoch: 1 step: 1317, loss is 0.048066768795251846\n",
      "epoch: 1 step: 1318, loss is 0.2126239389181137\n",
      "epoch: 1 step: 1319, loss is 0.3058006465435028\n",
      "epoch: 1 step: 1320, loss is 0.25945809483528137\n",
      "epoch: 1 step: 1321, loss is 0.1505933552980423\n",
      "epoch: 1 step: 1322, loss is 0.26184508204460144\n",
      "epoch: 1 step: 1323, loss is 0.21652677655220032\n",
      "epoch: 1 step: 1324, loss is 0.12487529963254929\n",
      "epoch: 1 step: 1325, loss is 0.15467678010463715\n",
      "epoch: 1 step: 1326, loss is 0.16400864720344543\n",
      "epoch: 1 step: 1327, loss is 0.13074469566345215\n",
      "epoch: 1 step: 1328, loss is 0.26251721382141113\n",
      "epoch: 1 step: 1329, loss is 0.05845549702644348\n",
      "epoch: 1 step: 1330, loss is 0.30344340205192566\n",
      "epoch: 1 step: 1331, loss is 0.2720928192138672\n",
      "epoch: 1 step: 1332, loss is 0.09267423301935196\n",
      "epoch: 1 step: 1333, loss is 0.0556710809469223\n",
      "epoch: 1 step: 1334, loss is 0.07121608406305313\n",
      "epoch: 1 step: 1335, loss is 0.043002765625715256\n",
      "epoch: 1 step: 1336, loss is 0.0736188292503357\n",
      "epoch: 1 step: 1337, loss is 0.08979522436857224\n",
      "epoch: 1 step: 1338, loss is 0.2224358320236206\n",
      "epoch: 1 step: 1339, loss is 0.015806635841727257\n",
      "epoch: 1 step: 1340, loss is 0.08732452988624573\n",
      "epoch: 1 step: 1341, loss is 0.09394737333059311\n",
      "epoch: 1 step: 1342, loss is 0.13618029654026031\n",
      "epoch: 1 step: 1343, loss is 0.2242429256439209\n",
      "epoch: 1 step: 1344, loss is 0.4808960556983948\n",
      "epoch: 1 step: 1345, loss is 0.1958097219467163\n",
      "epoch: 1 step: 1346, loss is 0.09542232751846313\n",
      "epoch: 1 step: 1347, loss is 0.4647158682346344\n",
      "epoch: 1 step: 1348, loss is 0.13596944510936737\n",
      "epoch: 1 step: 1349, loss is 0.11308708786964417\n",
      "epoch: 1 step: 1350, loss is 0.018291933462023735\n",
      "epoch: 1 step: 1351, loss is 0.1467258185148239\n",
      "epoch: 1 step: 1352, loss is 0.22117821872234344\n",
      "epoch: 1 step: 1353, loss is 0.12707291543483734\n",
      "epoch: 1 step: 1354, loss is 0.11156409233808517\n",
      "epoch: 1 step: 1355, loss is 0.2076820433139801\n",
      "epoch: 1 step: 1356, loss is 0.169033482670784\n",
      "epoch: 1 step: 1357, loss is 0.0632162094116211\n",
      "epoch: 1 step: 1358, loss is 0.03359483182430267\n",
      "epoch: 1 step: 1359, loss is 0.43572530150413513\n",
      "epoch: 1 step: 1360, loss is 0.2195078432559967\n",
      "epoch: 1 step: 1361, loss is 0.019837327301502228\n",
      "epoch: 1 step: 1362, loss is 0.26008981466293335\n",
      "epoch: 1 step: 1363, loss is 0.14402642846107483\n",
      "epoch: 1 step: 1364, loss is 0.05935679003596306\n",
      "epoch: 1 step: 1365, loss is 0.28834280371665955\n",
      "epoch: 1 step: 1366, loss is 0.15212686359882355\n",
      "epoch: 1 step: 1367, loss is 0.2714555263519287\n",
      "epoch: 1 step: 1368, loss is 0.09504136443138123\n",
      "epoch: 1 step: 1369, loss is 0.28933772444725037\n",
      "epoch: 1 step: 1370, loss is 0.14560070633888245\n",
      "epoch: 1 step: 1371, loss is 0.3072494864463806\n",
      "epoch: 1 step: 1372, loss is 0.1558079868555069\n",
      "epoch: 1 step: 1373, loss is 0.1220991313457489\n",
      "epoch: 1 step: 1374, loss is 0.18953067064285278\n",
      "epoch: 1 step: 1375, loss is 0.034543294459581375\n",
      "epoch: 1 step: 1376, loss is 0.10328174382448196\n",
      "epoch: 1 step: 1377, loss is 0.024781575426459312\n",
      "epoch: 1 step: 1378, loss is 0.07750483602285385\n",
      "epoch: 1 step: 1379, loss is 0.026241963729262352\n",
      "epoch: 1 step: 1380, loss is 0.14996883273124695\n",
      "epoch: 1 step: 1381, loss is 0.33437561988830566\n",
      "epoch: 1 step: 1382, loss is 0.28114795684814453\n",
      "epoch: 1 step: 1383, loss is 0.46972352266311646\n",
      "epoch: 1 step: 1384, loss is 0.08700280636548996\n",
      "epoch: 1 step: 1385, loss is 0.02692125365138054\n",
      "epoch: 1 step: 1386, loss is 0.057440873235464096\n",
      "epoch: 1 step: 1387, loss is 0.06447600573301315\n",
      "epoch: 1 step: 1388, loss is 0.7306717038154602\n",
      "epoch: 1 step: 1389, loss is 0.18444092571735382\n",
      "epoch: 1 step: 1390, loss is 0.36090752482414246\n",
      "epoch: 1 step: 1391, loss is 0.033602211624383926\n",
      "epoch: 1 step: 1392, loss is 0.08548524230718613\n",
      "epoch: 1 step: 1393, loss is 0.07800491154193878\n",
      "epoch: 1 step: 1394, loss is 0.09373381733894348\n",
      "epoch: 1 step: 1395, loss is 0.33743420243263245\n",
      "epoch: 1 step: 1396, loss is 0.29771479964256287\n",
      "epoch: 1 step: 1397, loss is 0.05129529908299446\n",
      "epoch: 1 step: 1398, loss is 0.15602105855941772\n",
      "epoch: 1 step: 1399, loss is 0.17974668741226196\n",
      "epoch: 1 step: 1400, loss is 0.16337627172470093\n",
      "epoch: 1 step: 1401, loss is 0.13323280215263367\n",
      "epoch: 1 step: 1402, loss is 0.0887790098786354\n",
      "epoch: 1 step: 1403, loss is 0.1177726686000824\n",
      "epoch: 1 step: 1404, loss is 0.09837545454502106\n",
      "epoch: 1 step: 1405, loss is 0.2269888073205948\n",
      "epoch: 1 step: 1406, loss is 0.06652004271745682\n",
      "epoch: 1 step: 1407, loss is 0.20110495388507843\n",
      "epoch: 1 step: 1408, loss is 0.057982657104730606\n",
      "epoch: 1 step: 1409, loss is 0.3883281946182251\n",
      "epoch: 1 step: 1410, loss is 0.04091497138142586\n",
      "epoch: 1 step: 1411, loss is 0.1952219307422638\n",
      "epoch: 1 step: 1412, loss is 0.05010809749364853\n",
      "epoch: 1 step: 1413, loss is 0.17181402444839478\n",
      "epoch: 1 step: 1414, loss is 0.0798921287059784\n",
      "epoch: 1 step: 1415, loss is 0.01838105171918869\n",
      "epoch: 1 step: 1416, loss is 0.22048871219158173\n",
      "epoch: 1 step: 1417, loss is 0.08170805871486664\n",
      "epoch: 1 step: 1418, loss is 0.24567317962646484\n",
      "epoch: 1 step: 1419, loss is 0.09216776490211487\n",
      "epoch: 1 step: 1420, loss is 0.23393210768699646\n",
      "epoch: 1 step: 1421, loss is 0.07191388309001923\n",
      "epoch: 1 step: 1422, loss is 0.1694338023662567\n",
      "epoch: 1 step: 1423, loss is 0.052910979837179184\n",
      "epoch: 1 step: 1424, loss is 0.039091095328330994\n",
      "epoch: 1 step: 1425, loss is 0.15625153481960297\n",
      "epoch: 1 step: 1426, loss is 0.1328132152557373\n",
      "epoch: 1 step: 1427, loss is 0.05261189118027687\n",
      "epoch: 1 step: 1428, loss is 0.01887013390660286\n",
      "epoch: 1 step: 1429, loss is 0.028072640299797058\n",
      "epoch: 1 step: 1430, loss is 0.09676560759544373\n",
      "epoch: 1 step: 1431, loss is 0.05219357833266258\n",
      "epoch: 1 step: 1432, loss is 0.07315757125616074\n",
      "epoch: 1 step: 1433, loss is 0.05427654832601547\n",
      "epoch: 1 step: 1434, loss is 0.021873943507671356\n",
      "epoch: 1 step: 1435, loss is 0.25929391384124756\n",
      "epoch: 1 step: 1436, loss is 0.012056311592459679\n",
      "epoch: 1 step: 1437, loss is 0.11455921083688736\n",
      "epoch: 1 step: 1438, loss is 0.15867996215820312\n",
      "epoch: 1 step: 1439, loss is 0.12209708988666534\n",
      "epoch: 1 step: 1440, loss is 0.07146202772855759\n",
      "epoch: 1 step: 1441, loss is 0.12369588017463684\n",
      "epoch: 1 step: 1442, loss is 0.03434408828616142\n",
      "epoch: 1 step: 1443, loss is 0.040385760366916656\n",
      "epoch: 1 step: 1444, loss is 0.17482224106788635\n",
      "epoch: 1 step: 1445, loss is 0.28313755989074707\n",
      "epoch: 1 step: 1446, loss is 0.031030435115098953\n",
      "epoch: 1 step: 1447, loss is 0.03188537806272507\n",
      "epoch: 1 step: 1448, loss is 0.07331853359937668\n",
      "epoch: 1 step: 1449, loss is 0.048723433166742325\n",
      "epoch: 1 step: 1450, loss is 0.09708801656961441\n",
      "epoch: 1 step: 1451, loss is 0.0718473568558693\n",
      "epoch: 1 step: 1452, loss is 0.2047860026359558\n",
      "epoch: 1 step: 1453, loss is 0.19749121367931366\n",
      "epoch: 1 step: 1454, loss is 0.297152578830719\n",
      "epoch: 1 step: 1455, loss is 0.08628734946250916\n",
      "epoch: 1 step: 1456, loss is 0.20883627235889435\n",
      "epoch: 1 step: 1457, loss is 0.056395433843135834\n",
      "epoch: 1 step: 1458, loss is 0.5454553365707397\n",
      "epoch: 1 step: 1459, loss is 0.2567904591560364\n",
      "epoch: 1 step: 1460, loss is 0.22416573762893677\n",
      "epoch: 1 step: 1461, loss is 0.10050962120294571\n",
      "epoch: 1 step: 1462, loss is 0.19576627016067505\n",
      "epoch: 1 step: 1463, loss is 0.006554685067385435\n",
      "epoch: 1 step: 1464, loss is 0.02509305067360401\n",
      "epoch: 1 step: 1465, loss is 0.2546387314796448\n",
      "epoch: 1 step: 1466, loss is 0.04929540306329727\n",
      "epoch: 1 step: 1467, loss is 0.018423838540911674\n",
      "epoch: 1 step: 1468, loss is 0.17918674647808075\n",
      "epoch: 1 step: 1469, loss is 0.04921963810920715\n",
      "epoch: 1 step: 1470, loss is 0.2652764916419983\n",
      "epoch: 1 step: 1471, loss is 0.1364193558692932\n",
      "epoch: 1 step: 1472, loss is 0.10425779968500137\n",
      "epoch: 1 step: 1473, loss is 0.07863249629735947\n",
      "epoch: 1 step: 1474, loss is 0.35169821977615356\n",
      "epoch: 1 step: 1475, loss is 0.2210002839565277\n",
      "epoch: 1 step: 1476, loss is 0.2159809172153473\n",
      "epoch: 1 step: 1477, loss is 0.1550455242395401\n",
      "epoch: 1 step: 1478, loss is 0.46440786123275757\n",
      "epoch: 1 step: 1479, loss is 0.037987422198057175\n",
      "epoch: 1 step: 1480, loss is 0.020251788198947906\n",
      "epoch: 1 step: 1481, loss is 0.1784154623746872\n",
      "epoch: 1 step: 1482, loss is 0.2037145495414734\n",
      "epoch: 1 step: 1483, loss is 0.16853080689907074\n",
      "epoch: 1 step: 1484, loss is 0.14439542591571808\n",
      "epoch: 1 step: 1485, loss is 0.07816161215305328\n",
      "epoch: 1 step: 1486, loss is 0.054276444017887115\n",
      "epoch: 1 step: 1487, loss is 0.09724778681993484\n",
      "epoch: 1 step: 1488, loss is 0.17393063008785248\n",
      "epoch: 1 step: 1489, loss is 0.13600711524486542\n",
      "epoch: 1 step: 1490, loss is 0.1272890567779541\n",
      "epoch: 1 step: 1491, loss is 0.05846559256315231\n",
      "epoch: 1 step: 1492, loss is 0.30096185207366943\n",
      "epoch: 1 step: 1493, loss is 0.056587591767311096\n",
      "epoch: 1 step: 1494, loss is 0.3044605553150177\n",
      "epoch: 1 step: 1495, loss is 0.10781939327716827\n",
      "epoch: 1 step: 1496, loss is 0.03467817232012749\n",
      "epoch: 1 step: 1497, loss is 0.06116480007767677\n",
      "epoch: 1 step: 1498, loss is 0.2114979326725006\n",
      "epoch: 1 step: 1499, loss is 0.10728194564580917\n",
      "epoch: 1 step: 1500, loss is 0.28050386905670166\n",
      "epoch: 1 step: 1501, loss is 0.14669840037822723\n",
      "epoch: 1 step: 1502, loss is 0.11508399248123169\n",
      "epoch: 1 step: 1503, loss is 0.09930217266082764\n",
      "epoch: 1 step: 1504, loss is 0.1918395310640335\n",
      "epoch: 1 step: 1505, loss is 0.05064164847135544\n",
      "epoch: 1 step: 1506, loss is 0.041521210223436356\n",
      "epoch: 1 step: 1507, loss is 0.124658964574337\n",
      "epoch: 1 step: 1508, loss is 0.47546204924583435\n",
      "epoch: 1 step: 1509, loss is 0.06759728491306305\n",
      "epoch: 1 step: 1510, loss is 0.10070259124040604\n",
      "epoch: 1 step: 1511, loss is 0.06761892139911652\n",
      "epoch: 1 step: 1512, loss is 0.07311911880970001\n",
      "epoch: 1 step: 1513, loss is 0.005556278862059116\n",
      "epoch: 1 step: 1514, loss is 0.2220516949892044\n",
      "epoch: 1 step: 1515, loss is 0.186014786362648\n",
      "epoch: 1 step: 1516, loss is 0.21809324622154236\n",
      "epoch: 1 step: 1517, loss is 0.2329763025045395\n",
      "epoch: 1 step: 1518, loss is 0.2293156236410141\n",
      "epoch: 1 step: 1519, loss is 0.07675795257091522\n",
      "epoch: 1 step: 1520, loss is 0.11269326508045197\n",
      "epoch: 1 step: 1521, loss is 0.11485247313976288\n",
      "epoch: 1 step: 1522, loss is 0.06248275190591812\n",
      "epoch: 1 step: 1523, loss is 0.3399321138858795\n",
      "epoch: 1 step: 1524, loss is 0.3342764675617218\n",
      "epoch: 1 step: 1525, loss is 0.2731980085372925\n",
      "epoch: 1 step: 1526, loss is 0.06266499310731888\n",
      "epoch: 1 step: 1527, loss is 0.3271467983722687\n",
      "epoch: 1 step: 1528, loss is 0.11887021362781525\n",
      "epoch: 1 step: 1529, loss is 0.036537330597639084\n",
      "epoch: 1 step: 1530, loss is 0.16653010249137878\n",
      "epoch: 1 step: 1531, loss is 0.3741667866706848\n",
      "epoch: 1 step: 1532, loss is 0.10286448150873184\n",
      "epoch: 1 step: 1533, loss is 0.13977837562561035\n",
      "epoch: 1 step: 1534, loss is 0.04878547042608261\n",
      "epoch: 1 step: 1535, loss is 0.3164782226085663\n",
      "epoch: 1 step: 1536, loss is 0.09918540716171265\n",
      "epoch: 1 step: 1537, loss is 0.40136948227882385\n",
      "epoch: 1 step: 1538, loss is 0.05488870292901993\n",
      "epoch: 1 step: 1539, loss is 0.0828929916024208\n",
      "epoch: 1 step: 1540, loss is 0.23642946779727936\n",
      "epoch: 1 step: 1541, loss is 0.11880876123905182\n",
      "epoch: 1 step: 1542, loss is 0.11015676707029343\n",
      "epoch: 1 step: 1543, loss is 0.15872545540332794\n",
      "epoch: 1 step: 1544, loss is 0.07101665437221527\n",
      "epoch: 1 step: 1545, loss is 0.19502249360084534\n",
      "epoch: 1 step: 1546, loss is 0.20549176633358002\n",
      "epoch: 1 step: 1547, loss is 0.043298039585351944\n",
      "epoch: 1 step: 1548, loss is 0.07984894514083862\n",
      "epoch: 1 step: 1549, loss is 0.37750905752182007\n",
      "epoch: 1 step: 1550, loss is 0.1980091780424118\n",
      "epoch: 1 step: 1551, loss is 0.046742700040340424\n",
      "epoch: 1 step: 1552, loss is 0.1170143336057663\n",
      "epoch: 1 step: 1553, loss is 0.06303614377975464\n",
      "epoch: 1 step: 1554, loss is 0.017340239137411118\n",
      "epoch: 1 step: 1555, loss is 0.07300490885972977\n",
      "epoch: 1 step: 1556, loss is 0.06474298983812332\n",
      "epoch: 1 step: 1557, loss is 0.3025655746459961\n",
      "epoch: 1 step: 1558, loss is 0.05803583562374115\n",
      "epoch: 1 step: 1559, loss is 0.02754710055887699\n",
      "epoch: 1 step: 1560, loss is 0.2621856927871704\n",
      "epoch: 1 step: 1561, loss is 0.0974612757563591\n",
      "epoch: 1 step: 1562, loss is 0.07475674152374268\n",
      "epoch: 1 step: 1563, loss is 0.020941205322742462\n",
      "epoch: 1 step: 1564, loss is 0.22043144702911377\n",
      "epoch: 1 step: 1565, loss is 0.013149172067642212\n",
      "epoch: 1 step: 1566, loss is 0.10105685889720917\n",
      "epoch: 1 step: 1567, loss is 0.1621442437171936\n",
      "epoch: 1 step: 1568, loss is 0.13530243933200836\n",
      "epoch: 1 step: 1569, loss is 0.13873843848705292\n",
      "epoch: 1 step: 1570, loss is 0.020207034423947334\n",
      "epoch: 1 step: 1571, loss is 0.24809293448925018\n",
      "epoch: 1 step: 1572, loss is 0.00549792917445302\n",
      "epoch: 1 step: 1573, loss is 0.6507671475410461\n",
      "epoch: 1 step: 1574, loss is 0.16503407061100006\n",
      "epoch: 1 step: 1575, loss is 0.2083514928817749\n",
      "epoch: 1 step: 1576, loss is 0.04452043026685715\n",
      "epoch: 1 step: 1577, loss is 0.13231390714645386\n",
      "epoch: 1 step: 1578, loss is 0.17151278257369995\n",
      "epoch: 1 step: 1579, loss is 0.09924343973398209\n",
      "epoch: 1 step: 1580, loss is 0.030794141814112663\n",
      "epoch: 1 step: 1581, loss is 0.028710562735795975\n",
      "epoch: 1 step: 1582, loss is 0.2402440309524536\n",
      "epoch: 1 step: 1583, loss is 0.07794973254203796\n",
      "epoch: 1 step: 1584, loss is 0.20379038155078888\n",
      "epoch: 1 step: 1585, loss is 0.06433926522731781\n",
      "epoch: 1 step: 1586, loss is 0.09248331189155579\n",
      "epoch: 1 step: 1587, loss is 0.017960388213396072\n",
      "epoch: 1 step: 1588, loss is 0.01090884581208229\n",
      "epoch: 1 step: 1589, loss is 0.26382169127464294\n",
      "epoch: 1 step: 1590, loss is 0.34341683983802795\n",
      "epoch: 1 step: 1591, loss is 0.12068083882331848\n",
      "epoch: 1 step: 1592, loss is 0.12029741704463959\n",
      "epoch: 1 step: 1593, loss is 0.023491492494940758\n",
      "epoch: 1 step: 1594, loss is 0.12026556581258774\n",
      "epoch: 1 step: 1595, loss is 0.18338680267333984\n",
      "epoch: 1 step: 1596, loss is 0.07631499320268631\n",
      "epoch: 1 step: 1597, loss is 0.15209154784679413\n",
      "epoch: 1 step: 1598, loss is 0.02756298892199993\n",
      "epoch: 1 step: 1599, loss is 0.1596958339214325\n",
      "epoch: 1 step: 1600, loss is 0.2589413523674011\n",
      "epoch: 1 step: 1601, loss is 0.03289264813065529\n",
      "epoch: 1 step: 1602, loss is 0.0771522969007492\n",
      "epoch: 1 step: 1603, loss is 0.14689278602600098\n",
      "epoch: 1 step: 1604, loss is 0.1983301192522049\n",
      "epoch: 1 step: 1605, loss is 0.12119576334953308\n",
      "epoch: 1 step: 1606, loss is 0.1528317779302597\n",
      "epoch: 1 step: 1607, loss is 0.010388913564383984\n",
      "epoch: 1 step: 1608, loss is 0.2407214194536209\n",
      "epoch: 1 step: 1609, loss is 0.07244909554719925\n",
      "epoch: 1 step: 1610, loss is 0.07461439073085785\n",
      "epoch: 1 step: 1611, loss is 0.0211598239839077\n",
      "epoch: 1 step: 1612, loss is 0.06320201605558395\n",
      "epoch: 1 step: 1613, loss is 0.09509669244289398\n",
      "epoch: 1 step: 1614, loss is 0.1335451602935791\n",
      "epoch: 1 step: 1615, loss is 0.2605176568031311\n",
      "epoch: 1 step: 1616, loss is 0.041359517723321915\n",
      "epoch: 1 step: 1617, loss is 0.014320289716124535\n",
      "epoch: 1 step: 1618, loss is 0.10983192175626755\n",
      "epoch: 1 step: 1619, loss is 0.27480846643447876\n",
      "epoch: 1 step: 1620, loss is 0.30496278405189514\n",
      "epoch: 1 step: 1621, loss is 0.37757065892219543\n",
      "epoch: 1 step: 1622, loss is 0.09897708147764206\n",
      "epoch: 1 step: 1623, loss is 0.22425630688667297\n",
      "epoch: 1 step: 1624, loss is 0.09014703333377838\n",
      "epoch: 1 step: 1625, loss is 0.010669564828276634\n",
      "epoch: 1 step: 1626, loss is 0.029319999739527702\n",
      "epoch: 1 step: 1627, loss is 0.16831612586975098\n",
      "epoch: 1 step: 1628, loss is 0.09236060082912445\n",
      "epoch: 1 step: 1629, loss is 0.007903505116701126\n",
      "epoch: 1 step: 1630, loss is 0.2413640320301056\n",
      "epoch: 1 step: 1631, loss is 0.10835936665534973\n",
      "epoch: 1 step: 1632, loss is 0.17484332621097565\n",
      "epoch: 1 step: 1633, loss is 0.11519846320152283\n",
      "epoch: 1 step: 1634, loss is 0.20500478148460388\n",
      "epoch: 1 step: 1635, loss is 0.05751138553023338\n",
      "epoch: 1 step: 1636, loss is 0.05237097665667534\n",
      "epoch: 1 step: 1637, loss is 0.054251763969659805\n",
      "epoch: 1 step: 1638, loss is 0.409811794757843\n",
      "epoch: 1 step: 1639, loss is 0.06427507847547531\n",
      "epoch: 1 step: 1640, loss is 0.12029985338449478\n",
      "epoch: 1 step: 1641, loss is 0.041799597442150116\n",
      "epoch: 1 step: 1642, loss is 0.14597231149673462\n",
      "epoch: 1 step: 1643, loss is 0.02201695181429386\n",
      "epoch: 1 step: 1644, loss is 0.02900206670165062\n",
      "epoch: 1 step: 1645, loss is 0.07078192383050919\n",
      "epoch: 1 step: 1646, loss is 0.2975178360939026\n",
      "epoch: 1 step: 1647, loss is 0.04876647889614105\n",
      "epoch: 1 step: 1648, loss is 0.07607320696115494\n",
      "epoch: 1 step: 1649, loss is 0.0980563536286354\n",
      "epoch: 1 step: 1650, loss is 0.31454047560691833\n",
      "epoch: 1 step: 1651, loss is 0.028296463191509247\n",
      "epoch: 1 step: 1652, loss is 0.08749726414680481\n",
      "epoch: 1 step: 1653, loss is 0.035694655030965805\n",
      "epoch: 1 step: 1654, loss is 0.07607834041118622\n",
      "epoch: 1 step: 1655, loss is 0.030342916026711464\n",
      "epoch: 1 step: 1656, loss is 0.08536586910486221\n",
      "epoch: 1 step: 1657, loss is 0.027781372889876366\n",
      "epoch: 1 step: 1658, loss is 0.01391461119055748\n",
      "epoch: 1 step: 1659, loss is 0.0484161302447319\n",
      "epoch: 1 step: 1660, loss is 0.13309630751609802\n",
      "epoch: 1 step: 1661, loss is 0.14401480555534363\n",
      "epoch: 1 step: 1662, loss is 0.17030081152915955\n",
      "epoch: 1 step: 1663, loss is 0.4062753915786743\n",
      "epoch: 1 step: 1664, loss is 0.03887251764535904\n",
      "epoch: 1 step: 1665, loss is 0.18468037247657776\n",
      "epoch: 1 step: 1666, loss is 0.008213898167014122\n",
      "epoch: 1 step: 1667, loss is 0.10445800423622131\n",
      "epoch: 1 step: 1668, loss is 0.18887770175933838\n",
      "epoch: 1 step: 1669, loss is 0.1746303290128708\n",
      "epoch: 1 step: 1670, loss is 0.4184587299823761\n",
      "epoch: 1 step: 1671, loss is 0.055245570838451385\n",
      "epoch: 1 step: 1672, loss is 0.11157296597957611\n",
      "epoch: 1 step: 1673, loss is 0.01898561604321003\n",
      "epoch: 1 step: 1674, loss is 0.03489430248737335\n",
      "epoch: 1 step: 1675, loss is 0.18375492095947266\n",
      "epoch: 1 step: 1676, loss is 0.2046038955450058\n",
      "epoch: 1 step: 1677, loss is 0.07642065733671188\n",
      "epoch: 1 step: 1678, loss is 0.2782743573188782\n",
      "epoch: 1 step: 1679, loss is 0.0245867520570755\n",
      "epoch: 1 step: 1680, loss is 0.08979495614767075\n",
      "epoch: 1 step: 1681, loss is 0.02549329772591591\n",
      "epoch: 1 step: 1682, loss is 0.029929272830486298\n",
      "epoch: 1 step: 1683, loss is 0.015523058362305164\n",
      "epoch: 1 step: 1684, loss is 0.03592991083860397\n",
      "epoch: 1 step: 1685, loss is 0.35594290494918823\n",
      "epoch: 1 step: 1686, loss is 0.15021337568759918\n",
      "epoch: 1 step: 1687, loss is 0.2054648995399475\n",
      "epoch: 1 step: 1688, loss is 0.08543266355991364\n",
      "epoch: 1 step: 1689, loss is 0.066279336810112\n",
      "epoch: 1 step: 1690, loss is 0.08407963812351227\n",
      "epoch: 1 step: 1691, loss is 0.16565948724746704\n",
      "epoch: 1 step: 1692, loss is 0.05791683867573738\n",
      "epoch: 1 step: 1693, loss is 0.19675394892692566\n",
      "epoch: 1 step: 1694, loss is 0.01539982296526432\n",
      "epoch: 1 step: 1695, loss is 0.16015145182609558\n",
      "epoch: 1 step: 1696, loss is 0.10682174563407898\n",
      "epoch: 1 step: 1697, loss is 0.014655672013759613\n",
      "epoch: 1 step: 1698, loss is 0.1913754791021347\n",
      "epoch: 1 step: 1699, loss is 0.06901054084300995\n",
      "epoch: 1 step: 1700, loss is 0.6273546814918518\n",
      "epoch: 1 step: 1701, loss is 0.10472513735294342\n",
      "epoch: 1 step: 1702, loss is 0.21966330707073212\n",
      "epoch: 1 step: 1703, loss is 0.07289741933345795\n",
      "epoch: 1 step: 1704, loss is 0.00976849440485239\n",
      "epoch: 1 step: 1705, loss is 0.05756131932139397\n",
      "epoch: 1 step: 1706, loss is 0.24489383399486542\n",
      "epoch: 1 step: 1707, loss is 0.017681866884231567\n",
      "epoch: 1 step: 1708, loss is 0.2581727206707001\n",
      "epoch: 1 step: 1709, loss is 0.0608975775539875\n",
      "epoch: 1 step: 1710, loss is 0.24140413105487823\n",
      "epoch: 1 step: 1711, loss is 0.26796358823776245\n",
      "epoch: 1 step: 1712, loss is 0.10240784287452698\n",
      "epoch: 1 step: 1713, loss is 0.10965131968259811\n",
      "epoch: 1 step: 1714, loss is 0.18139006197452545\n",
      "epoch: 1 step: 1715, loss is 0.07462964206933975\n",
      "epoch: 1 step: 1716, loss is 0.09010971337556839\n",
      "epoch: 1 step: 1717, loss is 0.029005663469433784\n",
      "epoch: 1 step: 1718, loss is 0.010452251881361008\n",
      "epoch: 1 step: 1719, loss is 0.03143797814846039\n",
      "epoch: 1 step: 1720, loss is 0.10758035629987717\n",
      "epoch: 1 step: 1721, loss is 0.014692637138068676\n",
      "epoch: 1 step: 1722, loss is 0.021409254521131516\n",
      "epoch: 1 step: 1723, loss is 0.019780877977609634\n",
      "epoch: 1 step: 1724, loss is 0.03008936159312725\n",
      "epoch: 1 step: 1725, loss is 0.04883482679724693\n",
      "epoch: 1 step: 1726, loss is 0.25349298119544983\n",
      "epoch: 1 step: 1727, loss is 0.016734303906559944\n",
      "epoch: 1 step: 1728, loss is 0.030867796391248703\n",
      "epoch: 1 step: 1729, loss is 0.018460005521774292\n",
      "epoch: 1 step: 1730, loss is 0.014980184845626354\n",
      "epoch: 1 step: 1731, loss is 0.09788179397583008\n",
      "epoch: 1 step: 1732, loss is 0.03142639249563217\n",
      "epoch: 1 step: 1733, loss is 0.00956147350370884\n",
      "epoch: 1 step: 1734, loss is 0.10851603746414185\n",
      "epoch: 1 step: 1735, loss is 0.022036468610167503\n",
      "epoch: 1 step: 1736, loss is 0.00976280216127634\n",
      "epoch: 1 step: 1737, loss is 0.019884932786226273\n",
      "epoch: 1 step: 1738, loss is 0.16351567208766937\n",
      "epoch: 1 step: 1739, loss is 0.214320570230484\n",
      "epoch: 1 step: 1740, loss is 0.02586180716753006\n",
      "epoch: 1 step: 1741, loss is 0.013898380100727081\n",
      "epoch: 1 step: 1742, loss is 0.010945689864456654\n",
      "epoch: 1 step: 1743, loss is 0.07368845492601395\n",
      "epoch: 1 step: 1744, loss is 0.10123108327388763\n",
      "epoch: 1 step: 1745, loss is 0.15375590324401855\n",
      "epoch: 1 step: 1746, loss is 0.38515034317970276\n",
      "epoch: 1 step: 1747, loss is 0.1730705350637436\n",
      "epoch: 1 step: 1748, loss is 0.016295667737722397\n",
      "epoch: 1 step: 1749, loss is 0.07752770185470581\n",
      "epoch: 1 step: 1750, loss is 0.07023311406373978\n",
      "epoch: 1 step: 1751, loss is 0.059259455651044846\n",
      "epoch: 1 step: 1752, loss is 0.054530203342437744\n",
      "epoch: 1 step: 1753, loss is 0.2066079080104828\n",
      "epoch: 1 step: 1754, loss is 0.16587215662002563\n",
      "epoch: 1 step: 1755, loss is 0.05970202386379242\n",
      "epoch: 1 step: 1756, loss is 0.10136004537343979\n",
      "epoch: 1 step: 1757, loss is 0.4907902777194977\n",
      "epoch: 1 step: 1758, loss is 0.07927273213863373\n",
      "epoch: 1 step: 1759, loss is 0.212630033493042\n",
      "epoch: 1 step: 1760, loss is 0.025355324149131775\n",
      "epoch: 1 step: 1761, loss is 0.019884157925844193\n",
      "epoch: 1 step: 1762, loss is 0.05167577415704727\n",
      "epoch: 1 step: 1763, loss is 0.08866280317306519\n",
      "epoch: 1 step: 1764, loss is 0.03157341107726097\n",
      "epoch: 1 step: 1765, loss is 0.01943378895521164\n",
      "epoch: 1 step: 1766, loss is 0.02391415648162365\n",
      "epoch: 1 step: 1767, loss is 0.03855998069047928\n",
      "epoch: 1 step: 1768, loss is 0.18299712240695953\n",
      "epoch: 1 step: 1769, loss is 0.02040485292673111\n",
      "epoch: 1 step: 1770, loss is 0.1941758543252945\n",
      "epoch: 1 step: 1771, loss is 0.05201883614063263\n",
      "epoch: 1 step: 1772, loss is 0.0711822360754013\n",
      "epoch: 1 step: 1773, loss is 0.08613959699869156\n",
      "epoch: 1 step: 1774, loss is 0.006825544405728579\n",
      "epoch: 1 step: 1775, loss is 0.05714336782693863\n",
      "epoch: 1 step: 1776, loss is 0.19664333760738373\n",
      "epoch: 1 step: 1777, loss is 0.09383053332567215\n",
      "epoch: 1 step: 1778, loss is 0.02149919793009758\n",
      "epoch: 1 step: 1779, loss is 0.014021596871316433\n",
      "epoch: 1 step: 1780, loss is 0.00577160157263279\n",
      "epoch: 1 step: 1781, loss is 0.13634078204631805\n",
      "epoch: 1 step: 1782, loss is 0.04059967026114464\n",
      "epoch: 1 step: 1783, loss is 0.026369154453277588\n",
      "epoch: 1 step: 1784, loss is 0.14521488547325134\n",
      "epoch: 1 step: 1785, loss is 0.005794796161353588\n",
      "epoch: 1 step: 1786, loss is 0.16214174032211304\n",
      "epoch: 1 step: 1787, loss is 0.014049140736460686\n",
      "epoch: 1 step: 1788, loss is 0.09213568270206451\n",
      "epoch: 1 step: 1789, loss is 0.23975618183612823\n",
      "epoch: 1 step: 1790, loss is 0.1048930436372757\n",
      "epoch: 1 step: 1791, loss is 0.033271268010139465\n",
      "epoch: 1 step: 1792, loss is 0.0974343940615654\n",
      "epoch: 1 step: 1793, loss is 0.40927499532699585\n",
      "epoch: 1 step: 1794, loss is 0.04773325473070145\n",
      "epoch: 1 step: 1795, loss is 0.015276444144546986\n",
      "epoch: 1 step: 1796, loss is 0.10288873314857483\n",
      "epoch: 1 step: 1797, loss is 0.029957544058561325\n",
      "epoch: 1 step: 1798, loss is 0.06802913546562195\n",
      "epoch: 1 step: 1799, loss is 0.1170903891324997\n",
      "epoch: 1 step: 1800, loss is 0.2787867486476898\n",
      "epoch: 1 step: 1801, loss is 0.18404632806777954\n",
      "epoch: 1 step: 1802, loss is 0.02166074514389038\n",
      "epoch: 1 step: 1803, loss is 0.014219718053936958\n",
      "epoch: 1 step: 1804, loss is 0.04935401678085327\n",
      "epoch: 1 step: 1805, loss is 0.08941622823476791\n",
      "epoch: 1 step: 1806, loss is 0.14549434185028076\n",
      "epoch: 1 step: 1807, loss is 0.044649191200733185\n",
      "epoch: 1 step: 1808, loss is 0.07105795294046402\n",
      "epoch: 1 step: 1809, loss is 0.11198490858078003\n",
      "epoch: 1 step: 1810, loss is 0.07293909788131714\n",
      "epoch: 1 step: 1811, loss is 0.036080893129110336\n",
      "epoch: 1 step: 1812, loss is 0.029078099876642227\n",
      "epoch: 1 step: 1813, loss is 0.16024522483348846\n",
      "epoch: 1 step: 1814, loss is 0.029399363324046135\n",
      "epoch: 1 step: 1815, loss is 0.008247178047895432\n",
      "epoch: 1 step: 1816, loss is 0.04433334618806839\n",
      "epoch: 1 step: 1817, loss is 0.22066113352775574\n",
      "epoch: 1 step: 1818, loss is 0.01340701337903738\n",
      "epoch: 1 step: 1819, loss is 0.21128320693969727\n",
      "epoch: 1 step: 1820, loss is 0.06982085108757019\n",
      "epoch: 1 step: 1821, loss is 0.008843681775033474\n",
      "epoch: 1 step: 1822, loss is 0.3751882314682007\n",
      "epoch: 1 step: 1823, loss is 0.021786702796816826\n",
      "epoch: 1 step: 1824, loss is 0.010662718676030636\n",
      "epoch: 1 step: 1825, loss is 0.045318201184272766\n",
      "epoch: 1 step: 1826, loss is 0.07284853607416153\n",
      "epoch: 1 step: 1827, loss is 0.032251134514808655\n",
      "epoch: 1 step: 1828, loss is 0.03895188495516777\n",
      "epoch: 1 step: 1829, loss is 0.00367554253898561\n",
      "epoch: 1 step: 1830, loss is 0.15308769047260284\n",
      "epoch: 1 step: 1831, loss is 0.23440881073474884\n",
      "epoch: 1 step: 1832, loss is 0.41917291283607483\n",
      "epoch: 1 step: 1833, loss is 0.31411901116371155\n",
      "epoch: 1 step: 1834, loss is 0.18167176842689514\n",
      "epoch: 1 step: 1835, loss is 0.1255301982164383\n",
      "epoch: 1 step: 1836, loss is 0.0052153500728309155\n",
      "epoch: 1 step: 1837, loss is 0.24308426678180695\n",
      "epoch: 1 step: 1838, loss is 0.07472732663154602\n",
      "epoch: 1 step: 1839, loss is 0.18868811428546906\n",
      "epoch: 1 step: 1840, loss is 0.09860827773809433\n",
      "epoch: 1 step: 1841, loss is 0.2261127531528473\n",
      "epoch: 1 step: 1842, loss is 0.13774359226226807\n",
      "epoch: 1 step: 1843, loss is 0.01757628470659256\n",
      "epoch: 1 step: 1844, loss is 0.028199607506394386\n",
      "epoch: 1 step: 1845, loss is 0.021638348698616028\n",
      "epoch: 1 step: 1846, loss is 0.11878577619791031\n",
      "epoch: 1 step: 1847, loss is 0.02920476160943508\n",
      "epoch: 1 step: 1848, loss is 0.10071049630641937\n",
      "epoch: 1 step: 1849, loss is 0.03811080753803253\n",
      "epoch: 1 step: 1850, loss is 0.11139125376939774\n",
      "epoch: 1 step: 1851, loss is 0.03173115476965904\n",
      "epoch: 1 step: 1852, loss is 0.314448744058609\n",
      "epoch: 1 step: 1853, loss is 0.020211098715662956\n",
      "epoch: 1 step: 1854, loss is 0.12324129045009613\n",
      "epoch: 1 step: 1855, loss is 0.010382650420069695\n",
      "epoch: 1 step: 1856, loss is 0.0879737064242363\n",
      "epoch: 1 step: 1857, loss is 0.09846925735473633\n",
      "epoch: 1 step: 1858, loss is 0.16225501894950867\n",
      "epoch: 1 step: 1859, loss is 0.019513174891471863\n",
      "epoch: 1 step: 1860, loss is 0.13633118569850922\n",
      "epoch: 1 step: 1861, loss is 0.026997258886694908\n",
      "epoch: 1 step: 1862, loss is 0.10998320579528809\n",
      "epoch: 1 step: 1863, loss is 0.014276551082730293\n",
      "epoch: 1 step: 1864, loss is 0.12854571640491486\n",
      "epoch: 1 step: 1865, loss is 0.020835058763623238\n",
      "epoch: 1 step: 1866, loss is 0.058033108711242676\n",
      "epoch: 1 step: 1867, loss is 0.3371012806892395\n",
      "epoch: 1 step: 1868, loss is 0.1753598153591156\n",
      "epoch: 1 step: 1869, loss is 0.01240455824881792\n",
      "epoch: 1 step: 1870, loss is 0.04069151356816292\n",
      "epoch: 1 step: 1871, loss is 0.09086577594280243\n",
      "epoch: 1 step: 1872, loss is 0.05872992053627968\n",
      "epoch: 1 step: 1873, loss is 0.12331904470920563\n",
      "epoch: 1 step: 1874, loss is 0.15108846127986908\n",
      "epoch: 1 step: 1875, loss is 0.09645925462245941\n",
      "epoch: 2 step: 1, loss is 0.17253290116786957\n",
      "epoch: 2 step: 2, loss is 0.2846620976924896\n",
      "epoch: 2 step: 3, loss is 0.015409497544169426\n",
      "epoch: 2 step: 4, loss is 0.041811272501945496\n",
      "epoch: 2 step: 5, loss is 0.270072877407074\n",
      "epoch: 2 step: 6, loss is 0.1177072525024414\n",
      "epoch: 2 step: 7, loss is 0.005649576894938946\n",
      "epoch: 2 step: 8, loss is 0.07078034430742264\n",
      "epoch: 2 step: 9, loss is 0.07759492099285126\n",
      "epoch: 2 step: 10, loss is 0.011784589849412441\n",
      "epoch: 2 step: 11, loss is 0.12794648110866547\n",
      "epoch: 2 step: 12, loss is 0.04713668301701546\n",
      "epoch: 2 step: 13, loss is 0.14352703094482422\n",
      "epoch: 2 step: 14, loss is 0.00701499916613102\n",
      "epoch: 2 step: 15, loss is 0.027757607400417328\n",
      "epoch: 2 step: 16, loss is 0.07185598462820053\n",
      "epoch: 2 step: 17, loss is 0.012503262609243393\n",
      "epoch: 2 step: 18, loss is 0.24447837471961975\n",
      "epoch: 2 step: 19, loss is 0.056044455617666245\n",
      "epoch: 2 step: 20, loss is 0.0539325587451458\n",
      "epoch: 2 step: 21, loss is 0.19705286622047424\n",
      "epoch: 2 step: 22, loss is 0.052255019545555115\n",
      "epoch: 2 step: 23, loss is 0.00459101190790534\n",
      "epoch: 2 step: 24, loss is 0.0705639123916626\n",
      "epoch: 2 step: 25, loss is 0.11103469878435135\n",
      "epoch: 2 step: 26, loss is 0.24186056852340698\n",
      "epoch: 2 step: 27, loss is 0.043486110866069794\n",
      "epoch: 2 step: 28, loss is 0.18754954636096954\n",
      "epoch: 2 step: 29, loss is 0.11446234583854675\n",
      "epoch: 2 step: 30, loss is 0.17193253338336945\n",
      "epoch: 2 step: 31, loss is 0.05754724517464638\n",
      "epoch: 2 step: 32, loss is 0.08778861910104752\n",
      "epoch: 2 step: 33, loss is 0.04046173021197319\n",
      "epoch: 2 step: 34, loss is 0.02346033975481987\n",
      "epoch: 2 step: 35, loss is 0.09422429651021957\n",
      "epoch: 2 step: 36, loss is 0.017159875482320786\n",
      "epoch: 2 step: 37, loss is 0.08471974730491638\n",
      "epoch: 2 step: 38, loss is 0.03224996477365494\n",
      "epoch: 2 step: 39, loss is 0.2851065397262573\n",
      "epoch: 2 step: 40, loss is 0.09762438386678696\n",
      "epoch: 2 step: 41, loss is 0.0308093149214983\n",
      "epoch: 2 step: 42, loss is 0.2624499797821045\n",
      "epoch: 2 step: 43, loss is 0.3583134412765503\n",
      "epoch: 2 step: 44, loss is 0.106949083507061\n",
      "epoch: 2 step: 45, loss is 0.13184840977191925\n",
      "epoch: 2 step: 46, loss is 0.036759305745363235\n",
      "epoch: 2 step: 47, loss is 0.21238014101982117\n",
      "epoch: 2 step: 48, loss is 0.17218652367591858\n",
      "epoch: 2 step: 49, loss is 0.031440235674381256\n",
      "epoch: 2 step: 50, loss is 0.11612221598625183\n",
      "epoch: 2 step: 51, loss is 0.026256727054715157\n",
      "epoch: 2 step: 52, loss is 0.1243055909872055\n",
      "epoch: 2 step: 53, loss is 0.1656181961297989\n",
      "epoch: 2 step: 54, loss is 0.16992001235485077\n",
      "epoch: 2 step: 55, loss is 0.24009111523628235\n",
      "epoch: 2 step: 56, loss is 0.06414700299501419\n",
      "epoch: 2 step: 57, loss is 0.01596791297197342\n",
      "epoch: 2 step: 58, loss is 0.020531369373202324\n",
      "epoch: 2 step: 59, loss is 0.11599374562501907\n",
      "epoch: 2 step: 60, loss is 0.014304467476904392\n",
      "epoch: 2 step: 61, loss is 0.018593253567814827\n",
      "epoch: 2 step: 62, loss is 0.05780218541622162\n",
      "epoch: 2 step: 63, loss is 0.07603690773248672\n",
      "epoch: 2 step: 64, loss is 0.02629728801548481\n",
      "epoch: 2 step: 65, loss is 0.07334379106760025\n",
      "epoch: 2 step: 66, loss is 0.1553538739681244\n",
      "epoch: 2 step: 67, loss is 0.1070605218410492\n",
      "epoch: 2 step: 68, loss is 0.022413358092308044\n",
      "epoch: 2 step: 69, loss is 0.0861954614520073\n",
      "epoch: 2 step: 70, loss is 0.1292666494846344\n",
      "epoch: 2 step: 71, loss is 0.004130628891289234\n",
      "epoch: 2 step: 72, loss is 0.11280874162912369\n",
      "epoch: 2 step: 73, loss is 0.33757033944129944\n",
      "epoch: 2 step: 74, loss is 0.011309358291327953\n",
      "epoch: 2 step: 75, loss is 0.030587952584028244\n",
      "epoch: 2 step: 76, loss is 0.10138869285583496\n",
      "epoch: 2 step: 77, loss is 0.2914263606071472\n",
      "epoch: 2 step: 78, loss is 0.0423462837934494\n",
      "epoch: 2 step: 79, loss is 0.11381682753562927\n",
      "epoch: 2 step: 80, loss is 0.06951993703842163\n",
      "epoch: 2 step: 81, loss is 0.23034784197807312\n",
      "epoch: 2 step: 82, loss is 0.0880131721496582\n",
      "epoch: 2 step: 83, loss is 0.08149264752864838\n",
      "epoch: 2 step: 84, loss is 0.06800500303506851\n",
      "epoch: 2 step: 85, loss is 0.008360886946320534\n",
      "epoch: 2 step: 86, loss is 0.07162562757730484\n",
      "epoch: 2 step: 87, loss is 0.06613544374704361\n",
      "epoch: 2 step: 88, loss is 0.018399737775325775\n",
      "epoch: 2 step: 89, loss is 0.015298607759177685\n",
      "epoch: 2 step: 90, loss is 0.019001681357622147\n",
      "epoch: 2 step: 91, loss is 0.030200820416212082\n",
      "epoch: 2 step: 92, loss is 0.01560998521745205\n",
      "epoch: 2 step: 93, loss is 0.009678415022790432\n",
      "epoch: 2 step: 94, loss is 0.3133408725261688\n",
      "epoch: 2 step: 95, loss is 0.029847173020243645\n",
      "epoch: 2 step: 96, loss is 0.0806693434715271\n",
      "epoch: 2 step: 97, loss is 0.07183435559272766\n",
      "epoch: 2 step: 98, loss is 0.03562666103243828\n",
      "epoch: 2 step: 99, loss is 0.049462590366601944\n",
      "epoch: 2 step: 100, loss is 0.07790924608707428\n",
      "epoch: 2 step: 101, loss is 0.034581877291202545\n",
      "epoch: 2 step: 102, loss is 0.1398072987794876\n",
      "epoch: 2 step: 103, loss is 0.1410205215215683\n",
      "epoch: 2 step: 104, loss is 0.25626903772354126\n",
      "epoch: 2 step: 105, loss is 0.0727979764342308\n",
      "epoch: 2 step: 106, loss is 0.22598059475421906\n",
      "epoch: 2 step: 107, loss is 0.047126173973083496\n",
      "epoch: 2 step: 108, loss is 0.15214788913726807\n",
      "epoch: 2 step: 109, loss is 0.09444410353899002\n",
      "epoch: 2 step: 110, loss is 0.12707148492336273\n",
      "epoch: 2 step: 111, loss is 0.0984925851225853\n",
      "epoch: 2 step: 112, loss is 0.009956416673958302\n",
      "epoch: 2 step: 113, loss is 0.03954372555017471\n",
      "epoch: 2 step: 114, loss is 0.007203792221844196\n",
      "epoch: 2 step: 115, loss is 0.21564881503582\n",
      "epoch: 2 step: 116, loss is 0.050274621695280075\n",
      "epoch: 2 step: 117, loss is 0.07247857004404068\n",
      "epoch: 2 step: 118, loss is 0.19496609270572662\n",
      "epoch: 2 step: 119, loss is 0.1462605595588684\n",
      "epoch: 2 step: 120, loss is 0.5366461873054504\n",
      "epoch: 2 step: 121, loss is 0.18687669932842255\n",
      "epoch: 2 step: 122, loss is 0.15527787804603577\n",
      "epoch: 2 step: 123, loss is 0.24821937084197998\n",
      "epoch: 2 step: 124, loss is 0.29000890254974365\n",
      "epoch: 2 step: 125, loss is 0.18862570822238922\n",
      "epoch: 2 step: 126, loss is 0.06558946520090103\n",
      "epoch: 2 step: 127, loss is 0.16904838383197784\n",
      "epoch: 2 step: 128, loss is 0.11275923997163773\n",
      "epoch: 2 step: 129, loss is 0.1407000720500946\n",
      "epoch: 2 step: 130, loss is 0.19755834341049194\n",
      "epoch: 2 step: 131, loss is 0.010613750666379929\n",
      "epoch: 2 step: 132, loss is 0.02666829153895378\n",
      "epoch: 2 step: 133, loss is 0.29006871581077576\n",
      "epoch: 2 step: 134, loss is 0.2650066912174225\n",
      "epoch: 2 step: 135, loss is 0.10555204749107361\n",
      "epoch: 2 step: 136, loss is 0.09866935759782791\n",
      "epoch: 2 step: 137, loss is 0.039787113666534424\n",
      "epoch: 2 step: 138, loss is 0.27090999484062195\n",
      "epoch: 2 step: 139, loss is 0.0924551859498024\n",
      "epoch: 2 step: 140, loss is 0.22506780922412872\n",
      "epoch: 2 step: 141, loss is 0.044925373047590256\n",
      "epoch: 2 step: 142, loss is 0.017341457307338715\n",
      "epoch: 2 step: 143, loss is 0.08301295340061188\n",
      "epoch: 2 step: 144, loss is 0.023697461932897568\n",
      "epoch: 2 step: 145, loss is 0.09781194478273392\n",
      "epoch: 2 step: 146, loss is 0.029749302193522453\n",
      "epoch: 2 step: 147, loss is 0.09132245182991028\n",
      "epoch: 2 step: 148, loss is 0.08610228449106216\n",
      "epoch: 2 step: 149, loss is 0.05853060632944107\n",
      "epoch: 2 step: 150, loss is 0.16253846883773804\n",
      "epoch: 2 step: 151, loss is 0.014426806010305882\n",
      "epoch: 2 step: 152, loss is 0.10780221223831177\n",
      "epoch: 2 step: 153, loss is 0.0792185515165329\n",
      "epoch: 2 step: 154, loss is 0.18908540904521942\n",
      "epoch: 2 step: 155, loss is 0.020398510619997978\n",
      "epoch: 2 step: 156, loss is 0.1500038206577301\n",
      "epoch: 2 step: 157, loss is 0.0322757326066494\n",
      "epoch: 2 step: 158, loss is 0.07292622327804565\n",
      "epoch: 2 step: 159, loss is 0.04618343338370323\n",
      "epoch: 2 step: 160, loss is 0.03996926546096802\n",
      "epoch: 2 step: 161, loss is 0.06051182001829147\n",
      "epoch: 2 step: 162, loss is 0.06489618122577667\n",
      "epoch: 2 step: 163, loss is 0.046904806047677994\n",
      "epoch: 2 step: 164, loss is 0.026166439056396484\n",
      "epoch: 2 step: 165, loss is 0.060174230486154556\n",
      "epoch: 2 step: 166, loss is 0.12212127447128296\n",
      "epoch: 2 step: 167, loss is 0.2055296003818512\n",
      "epoch: 2 step: 168, loss is 0.12484761327505112\n",
      "epoch: 2 step: 169, loss is 0.2184515744447708\n",
      "epoch: 2 step: 170, loss is 0.13890421390533447\n",
      "epoch: 2 step: 171, loss is 0.029137998819351196\n",
      "epoch: 2 step: 172, loss is 0.059931907802820206\n",
      "epoch: 2 step: 173, loss is 0.04648321866989136\n",
      "epoch: 2 step: 174, loss is 0.16417326033115387\n",
      "epoch: 2 step: 175, loss is 0.0028150149155408144\n",
      "epoch: 2 step: 176, loss is 0.016358960419893265\n",
      "epoch: 2 step: 177, loss is 0.03130334988236427\n",
      "epoch: 2 step: 178, loss is 0.043990667909383774\n",
      "epoch: 2 step: 179, loss is 0.09594552218914032\n",
      "epoch: 2 step: 180, loss is 0.10987891256809235\n",
      "epoch: 2 step: 181, loss is 0.0016757150879129767\n",
      "epoch: 2 step: 182, loss is 0.036091893911361694\n",
      "epoch: 2 step: 183, loss is 0.03786709904670715\n",
      "epoch: 2 step: 184, loss is 0.021920019760727882\n",
      "epoch: 2 step: 185, loss is 0.18169860541820526\n",
      "epoch: 2 step: 186, loss is 0.08167730271816254\n",
      "epoch: 2 step: 187, loss is 0.07676944881677628\n",
      "epoch: 2 step: 188, loss is 0.007740046828985214\n",
      "epoch: 2 step: 189, loss is 0.05797531455755234\n",
      "epoch: 2 step: 190, loss is 0.11051058024168015\n",
      "epoch: 2 step: 191, loss is 0.25169283151626587\n",
      "epoch: 2 step: 192, loss is 0.05118992552161217\n",
      "epoch: 2 step: 193, loss is 0.02598864585161209\n",
      "epoch: 2 step: 194, loss is 0.08080749958753586\n",
      "epoch: 2 step: 195, loss is 0.07540398091077805\n",
      "epoch: 2 step: 196, loss is 0.08408122509717941\n",
      "epoch: 2 step: 197, loss is 0.024956881999969482\n",
      "epoch: 2 step: 198, loss is 0.029278596863150597\n",
      "epoch: 2 step: 199, loss is 0.0626533105969429\n",
      "epoch: 2 step: 200, loss is 0.14596979320049286\n",
      "epoch: 2 step: 201, loss is 0.07790062576532364\n",
      "epoch: 2 step: 202, loss is 0.050692394375801086\n",
      "epoch: 2 step: 203, loss is 0.12388471513986588\n",
      "epoch: 2 step: 204, loss is 0.014365659095346928\n",
      "epoch: 2 step: 205, loss is 0.021813703700900078\n",
      "epoch: 2 step: 206, loss is 0.011896464973688126\n",
      "epoch: 2 step: 207, loss is 0.03365486487746239\n",
      "epoch: 2 step: 208, loss is 0.17381693422794342\n",
      "epoch: 2 step: 209, loss is 0.03809383139014244\n",
      "epoch: 2 step: 210, loss is 1.0133275985717773\n",
      "epoch: 2 step: 211, loss is 0.01591997966170311\n",
      "epoch: 2 step: 212, loss is 0.043889425694942474\n",
      "epoch: 2 step: 213, loss is 0.0644940510392189\n",
      "epoch: 2 step: 214, loss is 0.40354421734809875\n",
      "epoch: 2 step: 215, loss is 0.02291775867342949\n",
      "epoch: 2 step: 216, loss is 0.045626044273376465\n",
      "epoch: 2 step: 217, loss is 0.11883008480072021\n",
      "epoch: 2 step: 218, loss is 0.31152787804603577\n",
      "epoch: 2 step: 219, loss is 0.24604280292987823\n",
      "epoch: 2 step: 220, loss is 0.15015222132205963\n",
      "epoch: 2 step: 221, loss is 0.10247713327407837\n",
      "epoch: 2 step: 222, loss is 0.04737522453069687\n",
      "epoch: 2 step: 223, loss is 0.12239707261323929\n",
      "epoch: 2 step: 224, loss is 0.019689932465553284\n",
      "epoch: 2 step: 225, loss is 0.18593524396419525\n",
      "epoch: 2 step: 226, loss is 0.0769352987408638\n",
      "epoch: 2 step: 227, loss is 0.013078434392809868\n",
      "epoch: 2 step: 228, loss is 0.08185460418462753\n",
      "epoch: 2 step: 229, loss is 0.03986280784010887\n",
      "epoch: 2 step: 230, loss is 0.005559965502470732\n",
      "epoch: 2 step: 231, loss is 0.15715964138507843\n",
      "epoch: 2 step: 232, loss is 0.015454188920557499\n",
      "epoch: 2 step: 233, loss is 0.037257757037878036\n",
      "epoch: 2 step: 234, loss is 0.11296112090349197\n",
      "epoch: 2 step: 235, loss is 0.024866634979844093\n",
      "epoch: 2 step: 236, loss is 0.20227806270122528\n",
      "epoch: 2 step: 237, loss is 0.11987795680761337\n",
      "epoch: 2 step: 238, loss is 0.09205231815576553\n",
      "epoch: 2 step: 239, loss is 0.1263168454170227\n",
      "epoch: 2 step: 240, loss is 0.15015560388565063\n",
      "epoch: 2 step: 241, loss is 0.053304869681596756\n",
      "epoch: 2 step: 242, loss is 0.0020428942516446114\n",
      "epoch: 2 step: 243, loss is 0.020909111946821213\n",
      "epoch: 2 step: 244, loss is 0.054791226983070374\n",
      "epoch: 2 step: 245, loss is 0.05321234092116356\n",
      "epoch: 2 step: 246, loss is 0.014521930366754532\n",
      "epoch: 2 step: 247, loss is 0.0979536771774292\n",
      "epoch: 2 step: 248, loss is 0.010413493029773235\n",
      "epoch: 2 step: 249, loss is 0.21567127108573914\n",
      "epoch: 2 step: 250, loss is 0.10750755667686462\n",
      "epoch: 2 step: 251, loss is 0.06288928538560867\n",
      "epoch: 2 step: 252, loss is 0.01102457381784916\n",
      "epoch: 2 step: 253, loss is 0.057264722883701324\n",
      "epoch: 2 step: 254, loss is 0.09321171045303345\n",
      "epoch: 2 step: 255, loss is 0.1761004477739334\n",
      "epoch: 2 step: 256, loss is 0.12254215031862259\n",
      "epoch: 2 step: 257, loss is 0.040176618844270706\n",
      "epoch: 2 step: 258, loss is 0.03786590322852135\n",
      "epoch: 2 step: 259, loss is 0.06729389727115631\n",
      "epoch: 2 step: 260, loss is 0.019929133355617523\n",
      "epoch: 2 step: 261, loss is 0.2227107286453247\n",
      "epoch: 2 step: 262, loss is 0.00829863641411066\n",
      "epoch: 2 step: 263, loss is 0.0486336387693882\n",
      "epoch: 2 step: 264, loss is 0.1823718547821045\n",
      "epoch: 2 step: 265, loss is 0.2192060500383377\n",
      "epoch: 2 step: 266, loss is 0.024866145104169846\n",
      "epoch: 2 step: 267, loss is 0.11460660398006439\n",
      "epoch: 2 step: 268, loss is 0.04205910861492157\n",
      "epoch: 2 step: 269, loss is 0.024091333150863647\n",
      "epoch: 2 step: 270, loss is 0.008899012580513954\n",
      "epoch: 2 step: 271, loss is 0.11592940986156464\n",
      "epoch: 2 step: 272, loss is 0.07745794206857681\n",
      "epoch: 2 step: 273, loss is 0.006270797457545996\n",
      "epoch: 2 step: 274, loss is 0.04478060454130173\n",
      "epoch: 2 step: 275, loss is 0.002751626307144761\n",
      "epoch: 2 step: 276, loss is 0.006418941542506218\n",
      "epoch: 2 step: 277, loss is 0.028958287090063095\n",
      "epoch: 2 step: 278, loss is 0.03867834433913231\n",
      "epoch: 2 step: 279, loss is 0.047801513224840164\n",
      "epoch: 2 step: 280, loss is 0.019863935187458992\n",
      "epoch: 2 step: 281, loss is 0.07480897754430771\n",
      "epoch: 2 step: 282, loss is 0.00758521631360054\n",
      "epoch: 2 step: 283, loss is 0.12492606788873672\n",
      "epoch: 2 step: 284, loss is 0.27115902304649353\n",
      "epoch: 2 step: 285, loss is 0.07162762433290482\n",
      "epoch: 2 step: 286, loss is 0.015170248225331306\n",
      "epoch: 2 step: 287, loss is 0.41513487696647644\n",
      "epoch: 2 step: 288, loss is 0.008387618698179722\n",
      "epoch: 2 step: 289, loss is 0.3028816282749176\n",
      "epoch: 2 step: 290, loss is 0.014077601954340935\n",
      "epoch: 2 step: 291, loss is 0.06376833468675613\n",
      "epoch: 2 step: 292, loss is 0.01545101311057806\n",
      "epoch: 2 step: 293, loss is 0.09782906621694565\n",
      "epoch: 2 step: 294, loss is 0.14853963255882263\n",
      "epoch: 2 step: 295, loss is 0.12091100215911865\n",
      "epoch: 2 step: 296, loss is 0.23329022526741028\n",
      "epoch: 2 step: 297, loss is 0.036567322909832\n",
      "epoch: 2 step: 298, loss is 0.031583189964294434\n",
      "epoch: 2 step: 299, loss is 0.10742159932851791\n",
      "epoch: 2 step: 300, loss is 0.006679837591946125\n",
      "epoch: 2 step: 301, loss is 0.016988087445497513\n",
      "epoch: 2 step: 302, loss is 0.026924027130007744\n",
      "epoch: 2 step: 303, loss is 0.05143547058105469\n",
      "epoch: 2 step: 304, loss is 0.08695012331008911\n",
      "epoch: 2 step: 305, loss is 0.1770646721124649\n",
      "epoch: 2 step: 306, loss is 0.13737450540065765\n",
      "epoch: 2 step: 307, loss is 0.06451700627803802\n",
      "epoch: 2 step: 308, loss is 0.12876877188682556\n",
      "epoch: 2 step: 309, loss is 0.22826199233531952\n",
      "epoch: 2 step: 310, loss is 0.07112904638051987\n",
      "epoch: 2 step: 311, loss is 0.035515230149030685\n",
      "epoch: 2 step: 312, loss is 0.05842123180627823\n",
      "epoch: 2 step: 313, loss is 0.13929806649684906\n",
      "epoch: 2 step: 314, loss is 0.09507240355014801\n",
      "epoch: 2 step: 315, loss is 0.02344231680035591\n",
      "epoch: 2 step: 316, loss is 0.04692106321454048\n",
      "epoch: 2 step: 317, loss is 0.030843041837215424\n",
      "epoch: 2 step: 318, loss is 0.07534976303577423\n",
      "epoch: 2 step: 319, loss is 0.037839777767658234\n",
      "epoch: 2 step: 320, loss is 0.09523887932300568\n",
      "epoch: 2 step: 321, loss is 0.05439595878124237\n",
      "epoch: 2 step: 322, loss is 0.5175135135650635\n",
      "epoch: 2 step: 323, loss is 0.17408454418182373\n",
      "epoch: 2 step: 324, loss is 0.0068602836690843105\n",
      "epoch: 2 step: 325, loss is 0.1458047777414322\n",
      "epoch: 2 step: 326, loss is 0.18476557731628418\n",
      "epoch: 2 step: 327, loss is 0.12101081758737564\n",
      "epoch: 2 step: 328, loss is 0.16177132725715637\n",
      "epoch: 2 step: 329, loss is 0.0187164805829525\n",
      "epoch: 2 step: 330, loss is 0.13593144714832306\n",
      "epoch: 2 step: 331, loss is 0.004293507896363735\n",
      "epoch: 2 step: 332, loss is 0.22922858595848083\n",
      "epoch: 2 step: 333, loss is 0.09933628141880035\n",
      "epoch: 2 step: 334, loss is 0.1335763782262802\n",
      "epoch: 2 step: 335, loss is 0.12359707057476044\n",
      "epoch: 2 step: 336, loss is 0.20541158318519592\n",
      "epoch: 2 step: 337, loss is 0.10839304327964783\n",
      "epoch: 2 step: 338, loss is 0.0188454557210207\n",
      "epoch: 2 step: 339, loss is 0.008727261796593666\n",
      "epoch: 2 step: 340, loss is 0.1072353720664978\n",
      "epoch: 2 step: 341, loss is 0.08524826914072037\n",
      "epoch: 2 step: 342, loss is 0.1398477852344513\n",
      "epoch: 2 step: 343, loss is 0.11606364697217941\n",
      "epoch: 2 step: 344, loss is 0.01419853512197733\n",
      "epoch: 2 step: 345, loss is 0.07715203613042831\n",
      "epoch: 2 step: 346, loss is 0.03857698291540146\n",
      "epoch: 2 step: 347, loss is 0.049695827066898346\n",
      "epoch: 2 step: 348, loss is 0.02917715720832348\n",
      "epoch: 2 step: 349, loss is 0.1787351369857788\n",
      "epoch: 2 step: 350, loss is 0.005461671389639378\n",
      "epoch: 2 step: 351, loss is 0.19549547135829926\n",
      "epoch: 2 step: 352, loss is 0.18279601633548737\n",
      "epoch: 2 step: 353, loss is 0.09557433426380157\n",
      "epoch: 2 step: 354, loss is 0.037784937769174576\n",
      "epoch: 2 step: 355, loss is 0.038412321358919144\n",
      "epoch: 2 step: 356, loss is 0.02616707980632782\n",
      "epoch: 2 step: 357, loss is 0.3647233247756958\n",
      "epoch: 2 step: 358, loss is 0.08075858652591705\n",
      "epoch: 2 step: 359, loss is 0.2705724239349365\n",
      "epoch: 2 step: 360, loss is 0.18832047283649445\n",
      "epoch: 2 step: 361, loss is 0.10783907026052475\n",
      "epoch: 2 step: 362, loss is 0.0333876758813858\n",
      "epoch: 2 step: 363, loss is 0.01365406159311533\n",
      "epoch: 2 step: 364, loss is 0.02198091521859169\n",
      "epoch: 2 step: 365, loss is 0.04844656214118004\n",
      "epoch: 2 step: 366, loss is 0.04602956771850586\n",
      "epoch: 2 step: 367, loss is 0.02546576038002968\n",
      "epoch: 2 step: 368, loss is 0.013024772517383099\n",
      "epoch: 2 step: 369, loss is 0.020370427519083023\n",
      "epoch: 2 step: 370, loss is 0.04548657685518265\n",
      "epoch: 2 step: 371, loss is 0.00412391684949398\n",
      "epoch: 2 step: 372, loss is 0.10803228616714478\n",
      "epoch: 2 step: 373, loss is 0.11004780977964401\n",
      "epoch: 2 step: 374, loss is 0.002769594080746174\n",
      "epoch: 2 step: 375, loss is 0.2256338894367218\n",
      "epoch: 2 step: 376, loss is 0.03390761464834213\n",
      "epoch: 2 step: 377, loss is 0.06551613658666611\n",
      "epoch: 2 step: 378, loss is 0.06716097891330719\n",
      "epoch: 2 step: 379, loss is 0.009485118091106415\n",
      "epoch: 2 step: 380, loss is 0.03495971858501434\n",
      "epoch: 2 step: 381, loss is 0.017352886497974396\n",
      "epoch: 2 step: 382, loss is 0.02395935356616974\n",
      "epoch: 2 step: 383, loss is 0.006404613144695759\n",
      "epoch: 2 step: 384, loss is 0.08759991079568863\n",
      "epoch: 2 step: 385, loss is 0.015663055703043938\n",
      "epoch: 2 step: 386, loss is 0.00477299140766263\n",
      "epoch: 2 step: 387, loss is 0.051020216196775436\n",
      "epoch: 2 step: 388, loss is 0.029316699132323265\n",
      "epoch: 2 step: 389, loss is 0.0605354867875576\n",
      "epoch: 2 step: 390, loss is 0.022121068090200424\n",
      "epoch: 2 step: 391, loss is 0.0009642677614465356\n",
      "epoch: 2 step: 392, loss is 0.03251443803310394\n",
      "epoch: 2 step: 393, loss is 0.11183661222457886\n",
      "epoch: 2 step: 394, loss is 0.006500771269202232\n",
      "epoch: 2 step: 395, loss is 0.03698718175292015\n",
      "epoch: 2 step: 396, loss is 0.013841592706739902\n",
      "epoch: 2 step: 397, loss is 0.020509816706180573\n",
      "epoch: 2 step: 398, loss is 0.007276312913745642\n",
      "epoch: 2 step: 399, loss is 0.010036414489150047\n",
      "epoch: 2 step: 400, loss is 0.17875227332115173\n",
      "epoch: 2 step: 401, loss is 0.0048696016892790794\n",
      "epoch: 2 step: 402, loss is 0.021487493067979813\n",
      "epoch: 2 step: 403, loss is 0.06056449934840202\n",
      "epoch: 2 step: 404, loss is 0.0813082680106163\n",
      "epoch: 2 step: 405, loss is 0.13501596450805664\n",
      "epoch: 2 step: 406, loss is 0.31189385056495667\n",
      "epoch: 2 step: 407, loss is 0.07404346019029617\n",
      "epoch: 2 step: 408, loss is 0.14367009699344635\n",
      "epoch: 2 step: 409, loss is 0.005441704299300909\n",
      "epoch: 2 step: 410, loss is 0.050627466291189194\n",
      "epoch: 2 step: 411, loss is 0.02425847016274929\n",
      "epoch: 2 step: 412, loss is 0.1916010081768036\n",
      "epoch: 2 step: 413, loss is 0.027009807527065277\n",
      "epoch: 2 step: 414, loss is 0.2318175882101059\n",
      "epoch: 2 step: 415, loss is 0.1328793466091156\n",
      "epoch: 2 step: 416, loss is 0.009367959573864937\n",
      "epoch: 2 step: 417, loss is 0.03163502737879753\n",
      "epoch: 2 step: 418, loss is 0.03918210417032242\n",
      "epoch: 2 step: 419, loss is 0.14538398385047913\n",
      "epoch: 2 step: 420, loss is 0.03665792569518089\n",
      "epoch: 2 step: 421, loss is 0.20396003127098083\n",
      "epoch: 2 step: 422, loss is 0.25840985774993896\n",
      "epoch: 2 step: 423, loss is 0.1835852712392807\n",
      "epoch: 2 step: 424, loss is 0.20607946813106537\n",
      "epoch: 2 step: 425, loss is 0.008913904428482056\n",
      "epoch: 2 step: 426, loss is 0.05200989171862602\n",
      "epoch: 2 step: 427, loss is 0.22471749782562256\n",
      "epoch: 2 step: 428, loss is 0.10566036403179169\n",
      "epoch: 2 step: 429, loss is 0.035148683935403824\n",
      "epoch: 2 step: 430, loss is 0.05674140527844429\n",
      "epoch: 2 step: 431, loss is 0.3178612291812897\n",
      "epoch: 2 step: 432, loss is 0.09518609941005707\n",
      "epoch: 2 step: 433, loss is 0.055517472326755524\n",
      "epoch: 2 step: 434, loss is 0.015760621055960655\n",
      "epoch: 2 step: 435, loss is 0.09385215491056442\n",
      "epoch: 2 step: 436, loss is 0.06447640061378479\n",
      "epoch: 2 step: 437, loss is 0.2012932300567627\n",
      "epoch: 2 step: 438, loss is 0.055691491812467575\n",
      "epoch: 2 step: 439, loss is 0.1426408439874649\n",
      "epoch: 2 step: 440, loss is 0.10640258342027664\n",
      "epoch: 2 step: 441, loss is 0.0356890894472599\n",
      "epoch: 2 step: 442, loss is 0.010120416060090065\n",
      "epoch: 2 step: 443, loss is 0.07067213952541351\n",
      "epoch: 2 step: 444, loss is 0.06438940763473511\n",
      "epoch: 2 step: 445, loss is 0.21400918066501617\n",
      "epoch: 2 step: 446, loss is 0.2452777773141861\n",
      "epoch: 2 step: 447, loss is 0.06444063037633896\n",
      "epoch: 2 step: 448, loss is 0.04682869464159012\n",
      "epoch: 2 step: 449, loss is 0.07842278480529785\n",
      "epoch: 2 step: 450, loss is 0.04246893152594566\n",
      "epoch: 2 step: 451, loss is 0.022547151893377304\n",
      "epoch: 2 step: 452, loss is 0.09875211864709854\n",
      "epoch: 2 step: 453, loss is 0.14792916178703308\n",
      "epoch: 2 step: 454, loss is 0.2167770266532898\n",
      "epoch: 2 step: 455, loss is 0.09914056956768036\n",
      "epoch: 2 step: 456, loss is 0.09056557714939117\n",
      "epoch: 2 step: 457, loss is 0.006676203571259975\n",
      "epoch: 2 step: 458, loss is 0.06959299743175507\n",
      "epoch: 2 step: 459, loss is 0.25008606910705566\n",
      "epoch: 2 step: 460, loss is 0.013705184683203697\n",
      "epoch: 2 step: 461, loss is 0.13672758638858795\n",
      "epoch: 2 step: 462, loss is 0.023589875549077988\n",
      "epoch: 2 step: 463, loss is 0.010823697783052921\n",
      "epoch: 2 step: 464, loss is 0.09293810278177261\n",
      "epoch: 2 step: 465, loss is 0.08728230744600296\n",
      "epoch: 2 step: 466, loss is 0.0557568296790123\n",
      "epoch: 2 step: 467, loss is 0.23130907118320465\n",
      "epoch: 2 step: 468, loss is 0.053503163158893585\n",
      "epoch: 2 step: 469, loss is 0.42746713757514954\n",
      "epoch: 2 step: 470, loss is 0.0061992863193154335\n",
      "epoch: 2 step: 471, loss is 0.23928549885749817\n",
      "epoch: 2 step: 472, loss is 0.020103314891457558\n",
      "epoch: 2 step: 473, loss is 0.04202323406934738\n",
      "epoch: 2 step: 474, loss is 0.0351351760327816\n",
      "epoch: 2 step: 475, loss is 0.04116082191467285\n",
      "epoch: 2 step: 476, loss is 0.03249407932162285\n",
      "epoch: 2 step: 477, loss is 0.1321643888950348\n",
      "epoch: 2 step: 478, loss is 0.10850866138935089\n",
      "epoch: 2 step: 479, loss is 0.18589936196804047\n",
      "epoch: 2 step: 480, loss is 0.03247029334306717\n",
      "epoch: 2 step: 481, loss is 0.1591053307056427\n",
      "epoch: 2 step: 482, loss is 0.11830771714448929\n",
      "epoch: 2 step: 483, loss is 0.23150886595249176\n",
      "epoch: 2 step: 484, loss is 0.10548773407936096\n",
      "epoch: 2 step: 485, loss is 0.013035905547440052\n",
      "epoch: 2 step: 486, loss is 0.3688262701034546\n",
      "epoch: 2 step: 487, loss is 0.15471522510051727\n",
      "epoch: 2 step: 488, loss is 0.016897864639759064\n",
      "epoch: 2 step: 489, loss is 0.027180230244994164\n",
      "epoch: 2 step: 490, loss is 0.3051482141017914\n",
      "epoch: 2 step: 491, loss is 0.050173304975032806\n",
      "epoch: 2 step: 492, loss is 0.11571555584669113\n",
      "epoch: 2 step: 493, loss is 0.05623685196042061\n",
      "epoch: 2 step: 494, loss is 0.3416369557380676\n",
      "epoch: 2 step: 495, loss is 0.11565785109996796\n",
      "epoch: 2 step: 496, loss is 0.022441072389483452\n",
      "epoch: 2 step: 497, loss is 0.2205863893032074\n",
      "epoch: 2 step: 498, loss is 0.06078290939331055\n",
      "epoch: 2 step: 499, loss is 0.16006483137607574\n",
      "epoch: 2 step: 500, loss is 0.07269283384084702\n",
      "epoch: 2 step: 501, loss is 0.019161492586135864\n",
      "epoch: 2 step: 502, loss is 0.082086943089962\n",
      "epoch: 2 step: 503, loss is 0.03154023736715317\n",
      "epoch: 2 step: 504, loss is 0.008081329986453056\n",
      "epoch: 2 step: 505, loss is 0.0628153458237648\n",
      "epoch: 2 step: 506, loss is 0.035151489078998566\n",
      "epoch: 2 step: 507, loss is 0.0028404309414327145\n",
      "epoch: 2 step: 508, loss is 0.005348555278033018\n",
      "epoch: 2 step: 509, loss is 0.19965960085391998\n",
      "epoch: 2 step: 510, loss is 0.41940146684646606\n",
      "epoch: 2 step: 511, loss is 0.0503375306725502\n",
      "epoch: 2 step: 512, loss is 0.04174119606614113\n",
      "epoch: 2 step: 513, loss is 0.1528513878583908\n",
      "epoch: 2 step: 514, loss is 0.12053224444389343\n",
      "epoch: 2 step: 515, loss is 0.10082691162824631\n",
      "epoch: 2 step: 516, loss is 0.05158016085624695\n",
      "epoch: 2 step: 517, loss is 0.22617179155349731\n",
      "epoch: 2 step: 518, loss is 0.06877903640270233\n",
      "epoch: 2 step: 519, loss is 0.1441248208284378\n",
      "epoch: 2 step: 520, loss is 0.0683240294456482\n",
      "epoch: 2 step: 521, loss is 0.010931931436061859\n",
      "epoch: 2 step: 522, loss is 0.07713772356510162\n",
      "epoch: 2 step: 523, loss is 0.015786152333021164\n",
      "epoch: 2 step: 524, loss is 0.020192625001072884\n",
      "epoch: 2 step: 525, loss is 0.15721900761127472\n",
      "epoch: 2 step: 526, loss is 0.07701878249645233\n",
      "epoch: 2 step: 527, loss is 0.05292053520679474\n",
      "epoch: 2 step: 528, loss is 0.09082743525505066\n",
      "epoch: 2 step: 529, loss is 0.09488964080810547\n",
      "epoch: 2 step: 530, loss is 0.06288362294435501\n",
      "epoch: 2 step: 531, loss is 0.039669908583164215\n",
      "epoch: 2 step: 532, loss is 0.02870897203683853\n",
      "epoch: 2 step: 533, loss is 0.012607098557054996\n",
      "epoch: 2 step: 534, loss is 0.03163222596049309\n",
      "epoch: 2 step: 535, loss is 0.003666533390060067\n",
      "epoch: 2 step: 536, loss is 0.0027459647972136736\n",
      "epoch: 2 step: 537, loss is 0.017261356115341187\n",
      "epoch: 2 step: 538, loss is 0.018669815734028816\n",
      "epoch: 2 step: 539, loss is 0.010372648946940899\n",
      "epoch: 2 step: 540, loss is 0.06793130934238434\n",
      "epoch: 2 step: 541, loss is 0.008650860749185085\n",
      "epoch: 2 step: 542, loss is 0.04312317818403244\n",
      "epoch: 2 step: 543, loss is 0.05917845293879509\n",
      "epoch: 2 step: 544, loss is 0.032270077615976334\n",
      "epoch: 2 step: 545, loss is 0.11287017911672592\n",
      "epoch: 2 step: 546, loss is 0.009825886227190495\n",
      "epoch: 2 step: 547, loss is 0.2561444044113159\n",
      "epoch: 2 step: 548, loss is 0.006775785703212023\n",
      "epoch: 2 step: 549, loss is 0.004646584391593933\n",
      "epoch: 2 step: 550, loss is 0.09788452088832855\n",
      "epoch: 2 step: 551, loss is 0.21544243395328522\n",
      "epoch: 2 step: 552, loss is 0.05526990443468094\n",
      "epoch: 2 step: 553, loss is 0.017785033211112022\n",
      "epoch: 2 step: 554, loss is 0.00605106120929122\n",
      "epoch: 2 step: 555, loss is 0.06668912619352341\n",
      "epoch: 2 step: 556, loss is 0.01456714142113924\n",
      "epoch: 2 step: 557, loss is 0.22724826633930206\n",
      "epoch: 2 step: 558, loss is 0.1263623833656311\n",
      "epoch: 2 step: 559, loss is 0.03913097828626633\n",
      "epoch: 2 step: 560, loss is 0.0037627979181706905\n",
      "epoch: 2 step: 561, loss is 0.024728810414671898\n",
      "epoch: 2 step: 562, loss is 0.035174161195755005\n",
      "epoch: 2 step: 563, loss is 0.22296059131622314\n",
      "epoch: 2 step: 564, loss is 0.16053497791290283\n",
      "epoch: 2 step: 565, loss is 0.013890795409679413\n",
      "epoch: 2 step: 566, loss is 0.24739810824394226\n",
      "epoch: 2 step: 567, loss is 0.0686655268073082\n",
      "epoch: 2 step: 568, loss is 0.03443656489253044\n",
      "epoch: 2 step: 569, loss is 0.015983592718839645\n",
      "epoch: 2 step: 570, loss is 0.024472350254654884\n",
      "epoch: 2 step: 571, loss is 0.016917075961828232\n",
      "epoch: 2 step: 572, loss is 0.24750050902366638\n",
      "epoch: 2 step: 573, loss is 0.08326803892850876\n",
      "epoch: 2 step: 574, loss is 0.010598373599350452\n",
      "epoch: 2 step: 575, loss is 0.1148899644613266\n",
      "epoch: 2 step: 576, loss is 0.09324685484170914\n",
      "epoch: 2 step: 577, loss is 0.004779580514878035\n",
      "epoch: 2 step: 578, loss is 0.07161439955234528\n",
      "epoch: 2 step: 579, loss is 0.07094739377498627\n",
      "epoch: 2 step: 580, loss is 0.10810134559869766\n",
      "epoch: 2 step: 581, loss is 0.006178637500852346\n",
      "epoch: 2 step: 582, loss is 0.21844303607940674\n",
      "epoch: 2 step: 583, loss is 0.03322691097855568\n",
      "epoch: 2 step: 584, loss is 0.07580944150686264\n",
      "epoch: 2 step: 585, loss is 0.06043587625026703\n",
      "epoch: 2 step: 586, loss is 0.1752295345067978\n",
      "epoch: 2 step: 587, loss is 0.013668890111148357\n",
      "epoch: 2 step: 588, loss is 0.04448961094021797\n",
      "epoch: 2 step: 589, loss is 0.0061075109988451\n",
      "epoch: 2 step: 590, loss is 0.169934943318367\n",
      "epoch: 2 step: 591, loss is 0.2350197285413742\n",
      "epoch: 2 step: 592, loss is 0.015588752925395966\n",
      "epoch: 2 step: 593, loss is 0.016165057197213173\n",
      "epoch: 2 step: 594, loss is 0.14079517126083374\n",
      "epoch: 2 step: 595, loss is 0.053329404443502426\n",
      "epoch: 2 step: 596, loss is 0.01847585290670395\n",
      "epoch: 2 step: 597, loss is 0.08952680975198746\n",
      "epoch: 2 step: 598, loss is 0.1353946030139923\n",
      "epoch: 2 step: 599, loss is 0.13351930677890778\n",
      "epoch: 2 step: 600, loss is 0.13846231997013092\n",
      "epoch: 2 step: 601, loss is 0.033429183065891266\n",
      "epoch: 2 step: 602, loss is 0.024723315611481667\n",
      "epoch: 2 step: 603, loss is 0.04281136021018028\n",
      "epoch: 2 step: 604, loss is 0.21764180064201355\n",
      "epoch: 2 step: 605, loss is 0.08555140346288681\n",
      "epoch: 2 step: 606, loss is 0.20413774251937866\n",
      "epoch: 2 step: 607, loss is 0.13016818463802338\n",
      "epoch: 2 step: 608, loss is 0.19880840182304382\n",
      "epoch: 2 step: 609, loss is 0.014033244922757149\n",
      "epoch: 2 step: 610, loss is 0.06471262872219086\n",
      "epoch: 2 step: 611, loss is 0.0731835588812828\n",
      "epoch: 2 step: 612, loss is 0.13357919454574585\n",
      "epoch: 2 step: 613, loss is 0.019389135763049126\n",
      "epoch: 2 step: 614, loss is 0.0461086630821228\n",
      "epoch: 2 step: 615, loss is 0.007858483120799065\n",
      "epoch: 2 step: 616, loss is 0.10549906641244888\n",
      "epoch: 2 step: 617, loss is 0.014951720833778381\n",
      "epoch: 2 step: 618, loss is 0.002444556914269924\n",
      "epoch: 2 step: 619, loss is 0.016509154811501503\n",
      "epoch: 2 step: 620, loss is 0.073536217212677\n",
      "epoch: 2 step: 621, loss is 0.05617181956768036\n",
      "epoch: 2 step: 622, loss is 0.12707571685314178\n",
      "epoch: 2 step: 623, loss is 0.0839042067527771\n",
      "epoch: 2 step: 624, loss is 0.04587351158261299\n",
      "epoch: 2 step: 625, loss is 0.10242968052625656\n",
      "epoch: 2 step: 626, loss is 0.00390001037158072\n",
      "epoch: 2 step: 627, loss is 0.047403231263160706\n",
      "epoch: 2 step: 628, loss is 0.10260552912950516\n",
      "epoch: 2 step: 629, loss is 0.02386724203824997\n",
      "epoch: 2 step: 630, loss is 0.0071139042265713215\n",
      "epoch: 2 step: 631, loss is 0.09682896733283997\n",
      "epoch: 2 step: 632, loss is 0.024703368544578552\n",
      "epoch: 2 step: 633, loss is 0.13610920310020447\n",
      "epoch: 2 step: 634, loss is 0.03130568563938141\n",
      "epoch: 2 step: 635, loss is 0.010031605139374733\n",
      "epoch: 2 step: 636, loss is 0.001762477564625442\n",
      "epoch: 2 step: 637, loss is 0.002905310830101371\n",
      "epoch: 2 step: 638, loss is 0.025306999683380127\n",
      "epoch: 2 step: 639, loss is 0.048351820558309555\n",
      "epoch: 2 step: 640, loss is 0.05114400014281273\n",
      "epoch: 2 step: 641, loss is 0.061702486127614975\n",
      "epoch: 2 step: 642, loss is 0.002503165975213051\n",
      "epoch: 2 step: 643, loss is 0.4655800759792328\n",
      "epoch: 2 step: 644, loss is 0.22665835916996002\n",
      "epoch: 2 step: 645, loss is 0.24208348989486694\n",
      "epoch: 2 step: 646, loss is 0.044272735714912415\n",
      "epoch: 2 step: 647, loss is 0.03716174513101578\n",
      "epoch: 2 step: 648, loss is 0.008989045396447182\n",
      "epoch: 2 step: 649, loss is 0.1859404593706131\n",
      "epoch: 2 step: 650, loss is 0.13027358055114746\n",
      "epoch: 2 step: 651, loss is 0.01644347980618477\n",
      "epoch: 2 step: 652, loss is 0.010863730683922768\n",
      "epoch: 2 step: 653, loss is 0.011699562892317772\n",
      "epoch: 2 step: 654, loss is 0.014366237446665764\n",
      "epoch: 2 step: 655, loss is 0.014302801340818405\n",
      "epoch: 2 step: 656, loss is 0.023692050948739052\n",
      "epoch: 2 step: 657, loss is 0.002314344048500061\n",
      "epoch: 2 step: 658, loss is 0.11387462168931961\n",
      "epoch: 2 step: 659, loss is 0.01153595931828022\n",
      "epoch: 2 step: 660, loss is 0.02713795378804207\n",
      "epoch: 2 step: 661, loss is 0.014296761713922024\n",
      "epoch: 2 step: 662, loss is 0.08956234157085419\n",
      "epoch: 2 step: 663, loss is 0.07609962671995163\n",
      "epoch: 2 step: 664, loss is 0.40416473150253296\n",
      "epoch: 2 step: 665, loss is 0.11785591393709183\n",
      "epoch: 2 step: 666, loss is 0.2652592062950134\n",
      "epoch: 2 step: 667, loss is 0.1801193505525589\n",
      "epoch: 2 step: 668, loss is 0.046426061540842056\n",
      "epoch: 2 step: 669, loss is 0.18492387235164642\n",
      "epoch: 2 step: 670, loss is 0.00471859984099865\n",
      "epoch: 2 step: 671, loss is 0.08030327409505844\n",
      "epoch: 2 step: 672, loss is 0.11426057666540146\n",
      "epoch: 2 step: 673, loss is 0.022449471056461334\n",
      "epoch: 2 step: 674, loss is 0.06806483119726181\n",
      "epoch: 2 step: 675, loss is 0.006168002262711525\n",
      "epoch: 2 step: 676, loss is 0.04388342425227165\n",
      "epoch: 2 step: 677, loss is 0.22731360793113708\n",
      "epoch: 2 step: 678, loss is 0.11410398781299591\n",
      "epoch: 2 step: 679, loss is 0.15058457851409912\n",
      "epoch: 2 step: 680, loss is 0.03505823388695717\n",
      "epoch: 2 step: 681, loss is 0.02404908463358879\n",
      "epoch: 2 step: 682, loss is 0.05049217864871025\n",
      "epoch: 2 step: 683, loss is 0.03385469317436218\n",
      "epoch: 2 step: 684, loss is 0.11033983528614044\n",
      "epoch: 2 step: 685, loss is 0.0257547739893198\n",
      "epoch: 2 step: 686, loss is 0.0718047022819519\n",
      "epoch: 2 step: 687, loss is 0.036291882395744324\n",
      "epoch: 2 step: 688, loss is 0.006453145295381546\n",
      "epoch: 2 step: 689, loss is 0.00564816826954484\n",
      "epoch: 2 step: 690, loss is 0.13379013538360596\n",
      "epoch: 2 step: 691, loss is 0.02536873333156109\n",
      "epoch: 2 step: 692, loss is 0.029464177787303925\n",
      "epoch: 2 step: 693, loss is 0.04564998671412468\n",
      "epoch: 2 step: 694, loss is 0.006903909146785736\n",
      "epoch: 2 step: 695, loss is 0.18638575077056885\n",
      "epoch: 2 step: 696, loss is 0.09478956460952759\n",
      "epoch: 2 step: 697, loss is 0.0035844361409544945\n",
      "epoch: 2 step: 698, loss is 0.19470813870429993\n",
      "epoch: 2 step: 699, loss is 0.007673781830817461\n",
      "epoch: 2 step: 700, loss is 0.11649630963802338\n",
      "epoch: 2 step: 701, loss is 0.048184458166360855\n",
      "epoch: 2 step: 702, loss is 0.07140560448169708\n",
      "epoch: 2 step: 703, loss is 0.026409268379211426\n",
      "epoch: 2 step: 704, loss is 0.15045277774333954\n",
      "epoch: 2 step: 705, loss is 0.053433991968631744\n",
      "epoch: 2 step: 706, loss is 0.00926875788718462\n",
      "epoch: 2 step: 707, loss is 0.03174668177962303\n",
      "epoch: 2 step: 708, loss is 0.03905981034040451\n",
      "epoch: 2 step: 709, loss is 0.03901366516947746\n",
      "epoch: 2 step: 710, loss is 0.22976849973201752\n",
      "epoch: 2 step: 711, loss is 0.14195863902568817\n",
      "epoch: 2 step: 712, loss is 0.09582331031560898\n",
      "epoch: 2 step: 713, loss is 0.1751629114151001\n",
      "epoch: 2 step: 714, loss is 0.025398120284080505\n",
      "epoch: 2 step: 715, loss is 0.0049253650940954685\n",
      "epoch: 2 step: 716, loss is 0.10778627544641495\n",
      "epoch: 2 step: 717, loss is 0.004223203752189875\n",
      "epoch: 2 step: 718, loss is 0.08749686926603317\n",
      "epoch: 2 step: 719, loss is 0.02167055755853653\n",
      "epoch: 2 step: 720, loss is 0.047863882035017014\n",
      "epoch: 2 step: 721, loss is 0.2789073884487152\n",
      "epoch: 2 step: 722, loss is 0.018592366948723793\n",
      "epoch: 2 step: 723, loss is 0.0062841023318469524\n",
      "epoch: 2 step: 724, loss is 0.24075011909008026\n",
      "epoch: 2 step: 725, loss is 0.03882874920964241\n",
      "epoch: 2 step: 726, loss is 0.1663551777601242\n",
      "epoch: 2 step: 727, loss is 0.03892658278346062\n",
      "epoch: 2 step: 728, loss is 0.014335132203996181\n",
      "epoch: 2 step: 729, loss is 0.2624959945678711\n",
      "epoch: 2 step: 730, loss is 0.03334539756178856\n",
      "epoch: 2 step: 731, loss is 0.20207689702510834\n",
      "epoch: 2 step: 732, loss is 0.06523807346820831\n",
      "epoch: 2 step: 733, loss is 0.23654496669769287\n",
      "epoch: 2 step: 734, loss is 0.1485002636909485\n",
      "epoch: 2 step: 735, loss is 0.1843869686126709\n",
      "epoch: 2 step: 736, loss is 0.1121007427573204\n",
      "epoch: 2 step: 737, loss is 0.028275782242417336\n",
      "epoch: 2 step: 738, loss is 0.021170329302549362\n",
      "epoch: 2 step: 739, loss is 0.2268887162208557\n",
      "epoch: 2 step: 740, loss is 0.021056978031992912\n",
      "epoch: 2 step: 741, loss is 0.05460371449589729\n",
      "epoch: 2 step: 742, loss is 0.020940598100423813\n",
      "epoch: 2 step: 743, loss is 0.08652652055025101\n",
      "epoch: 2 step: 744, loss is 0.13586069643497467\n",
      "epoch: 2 step: 745, loss is 0.011493141762912273\n",
      "epoch: 2 step: 746, loss is 0.03573872894048691\n",
      "epoch: 2 step: 747, loss is 0.025272123515605927\n",
      "epoch: 2 step: 748, loss is 0.008444452658295631\n",
      "epoch: 2 step: 749, loss is 0.1344791054725647\n",
      "epoch: 2 step: 750, loss is 0.08042442798614502\n",
      "epoch: 2 step: 751, loss is 0.016129348427057266\n",
      "epoch: 2 step: 752, loss is 0.09384676069021225\n",
      "epoch: 2 step: 753, loss is 0.06821076571941376\n",
      "epoch: 2 step: 754, loss is 0.10191673785448074\n",
      "epoch: 2 step: 755, loss is 0.05098692700266838\n",
      "epoch: 2 step: 756, loss is 0.005977608263492584\n",
      "epoch: 2 step: 757, loss is 0.011369653046131134\n",
      "epoch: 2 step: 758, loss is 0.09568208456039429\n",
      "epoch: 2 step: 759, loss is 0.021323537454009056\n",
      "epoch: 2 step: 760, loss is 0.036267027258872986\n",
      "epoch: 2 step: 761, loss is 0.010114832781255245\n",
      "epoch: 2 step: 762, loss is 0.019319606944918633\n",
      "epoch: 2 step: 763, loss is 0.04277363047003746\n",
      "epoch: 2 step: 764, loss is 0.15998223423957825\n",
      "epoch: 2 step: 765, loss is 0.005023104604333639\n",
      "epoch: 2 step: 766, loss is 0.026445182040333748\n",
      "epoch: 2 step: 767, loss is 0.2646273970603943\n",
      "epoch: 2 step: 768, loss is 0.004597513936460018\n",
      "epoch: 2 step: 769, loss is 0.025724630802869797\n",
      "epoch: 2 step: 770, loss is 0.04174927994608879\n",
      "epoch: 2 step: 771, loss is 0.011855262331664562\n",
      "epoch: 2 step: 772, loss is 0.007992257364094257\n",
      "epoch: 2 step: 773, loss is 0.05171700194478035\n",
      "epoch: 2 step: 774, loss is 0.015054832212626934\n",
      "epoch: 2 step: 775, loss is 0.011182684451341629\n",
      "epoch: 2 step: 776, loss is 0.04591120779514313\n",
      "epoch: 2 step: 777, loss is 0.015168784186244011\n",
      "epoch: 2 step: 778, loss is 0.015103091485798359\n",
      "epoch: 2 step: 779, loss is 0.007117358967661858\n",
      "epoch: 2 step: 780, loss is 0.039652641862630844\n",
      "epoch: 2 step: 781, loss is 0.023580465465784073\n",
      "epoch: 2 step: 782, loss is 0.08825383335351944\n",
      "epoch: 2 step: 783, loss is 0.011402355507016182\n",
      "epoch: 2 step: 784, loss is 0.21991288661956787\n",
      "epoch: 2 step: 785, loss is 0.006868932396173477\n",
      "epoch: 2 step: 786, loss is 0.0021172533743083477\n",
      "epoch: 2 step: 787, loss is 0.007857575081288815\n",
      "epoch: 2 step: 788, loss is 0.012821147218346596\n",
      "epoch: 2 step: 789, loss is 0.3233969509601593\n",
      "epoch: 2 step: 790, loss is 0.04831906780600548\n",
      "epoch: 2 step: 791, loss is 0.0034720683470368385\n",
      "epoch: 2 step: 792, loss is 0.07270758599042892\n",
      "epoch: 2 step: 793, loss is 0.023993534967303276\n",
      "epoch: 2 step: 794, loss is 0.004627436399459839\n",
      "epoch: 2 step: 795, loss is 0.0280877947807312\n",
      "epoch: 2 step: 796, loss is 0.14386378228664398\n",
      "epoch: 2 step: 797, loss is 0.00579830352216959\n",
      "epoch: 2 step: 798, loss is 0.0006570442928932607\n",
      "epoch: 2 step: 799, loss is 0.07552116364240646\n",
      "epoch: 2 step: 800, loss is 0.10442240536212921\n",
      "epoch: 2 step: 801, loss is 0.1260305792093277\n",
      "epoch: 2 step: 802, loss is 0.019623268395662308\n",
      "epoch: 2 step: 803, loss is 0.008151473477482796\n",
      "epoch: 2 step: 804, loss is 0.01389075256884098\n",
      "epoch: 2 step: 805, loss is 0.018410110846161842\n",
      "epoch: 2 step: 806, loss is 0.21902954578399658\n",
      "epoch: 2 step: 807, loss is 0.058721791952848434\n",
      "epoch: 2 step: 808, loss is 0.05728645995259285\n",
      "epoch: 2 step: 809, loss is 0.08740232139825821\n",
      "epoch: 2 step: 810, loss is 0.002881231252104044\n",
      "epoch: 2 step: 811, loss is 0.0019595997873693705\n",
      "epoch: 2 step: 812, loss is 0.0269034281373024\n",
      "epoch: 2 step: 813, loss is 0.1559751182794571\n",
      "epoch: 2 step: 814, loss is 0.015153514221310616\n",
      "epoch: 2 step: 815, loss is 0.09623782336711884\n",
      "epoch: 2 step: 816, loss is 0.13037188351154327\n",
      "epoch: 2 step: 817, loss is 0.09045925736427307\n",
      "epoch: 2 step: 818, loss is 0.08543065190315247\n",
      "epoch: 2 step: 819, loss is 0.18774628639221191\n",
      "epoch: 2 step: 820, loss is 0.037270501255989075\n",
      "epoch: 2 step: 821, loss is 0.2065029889345169\n",
      "epoch: 2 step: 822, loss is 0.1388791799545288\n",
      "epoch: 2 step: 823, loss is 0.10478819161653519\n",
      "epoch: 2 step: 824, loss is 0.007576963864266872\n",
      "epoch: 2 step: 825, loss is 0.0034046361688524485\n",
      "epoch: 2 step: 826, loss is 0.08564557135105133\n",
      "epoch: 2 step: 827, loss is 0.015974486246705055\n",
      "epoch: 2 step: 828, loss is 0.05385472625494003\n",
      "epoch: 2 step: 829, loss is 0.007838127203285694\n",
      "epoch: 2 step: 830, loss is 0.011344991624355316\n",
      "epoch: 2 step: 831, loss is 0.023863820359110832\n",
      "epoch: 2 step: 832, loss is 0.006844695191830397\n",
      "epoch: 2 step: 833, loss is 0.0532958023250103\n",
      "epoch: 2 step: 834, loss is 0.007294408977031708\n",
      "epoch: 2 step: 835, loss is 0.004145352635532618\n",
      "epoch: 2 step: 836, loss is 0.016878506168723106\n",
      "epoch: 2 step: 837, loss is 0.009975334629416466\n",
      "epoch: 2 step: 838, loss is 0.0069025540724396706\n",
      "epoch: 2 step: 839, loss is 0.03891846537590027\n",
      "epoch: 2 step: 840, loss is 0.09192754328250885\n",
      "epoch: 2 step: 841, loss is 0.0749245136976242\n",
      "epoch: 2 step: 842, loss is 0.06430348753929138\n",
      "epoch: 2 step: 843, loss is 0.00890857819467783\n",
      "epoch: 2 step: 844, loss is 0.027884790673851967\n",
      "epoch: 2 step: 845, loss is 0.007791164796799421\n",
      "epoch: 2 step: 846, loss is 0.005401618778705597\n",
      "epoch: 2 step: 847, loss is 0.024737723171710968\n",
      "epoch: 2 step: 848, loss is 0.02069297432899475\n",
      "epoch: 2 step: 849, loss is 0.007515864912420511\n",
      "epoch: 2 step: 850, loss is 0.11467330157756805\n",
      "epoch: 2 step: 851, loss is 0.017283035442233086\n",
      "epoch: 2 step: 852, loss is 0.0023176607210189104\n",
      "epoch: 2 step: 853, loss is 0.025396306067705154\n",
      "epoch: 2 step: 854, loss is 0.05913180857896805\n",
      "epoch: 2 step: 855, loss is 0.28203949332237244\n",
      "epoch: 2 step: 856, loss is 0.10042314231395721\n",
      "epoch: 2 step: 857, loss is 0.05555607005953789\n",
      "epoch: 2 step: 858, loss is 0.031246228143572807\n",
      "epoch: 2 step: 859, loss is 0.0869595855474472\n",
      "epoch: 2 step: 860, loss is 0.22451603412628174\n",
      "epoch: 2 step: 861, loss is 0.013764564879238605\n",
      "epoch: 2 step: 862, loss is 0.015874983742833138\n",
      "epoch: 2 step: 863, loss is 0.0608360655605793\n",
      "epoch: 2 step: 864, loss is 0.12594294548034668\n",
      "epoch: 2 step: 865, loss is 0.2299756109714508\n",
      "epoch: 2 step: 866, loss is 0.013751573860645294\n",
      "epoch: 2 step: 867, loss is 0.031129352748394012\n",
      "epoch: 2 step: 868, loss is 0.0735427588224411\n",
      "epoch: 2 step: 869, loss is 0.18305949866771698\n",
      "epoch: 2 step: 870, loss is 0.08886595070362091\n",
      "epoch: 2 step: 871, loss is 0.037154681980609894\n",
      "epoch: 2 step: 872, loss is 0.09149859845638275\n",
      "epoch: 2 step: 873, loss is 0.012327022850513458\n",
      "epoch: 2 step: 874, loss is 0.012411501258611679\n",
      "epoch: 2 step: 875, loss is 0.012021739967167377\n",
      "epoch: 2 step: 876, loss is 0.007274269126355648\n",
      "epoch: 2 step: 877, loss is 0.002439741510897875\n",
      "epoch: 2 step: 878, loss is 0.17375704646110535\n",
      "epoch: 2 step: 879, loss is 0.3011261820793152\n",
      "epoch: 2 step: 880, loss is 0.1712409406900406\n",
      "epoch: 2 step: 881, loss is 0.09126418083906174\n",
      "epoch: 2 step: 882, loss is 0.020832903683185577\n",
      "epoch: 2 step: 883, loss is 0.0570332296192646\n",
      "epoch: 2 step: 884, loss is 0.12442722916603088\n",
      "epoch: 2 step: 885, loss is 0.08574895560741425\n",
      "epoch: 2 step: 886, loss is 0.1306442767381668\n",
      "epoch: 2 step: 887, loss is 0.004371698014438152\n",
      "epoch: 2 step: 888, loss is 0.059716641902923584\n",
      "epoch: 2 step: 889, loss is 0.08917445689439774\n",
      "epoch: 2 step: 890, loss is 0.36877894401550293\n",
      "epoch: 2 step: 891, loss is 0.01655542105436325\n",
      "epoch: 2 step: 892, loss is 0.035585932433605194\n",
      "epoch: 2 step: 893, loss is 0.008357465267181396\n",
      "epoch: 2 step: 894, loss is 0.016598602756857872\n",
      "epoch: 2 step: 895, loss is 0.01660197600722313\n",
      "epoch: 2 step: 896, loss is 0.06927201896905899\n",
      "epoch: 2 step: 897, loss is 0.0939917340874672\n",
      "epoch: 2 step: 898, loss is 0.07966116070747375\n",
      "epoch: 2 step: 899, loss is 0.10639846324920654\n",
      "epoch: 2 step: 900, loss is 0.13400782644748688\n",
      "epoch: 2 step: 901, loss is 0.023119458928704262\n",
      "epoch: 2 step: 902, loss is 0.19999410212039948\n",
      "epoch: 2 step: 903, loss is 0.06596380472183228\n",
      "epoch: 2 step: 904, loss is 0.0017733825370669365\n",
      "epoch: 2 step: 905, loss is 0.014253316447138786\n",
      "epoch: 2 step: 906, loss is 0.28179171681404114\n",
      "epoch: 2 step: 907, loss is 0.10410909354686737\n",
      "epoch: 2 step: 908, loss is 0.09659304469823837\n",
      "epoch: 2 step: 909, loss is 0.007301975507289171\n",
      "epoch: 2 step: 910, loss is 0.060641512274742126\n",
      "epoch: 2 step: 911, loss is 0.029209835454821587\n",
      "epoch: 2 step: 912, loss is 0.05334295704960823\n",
      "epoch: 2 step: 913, loss is 0.061971284449100494\n",
      "epoch: 2 step: 914, loss is 0.04668804258108139\n",
      "epoch: 2 step: 915, loss is 0.020218942314386368\n",
      "epoch: 2 step: 916, loss is 0.039539437741041183\n",
      "epoch: 2 step: 917, loss is 0.22378455102443695\n",
      "epoch: 2 step: 918, loss is 0.03638840094208717\n",
      "epoch: 2 step: 919, loss is 0.1665191650390625\n",
      "epoch: 2 step: 920, loss is 0.15826955437660217\n",
      "epoch: 2 step: 921, loss is 0.15143346786499023\n",
      "epoch: 2 step: 922, loss is 0.19524289667606354\n",
      "epoch: 2 step: 923, loss is 0.36038610339164734\n",
      "epoch: 2 step: 924, loss is 0.12957867980003357\n",
      "epoch: 2 step: 925, loss is 0.010384068824350834\n",
      "epoch: 2 step: 926, loss is 0.004411159548908472\n",
      "epoch: 2 step: 927, loss is 0.01329694502055645\n",
      "epoch: 2 step: 928, loss is 0.04535963386297226\n",
      "epoch: 2 step: 929, loss is 0.13702307641506195\n",
      "epoch: 2 step: 930, loss is 0.12402545660734177\n",
      "epoch: 2 step: 931, loss is 0.05486861243844032\n",
      "epoch: 2 step: 932, loss is 0.012890190817415714\n",
      "epoch: 2 step: 933, loss is 0.2577354609966278\n",
      "epoch: 2 step: 934, loss is 0.2733553349971771\n",
      "epoch: 2 step: 935, loss is 0.1407904326915741\n",
      "epoch: 2 step: 936, loss is 0.02820650301873684\n",
      "epoch: 2 step: 937, loss is 0.009768261574208736\n",
      "epoch: 2 step: 938, loss is 0.009761171415448189\n",
      "epoch: 2 step: 939, loss is 0.04230570048093796\n",
      "epoch: 2 step: 940, loss is 0.2105102390050888\n",
      "epoch: 2 step: 941, loss is 0.06831567734479904\n",
      "epoch: 2 step: 942, loss is 0.30389657616615295\n",
      "epoch: 2 step: 943, loss is 0.05576645955443382\n",
      "epoch: 2 step: 944, loss is 0.058362822979688644\n",
      "epoch: 2 step: 945, loss is 0.052280403673648834\n",
      "epoch: 2 step: 946, loss is 0.018132764846086502\n",
      "epoch: 2 step: 947, loss is 0.10091602802276611\n",
      "epoch: 2 step: 948, loss is 0.07291415333747864\n",
      "epoch: 2 step: 949, loss is 0.2535971403121948\n",
      "epoch: 2 step: 950, loss is 0.05360446125268936\n",
      "epoch: 2 step: 951, loss is 0.15192633867263794\n",
      "epoch: 2 step: 952, loss is 0.029602913185954094\n",
      "epoch: 2 step: 953, loss is 0.02627999521791935\n",
      "epoch: 2 step: 954, loss is 0.08670873194932938\n",
      "epoch: 2 step: 955, loss is 0.012830965220928192\n",
      "epoch: 2 step: 956, loss is 0.044492416083812714\n",
      "epoch: 2 step: 957, loss is 0.01171140931546688\n",
      "epoch: 2 step: 958, loss is 0.03760852664709091\n",
      "epoch: 2 step: 959, loss is 0.11029894649982452\n",
      "epoch: 2 step: 960, loss is 0.09911458194255829\n",
      "epoch: 2 step: 961, loss is 0.025526689365506172\n",
      "epoch: 2 step: 962, loss is 0.0692252516746521\n",
      "epoch: 2 step: 963, loss is 0.07250531762838364\n",
      "epoch: 2 step: 964, loss is 0.02337370067834854\n",
      "epoch: 2 step: 965, loss is 0.1391400843858719\n",
      "epoch: 2 step: 966, loss is 0.11169213801622391\n",
      "epoch: 2 step: 967, loss is 0.3348337709903717\n",
      "epoch: 2 step: 968, loss is 0.08789021521806717\n",
      "epoch: 2 step: 969, loss is 0.1398480236530304\n",
      "epoch: 2 step: 970, loss is 0.1871347278356552\n",
      "epoch: 2 step: 971, loss is 0.09881572425365448\n",
      "epoch: 2 step: 972, loss is 0.004438227042555809\n",
      "epoch: 2 step: 973, loss is 0.06857961416244507\n",
      "epoch: 2 step: 974, loss is 0.024041051045060158\n",
      "epoch: 2 step: 975, loss is 0.04202952980995178\n",
      "epoch: 2 step: 976, loss is 0.015522029250860214\n",
      "epoch: 2 step: 977, loss is 0.05822012946009636\n",
      "epoch: 2 step: 978, loss is 0.22293418645858765\n",
      "epoch: 2 step: 979, loss is 0.07227068394422531\n",
      "epoch: 2 step: 980, loss is 0.06103036552667618\n",
      "epoch: 2 step: 981, loss is 0.12061861902475357\n",
      "epoch: 2 step: 982, loss is 0.02216724492609501\n",
      "epoch: 2 step: 983, loss is 0.0037665131967514753\n",
      "epoch: 2 step: 984, loss is 0.014930475503206253\n",
      "epoch: 2 step: 985, loss is 0.05579760670661926\n",
      "epoch: 2 step: 986, loss is 0.20203669369220734\n",
      "epoch: 2 step: 987, loss is 0.07869871705770493\n",
      "epoch: 2 step: 988, loss is 0.013851127587258816\n",
      "epoch: 2 step: 989, loss is 0.006690342910587788\n",
      "epoch: 2 step: 990, loss is 0.16150625050067902\n",
      "epoch: 2 step: 991, loss is 0.01090581901371479\n",
      "epoch: 2 step: 992, loss is 0.027601052075624466\n",
      "epoch: 2 step: 993, loss is 0.018088562414050102\n",
      "epoch: 2 step: 994, loss is 0.19554957747459412\n",
      "epoch: 2 step: 995, loss is 0.04193414747714996\n",
      "epoch: 2 step: 996, loss is 0.021279199048876762\n",
      "epoch: 2 step: 997, loss is 0.008793553337454796\n",
      "epoch: 2 step: 998, loss is 0.031433604657649994\n",
      "epoch: 2 step: 999, loss is 0.00454590143635869\n",
      "epoch: 2 step: 1000, loss is 0.014187728986144066\n",
      "epoch: 2 step: 1001, loss is 0.12007516622543335\n",
      "epoch: 2 step: 1002, loss is 0.018522534519433975\n",
      "epoch: 2 step: 1003, loss is 0.018071478232741356\n",
      "epoch: 2 step: 1004, loss is 0.12765395641326904\n",
      "epoch: 2 step: 1005, loss is 0.03535033017396927\n",
      "epoch: 2 step: 1006, loss is 0.03112589381635189\n",
      "epoch: 2 step: 1007, loss is 0.028307544067502022\n",
      "epoch: 2 step: 1008, loss is 0.0040572816506028175\n",
      "epoch: 2 step: 1009, loss is 0.028144674375653267\n",
      "epoch: 2 step: 1010, loss is 0.0072977435775101185\n",
      "epoch: 2 step: 1011, loss is 0.03810551017522812\n",
      "epoch: 2 step: 1012, loss is 0.03213631734251976\n",
      "epoch: 2 step: 1013, loss is 0.10600633174180984\n",
      "epoch: 2 step: 1014, loss is 0.09406638890504837\n",
      "epoch: 2 step: 1015, loss is 0.208236426115036\n",
      "epoch: 2 step: 1016, loss is 0.07799351960420609\n",
      "epoch: 2 step: 1017, loss is 0.002259042114019394\n",
      "epoch: 2 step: 1018, loss is 0.022279009222984314\n",
      "epoch: 2 step: 1019, loss is 0.037026528269052505\n",
      "epoch: 2 step: 1020, loss is 0.03320701792836189\n",
      "epoch: 2 step: 1021, loss is 0.05104280635714531\n",
      "epoch: 2 step: 1022, loss is 0.0989425927400589\n",
      "epoch: 2 step: 1023, loss is 0.022459648549556732\n",
      "epoch: 2 step: 1024, loss is 0.04513774439692497\n",
      "epoch: 2 step: 1025, loss is 0.0022112189326435328\n",
      "epoch: 2 step: 1026, loss is 0.05284503102302551\n",
      "epoch: 2 step: 1027, loss is 0.018159300088882446\n",
      "epoch: 2 step: 1028, loss is 0.22437459230422974\n",
      "epoch: 2 step: 1029, loss is 0.09179408103227615\n",
      "epoch: 2 step: 1030, loss is 0.2382238656282425\n",
      "epoch: 2 step: 1031, loss is 0.17552286386489868\n",
      "epoch: 2 step: 1032, loss is 0.027974829077720642\n",
      "epoch: 2 step: 1033, loss is 0.012590768747031689\n",
      "epoch: 2 step: 1034, loss is 0.034427959471940994\n",
      "epoch: 2 step: 1035, loss is 0.021787794306874275\n",
      "epoch: 2 step: 1036, loss is 0.04886665940284729\n",
      "epoch: 2 step: 1037, loss is 0.00586061691865325\n",
      "epoch: 2 step: 1038, loss is 0.002292415825650096\n",
      "epoch: 2 step: 1039, loss is 0.21483968198299408\n",
      "epoch: 2 step: 1040, loss is 0.12078217417001724\n",
      "epoch: 2 step: 1041, loss is 0.27101460099220276\n",
      "epoch: 2 step: 1042, loss is 0.0946471244096756\n",
      "epoch: 2 step: 1043, loss is 0.08489595353603363\n",
      "epoch: 2 step: 1044, loss is 0.13867780566215515\n",
      "epoch: 2 step: 1045, loss is 0.004996593110263348\n",
      "epoch: 2 step: 1046, loss is 0.011310538277029991\n",
      "epoch: 2 step: 1047, loss is 0.010109234601259232\n",
      "epoch: 2 step: 1048, loss is 0.1895674616098404\n",
      "epoch: 2 step: 1049, loss is 0.12440706044435501\n",
      "epoch: 2 step: 1050, loss is 0.1479829102754593\n",
      "epoch: 2 step: 1051, loss is 0.011462298221886158\n",
      "epoch: 2 step: 1052, loss is 0.016096798703074455\n",
      "epoch: 2 step: 1053, loss is 0.021830713376402855\n",
      "epoch: 2 step: 1054, loss is 0.08062059432268143\n",
      "epoch: 2 step: 1055, loss is 0.15019670128822327\n",
      "epoch: 2 step: 1056, loss is 0.06818201392889023\n",
      "epoch: 2 step: 1057, loss is 0.07736626267433167\n",
      "epoch: 2 step: 1058, loss is 0.27988743782043457\n",
      "epoch: 2 step: 1059, loss is 0.1092422679066658\n",
      "epoch: 2 step: 1060, loss is 0.03017476759850979\n",
      "epoch: 2 step: 1061, loss is 0.024560704827308655\n",
      "epoch: 2 step: 1062, loss is 0.1703517585992813\n",
      "epoch: 2 step: 1063, loss is 0.13396435976028442\n",
      "epoch: 2 step: 1064, loss is 0.022579215466976166\n",
      "epoch: 2 step: 1065, loss is 0.16903705894947052\n",
      "epoch: 2 step: 1066, loss is 0.05602583661675453\n",
      "epoch: 2 step: 1067, loss is 0.03364285081624985\n",
      "epoch: 2 step: 1068, loss is 0.24111893773078918\n",
      "epoch: 2 step: 1069, loss is 0.048666369169950485\n",
      "epoch: 2 step: 1070, loss is 0.010534470900893211\n",
      "epoch: 2 step: 1071, loss is 0.007318512536585331\n",
      "epoch: 2 step: 1072, loss is 0.08471296727657318\n",
      "epoch: 2 step: 1073, loss is 0.04486232250928879\n",
      "epoch: 2 step: 1074, loss is 0.1479424089193344\n",
      "epoch: 2 step: 1075, loss is 0.1333562582731247\n",
      "epoch: 2 step: 1076, loss is 0.1442299783229828\n",
      "epoch: 2 step: 1077, loss is 0.09672795236110687\n",
      "epoch: 2 step: 1078, loss is 0.04626964405179024\n",
      "epoch: 2 step: 1079, loss is 0.03448599576950073\n",
      "epoch: 2 step: 1080, loss is 0.11030355095863342\n",
      "epoch: 2 step: 1081, loss is 0.010993856005370617\n",
      "epoch: 2 step: 1082, loss is 0.01560693234205246\n",
      "epoch: 2 step: 1083, loss is 0.016106996685266495\n",
      "epoch: 2 step: 1084, loss is 0.08620195090770721\n",
      "epoch: 2 step: 1085, loss is 0.007666470482945442\n",
      "epoch: 2 step: 1086, loss is 0.04619746282696724\n",
      "epoch: 2 step: 1087, loss is 0.08509405702352524\n",
      "epoch: 2 step: 1088, loss is 0.051589544862508774\n",
      "epoch: 2 step: 1089, loss is 0.16353774070739746\n",
      "epoch: 2 step: 1090, loss is 0.009582358412444592\n",
      "epoch: 2 step: 1091, loss is 0.012582287192344666\n",
      "epoch: 2 step: 1092, loss is 0.11641617119312286\n",
      "epoch: 2 step: 1093, loss is 0.03584097698330879\n",
      "epoch: 2 step: 1094, loss is 0.04244748130440712\n",
      "epoch: 2 step: 1095, loss is 0.03917412459850311\n",
      "epoch: 2 step: 1096, loss is 0.09566248953342438\n",
      "epoch: 2 step: 1097, loss is 0.02706390991806984\n",
      "epoch: 2 step: 1098, loss is 0.24212248623371124\n",
      "epoch: 2 step: 1099, loss is 0.05625082179903984\n",
      "epoch: 2 step: 1100, loss is 0.02000226080417633\n",
      "epoch: 2 step: 1101, loss is 0.04333562031388283\n",
      "epoch: 2 step: 1102, loss is 0.1295931041240692\n",
      "epoch: 2 step: 1103, loss is 0.017553171142935753\n",
      "epoch: 2 step: 1104, loss is 0.16750332713127136\n",
      "epoch: 2 step: 1105, loss is 0.019201593473553658\n",
      "epoch: 2 step: 1106, loss is 0.006061722058802843\n",
      "epoch: 2 step: 1107, loss is 0.07758226245641708\n",
      "epoch: 2 step: 1108, loss is 0.22161999344825745\n",
      "epoch: 2 step: 1109, loss is 0.06116703525185585\n",
      "epoch: 2 step: 1110, loss is 0.11735781282186508\n",
      "epoch: 2 step: 1111, loss is 0.004695217125117779\n",
      "epoch: 2 step: 1112, loss is 0.168477401137352\n",
      "epoch: 2 step: 1113, loss is 0.03305080160498619\n",
      "epoch: 2 step: 1114, loss is 0.005730480886995792\n",
      "epoch: 2 step: 1115, loss is 0.004701570142060518\n",
      "epoch: 2 step: 1116, loss is 0.05291314423084259\n",
      "epoch: 2 step: 1117, loss is 0.12604433298110962\n",
      "epoch: 2 step: 1118, loss is 0.00780784897506237\n",
      "epoch: 2 step: 1119, loss is 0.02810082584619522\n",
      "epoch: 2 step: 1120, loss is 0.09212938696146011\n",
      "epoch: 2 step: 1121, loss is 0.013822020031511784\n",
      "epoch: 2 step: 1122, loss is 0.013294801115989685\n",
      "epoch: 2 step: 1123, loss is 0.32643264532089233\n",
      "epoch: 2 step: 1124, loss is 0.24133895337581635\n",
      "epoch: 2 step: 1125, loss is 0.04122355207800865\n",
      "epoch: 2 step: 1126, loss is 0.12217223644256592\n",
      "epoch: 2 step: 1127, loss is 0.03601579740643501\n",
      "epoch: 2 step: 1128, loss is 0.029518455266952515\n",
      "epoch: 2 step: 1129, loss is 0.19992347061634064\n",
      "epoch: 2 step: 1130, loss is 0.14399079978466034\n",
      "epoch: 2 step: 1131, loss is 0.0154210040345788\n",
      "epoch: 2 step: 1132, loss is 0.015219194814562798\n",
      "epoch: 2 step: 1133, loss is 0.02357909083366394\n",
      "epoch: 2 step: 1134, loss is 0.036637481302022934\n",
      "epoch: 2 step: 1135, loss is 0.0074016940779984\n",
      "epoch: 2 step: 1136, loss is 0.0284096859395504\n",
      "epoch: 2 step: 1137, loss is 0.10894407331943512\n",
      "epoch: 2 step: 1138, loss is 0.08165071159601212\n",
      "epoch: 2 step: 1139, loss is 0.07451148331165314\n",
      "epoch: 2 step: 1140, loss is 0.0035668611526489258\n",
      "epoch: 2 step: 1141, loss is 0.11128535866737366\n",
      "epoch: 2 step: 1142, loss is 0.014202459715306759\n",
      "epoch: 2 step: 1143, loss is 0.15633194148540497\n",
      "epoch: 2 step: 1144, loss is 0.010354310274124146\n",
      "epoch: 2 step: 1145, loss is 0.007085409015417099\n",
      "epoch: 2 step: 1146, loss is 0.04432738944888115\n",
      "epoch: 2 step: 1147, loss is 0.1040852889418602\n",
      "epoch: 2 step: 1148, loss is 0.012368346564471722\n",
      "epoch: 2 step: 1149, loss is 0.17497318983078003\n",
      "epoch: 2 step: 1150, loss is 0.011110392399132252\n",
      "epoch: 2 step: 1151, loss is 0.1477210372686386\n",
      "epoch: 2 step: 1152, loss is 0.01118885912001133\n",
      "epoch: 2 step: 1153, loss is 0.009354368783533573\n",
      "epoch: 2 step: 1154, loss is 0.06372692435979843\n",
      "epoch: 2 step: 1155, loss is 0.0060868822038173676\n",
      "epoch: 2 step: 1156, loss is 0.120893195271492\n",
      "epoch: 2 step: 1157, loss is 0.00566131854429841\n",
      "epoch: 2 step: 1158, loss is 0.012056632898747921\n",
      "epoch: 2 step: 1159, loss is 0.020276207476854324\n",
      "epoch: 2 step: 1160, loss is 0.026213817298412323\n",
      "epoch: 2 step: 1161, loss is 0.13001687824726105\n",
      "epoch: 2 step: 1162, loss is 0.022373035550117493\n",
      "epoch: 2 step: 1163, loss is 0.058990489691495895\n",
      "epoch: 2 step: 1164, loss is 0.04736257717013359\n",
      "epoch: 2 step: 1165, loss is 0.010775988921523094\n",
      "epoch: 2 step: 1166, loss is 0.04140990599989891\n",
      "epoch: 2 step: 1167, loss is 0.004774394445121288\n",
      "epoch: 2 step: 1168, loss is 0.014015167020261288\n",
      "epoch: 2 step: 1169, loss is 0.04962879791855812\n",
      "epoch: 2 step: 1170, loss is 0.010327107273042202\n",
      "epoch: 2 step: 1171, loss is 0.021482504904270172\n",
      "epoch: 2 step: 1172, loss is 0.0505635142326355\n",
      "epoch: 2 step: 1173, loss is 0.044641003012657166\n",
      "epoch: 2 step: 1174, loss is 0.1082453802227974\n",
      "epoch: 2 step: 1175, loss is 0.006422305013984442\n",
      "epoch: 2 step: 1176, loss is 0.019022753462195396\n",
      "epoch: 2 step: 1177, loss is 0.01805945858359337\n",
      "epoch: 2 step: 1178, loss is 0.03464067354798317\n",
      "epoch: 2 step: 1179, loss is 0.14734946191310883\n",
      "epoch: 2 step: 1180, loss is 0.04868537560105324\n",
      "epoch: 2 step: 1181, loss is 0.24158795177936554\n",
      "epoch: 2 step: 1182, loss is 0.0169238168746233\n",
      "epoch: 2 step: 1183, loss is 0.013317771255970001\n",
      "epoch: 2 step: 1184, loss is 0.1854478120803833\n",
      "epoch: 2 step: 1185, loss is 0.10806809365749359\n",
      "epoch: 2 step: 1186, loss is 0.03145799785852432\n",
      "epoch: 2 step: 1187, loss is 0.020362459123134613\n",
      "epoch: 2 step: 1188, loss is 0.002337933983653784\n",
      "epoch: 2 step: 1189, loss is 0.06456242501735687\n",
      "epoch: 2 step: 1190, loss is 0.05927585810422897\n",
      "epoch: 2 step: 1191, loss is 0.020636552944779396\n",
      "epoch: 2 step: 1192, loss is 0.010241097770631313\n",
      "epoch: 2 step: 1193, loss is 0.005510680843144655\n",
      "epoch: 2 step: 1194, loss is 0.010082632303237915\n",
      "epoch: 2 step: 1195, loss is 0.04694562405347824\n",
      "epoch: 2 step: 1196, loss is 0.014653429388999939\n",
      "epoch: 2 step: 1197, loss is 0.01666569523513317\n",
      "epoch: 2 step: 1198, loss is 0.06328047066926956\n",
      "epoch: 2 step: 1199, loss is 0.05144229903817177\n",
      "epoch: 2 step: 1200, loss is 0.001864155288785696\n",
      "epoch: 2 step: 1201, loss is 0.25418731570243835\n",
      "epoch: 2 step: 1202, loss is 0.06348013132810593\n",
      "epoch: 2 step: 1203, loss is 0.061278849840164185\n",
      "epoch: 2 step: 1204, loss is 0.044524792581796646\n",
      "epoch: 2 step: 1205, loss is 0.09793219715356827\n",
      "epoch: 2 step: 1206, loss is 0.1750108003616333\n",
      "epoch: 2 step: 1207, loss is 0.014016108587384224\n",
      "epoch: 2 step: 1208, loss is 0.050244659185409546\n",
      "epoch: 2 step: 1209, loss is 0.004044839181005955\n",
      "epoch: 2 step: 1210, loss is 0.07739987224340439\n",
      "epoch: 2 step: 1211, loss is 0.04000287503004074\n",
      "epoch: 2 step: 1212, loss is 0.010515877045691013\n",
      "epoch: 2 step: 1213, loss is 0.011537650600075722\n",
      "epoch: 2 step: 1214, loss is 0.016967402771115303\n",
      "epoch: 2 step: 1215, loss is 0.06570657342672348\n",
      "epoch: 2 step: 1216, loss is 0.3017938435077667\n",
      "epoch: 2 step: 1217, loss is 0.07003819197416306\n",
      "epoch: 2 step: 1218, loss is 0.06313668191432953\n",
      "epoch: 2 step: 1219, loss is 0.0051538096740841866\n",
      "epoch: 2 step: 1220, loss is 0.02294575609266758\n",
      "epoch: 2 step: 1221, loss is 0.17399384081363678\n",
      "epoch: 2 step: 1222, loss is 0.22754187881946564\n",
      "epoch: 2 step: 1223, loss is 0.028243601322174072\n",
      "epoch: 2 step: 1224, loss is 0.08264829218387604\n",
      "epoch: 2 step: 1225, loss is 0.22614751756191254\n",
      "epoch: 2 step: 1226, loss is 0.0008241031900979578\n",
      "epoch: 2 step: 1227, loss is 0.04166189581155777\n",
      "epoch: 2 step: 1228, loss is 0.2933168411254883\n",
      "epoch: 2 step: 1229, loss is 0.03153134509921074\n",
      "epoch: 2 step: 1230, loss is 0.1737559735774994\n",
      "epoch: 2 step: 1231, loss is 0.019565384835004807\n",
      "epoch: 2 step: 1232, loss is 0.01911095157265663\n",
      "epoch: 2 step: 1233, loss is 0.06518211960792542\n",
      "epoch: 2 step: 1234, loss is 0.06712818145751953\n",
      "epoch: 2 step: 1235, loss is 0.060988686978816986\n",
      "epoch: 2 step: 1236, loss is 0.056418705731630325\n",
      "epoch: 2 step: 1237, loss is 0.006574091035872698\n",
      "epoch: 2 step: 1238, loss is 0.02967180125415325\n",
      "epoch: 2 step: 1239, loss is 0.10224306583404541\n",
      "epoch: 2 step: 1240, loss is 0.09365732222795486\n",
      "epoch: 2 step: 1241, loss is 0.08855950832366943\n",
      "epoch: 2 step: 1242, loss is 0.046237774193286896\n",
      "epoch: 2 step: 1243, loss is 0.0732196718454361\n",
      "epoch: 2 step: 1244, loss is 0.11170173436403275\n",
      "epoch: 2 step: 1245, loss is 0.025761669501662254\n",
      "epoch: 2 step: 1246, loss is 0.011644035577774048\n",
      "epoch: 2 step: 1247, loss is 0.14228545129299164\n",
      "epoch: 2 step: 1248, loss is 0.0033595282584428787\n",
      "epoch: 2 step: 1249, loss is 0.19001691043376923\n",
      "epoch: 2 step: 1250, loss is 0.013532535172998905\n",
      "epoch: 2 step: 1251, loss is 0.15581928193569183\n",
      "epoch: 2 step: 1252, loss is 0.021686242893338203\n",
      "epoch: 2 step: 1253, loss is 0.20036357641220093\n",
      "epoch: 2 step: 1254, loss is 0.07770521938800812\n",
      "epoch: 2 step: 1255, loss is 0.03468921035528183\n",
      "epoch: 2 step: 1256, loss is 0.01047939620912075\n",
      "epoch: 2 step: 1257, loss is 0.012400360777974129\n",
      "epoch: 2 step: 1258, loss is 0.03931242972612381\n",
      "epoch: 2 step: 1259, loss is 0.15874916315078735\n",
      "epoch: 2 step: 1260, loss is 0.37994229793548584\n",
      "epoch: 2 step: 1261, loss is 0.13712678849697113\n",
      "epoch: 2 step: 1262, loss is 0.15335288643836975\n",
      "epoch: 2 step: 1263, loss is 0.037743981927633286\n",
      "epoch: 2 step: 1264, loss is 0.018630601465702057\n",
      "epoch: 2 step: 1265, loss is 0.04572948440909386\n",
      "epoch: 2 step: 1266, loss is 0.040744949132204056\n",
      "epoch: 2 step: 1267, loss is 0.005751084536314011\n",
      "epoch: 2 step: 1268, loss is 0.10794296860694885\n",
      "epoch: 2 step: 1269, loss is 0.020307352766394615\n",
      "epoch: 2 step: 1270, loss is 0.0069030337035655975\n",
      "epoch: 2 step: 1271, loss is 0.00808795914053917\n",
      "epoch: 2 step: 1272, loss is 0.019472938030958176\n",
      "epoch: 2 step: 1273, loss is 0.008250408805906773\n",
      "epoch: 2 step: 1274, loss is 0.11135866492986679\n",
      "epoch: 2 step: 1275, loss is 0.1254580169916153\n",
      "epoch: 2 step: 1276, loss is 0.037353336811065674\n",
      "epoch: 2 step: 1277, loss is 0.012183121405541897\n",
      "epoch: 2 step: 1278, loss is 0.019215652719140053\n",
      "epoch: 2 step: 1279, loss is 0.00654214434325695\n",
      "epoch: 2 step: 1280, loss is 0.008230000734329224\n",
      "epoch: 2 step: 1281, loss is 0.0338178314268589\n",
      "epoch: 2 step: 1282, loss is 0.04762580990791321\n",
      "epoch: 2 step: 1283, loss is 0.005908202845603228\n",
      "epoch: 2 step: 1284, loss is 0.043466199189424515\n",
      "epoch: 2 step: 1285, loss is 0.21434977650642395\n",
      "epoch: 2 step: 1286, loss is 0.06636751443147659\n",
      "epoch: 2 step: 1287, loss is 0.14259964227676392\n",
      "epoch: 2 step: 1288, loss is 0.002877937164157629\n",
      "epoch: 2 step: 1289, loss is 0.07803760468959808\n",
      "epoch: 2 step: 1290, loss is 0.026785163208842278\n",
      "epoch: 2 step: 1291, loss is 0.03294255584478378\n",
      "epoch: 2 step: 1292, loss is 0.16586223244667053\n",
      "epoch: 2 step: 1293, loss is 0.023467719554901123\n",
      "epoch: 2 step: 1294, loss is 0.2113284021615982\n",
      "epoch: 2 step: 1295, loss is 0.25524279475212097\n",
      "epoch: 2 step: 1296, loss is 0.03470420092344284\n",
      "epoch: 2 step: 1297, loss is 0.12561233341693878\n",
      "epoch: 2 step: 1298, loss is 0.23574428260326385\n",
      "epoch: 2 step: 1299, loss is 0.007527303881943226\n",
      "epoch: 2 step: 1300, loss is 0.0796775296330452\n",
      "epoch: 2 step: 1301, loss is 0.013655073940753937\n",
      "epoch: 2 step: 1302, loss is 0.0028531220741569996\n",
      "epoch: 2 step: 1303, loss is 0.010381786152720451\n",
      "epoch: 2 step: 1304, loss is 0.022032342851161957\n",
      "epoch: 2 step: 1305, loss is 0.12371444702148438\n",
      "epoch: 2 step: 1306, loss is 0.01002125721424818\n",
      "epoch: 2 step: 1307, loss is 0.017696723341941833\n",
      "epoch: 2 step: 1308, loss is 0.03118341602385044\n",
      "epoch: 2 step: 1309, loss is 0.18892070651054382\n",
      "epoch: 2 step: 1310, loss is 0.036053940653800964\n",
      "epoch: 2 step: 1311, loss is 0.1050024926662445\n",
      "epoch: 2 step: 1312, loss is 0.046132370829582214\n",
      "epoch: 2 step: 1313, loss is 0.0035298755392432213\n",
      "epoch: 2 step: 1314, loss is 0.18392033874988556\n",
      "epoch: 2 step: 1315, loss is 0.115843765437603\n",
      "epoch: 2 step: 1316, loss is 0.03973418474197388\n",
      "epoch: 2 step: 1317, loss is 0.01624983176589012\n",
      "epoch: 2 step: 1318, loss is 0.09105204790830612\n",
      "epoch: 2 step: 1319, loss is 0.05019901320338249\n",
      "epoch: 2 step: 1320, loss is 0.006639639846980572\n",
      "epoch: 2 step: 1321, loss is 0.0174791868776083\n",
      "epoch: 2 step: 1322, loss is 0.21812793612480164\n",
      "epoch: 2 step: 1323, loss is 0.06575743854045868\n",
      "epoch: 2 step: 1324, loss is 0.1016082763671875\n",
      "epoch: 2 step: 1325, loss is 0.017207536846399307\n",
      "epoch: 2 step: 1326, loss is 0.11023879796266556\n",
      "epoch: 2 step: 1327, loss is 0.318819522857666\n",
      "epoch: 2 step: 1328, loss is 0.011152452789247036\n",
      "epoch: 2 step: 1329, loss is 0.09252782166004181\n",
      "epoch: 2 step: 1330, loss is 0.05702902749180794\n",
      "epoch: 2 step: 1331, loss is 0.014610149897634983\n",
      "epoch: 2 step: 1332, loss is 0.0036566380877047777\n",
      "epoch: 2 step: 1333, loss is 0.00861547514796257\n",
      "epoch: 2 step: 1334, loss is 0.0913090929389\n",
      "epoch: 2 step: 1335, loss is 0.01712610200047493\n",
      "epoch: 2 step: 1336, loss is 0.06239508464932442\n",
      "epoch: 2 step: 1337, loss is 0.04976596683263779\n",
      "epoch: 2 step: 1338, loss is 0.010128110647201538\n",
      "epoch: 2 step: 1339, loss is 0.08783896267414093\n",
      "epoch: 2 step: 1340, loss is 0.07277995347976685\n",
      "epoch: 2 step: 1341, loss is 0.051723163574934006\n",
      "epoch: 2 step: 1342, loss is 0.14373090863227844\n",
      "epoch: 2 step: 1343, loss is 0.04336792603135109\n",
      "epoch: 2 step: 1344, loss is 0.15015453100204468\n",
      "epoch: 2 step: 1345, loss is 0.0187925286591053\n",
      "epoch: 2 step: 1346, loss is 0.01611252874135971\n",
      "epoch: 2 step: 1347, loss is 0.1123298928141594\n",
      "epoch: 2 step: 1348, loss is 0.03291971608996391\n",
      "epoch: 2 step: 1349, loss is 0.02026478387415409\n",
      "epoch: 2 step: 1350, loss is 0.08320900797843933\n",
      "epoch: 2 step: 1351, loss is 0.054671283811330795\n",
      "epoch: 2 step: 1352, loss is 0.018231362104415894\n",
      "epoch: 2 step: 1353, loss is 0.0886082872748375\n",
      "epoch: 2 step: 1354, loss is 0.004666087683290243\n",
      "epoch: 2 step: 1355, loss is 0.11138473451137543\n",
      "epoch: 2 step: 1356, loss is 0.0842001661658287\n",
      "epoch: 2 step: 1357, loss is 0.12182746827602386\n",
      "epoch: 2 step: 1358, loss is 0.01912325993180275\n",
      "epoch: 2 step: 1359, loss is 0.0063224537298083305\n",
      "epoch: 2 step: 1360, loss is 0.05063895881175995\n",
      "epoch: 2 step: 1361, loss is 0.025369729846715927\n",
      "epoch: 2 step: 1362, loss is 0.2063380479812622\n",
      "epoch: 2 step: 1363, loss is 0.05633079260587692\n",
      "epoch: 2 step: 1364, loss is 0.12831158936023712\n",
      "epoch: 2 step: 1365, loss is 0.014314563944935799\n",
      "epoch: 2 step: 1366, loss is 0.10121730715036392\n",
      "epoch: 2 step: 1367, loss is 0.15077176690101624\n",
      "epoch: 2 step: 1368, loss is 0.07882378250360489\n",
      "epoch: 2 step: 1369, loss is 0.07771243900060654\n",
      "epoch: 2 step: 1370, loss is 0.024623438715934753\n",
      "epoch: 2 step: 1371, loss is 0.15202659368515015\n",
      "epoch: 2 step: 1372, loss is 0.07245616614818573\n",
      "epoch: 2 step: 1373, loss is 0.04856466129422188\n",
      "epoch: 2 step: 1374, loss is 0.007518130820244551\n",
      "epoch: 2 step: 1375, loss is 0.03465446084737778\n",
      "epoch: 2 step: 1376, loss is 0.04795138165354729\n",
      "epoch: 2 step: 1377, loss is 0.057968877255916595\n",
      "epoch: 2 step: 1378, loss is 0.15824630856513977\n",
      "epoch: 2 step: 1379, loss is 0.13535569608211517\n",
      "epoch: 2 step: 1380, loss is 0.0033670340199023485\n",
      "epoch: 2 step: 1381, loss is 0.0181560255587101\n",
      "epoch: 2 step: 1382, loss is 0.006437062285840511\n",
      "epoch: 2 step: 1383, loss is 0.012466886080801487\n",
      "epoch: 2 step: 1384, loss is 0.010851064696907997\n",
      "epoch: 2 step: 1385, loss is 0.1909051537513733\n",
      "epoch: 2 step: 1386, loss is 0.019615180790424347\n",
      "epoch: 2 step: 1387, loss is 0.25693026185035706\n",
      "epoch: 2 step: 1388, loss is 0.019964417442679405\n",
      "epoch: 2 step: 1389, loss is 0.26609060168266296\n",
      "epoch: 2 step: 1390, loss is 0.01627078466117382\n",
      "epoch: 2 step: 1391, loss is 0.05860881134867668\n",
      "epoch: 2 step: 1392, loss is 0.0688520222902298\n",
      "epoch: 2 step: 1393, loss is 0.006355071906000376\n",
      "epoch: 2 step: 1394, loss is 0.007228496950119734\n",
      "epoch: 2 step: 1395, loss is 0.09299203753471375\n",
      "epoch: 2 step: 1396, loss is 0.07014953345060349\n",
      "epoch: 2 step: 1397, loss is 0.002509689424186945\n",
      "epoch: 2 step: 1398, loss is 0.07806095480918884\n",
      "epoch: 2 step: 1399, loss is 0.04571427032351494\n",
      "epoch: 2 step: 1400, loss is 0.02078627608716488\n",
      "epoch: 2 step: 1401, loss is 0.019531769677996635\n",
      "epoch: 2 step: 1402, loss is 0.007953721098601818\n",
      "epoch: 2 step: 1403, loss is 0.25734561681747437\n",
      "epoch: 2 step: 1404, loss is 0.030135711655020714\n",
      "epoch: 2 step: 1405, loss is 0.08589200675487518\n",
      "epoch: 2 step: 1406, loss is 0.007724246475845575\n",
      "epoch: 2 step: 1407, loss is 0.008641785010695457\n",
      "epoch: 2 step: 1408, loss is 0.008690183982253075\n",
      "epoch: 2 step: 1409, loss is 0.019058775156736374\n",
      "epoch: 2 step: 1410, loss is 0.15505962073802948\n",
      "epoch: 2 step: 1411, loss is 0.3160375654697418\n",
      "epoch: 2 step: 1412, loss is 0.017266595736145973\n",
      "epoch: 2 step: 1413, loss is 0.09266512095928192\n",
      "epoch: 2 step: 1414, loss is 0.014285031706094742\n",
      "epoch: 2 step: 1415, loss is 0.012249669060111046\n",
      "epoch: 2 step: 1416, loss is 0.01164325326681137\n",
      "epoch: 2 step: 1417, loss is 0.018431086093187332\n",
      "epoch: 2 step: 1418, loss is 0.010756068862974644\n",
      "epoch: 2 step: 1419, loss is 0.007564190309494734\n",
      "epoch: 2 step: 1420, loss is 0.015207537449896336\n",
      "epoch: 2 step: 1421, loss is 0.07053716480731964\n",
      "epoch: 2 step: 1422, loss is 0.026611901819705963\n",
      "epoch: 2 step: 1423, loss is 0.37235110998153687\n",
      "epoch: 2 step: 1424, loss is 0.004960608668625355\n",
      "epoch: 2 step: 1425, loss is 0.11018440127372742\n",
      "epoch: 2 step: 1426, loss is 0.011757409200072289\n",
      "epoch: 2 step: 1427, loss is 0.23204699158668518\n",
      "epoch: 2 step: 1428, loss is 0.059769049286842346\n",
      "epoch: 2 step: 1429, loss is 0.00165280990768224\n",
      "epoch: 2 step: 1430, loss is 0.0744381844997406\n",
      "epoch: 2 step: 1431, loss is 0.012654628604650497\n",
      "epoch: 2 step: 1432, loss is 0.39479517936706543\n",
      "epoch: 2 step: 1433, loss is 0.02355901524424553\n",
      "epoch: 2 step: 1434, loss is 0.01500059012323618\n",
      "epoch: 2 step: 1435, loss is 0.03342050686478615\n",
      "epoch: 2 step: 1436, loss is 0.0100178774446249\n",
      "epoch: 2 step: 1437, loss is 0.022074880078434944\n",
      "epoch: 2 step: 1438, loss is 0.2923356294631958\n",
      "epoch: 2 step: 1439, loss is 0.026595160365104675\n",
      "epoch: 2 step: 1440, loss is 0.010743803344666958\n",
      "epoch: 2 step: 1441, loss is 0.012628717347979546\n",
      "epoch: 2 step: 1442, loss is 0.06675687432289124\n",
      "epoch: 2 step: 1443, loss is 0.005850279238075018\n",
      "epoch: 2 step: 1444, loss is 0.005206838250160217\n",
      "epoch: 2 step: 1445, loss is 0.011901511810719967\n",
      "epoch: 2 step: 1446, loss is 0.007668823003768921\n",
      "epoch: 2 step: 1447, loss is 0.018615171313285828\n",
      "epoch: 2 step: 1448, loss is 0.09659364074468613\n",
      "epoch: 2 step: 1449, loss is 0.0705946683883667\n",
      "epoch: 2 step: 1450, loss is 0.026237210258841515\n",
      "epoch: 2 step: 1451, loss is 0.20633457601070404\n",
      "epoch: 2 step: 1452, loss is 0.023513542488217354\n",
      "epoch: 2 step: 1453, loss is 0.07315972447395325\n",
      "epoch: 2 step: 1454, loss is 0.20272792875766754\n",
      "epoch: 2 step: 1455, loss is 0.09528066217899323\n",
      "epoch: 2 step: 1456, loss is 0.01374808233231306\n",
      "epoch: 2 step: 1457, loss is 0.04442496597766876\n",
      "epoch: 2 step: 1458, loss is 0.08130227029323578\n",
      "epoch: 2 step: 1459, loss is 0.22344475984573364\n",
      "epoch: 2 step: 1460, loss is 0.013330554589629173\n",
      "epoch: 2 step: 1461, loss is 0.009494826197624207\n",
      "epoch: 2 step: 1462, loss is 0.015145712532103062\n",
      "epoch: 2 step: 1463, loss is 0.008853966370224953\n",
      "epoch: 2 step: 1464, loss is 0.007441026158630848\n",
      "epoch: 2 step: 1465, loss is 0.015931058675050735\n",
      "epoch: 2 step: 1466, loss is 0.012480361387133598\n",
      "epoch: 2 step: 1467, loss is 0.10510600358247757\n",
      "epoch: 2 step: 1468, loss is 0.09663503617048264\n",
      "epoch: 2 step: 1469, loss is 0.014990255236625671\n",
      "epoch: 2 step: 1470, loss is 0.2411089390516281\n",
      "epoch: 2 step: 1471, loss is 0.1721521019935608\n",
      "epoch: 2 step: 1472, loss is 0.07937794923782349\n",
      "epoch: 2 step: 1473, loss is 0.008176575414836407\n",
      "epoch: 2 step: 1474, loss is 0.01797768659889698\n",
      "epoch: 2 step: 1475, loss is 0.08100146800279617\n",
      "epoch: 2 step: 1476, loss is 0.028371399268507957\n",
      "epoch: 2 step: 1477, loss is 0.029586782678961754\n",
      "epoch: 2 step: 1478, loss is 0.027510350570082664\n",
      "epoch: 2 step: 1479, loss is 0.05795542895793915\n",
      "epoch: 2 step: 1480, loss is 0.09233047068119049\n",
      "epoch: 2 step: 1481, loss is 0.1228240579366684\n",
      "epoch: 2 step: 1482, loss is 0.13895104825496674\n",
      "epoch: 2 step: 1483, loss is 0.03206200897693634\n",
      "epoch: 2 step: 1484, loss is 0.029789427295327187\n",
      "epoch: 2 step: 1485, loss is 0.04719298332929611\n",
      "epoch: 2 step: 1486, loss is 0.30639055371284485\n",
      "epoch: 2 step: 1487, loss is 0.1236632764339447\n",
      "epoch: 2 step: 1488, loss is 0.419869989156723\n",
      "epoch: 2 step: 1489, loss is 0.14744451642036438\n",
      "epoch: 2 step: 1490, loss is 0.0970769077539444\n",
      "epoch: 2 step: 1491, loss is 0.006421101279556751\n",
      "epoch: 2 step: 1492, loss is 0.01149189192801714\n",
      "epoch: 2 step: 1493, loss is 0.22702300548553467\n",
      "epoch: 2 step: 1494, loss is 0.01746387593448162\n",
      "epoch: 2 step: 1495, loss is 0.021700279787182808\n",
      "epoch: 2 step: 1496, loss is 0.06343478709459305\n",
      "epoch: 2 step: 1497, loss is 0.020222341641783714\n",
      "epoch: 2 step: 1498, loss is 0.074515201151371\n",
      "epoch: 2 step: 1499, loss is 0.034278031438589096\n",
      "epoch: 2 step: 1500, loss is 0.011245903559029102\n",
      "epoch: 2 step: 1501, loss is 0.02927502989768982\n",
      "epoch: 2 step: 1502, loss is 0.06948547065258026\n",
      "epoch: 2 step: 1503, loss is 0.02740436978638172\n",
      "epoch: 2 step: 1504, loss is 0.05437293276190758\n",
      "epoch: 2 step: 1505, loss is 0.1453203707933426\n",
      "epoch: 2 step: 1506, loss is 0.09172961860895157\n",
      "epoch: 2 step: 1507, loss is 0.15634487569332123\n",
      "epoch: 2 step: 1508, loss is 0.008141953498125076\n",
      "epoch: 2 step: 1509, loss is 0.0017500524409115314\n",
      "epoch: 2 step: 1510, loss is 0.01937149465084076\n",
      "epoch: 2 step: 1511, loss is 0.012605046853423119\n",
      "epoch: 2 step: 1512, loss is 0.003772053634747863\n",
      "epoch: 2 step: 1513, loss is 0.060304947197437286\n",
      "epoch: 2 step: 1514, loss is 0.01173532847315073\n",
      "epoch: 2 step: 1515, loss is 0.021707871928811073\n",
      "epoch: 2 step: 1516, loss is 0.003945892211049795\n",
      "epoch: 2 step: 1517, loss is 0.15250319242477417\n",
      "epoch: 2 step: 1518, loss is 0.16997967660427094\n",
      "epoch: 2 step: 1519, loss is 0.08775833994150162\n",
      "epoch: 2 step: 1520, loss is 0.08619817346334457\n",
      "epoch: 2 step: 1521, loss is 0.01136947050690651\n",
      "epoch: 2 step: 1522, loss is 0.1762576699256897\n",
      "epoch: 2 step: 1523, loss is 0.018876375630497932\n",
      "epoch: 2 step: 1524, loss is 0.038869183510541916\n",
      "epoch: 2 step: 1525, loss is 0.03736964985728264\n",
      "epoch: 2 step: 1526, loss is 0.4344501793384552\n",
      "epoch: 2 step: 1527, loss is 0.08996965736150742\n",
      "epoch: 2 step: 1528, loss is 0.07519672065973282\n",
      "epoch: 2 step: 1529, loss is 0.03471452742815018\n",
      "epoch: 2 step: 1530, loss is 0.05185892432928085\n",
      "epoch: 2 step: 1531, loss is 0.10154233127832413\n",
      "epoch: 2 step: 1532, loss is 0.011325855739414692\n",
      "epoch: 2 step: 1533, loss is 0.1185993030667305\n",
      "epoch: 2 step: 1534, loss is 0.03460881486535072\n",
      "epoch: 2 step: 1535, loss is 0.012495282106101513\n",
      "epoch: 2 step: 1536, loss is 0.03060859628021717\n",
      "epoch: 2 step: 1537, loss is 0.059580616652965546\n",
      "epoch: 2 step: 1538, loss is 0.02390141971409321\n",
      "epoch: 2 step: 1539, loss is 0.03872644901275635\n",
      "epoch: 2 step: 1540, loss is 0.1424189805984497\n",
      "epoch: 2 step: 1541, loss is 0.05001118779182434\n",
      "epoch: 2 step: 1542, loss is 0.27462083101272583\n",
      "epoch: 2 step: 1543, loss is 0.020200524479150772\n",
      "epoch: 2 step: 1544, loss is 0.1754285991191864\n",
      "epoch: 2 step: 1545, loss is 0.08737309277057648\n",
      "epoch: 2 step: 1546, loss is 0.02637445367872715\n",
      "epoch: 2 step: 1547, loss is 0.010163295082747936\n",
      "epoch: 2 step: 1548, loss is 0.13740524649620056\n",
      "epoch: 2 step: 1549, loss is 0.026477595791220665\n",
      "epoch: 2 step: 1550, loss is 0.00813059601932764\n",
      "epoch: 2 step: 1551, loss is 0.003861136268824339\n",
      "epoch: 2 step: 1552, loss is 0.16581743955612183\n",
      "epoch: 2 step: 1553, loss is 0.17101852595806122\n",
      "epoch: 2 step: 1554, loss is 0.10478103905916214\n",
      "epoch: 2 step: 1555, loss is 0.15234403312206268\n",
      "epoch: 2 step: 1556, loss is 0.014213521964848042\n",
      "epoch: 2 step: 1557, loss is 0.08486706763505936\n",
      "epoch: 2 step: 1558, loss is 0.002545051509514451\n",
      "epoch: 2 step: 1559, loss is 0.09056302160024643\n",
      "epoch: 2 step: 1560, loss is 0.05983632057905197\n",
      "epoch: 2 step: 1561, loss is 0.2819877564907074\n",
      "epoch: 2 step: 1562, loss is 0.0626576766371727\n",
      "epoch: 2 step: 1563, loss is 0.010357965715229511\n",
      "epoch: 2 step: 1564, loss is 0.039198171347379684\n",
      "epoch: 2 step: 1565, loss is 0.13633017241954803\n",
      "epoch: 2 step: 1566, loss is 0.02657722681760788\n",
      "epoch: 2 step: 1567, loss is 0.07939571142196655\n",
      "epoch: 2 step: 1568, loss is 0.05046340450644493\n",
      "epoch: 2 step: 1569, loss is 0.12941403687000275\n",
      "epoch: 2 step: 1570, loss is 0.03170366957783699\n",
      "epoch: 2 step: 1571, loss is 0.02248821221292019\n",
      "epoch: 2 step: 1572, loss is 0.11947758495807648\n",
      "epoch: 2 step: 1573, loss is 0.03698011115193367\n",
      "epoch: 2 step: 1574, loss is 0.1796903759241104\n",
      "epoch: 2 step: 1575, loss is 0.018652820959687233\n",
      "epoch: 2 step: 1576, loss is 0.029844829812645912\n",
      "epoch: 2 step: 1577, loss is 0.0022401243913918734\n",
      "epoch: 2 step: 1578, loss is 0.06425240635871887\n",
      "epoch: 2 step: 1579, loss is 0.0033200853504240513\n",
      "epoch: 2 step: 1580, loss is 0.013942573219537735\n",
      "epoch: 2 step: 1581, loss is 0.018725840374827385\n",
      "epoch: 2 step: 1582, loss is 0.14538389444351196\n",
      "epoch: 2 step: 1583, loss is 0.015123938210308552\n",
      "epoch: 2 step: 1584, loss is 0.1165345087647438\n",
      "epoch: 2 step: 1585, loss is 0.004634014796465635\n",
      "epoch: 2 step: 1586, loss is 0.11889602243900299\n",
      "epoch: 2 step: 1587, loss is 0.1426706463098526\n",
      "epoch: 2 step: 1588, loss is 0.06872226297855377\n",
      "epoch: 2 step: 1589, loss is 0.04181410372257233\n",
      "epoch: 2 step: 1590, loss is 0.2438407987356186\n",
      "epoch: 2 step: 1591, loss is 0.18714016675949097\n",
      "epoch: 2 step: 1592, loss is 0.04776911810040474\n",
      "epoch: 2 step: 1593, loss is 0.17278838157653809\n",
      "epoch: 2 step: 1594, loss is 0.05297264829277992\n",
      "epoch: 2 step: 1595, loss is 0.2500894367694855\n",
      "epoch: 2 step: 1596, loss is 0.2353353351354599\n",
      "epoch: 2 step: 1597, loss is 0.013116922229528427\n",
      "epoch: 2 step: 1598, loss is 0.0952402725815773\n",
      "epoch: 2 step: 1599, loss is 0.02337350882589817\n",
      "epoch: 2 step: 1600, loss is 0.06768207997083664\n",
      "epoch: 2 step: 1601, loss is 0.1478632688522339\n",
      "epoch: 2 step: 1602, loss is 0.005468376446515322\n",
      "epoch: 2 step: 1603, loss is 0.2112458199262619\n",
      "epoch: 2 step: 1604, loss is 0.03372233361005783\n",
      "epoch: 2 step: 1605, loss is 0.018297599628567696\n",
      "epoch: 2 step: 1606, loss is 0.09231189638376236\n",
      "epoch: 2 step: 1607, loss is 0.06618674844503403\n",
      "epoch: 2 step: 1608, loss is 0.11945392191410065\n",
      "epoch: 2 step: 1609, loss is 0.15592068433761597\n",
      "epoch: 2 step: 1610, loss is 0.017043303698301315\n",
      "epoch: 2 step: 1611, loss is 0.03218555450439453\n",
      "epoch: 2 step: 1612, loss is 0.022099623456597328\n",
      "epoch: 2 step: 1613, loss is 0.03723272308707237\n",
      "epoch: 2 step: 1614, loss is 0.03692205995321274\n",
      "epoch: 2 step: 1615, loss is 0.01437537744641304\n",
      "epoch: 2 step: 1616, loss is 0.19862565398216248\n",
      "epoch: 2 step: 1617, loss is 0.004275091458112001\n",
      "epoch: 2 step: 1618, loss is 0.03720133379101753\n",
      "epoch: 2 step: 1619, loss is 0.202765554189682\n",
      "epoch: 2 step: 1620, loss is 0.16071374714374542\n",
      "epoch: 2 step: 1621, loss is 0.004385084845125675\n",
      "epoch: 2 step: 1622, loss is 0.005849552806466818\n",
      "epoch: 2 step: 1623, loss is 0.024430537596344948\n",
      "epoch: 2 step: 1624, loss is 0.0010951361618936062\n",
      "epoch: 2 step: 1625, loss is 0.09540329873561859\n",
      "epoch: 2 step: 1626, loss is 0.1990302950143814\n",
      "epoch: 2 step: 1627, loss is 0.03231992945075035\n",
      "epoch: 2 step: 1628, loss is 0.005202862899750471\n",
      "epoch: 2 step: 1629, loss is 0.061637405306100845\n",
      "epoch: 2 step: 1630, loss is 0.004380376078188419\n",
      "epoch: 2 step: 1631, loss is 0.043105483055114746\n",
      "epoch: 2 step: 1632, loss is 0.07759241759777069\n",
      "epoch: 2 step: 1633, loss is 0.10677257925271988\n",
      "epoch: 2 step: 1634, loss is 0.004641070030629635\n",
      "epoch: 2 step: 1635, loss is 0.009328389540314674\n",
      "epoch: 2 step: 1636, loss is 0.12366683781147003\n",
      "epoch: 2 step: 1637, loss is 0.011632542125880718\n",
      "epoch: 2 step: 1638, loss is 0.020149869844317436\n",
      "epoch: 2 step: 1639, loss is 0.010092416778206825\n",
      "epoch: 2 step: 1640, loss is 0.022894032299518585\n",
      "epoch: 2 step: 1641, loss is 0.04618050530552864\n",
      "epoch: 2 step: 1642, loss is 0.06271782517433167\n",
      "epoch: 2 step: 1643, loss is 0.08647198975086212\n",
      "epoch: 2 step: 1644, loss is 0.041939400136470795\n",
      "epoch: 2 step: 1645, loss is 0.004316866397857666\n",
      "epoch: 2 step: 1646, loss is 0.014497805386781693\n",
      "epoch: 2 step: 1647, loss is 0.0036560052540153265\n",
      "epoch: 2 step: 1648, loss is 0.03350380063056946\n",
      "epoch: 2 step: 1649, loss is 0.0631776675581932\n",
      "epoch: 2 step: 1650, loss is 0.09029890596866608\n",
      "epoch: 2 step: 1651, loss is 0.023912664502859116\n",
      "epoch: 2 step: 1652, loss is 0.08112024515867233\n",
      "epoch: 2 step: 1653, loss is 0.2308274358510971\n",
      "epoch: 2 step: 1654, loss is 0.05767001211643219\n",
      "epoch: 2 step: 1655, loss is 0.06845416873693466\n",
      "epoch: 2 step: 1656, loss is 0.09000971168279648\n",
      "epoch: 2 step: 1657, loss is 0.026467230170965195\n",
      "epoch: 2 step: 1658, loss is 0.003926839679479599\n",
      "epoch: 2 step: 1659, loss is 0.0018389236647635698\n",
      "epoch: 2 step: 1660, loss is 0.003475574776530266\n",
      "epoch: 2 step: 1661, loss is 0.004044810775667429\n",
      "epoch: 2 step: 1662, loss is 0.030070746317505836\n",
      "epoch: 2 step: 1663, loss is 0.010205025784671307\n",
      "epoch: 2 step: 1664, loss is 0.05566911771893501\n",
      "epoch: 2 step: 1665, loss is 0.021899787709116936\n",
      "epoch: 2 step: 1666, loss is 0.15928179025650024\n",
      "epoch: 2 step: 1667, loss is 0.027322672307491302\n",
      "epoch: 2 step: 1668, loss is 0.21644511818885803\n",
      "epoch: 2 step: 1669, loss is 0.0018077633576467633\n",
      "epoch: 2 step: 1670, loss is 0.1932993084192276\n",
      "epoch: 2 step: 1671, loss is 0.14630897343158722\n",
      "epoch: 2 step: 1672, loss is 0.15553931891918182\n",
      "epoch: 2 step: 1673, loss is 0.010208227671682835\n",
      "epoch: 2 step: 1674, loss is 0.020838424563407898\n",
      "epoch: 2 step: 1675, loss is 0.007271535694599152\n",
      "epoch: 2 step: 1676, loss is 0.017050726339221\n",
      "epoch: 2 step: 1677, loss is 0.05034097284078598\n",
      "epoch: 2 step: 1678, loss is 0.013412756845355034\n",
      "epoch: 2 step: 1679, loss is 0.020802248269319534\n",
      "epoch: 2 step: 1680, loss is 0.004321931395679712\n",
      "epoch: 2 step: 1681, loss is 0.05461516231298447\n",
      "epoch: 2 step: 1682, loss is 0.04033145308494568\n",
      "epoch: 2 step: 1683, loss is 0.005484102293848991\n",
      "epoch: 2 step: 1684, loss is 0.029474744573235512\n",
      "epoch: 2 step: 1685, loss is 0.03146229684352875\n",
      "epoch: 2 step: 1686, loss is 0.012249527499079704\n",
      "epoch: 2 step: 1687, loss is 0.008952825330197811\n",
      "epoch: 2 step: 1688, loss is 0.06191321834921837\n",
      "epoch: 2 step: 1689, loss is 0.25400015711784363\n",
      "epoch: 2 step: 1690, loss is 0.1543048769235611\n",
      "epoch: 2 step: 1691, loss is 0.029423648491501808\n",
      "epoch: 2 step: 1692, loss is 0.1492806077003479\n",
      "epoch: 2 step: 1693, loss is 0.006918859668076038\n",
      "epoch: 2 step: 1694, loss is 0.0021138323936611414\n",
      "epoch: 2 step: 1695, loss is 0.003414511913433671\n",
      "epoch: 2 step: 1696, loss is 0.08178789913654327\n",
      "epoch: 2 step: 1697, loss is 0.023304834961891174\n",
      "epoch: 2 step: 1698, loss is 0.14605754613876343\n",
      "epoch: 2 step: 1699, loss is 0.10205116868019104\n",
      "epoch: 2 step: 1700, loss is 0.004339320585131645\n",
      "epoch: 2 step: 1701, loss is 0.004986519459635019\n",
      "epoch: 2 step: 1702, loss is 0.011427266523241997\n",
      "epoch: 2 step: 1703, loss is 0.015034142881631851\n",
      "epoch: 2 step: 1704, loss is 0.0017393874004483223\n",
      "epoch: 2 step: 1705, loss is 0.01064501516520977\n",
      "epoch: 2 step: 1706, loss is 0.028969382867217064\n",
      "epoch: 2 step: 1707, loss is 0.004273298662155867\n",
      "epoch: 2 step: 1708, loss is 0.004899593070149422\n",
      "epoch: 2 step: 1709, loss is 0.028641700744628906\n",
      "epoch: 2 step: 1710, loss is 0.06977710127830505\n",
      "epoch: 2 step: 1711, loss is 0.13483896851539612\n",
      "epoch: 2 step: 1712, loss is 0.07963045686483383\n",
      "epoch: 2 step: 1713, loss is 0.0641992911696434\n",
      "epoch: 2 step: 1714, loss is 0.2998400330543518\n",
      "epoch: 2 step: 1715, loss is 0.13630741834640503\n",
      "epoch: 2 step: 1716, loss is 0.22435729205608368\n",
      "epoch: 2 step: 1717, loss is 0.008111548610031605\n",
      "epoch: 2 step: 1718, loss is 0.003910624887794256\n",
      "epoch: 2 step: 1719, loss is 0.058130331337451935\n",
      "epoch: 2 step: 1720, loss is 0.08073006570339203\n",
      "epoch: 2 step: 1721, loss is 0.010458194650709629\n",
      "epoch: 2 step: 1722, loss is 0.03298308327794075\n",
      "epoch: 2 step: 1723, loss is 0.026608744636178017\n",
      "epoch: 2 step: 1724, loss is 0.008562005124986172\n",
      "epoch: 2 step: 1725, loss is 0.19689823687076569\n",
      "epoch: 2 step: 1726, loss is 0.1643696278333664\n",
      "epoch: 2 step: 1727, loss is 0.10065139085054398\n",
      "epoch: 2 step: 1728, loss is 0.08578301221132278\n",
      "epoch: 2 step: 1729, loss is 0.1476345956325531\n",
      "epoch: 2 step: 1730, loss is 0.03260481730103493\n",
      "epoch: 2 step: 1731, loss is 0.013200164772570133\n",
      "epoch: 2 step: 1732, loss is 0.0019470711704343557\n",
      "epoch: 2 step: 1733, loss is 0.0030507943592965603\n",
      "epoch: 2 step: 1734, loss is 0.13832788169384003\n",
      "epoch: 2 step: 1735, loss is 0.11780336499214172\n",
      "epoch: 2 step: 1736, loss is 0.05368547886610031\n",
      "epoch: 2 step: 1737, loss is 0.17287148535251617\n",
      "epoch: 2 step: 1738, loss is 0.11441348493099213\n",
      "epoch: 2 step: 1739, loss is 0.03998139500617981\n",
      "epoch: 2 step: 1740, loss is 0.34648028016090393\n",
      "epoch: 2 step: 1741, loss is 0.009635314345359802\n",
      "epoch: 2 step: 1742, loss is 0.013313278555870056\n",
      "epoch: 2 step: 1743, loss is 0.003830491565167904\n",
      "epoch: 2 step: 1744, loss is 0.015843218192458153\n",
      "epoch: 2 step: 1745, loss is 0.021374888718128204\n",
      "epoch: 2 step: 1746, loss is 0.12873435020446777\n",
      "epoch: 2 step: 1747, loss is 0.04617668688297272\n",
      "epoch: 2 step: 1748, loss is 0.0020369472913444042\n",
      "epoch: 2 step: 1749, loss is 0.25286784768104553\n",
      "epoch: 2 step: 1750, loss is 0.14871282875537872\n",
      "epoch: 2 step: 1751, loss is 0.1172177791595459\n",
      "epoch: 2 step: 1752, loss is 0.11686519533395767\n",
      "epoch: 2 step: 1753, loss is 0.05224724858999252\n",
      "epoch: 2 step: 1754, loss is 0.01961449161171913\n",
      "epoch: 2 step: 1755, loss is 0.09551164507865906\n",
      "epoch: 2 step: 1756, loss is 0.001430876087397337\n",
      "epoch: 2 step: 1757, loss is 0.06695921719074249\n",
      "epoch: 2 step: 1758, loss is 0.04004181921482086\n",
      "epoch: 2 step: 1759, loss is 0.016507981345057487\n",
      "epoch: 2 step: 1760, loss is 0.007374720182269812\n",
      "epoch: 2 step: 1761, loss is 0.005346033722162247\n",
      "epoch: 2 step: 1762, loss is 0.4544970691204071\n",
      "epoch: 2 step: 1763, loss is 0.04026214778423309\n",
      "epoch: 2 step: 1764, loss is 0.05484820529818535\n",
      "epoch: 2 step: 1765, loss is 0.02149277739226818\n",
      "epoch: 2 step: 1766, loss is 0.023447420448064804\n",
      "epoch: 2 step: 1767, loss is 0.050873804837465286\n",
      "epoch: 2 step: 1768, loss is 0.09369039535522461\n",
      "epoch: 2 step: 1769, loss is 0.07480455935001373\n",
      "epoch: 2 step: 1770, loss is 0.08081520348787308\n",
      "epoch: 2 step: 1771, loss is 0.25759515166282654\n",
      "epoch: 2 step: 1772, loss is 0.2766280174255371\n",
      "epoch: 2 step: 1773, loss is 0.005164611618965864\n",
      "epoch: 2 step: 1774, loss is 0.02309424802660942\n",
      "epoch: 2 step: 1775, loss is 0.01641615852713585\n",
      "epoch: 2 step: 1776, loss is 0.038678571581840515\n",
      "epoch: 2 step: 1777, loss is 0.030273066833615303\n",
      "epoch: 2 step: 1778, loss is 0.014589314348995686\n",
      "epoch: 2 step: 1779, loss is 0.054206483066082\n",
      "epoch: 2 step: 1780, loss is 0.1824236363172531\n",
      "epoch: 2 step: 1781, loss is 0.013326968066394329\n",
      "epoch: 2 step: 1782, loss is 0.11850400269031525\n",
      "epoch: 2 step: 1783, loss is 0.11893757432699203\n",
      "epoch: 2 step: 1784, loss is 0.2931077778339386\n",
      "epoch: 2 step: 1785, loss is 0.10292813926935196\n",
      "epoch: 2 step: 1786, loss is 0.004002784378826618\n",
      "epoch: 2 step: 1787, loss is 0.006608195137232542\n",
      "epoch: 2 step: 1788, loss is 0.18610318005084991\n",
      "epoch: 2 step: 1789, loss is 0.1066625714302063\n",
      "epoch: 2 step: 1790, loss is 0.23502516746520996\n",
      "epoch: 2 step: 1791, loss is 0.1957016885280609\n",
      "epoch: 2 step: 1792, loss is 0.23429900407791138\n",
      "epoch: 2 step: 1793, loss is 0.02353193797171116\n",
      "epoch: 2 step: 1794, loss is 0.05910898372530937\n",
      "epoch: 2 step: 1795, loss is 0.13594651222229004\n",
      "epoch: 2 step: 1796, loss is 0.0494876466691494\n",
      "epoch: 2 step: 1797, loss is 0.05734441429376602\n",
      "epoch: 2 step: 1798, loss is 0.13432657718658447\n",
      "epoch: 2 step: 1799, loss is 0.043769754469394684\n",
      "epoch: 2 step: 1800, loss is 0.05073367431759834\n",
      "epoch: 2 step: 1801, loss is 0.02403351664543152\n",
      "epoch: 2 step: 1802, loss is 0.03908772021532059\n",
      "epoch: 2 step: 1803, loss is 0.04555811360478401\n",
      "epoch: 2 step: 1804, loss is 0.10237962752580643\n",
      "epoch: 2 step: 1805, loss is 0.09837669879198074\n",
      "epoch: 2 step: 1806, loss is 0.04078088328242302\n",
      "epoch: 2 step: 1807, loss is 0.013479565270245075\n",
      "epoch: 2 step: 1808, loss is 0.07787672430276871\n",
      "epoch: 2 step: 1809, loss is 0.013904411345720291\n",
      "epoch: 2 step: 1810, loss is 0.025709591805934906\n",
      "epoch: 2 step: 1811, loss is 0.13857318460941315\n",
      "epoch: 2 step: 1812, loss is 0.013051888905465603\n",
      "epoch: 2 step: 1813, loss is 0.05612063780426979\n",
      "epoch: 2 step: 1814, loss is 0.02175433188676834\n",
      "epoch: 2 step: 1815, loss is 0.013197045773267746\n",
      "epoch: 2 step: 1816, loss is 0.08907771110534668\n",
      "epoch: 2 step: 1817, loss is 0.006545806769281626\n",
      "epoch: 2 step: 1818, loss is 0.016705261543393135\n",
      "epoch: 2 step: 1819, loss is 0.014750679954886436\n",
      "epoch: 2 step: 1820, loss is 0.011531057767570019\n",
      "epoch: 2 step: 1821, loss is 0.008494650945067406\n",
      "epoch: 2 step: 1822, loss is 0.03009926900267601\n",
      "epoch: 2 step: 1823, loss is 0.055774398148059845\n",
      "epoch: 2 step: 1824, loss is 0.04566020146012306\n",
      "epoch: 2 step: 1825, loss is 0.08066852390766144\n",
      "epoch: 2 step: 1826, loss is 0.035349998623132706\n",
      "epoch: 2 step: 1827, loss is 0.09302647411823273\n",
      "epoch: 2 step: 1828, loss is 0.020493872463703156\n",
      "epoch: 2 step: 1829, loss is 0.05417335033416748\n",
      "epoch: 2 step: 1830, loss is 0.01821773126721382\n",
      "epoch: 2 step: 1831, loss is 0.012419842183589935\n",
      "epoch: 2 step: 1832, loss is 0.4199308454990387\n",
      "epoch: 2 step: 1833, loss is 0.0697455108165741\n",
      "epoch: 2 step: 1834, loss is 0.039749253541231155\n",
      "epoch: 2 step: 1835, loss is 0.005147347692400217\n",
      "epoch: 2 step: 1836, loss is 0.009745948016643524\n",
      "epoch: 2 step: 1837, loss is 0.012601024471223354\n",
      "epoch: 2 step: 1838, loss is 0.08521636575460434\n",
      "epoch: 2 step: 1839, loss is 0.013844928704202175\n",
      "epoch: 2 step: 1840, loss is 0.015402561984956264\n",
      "epoch: 2 step: 1841, loss is 0.2482929527759552\n",
      "epoch: 2 step: 1842, loss is 0.032096270471811295\n",
      "epoch: 2 step: 1843, loss is 0.010700556449592113\n",
      "epoch: 2 step: 1844, loss is 0.006674652453511953\n",
      "epoch: 2 step: 1845, loss is 0.03249893710017204\n",
      "epoch: 2 step: 1846, loss is 0.005811154842376709\n",
      "epoch: 2 step: 1847, loss is 0.04220282658934593\n",
      "epoch: 2 step: 1848, loss is 0.04099571704864502\n",
      "epoch: 2 step: 1849, loss is 0.16775596141815186\n",
      "epoch: 2 step: 1850, loss is 0.006579336244612932\n",
      "epoch: 2 step: 1851, loss is 0.004444332327693701\n",
      "epoch: 2 step: 1852, loss is 0.010200046002864838\n",
      "epoch: 2 step: 1853, loss is 0.09592987596988678\n",
      "epoch: 2 step: 1854, loss is 0.10070563852787018\n",
      "epoch: 2 step: 1855, loss is 0.022588679566979408\n",
      "epoch: 2 step: 1856, loss is 0.05768624693155289\n",
      "epoch: 2 step: 1857, loss is 0.0018492364324629307\n",
      "epoch: 2 step: 1858, loss is 0.023473404347896576\n",
      "epoch: 2 step: 1859, loss is 0.0853162333369255\n",
      "epoch: 2 step: 1860, loss is 0.00193381670396775\n",
      "epoch: 2 step: 1861, loss is 0.010854029096662998\n",
      "epoch: 2 step: 1862, loss is 0.053764719516038895\n",
      "epoch: 2 step: 1863, loss is 0.041833098977804184\n",
      "epoch: 2 step: 1864, loss is 0.1338741034269333\n",
      "epoch: 2 step: 1865, loss is 0.020869456231594086\n",
      "epoch: 2 step: 1866, loss is 0.019252844154834747\n",
      "epoch: 2 step: 1867, loss is 0.00516161322593689\n",
      "epoch: 2 step: 1868, loss is 0.0017271364340558648\n",
      "epoch: 2 step: 1869, loss is 0.005362313240766525\n",
      "epoch: 2 step: 1870, loss is 0.001540183206088841\n",
      "epoch: 2 step: 1871, loss is 0.10012821108102798\n",
      "epoch: 2 step: 1872, loss is 0.013646782375872135\n",
      "epoch: 2 step: 1873, loss is 0.02504517324268818\n",
      "epoch: 2 step: 1874, loss is 0.10389972478151321\n",
      "epoch: 2 step: 1875, loss is 0.0008255192078649998\n",
      "epoch: 3 step: 1, loss is 0.06241178512573242\n",
      "epoch: 3 step: 2, loss is 0.003768792375922203\n",
      "epoch: 3 step: 3, loss is 0.020893579348921776\n",
      "epoch: 3 step: 4, loss is 0.0005419516237452626\n",
      "epoch: 3 step: 5, loss is 0.25377705693244934\n",
      "epoch: 3 step: 6, loss is 0.019594084471464157\n",
      "epoch: 3 step: 7, loss is 0.008750197477638721\n",
      "epoch: 3 step: 8, loss is 0.006718556862324476\n",
      "epoch: 3 step: 9, loss is 0.00864664651453495\n",
      "epoch: 3 step: 10, loss is 0.11700812727212906\n",
      "epoch: 3 step: 11, loss is 0.06696323305368423\n",
      "epoch: 3 step: 12, loss is 0.08573298901319504\n",
      "epoch: 3 step: 13, loss is 0.29856210947036743\n",
      "epoch: 3 step: 14, loss is 0.26819342374801636\n",
      "epoch: 3 step: 15, loss is 0.02683354914188385\n",
      "epoch: 3 step: 16, loss is 0.05853580683469772\n",
      "epoch: 3 step: 17, loss is 0.0122850202023983\n",
      "epoch: 3 step: 18, loss is 0.13691693544387817\n",
      "epoch: 3 step: 19, loss is 0.03854463994503021\n",
      "epoch: 3 step: 20, loss is 0.006480603478848934\n",
      "epoch: 3 step: 21, loss is 0.04395456984639168\n",
      "epoch: 3 step: 22, loss is 0.05017764866352081\n",
      "epoch: 3 step: 23, loss is 0.03134084492921829\n",
      "epoch: 3 step: 24, loss is 0.04181770980358124\n",
      "epoch: 3 step: 25, loss is 0.0915587991476059\n",
      "epoch: 3 step: 26, loss is 0.03789067268371582\n",
      "epoch: 3 step: 27, loss is 0.006082274951040745\n",
      "epoch: 3 step: 28, loss is 0.07917209714651108\n",
      "epoch: 3 step: 29, loss is 0.01918390579521656\n",
      "epoch: 3 step: 30, loss is 0.01973770186305046\n",
      "epoch: 3 step: 31, loss is 0.062408071011304855\n",
      "epoch: 3 step: 32, loss is 0.0028988418634980917\n",
      "epoch: 3 step: 33, loss is 0.21936875581741333\n",
      "epoch: 3 step: 34, loss is 0.06534041464328766\n",
      "epoch: 3 step: 35, loss is 0.3161206543445587\n",
      "epoch: 3 step: 36, loss is 0.004012714605778456\n",
      "epoch: 3 step: 37, loss is 0.14865520596504211\n",
      "epoch: 3 step: 38, loss is 0.04762181267142296\n",
      "epoch: 3 step: 39, loss is 0.0022944353986531496\n",
      "epoch: 3 step: 40, loss is 0.12435347586870193\n",
      "epoch: 3 step: 41, loss is 0.0020909470040351152\n",
      "epoch: 3 step: 42, loss is 0.2530808746814728\n",
      "epoch: 3 step: 43, loss is 0.04582574963569641\n",
      "epoch: 3 step: 44, loss is 0.0027492588851600885\n",
      "epoch: 3 step: 45, loss is 0.021172625944018364\n",
      "epoch: 3 step: 46, loss is 0.05117833614349365\n",
      "epoch: 3 step: 47, loss is 0.022354671731591225\n",
      "epoch: 3 step: 48, loss is 0.027260910719633102\n",
      "epoch: 3 step: 49, loss is 0.051432330161333084\n",
      "epoch: 3 step: 50, loss is 0.04520973563194275\n",
      "epoch: 3 step: 51, loss is 0.0124721210449934\n",
      "epoch: 3 step: 52, loss is 0.011183050461113453\n",
      "epoch: 3 step: 53, loss is 0.17399252951145172\n",
      "epoch: 3 step: 54, loss is 0.03324035182595253\n",
      "epoch: 3 step: 55, loss is 0.00937550887465477\n",
      "epoch: 3 step: 56, loss is 0.005501077510416508\n",
      "epoch: 3 step: 57, loss is 0.006655267905443907\n",
      "epoch: 3 step: 58, loss is 0.026361705735325813\n",
      "epoch: 3 step: 59, loss is 0.08159890025854111\n",
      "epoch: 3 step: 60, loss is 0.047183506190776825\n",
      "epoch: 3 step: 61, loss is 0.0907549187541008\n",
      "epoch: 3 step: 62, loss is 0.035514574497938156\n",
      "epoch: 3 step: 63, loss is 0.016411522403359413\n",
      "epoch: 3 step: 64, loss is 0.028493791818618774\n",
      "epoch: 3 step: 65, loss is 0.037701476365327835\n",
      "epoch: 3 step: 66, loss is 0.006968523841351271\n",
      "epoch: 3 step: 67, loss is 0.14260143041610718\n",
      "epoch: 3 step: 68, loss is 0.013698330149054527\n",
      "epoch: 3 step: 69, loss is 0.01081144530326128\n",
      "epoch: 3 step: 70, loss is 0.05855659022927284\n",
      "epoch: 3 step: 71, loss is 0.006189803127199411\n",
      "epoch: 3 step: 72, loss is 0.016582177951931953\n",
      "epoch: 3 step: 73, loss is 0.10488031059503555\n",
      "epoch: 3 step: 74, loss is 0.012409543618559837\n",
      "epoch: 3 step: 75, loss is 0.0008192179375328124\n",
      "epoch: 3 step: 76, loss is 0.00976601243019104\n",
      "epoch: 3 step: 77, loss is 0.019657745957374573\n",
      "epoch: 3 step: 78, loss is 0.06883452087640762\n",
      "epoch: 3 step: 79, loss is 0.059516776353120804\n",
      "epoch: 3 step: 80, loss is 0.17636141180992126\n",
      "epoch: 3 step: 81, loss is 0.10917109251022339\n",
      "epoch: 3 step: 82, loss is 0.010326048359274864\n",
      "epoch: 3 step: 83, loss is 0.017839476466178894\n",
      "epoch: 3 step: 84, loss is 0.03155149891972542\n",
      "epoch: 3 step: 85, loss is 0.017968427389860153\n",
      "epoch: 3 step: 86, loss is 0.06190282478928566\n",
      "epoch: 3 step: 87, loss is 0.015047825872898102\n",
      "epoch: 3 step: 88, loss is 0.003493766998872161\n",
      "epoch: 3 step: 89, loss is 0.024587482213974\n",
      "epoch: 3 step: 90, loss is 0.027304479852318764\n",
      "epoch: 3 step: 91, loss is 0.10978072136640549\n",
      "epoch: 3 step: 92, loss is 0.026273196563124657\n",
      "epoch: 3 step: 93, loss is 0.019984310492873192\n",
      "epoch: 3 step: 94, loss is 0.010059035383164883\n",
      "epoch: 3 step: 95, loss is 0.01938793808221817\n",
      "epoch: 3 step: 96, loss is 0.007378768641501665\n",
      "epoch: 3 step: 97, loss is 0.043519277125597\n",
      "epoch: 3 step: 98, loss is 0.020987078547477722\n",
      "epoch: 3 step: 99, loss is 0.012036910280585289\n",
      "epoch: 3 step: 100, loss is 0.001554364338517189\n",
      "epoch: 3 step: 101, loss is 0.0006215911125764251\n",
      "epoch: 3 step: 102, loss is 0.004752721171826124\n",
      "epoch: 3 step: 103, loss is 0.017769288271665573\n",
      "epoch: 3 step: 104, loss is 0.002073135459795594\n",
      "epoch: 3 step: 105, loss is 0.04229061305522919\n",
      "epoch: 3 step: 106, loss is 0.01375243067741394\n",
      "epoch: 3 step: 107, loss is 0.22884276509284973\n",
      "epoch: 3 step: 108, loss is 0.06016286835074425\n",
      "epoch: 3 step: 109, loss is 0.004904014058411121\n",
      "epoch: 3 step: 110, loss is 0.006362962536513805\n",
      "epoch: 3 step: 111, loss is 0.01019945740699768\n",
      "epoch: 3 step: 112, loss is 0.001669157762080431\n",
      "epoch: 3 step: 113, loss is 0.07965079694986343\n",
      "epoch: 3 step: 114, loss is 0.04512978717684746\n",
      "epoch: 3 step: 115, loss is 0.1434764713048935\n",
      "epoch: 3 step: 116, loss is 0.09872608631849289\n",
      "epoch: 3 step: 117, loss is 0.04084888473153114\n",
      "epoch: 3 step: 118, loss is 0.011709192767739296\n",
      "epoch: 3 step: 119, loss is 0.11564832180738449\n",
      "epoch: 3 step: 120, loss is 0.006958826445043087\n",
      "epoch: 3 step: 121, loss is 0.026103001087903976\n",
      "epoch: 3 step: 122, loss is 0.036002546548843384\n",
      "epoch: 3 step: 123, loss is 0.09931343793869019\n",
      "epoch: 3 step: 124, loss is 0.12320108711719513\n",
      "epoch: 3 step: 125, loss is 0.01061903964728117\n",
      "epoch: 3 step: 126, loss is 0.002206337172538042\n",
      "epoch: 3 step: 127, loss is 0.0028375242836773396\n",
      "epoch: 3 step: 128, loss is 0.058128949254751205\n",
      "epoch: 3 step: 129, loss is 0.002265401417389512\n",
      "epoch: 3 step: 130, loss is 0.03849535062909126\n",
      "epoch: 3 step: 131, loss is 0.14605650305747986\n",
      "epoch: 3 step: 132, loss is 0.008765850216150284\n",
      "epoch: 3 step: 133, loss is 0.040097832679748535\n",
      "epoch: 3 step: 134, loss is 0.01162958238273859\n",
      "epoch: 3 step: 135, loss is 0.1587304025888443\n",
      "epoch: 3 step: 136, loss is 0.03188248351216316\n",
      "epoch: 3 step: 137, loss is 0.30417555570602417\n",
      "epoch: 3 step: 138, loss is 0.01105355191975832\n",
      "epoch: 3 step: 139, loss is 0.024044333025813103\n",
      "epoch: 3 step: 140, loss is 0.05278322473168373\n",
      "epoch: 3 step: 141, loss is 0.10336421430110931\n",
      "epoch: 3 step: 142, loss is 0.02127312310039997\n",
      "epoch: 3 step: 143, loss is 0.0033994580153375864\n",
      "epoch: 3 step: 144, loss is 0.17815957963466644\n",
      "epoch: 3 step: 145, loss is 0.080324687063694\n",
      "epoch: 3 step: 146, loss is 0.14329305291175842\n",
      "epoch: 3 step: 147, loss is 0.28701335191726685\n",
      "epoch: 3 step: 148, loss is 0.005542401690036058\n",
      "epoch: 3 step: 149, loss is 0.17052479088306427\n",
      "epoch: 3 step: 150, loss is 0.03916286304593086\n",
      "epoch: 3 step: 151, loss is 0.006740266922861338\n",
      "epoch: 3 step: 152, loss is 0.02329161949455738\n",
      "epoch: 3 step: 153, loss is 0.02535521425306797\n",
      "epoch: 3 step: 154, loss is 0.008997447788715363\n",
      "epoch: 3 step: 155, loss is 0.034659501165151596\n",
      "epoch: 3 step: 156, loss is 0.015629496425390244\n",
      "epoch: 3 step: 157, loss is 0.013487349264323711\n",
      "epoch: 3 step: 158, loss is 0.018262119963765144\n",
      "epoch: 3 step: 159, loss is 0.005747935734689236\n",
      "epoch: 3 step: 160, loss is 0.059944361448287964\n",
      "epoch: 3 step: 161, loss is 0.043725185096263885\n",
      "epoch: 3 step: 162, loss is 0.19626757502555847\n",
      "epoch: 3 step: 163, loss is 0.00866310391575098\n",
      "epoch: 3 step: 164, loss is 0.019724242389202118\n",
      "epoch: 3 step: 165, loss is 0.003543238388374448\n",
      "epoch: 3 step: 166, loss is 0.02030564844608307\n",
      "epoch: 3 step: 167, loss is 0.041451845318078995\n",
      "epoch: 3 step: 168, loss is 0.01122145727276802\n",
      "epoch: 3 step: 169, loss is 0.004102557897567749\n",
      "epoch: 3 step: 170, loss is 0.07580610364675522\n",
      "epoch: 3 step: 171, loss is 0.04719283804297447\n",
      "epoch: 3 step: 172, loss is 0.03697732090950012\n",
      "epoch: 3 step: 173, loss is 0.01634511724114418\n",
      "epoch: 3 step: 174, loss is 0.01803199015557766\n",
      "epoch: 3 step: 175, loss is 0.04707960784435272\n",
      "epoch: 3 step: 176, loss is 0.0694768875837326\n",
      "epoch: 3 step: 177, loss is 0.009479879401624203\n",
      "epoch: 3 step: 178, loss is 0.023397833108901978\n",
      "epoch: 3 step: 179, loss is 0.031162720173597336\n",
      "epoch: 3 step: 180, loss is 0.005862316582351923\n",
      "epoch: 3 step: 181, loss is 0.004365566652268171\n",
      "epoch: 3 step: 182, loss is 0.18213647603988647\n",
      "epoch: 3 step: 183, loss is 0.005219447426497936\n",
      "epoch: 3 step: 184, loss is 0.0014248252846300602\n",
      "epoch: 3 step: 185, loss is 0.018436426296830177\n",
      "epoch: 3 step: 186, loss is 0.19028516113758087\n",
      "epoch: 3 step: 187, loss is 0.006746785715222359\n",
      "epoch: 3 step: 188, loss is 0.010723904706537724\n",
      "epoch: 3 step: 189, loss is 0.004176972899585962\n",
      "epoch: 3 step: 190, loss is 0.0006169606349430978\n",
      "epoch: 3 step: 191, loss is 0.0005597493145614862\n",
      "epoch: 3 step: 192, loss is 0.04784385859966278\n",
      "epoch: 3 step: 193, loss is 0.018477706238627434\n",
      "epoch: 3 step: 194, loss is 0.1770908683538437\n",
      "epoch: 3 step: 195, loss is 0.07784966379404068\n",
      "epoch: 3 step: 196, loss is 0.014637577347457409\n",
      "epoch: 3 step: 197, loss is 0.04536492004990578\n",
      "epoch: 3 step: 198, loss is 0.0463295616209507\n",
      "epoch: 3 step: 199, loss is 0.11912316828966141\n",
      "epoch: 3 step: 200, loss is 0.07459266483783722\n",
      "epoch: 3 step: 201, loss is 0.03477289155125618\n",
      "epoch: 3 step: 202, loss is 0.00874642189592123\n",
      "epoch: 3 step: 203, loss is 0.12075777351856232\n",
      "epoch: 3 step: 204, loss is 0.00063793093431741\n",
      "epoch: 3 step: 205, loss is 0.007418172433972359\n",
      "epoch: 3 step: 206, loss is 0.15458504855632782\n",
      "epoch: 3 step: 207, loss is 0.13695769011974335\n",
      "epoch: 3 step: 208, loss is 0.0018985950155183673\n",
      "epoch: 3 step: 209, loss is 0.17067766189575195\n",
      "epoch: 3 step: 210, loss is 0.04882470890879631\n",
      "epoch: 3 step: 211, loss is 0.02859775722026825\n",
      "epoch: 3 step: 212, loss is 0.03226417675614357\n",
      "epoch: 3 step: 213, loss is 0.02691589668393135\n",
      "epoch: 3 step: 214, loss is 0.06743647903203964\n",
      "epoch: 3 step: 215, loss is 0.006630824878811836\n",
      "epoch: 3 step: 216, loss is 0.009811410680413246\n",
      "epoch: 3 step: 217, loss is 0.001895281602628529\n",
      "epoch: 3 step: 218, loss is 0.10881643742322922\n",
      "epoch: 3 step: 219, loss is 0.26304227113723755\n",
      "epoch: 3 step: 220, loss is 0.011635137721896172\n",
      "epoch: 3 step: 221, loss is 0.017866384238004684\n",
      "epoch: 3 step: 222, loss is 0.05684160068631172\n",
      "epoch: 3 step: 223, loss is 0.12660330533981323\n",
      "epoch: 3 step: 224, loss is 0.009061004035174847\n",
      "epoch: 3 step: 225, loss is 0.01640346460044384\n",
      "epoch: 3 step: 226, loss is 0.016377849504351616\n",
      "epoch: 3 step: 227, loss is 0.06781946122646332\n",
      "epoch: 3 step: 228, loss is 0.008029879070818424\n",
      "epoch: 3 step: 229, loss is 0.06320036947727203\n",
      "epoch: 3 step: 230, loss is 0.01746455579996109\n",
      "epoch: 3 step: 231, loss is 0.0011702855117619038\n",
      "epoch: 3 step: 232, loss is 0.12784531712532043\n",
      "epoch: 3 step: 233, loss is 0.03835549205541611\n",
      "epoch: 3 step: 234, loss is 0.002966838190332055\n",
      "epoch: 3 step: 235, loss is 0.21258141100406647\n",
      "epoch: 3 step: 236, loss is 0.08853297680616379\n",
      "epoch: 3 step: 237, loss is 0.028980134055018425\n",
      "epoch: 3 step: 238, loss is 0.0041505382396280766\n",
      "epoch: 3 step: 239, loss is 0.01760895550251007\n",
      "epoch: 3 step: 240, loss is 0.008542286232113838\n",
      "epoch: 3 step: 241, loss is 0.003147534327581525\n",
      "epoch: 3 step: 242, loss is 0.014317099936306477\n",
      "epoch: 3 step: 243, loss is 0.3342374265193939\n",
      "epoch: 3 step: 244, loss is 0.007274875417351723\n",
      "epoch: 3 step: 245, loss is 0.007776870857924223\n",
      "epoch: 3 step: 246, loss is 0.03206635266542435\n",
      "epoch: 3 step: 247, loss is 0.006077915895730257\n",
      "epoch: 3 step: 248, loss is 0.24688488245010376\n",
      "epoch: 3 step: 249, loss is 0.003775286488234997\n",
      "epoch: 3 step: 250, loss is 0.002665688283741474\n",
      "epoch: 3 step: 251, loss is 0.016392095014452934\n",
      "epoch: 3 step: 252, loss is 0.0033291331492364407\n",
      "epoch: 3 step: 253, loss is 0.040299613028764725\n",
      "epoch: 3 step: 254, loss is 0.009887254796922207\n",
      "epoch: 3 step: 255, loss is 0.08128688484430313\n",
      "epoch: 3 step: 256, loss is 0.06489867717027664\n",
      "epoch: 3 step: 257, loss is 0.059949904680252075\n",
      "epoch: 3 step: 258, loss is 0.0021215002052485943\n",
      "epoch: 3 step: 259, loss is 0.004382521845400333\n",
      "epoch: 3 step: 260, loss is 0.008141189813613892\n",
      "epoch: 3 step: 261, loss is 0.03489169105887413\n",
      "epoch: 3 step: 262, loss is 0.01522182859480381\n",
      "epoch: 3 step: 263, loss is 0.0038628524634987116\n",
      "epoch: 3 step: 264, loss is 0.0023521066177636385\n",
      "epoch: 3 step: 265, loss is 0.10047280043363571\n",
      "epoch: 3 step: 266, loss is 0.11088018864393234\n",
      "epoch: 3 step: 267, loss is 0.02867087908089161\n",
      "epoch: 3 step: 268, loss is 0.03197549283504486\n",
      "epoch: 3 step: 269, loss is 0.0036477104295045137\n",
      "epoch: 3 step: 270, loss is 0.005755157209932804\n",
      "epoch: 3 step: 271, loss is 0.17211101949214935\n",
      "epoch: 3 step: 272, loss is 0.030534423887729645\n",
      "epoch: 3 step: 273, loss is 0.01438884437084198\n",
      "epoch: 3 step: 274, loss is 0.04540214687585831\n",
      "epoch: 3 step: 275, loss is 0.12450107932090759\n",
      "epoch: 3 step: 276, loss is 0.013468548655509949\n",
      "epoch: 3 step: 277, loss is 0.025611942633986473\n",
      "epoch: 3 step: 278, loss is 0.009426389820873737\n",
      "epoch: 3 step: 279, loss is 0.004722224082797766\n",
      "epoch: 3 step: 280, loss is 0.05342647433280945\n",
      "epoch: 3 step: 281, loss is 0.06724622845649719\n",
      "epoch: 3 step: 282, loss is 0.059383779764175415\n",
      "epoch: 3 step: 283, loss is 0.13655830919742584\n",
      "epoch: 3 step: 284, loss is 0.05276062712073326\n",
      "epoch: 3 step: 285, loss is 0.012460819445550442\n",
      "epoch: 3 step: 286, loss is 0.004662910010665655\n",
      "epoch: 3 step: 287, loss is 0.010439314879477024\n",
      "epoch: 3 step: 288, loss is 0.0580143928527832\n",
      "epoch: 3 step: 289, loss is 0.006121882703155279\n",
      "epoch: 3 step: 290, loss is 0.034054916352033615\n",
      "epoch: 3 step: 291, loss is 0.03450864925980568\n",
      "epoch: 3 step: 292, loss is 0.03139478713274002\n",
      "epoch: 3 step: 293, loss is 0.006265878211706877\n",
      "epoch: 3 step: 294, loss is 0.015329230576753616\n",
      "epoch: 3 step: 295, loss is 0.020543402060866356\n",
      "epoch: 3 step: 296, loss is 0.05797974020242691\n",
      "epoch: 3 step: 297, loss is 0.026027631014585495\n",
      "epoch: 3 step: 298, loss is 0.01020724605768919\n",
      "epoch: 3 step: 299, loss is 0.004556690342724323\n",
      "epoch: 3 step: 300, loss is 0.0012425953755155206\n",
      "epoch: 3 step: 301, loss is 0.02217378094792366\n",
      "epoch: 3 step: 302, loss is 0.004653117153793573\n",
      "epoch: 3 step: 303, loss is 0.05593153089284897\n",
      "epoch: 3 step: 304, loss is 0.33002346754074097\n",
      "epoch: 3 step: 305, loss is 0.004077148158103228\n",
      "epoch: 3 step: 306, loss is 0.0012930156663060188\n",
      "epoch: 3 step: 307, loss is 0.0025734989903867245\n",
      "epoch: 3 step: 308, loss is 0.0021452310029417276\n",
      "epoch: 3 step: 309, loss is 0.023471182212233543\n",
      "epoch: 3 step: 310, loss is 0.007180869113653898\n",
      "epoch: 3 step: 311, loss is 0.10327164083719254\n",
      "epoch: 3 step: 312, loss is 0.001790696056559682\n",
      "epoch: 3 step: 313, loss is 0.1568194031715393\n",
      "epoch: 3 step: 314, loss is 0.0005306117236614227\n",
      "epoch: 3 step: 315, loss is 0.06784515082836151\n",
      "epoch: 3 step: 316, loss is 0.31084924936294556\n",
      "epoch: 3 step: 317, loss is 0.0006488297949545085\n",
      "epoch: 3 step: 318, loss is 0.0008699266472831368\n",
      "epoch: 3 step: 319, loss is 0.004555586259812117\n",
      "epoch: 3 step: 320, loss is 0.07778298854827881\n",
      "epoch: 3 step: 321, loss is 0.01549635361880064\n",
      "epoch: 3 step: 322, loss is 0.21485595405101776\n",
      "epoch: 3 step: 323, loss is 0.11993061751127243\n",
      "epoch: 3 step: 324, loss is 0.08636075258255005\n",
      "epoch: 3 step: 325, loss is 0.040359027683734894\n",
      "epoch: 3 step: 326, loss is 0.006282212678343058\n",
      "epoch: 3 step: 327, loss is 0.015214202925562859\n",
      "epoch: 3 step: 328, loss is 0.007101557683199644\n",
      "epoch: 3 step: 329, loss is 0.008012999780476093\n",
      "epoch: 3 step: 330, loss is 0.007742930203676224\n",
      "epoch: 3 step: 331, loss is 0.049309760332107544\n",
      "epoch: 3 step: 332, loss is 0.011935911141335964\n",
      "epoch: 3 step: 333, loss is 0.03418216109275818\n",
      "epoch: 3 step: 334, loss is 0.02078220620751381\n",
      "epoch: 3 step: 335, loss is 0.0019469329854473472\n",
      "epoch: 3 step: 336, loss is 0.025368085131049156\n",
      "epoch: 3 step: 337, loss is 0.009535995312035084\n",
      "epoch: 3 step: 338, loss is 0.12113174051046371\n",
      "epoch: 3 step: 339, loss is 0.009210986085236073\n",
      "epoch: 3 step: 340, loss is 0.3236515522003174\n",
      "epoch: 3 step: 341, loss is 0.015240032225847244\n",
      "epoch: 3 step: 342, loss is 0.12077116221189499\n",
      "epoch: 3 step: 343, loss is 0.003608990926295519\n",
      "epoch: 3 step: 344, loss is 0.002492270665243268\n",
      "epoch: 3 step: 345, loss is 0.12608376145362854\n",
      "epoch: 3 step: 346, loss is 0.013343133963644505\n",
      "epoch: 3 step: 347, loss is 0.0012355581857264042\n",
      "epoch: 3 step: 348, loss is 0.003070301143452525\n",
      "epoch: 3 step: 349, loss is 0.020954469218850136\n",
      "epoch: 3 step: 350, loss is 0.0027933709789067507\n",
      "epoch: 3 step: 351, loss is 0.20174208283424377\n",
      "epoch: 3 step: 352, loss is 0.022412538528442383\n",
      "epoch: 3 step: 353, loss is 0.03164589777588844\n",
      "epoch: 3 step: 354, loss is 0.03475983813405037\n",
      "epoch: 3 step: 355, loss is 0.029120992869138718\n",
      "epoch: 3 step: 356, loss is 0.15363594889640808\n",
      "epoch: 3 step: 357, loss is 0.029531419277191162\n",
      "epoch: 3 step: 358, loss is 0.06924143433570862\n",
      "epoch: 3 step: 359, loss is 0.2998756766319275\n",
      "epoch: 3 step: 360, loss is 0.00236398633569479\n",
      "epoch: 3 step: 361, loss is 0.06560742110013962\n",
      "epoch: 3 step: 362, loss is 0.020322397351264954\n",
      "epoch: 3 step: 363, loss is 0.0034874037373811007\n",
      "epoch: 3 step: 364, loss is 0.0758344978094101\n",
      "epoch: 3 step: 365, loss is 0.0710320994257927\n",
      "epoch: 3 step: 366, loss is 0.02286640927195549\n",
      "epoch: 3 step: 367, loss is 0.03736042603850365\n",
      "epoch: 3 step: 368, loss is 0.016180584207177162\n",
      "epoch: 3 step: 369, loss is 0.14947229623794556\n",
      "epoch: 3 step: 370, loss is 0.10885795950889587\n",
      "epoch: 3 step: 371, loss is 0.3668133616447449\n",
      "epoch: 3 step: 372, loss is 0.037602219730615616\n",
      "epoch: 3 step: 373, loss is 0.000978479627519846\n",
      "epoch: 3 step: 374, loss is 0.014171579852700233\n",
      "epoch: 3 step: 375, loss is 0.0572255440056324\n",
      "epoch: 3 step: 376, loss is 0.027501752600073814\n",
      "epoch: 3 step: 377, loss is 0.10355117917060852\n",
      "epoch: 3 step: 378, loss is 0.0023674825206398964\n",
      "epoch: 3 step: 379, loss is 0.029727883636951447\n",
      "epoch: 3 step: 380, loss is 0.017014365643262863\n",
      "epoch: 3 step: 381, loss is 0.07123403996229172\n",
      "epoch: 3 step: 382, loss is 0.08300308883190155\n",
      "epoch: 3 step: 383, loss is 0.029460307210683823\n",
      "epoch: 3 step: 384, loss is 0.07391594350337982\n",
      "epoch: 3 step: 385, loss is 0.01604563184082508\n",
      "epoch: 3 step: 386, loss is 0.012896525673568249\n",
      "epoch: 3 step: 387, loss is 0.009371870197355747\n",
      "epoch: 3 step: 388, loss is 0.015926271677017212\n",
      "epoch: 3 step: 389, loss is 0.026537319645285606\n",
      "epoch: 3 step: 390, loss is 0.021079465746879578\n",
      "epoch: 3 step: 391, loss is 0.018021414056420326\n",
      "epoch: 3 step: 392, loss is 0.0032612106297165155\n",
      "epoch: 3 step: 393, loss is 0.02324187383055687\n",
      "epoch: 3 step: 394, loss is 0.0008180132717825472\n",
      "epoch: 3 step: 395, loss is 0.07787537574768066\n",
      "epoch: 3 step: 396, loss is 0.08755647391080856\n",
      "epoch: 3 step: 397, loss is 0.04023047164082527\n",
      "epoch: 3 step: 398, loss is 0.006517225876450539\n",
      "epoch: 3 step: 399, loss is 0.007599826902151108\n",
      "epoch: 3 step: 400, loss is 0.006802785210311413\n",
      "epoch: 3 step: 401, loss is 0.031788505613803864\n",
      "epoch: 3 step: 402, loss is 0.047774769365787506\n",
      "epoch: 3 step: 403, loss is 0.03446638956665993\n",
      "epoch: 3 step: 404, loss is 0.11456844210624695\n",
      "epoch: 3 step: 405, loss is 0.0021544124465435743\n",
      "epoch: 3 step: 406, loss is 0.274569034576416\n",
      "epoch: 3 step: 407, loss is 0.008857418783009052\n",
      "epoch: 3 step: 408, loss is 0.057073116302490234\n",
      "epoch: 3 step: 409, loss is 0.0017207082128152251\n",
      "epoch: 3 step: 410, loss is 0.04104696214199066\n",
      "epoch: 3 step: 411, loss is 0.14438335597515106\n",
      "epoch: 3 step: 412, loss is 0.0035236915573477745\n",
      "epoch: 3 step: 413, loss is 0.01884162239730358\n",
      "epoch: 3 step: 414, loss is 0.01122283935546875\n",
      "epoch: 3 step: 415, loss is 0.0325094535946846\n",
      "epoch: 3 step: 416, loss is 0.11016441136598587\n",
      "epoch: 3 step: 417, loss is 0.26469898223876953\n",
      "epoch: 3 step: 418, loss is 0.11944400519132614\n",
      "epoch: 3 step: 419, loss is 0.014123229309916496\n",
      "epoch: 3 step: 420, loss is 0.15604296326637268\n",
      "epoch: 3 step: 421, loss is 0.0317872054874897\n",
      "epoch: 3 step: 422, loss is 0.005106878001242876\n",
      "epoch: 3 step: 423, loss is 0.15316924452781677\n",
      "epoch: 3 step: 424, loss is 0.0029853270389139652\n",
      "epoch: 3 step: 425, loss is 0.035669442266225815\n",
      "epoch: 3 step: 426, loss is 0.06698980927467346\n",
      "epoch: 3 step: 427, loss is 0.06890065222978592\n",
      "epoch: 3 step: 428, loss is 0.005166072864085436\n",
      "epoch: 3 step: 429, loss is 0.005285070743411779\n",
      "epoch: 3 step: 430, loss is 0.056732505559921265\n",
      "epoch: 3 step: 431, loss is 0.026311593130230904\n",
      "epoch: 3 step: 432, loss is 0.018386371433734894\n",
      "epoch: 3 step: 433, loss is 0.01495254598557949\n",
      "epoch: 3 step: 434, loss is 0.0030797291547060013\n",
      "epoch: 3 step: 435, loss is 0.02328946255147457\n",
      "epoch: 3 step: 436, loss is 0.15307274460792542\n",
      "epoch: 3 step: 437, loss is 0.0017618195852264762\n",
      "epoch: 3 step: 438, loss is 0.018277237191796303\n",
      "epoch: 3 step: 439, loss is 0.04813776910305023\n",
      "epoch: 3 step: 440, loss is 0.11599037051200867\n",
      "epoch: 3 step: 441, loss is 0.13246284425258636\n",
      "epoch: 3 step: 442, loss is 0.0016829686937853694\n",
      "epoch: 3 step: 443, loss is 0.011219024658203125\n",
      "epoch: 3 step: 444, loss is 0.022052908316254616\n",
      "epoch: 3 step: 445, loss is 0.14865687489509583\n",
      "epoch: 3 step: 446, loss is 0.1391584873199463\n",
      "epoch: 3 step: 447, loss is 0.005339500494301319\n",
      "epoch: 3 step: 448, loss is 0.05793436989188194\n",
      "epoch: 3 step: 449, loss is 0.030449260026216507\n",
      "epoch: 3 step: 450, loss is 0.024276457726955414\n",
      "epoch: 3 step: 451, loss is 0.186744824051857\n",
      "epoch: 3 step: 452, loss is 0.04752686247229576\n",
      "epoch: 3 step: 453, loss is 0.05610935762524605\n",
      "epoch: 3 step: 454, loss is 0.18367083370685577\n",
      "epoch: 3 step: 455, loss is 0.007576076313853264\n",
      "epoch: 3 step: 456, loss is 0.22890792787075043\n",
      "epoch: 3 step: 457, loss is 0.09833253920078278\n",
      "epoch: 3 step: 458, loss is 0.04051456227898598\n",
      "epoch: 3 step: 459, loss is 0.021245015785098076\n",
      "epoch: 3 step: 460, loss is 0.028654368594288826\n",
      "epoch: 3 step: 461, loss is 0.05341004207730293\n",
      "epoch: 3 step: 462, loss is 0.13206224143505096\n",
      "epoch: 3 step: 463, loss is 0.0037471081595867872\n",
      "epoch: 3 step: 464, loss is 0.015002445317804813\n",
      "epoch: 3 step: 465, loss is 0.037716347724199295\n",
      "epoch: 3 step: 466, loss is 0.001697269850410521\n",
      "epoch: 3 step: 467, loss is 0.12306228280067444\n",
      "epoch: 3 step: 468, loss is 0.13246630132198334\n",
      "epoch: 3 step: 469, loss is 0.006219695322215557\n",
      "epoch: 3 step: 470, loss is 0.00558870006352663\n",
      "epoch: 3 step: 471, loss is 0.10127595067024231\n",
      "epoch: 3 step: 472, loss is 0.10621964186429977\n",
      "epoch: 3 step: 473, loss is 0.02710643783211708\n",
      "epoch: 3 step: 474, loss is 0.08370208740234375\n",
      "epoch: 3 step: 475, loss is 0.020532388240098953\n",
      "epoch: 3 step: 476, loss is 0.0344085693359375\n",
      "epoch: 3 step: 477, loss is 0.03993955999612808\n",
      "epoch: 3 step: 478, loss is 0.06216346472501755\n",
      "epoch: 3 step: 479, loss is 0.04145163297653198\n",
      "epoch: 3 step: 480, loss is 0.015241377055644989\n",
      "epoch: 3 step: 481, loss is 0.005303527228534222\n",
      "epoch: 3 step: 482, loss is 0.03437657281756401\n",
      "epoch: 3 step: 483, loss is 0.0859459936618805\n",
      "epoch: 3 step: 484, loss is 0.0025108247064054012\n",
      "epoch: 3 step: 485, loss is 0.014552394859492779\n",
      "epoch: 3 step: 486, loss is 0.010340812616050243\n",
      "epoch: 3 step: 487, loss is 0.06000266596674919\n",
      "epoch: 3 step: 488, loss is 0.007595686241984367\n",
      "epoch: 3 step: 489, loss is 0.1510496884584427\n",
      "epoch: 3 step: 490, loss is 0.005157201085239649\n",
      "epoch: 3 step: 491, loss is 0.09342169761657715\n",
      "epoch: 3 step: 492, loss is 0.013803727924823761\n",
      "epoch: 3 step: 493, loss is 0.04455049708485603\n",
      "epoch: 3 step: 494, loss is 0.06777630001306534\n",
      "epoch: 3 step: 495, loss is 0.01855466328561306\n",
      "epoch: 3 step: 496, loss is 0.02297857776284218\n",
      "epoch: 3 step: 497, loss is 0.010094919241964817\n",
      "epoch: 3 step: 498, loss is 0.008667197078466415\n",
      "epoch: 3 step: 499, loss is 0.007859691977500916\n",
      "epoch: 3 step: 500, loss is 0.06787831336259842\n",
      "epoch: 3 step: 501, loss is 0.028641534969210625\n",
      "epoch: 3 step: 502, loss is 0.06797617673873901\n",
      "epoch: 3 step: 503, loss is 0.003932546824216843\n",
      "epoch: 3 step: 504, loss is 0.07595229893922806\n",
      "epoch: 3 step: 505, loss is 0.39863404631614685\n",
      "epoch: 3 step: 506, loss is 0.010749725624918938\n",
      "epoch: 3 step: 507, loss is 0.00681414594873786\n",
      "epoch: 3 step: 508, loss is 0.012082806788384914\n",
      "epoch: 3 step: 509, loss is 0.005400964058935642\n",
      "epoch: 3 step: 510, loss is 0.13417866826057434\n",
      "epoch: 3 step: 511, loss is 0.16002696752548218\n",
      "epoch: 3 step: 512, loss is 0.12295118719339371\n",
      "epoch: 3 step: 513, loss is 0.11284687370061874\n",
      "epoch: 3 step: 514, loss is 0.00047371917753480375\n",
      "epoch: 3 step: 515, loss is 0.003490164875984192\n",
      "epoch: 3 step: 516, loss is 0.013265810906887054\n",
      "epoch: 3 step: 517, loss is 0.01495426520705223\n",
      "epoch: 3 step: 518, loss is 0.006305405404418707\n",
      "epoch: 3 step: 519, loss is 0.01842736266553402\n",
      "epoch: 3 step: 520, loss is 0.18140199780464172\n",
      "epoch: 3 step: 521, loss is 0.03896307572722435\n",
      "epoch: 3 step: 522, loss is 0.03374266251921654\n",
      "epoch: 3 step: 523, loss is 0.041157111525535583\n",
      "epoch: 3 step: 524, loss is 0.02136755734682083\n",
      "epoch: 3 step: 525, loss is 0.0128819290548563\n",
      "epoch: 3 step: 526, loss is 0.0035673563834279776\n",
      "epoch: 3 step: 527, loss is 0.06350059062242508\n",
      "epoch: 3 step: 528, loss is 0.3055990934371948\n",
      "epoch: 3 step: 529, loss is 0.003071254352107644\n",
      "epoch: 3 step: 530, loss is 0.0024135089479386806\n",
      "epoch: 3 step: 531, loss is 0.030087923631072044\n",
      "epoch: 3 step: 532, loss is 0.002704622456803918\n",
      "epoch: 3 step: 533, loss is 0.007506544701755047\n",
      "epoch: 3 step: 534, loss is 0.1840437650680542\n",
      "epoch: 3 step: 535, loss is 0.012338998727500439\n",
      "epoch: 3 step: 536, loss is 0.07241754978895187\n",
      "epoch: 3 step: 537, loss is 0.06777589023113251\n",
      "epoch: 3 step: 538, loss is 0.10571809858083725\n",
      "epoch: 3 step: 539, loss is 0.01509665884077549\n",
      "epoch: 3 step: 540, loss is 0.048670243471860886\n",
      "epoch: 3 step: 541, loss is 0.0023571343626827\n",
      "epoch: 3 step: 542, loss is 0.022097067907452583\n",
      "epoch: 3 step: 543, loss is 0.018933838233351707\n",
      "epoch: 3 step: 544, loss is 0.004384962376207113\n",
      "epoch: 3 step: 545, loss is 0.013942361809313297\n",
      "epoch: 3 step: 546, loss is 0.2532998025417328\n",
      "epoch: 3 step: 547, loss is 0.007893647067248821\n",
      "epoch: 3 step: 548, loss is 0.17780600488185883\n",
      "epoch: 3 step: 549, loss is 0.05468537285923958\n",
      "epoch: 3 step: 550, loss is 0.17141903936862946\n",
      "epoch: 3 step: 551, loss is 0.022954842075705528\n",
      "epoch: 3 step: 552, loss is 0.23950085043907166\n",
      "epoch: 3 step: 553, loss is 0.007201989181339741\n",
      "epoch: 3 step: 554, loss is 0.08547022193670273\n",
      "epoch: 3 step: 555, loss is 0.0018919870490208268\n",
      "epoch: 3 step: 556, loss is 0.07546894252300262\n",
      "epoch: 3 step: 557, loss is 0.12436414510011673\n",
      "epoch: 3 step: 558, loss is 0.2540125846862793\n",
      "epoch: 3 step: 559, loss is 0.005332363303750753\n",
      "epoch: 3 step: 560, loss is 0.049633920192718506\n",
      "epoch: 3 step: 561, loss is 0.06080859526991844\n",
      "epoch: 3 step: 562, loss is 0.004300026223063469\n",
      "epoch: 3 step: 563, loss is 0.07984746992588043\n",
      "epoch: 3 step: 564, loss is 0.02309800311923027\n",
      "epoch: 3 step: 565, loss is 0.014138606376945972\n",
      "epoch: 3 step: 566, loss is 0.010003226809203625\n",
      "epoch: 3 step: 567, loss is 0.13153183460235596\n",
      "epoch: 3 step: 568, loss is 0.03946935757994652\n",
      "epoch: 3 step: 569, loss is 0.007744285278022289\n",
      "epoch: 3 step: 570, loss is 0.007210219744592905\n",
      "epoch: 3 step: 571, loss is 0.024039946496486664\n",
      "epoch: 3 step: 572, loss is 0.13255687057971954\n",
      "epoch: 3 step: 573, loss is 0.22838689386844635\n",
      "epoch: 3 step: 574, loss is 0.10529838502407074\n",
      "epoch: 3 step: 575, loss is 0.01893301121890545\n",
      "epoch: 3 step: 576, loss is 0.02433639205992222\n",
      "epoch: 3 step: 577, loss is 0.004016237799078226\n",
      "epoch: 3 step: 578, loss is 0.06335189938545227\n",
      "epoch: 3 step: 579, loss is 0.10062108933925629\n",
      "epoch: 3 step: 580, loss is 0.04331546276807785\n",
      "epoch: 3 step: 581, loss is 0.18796908855438232\n",
      "epoch: 3 step: 582, loss is 0.0032229924108833075\n",
      "epoch: 3 step: 583, loss is 0.013656055554747581\n",
      "epoch: 3 step: 584, loss is 0.03050028719007969\n",
      "epoch: 3 step: 585, loss is 0.006419898010790348\n",
      "epoch: 3 step: 586, loss is 0.019516119733452797\n",
      "epoch: 3 step: 587, loss is 0.0028115599416196346\n",
      "epoch: 3 step: 588, loss is 0.08495507389307022\n",
      "epoch: 3 step: 589, loss is 0.0911344364285469\n",
      "epoch: 3 step: 590, loss is 0.06812914460897446\n",
      "epoch: 3 step: 591, loss is 0.0006291594472713768\n",
      "epoch: 3 step: 592, loss is 0.047702938318252563\n",
      "epoch: 3 step: 593, loss is 0.1063491478562355\n",
      "epoch: 3 step: 594, loss is 0.006569492630660534\n",
      "epoch: 3 step: 595, loss is 0.0028344730380922556\n",
      "epoch: 3 step: 596, loss is 0.014628894627094269\n",
      "epoch: 3 step: 597, loss is 0.003262162674218416\n",
      "epoch: 3 step: 598, loss is 0.013874884694814682\n",
      "epoch: 3 step: 599, loss is 0.024525640532374382\n",
      "epoch: 3 step: 600, loss is 0.005177549086511135\n",
      "epoch: 3 step: 601, loss is 0.004624795634299517\n",
      "epoch: 3 step: 602, loss is 0.020606067031621933\n",
      "epoch: 3 step: 603, loss is 0.01620912365615368\n",
      "epoch: 3 step: 604, loss is 0.052676115185022354\n",
      "epoch: 3 step: 605, loss is 0.04099055752158165\n",
      "epoch: 3 step: 606, loss is 0.14069914817810059\n",
      "epoch: 3 step: 607, loss is 0.00276296678930521\n",
      "epoch: 3 step: 608, loss is 0.008868996053934097\n",
      "epoch: 3 step: 609, loss is 0.005900360643863678\n",
      "epoch: 3 step: 610, loss is 0.016852714121341705\n",
      "epoch: 3 step: 611, loss is 0.21915754675865173\n",
      "epoch: 3 step: 612, loss is 0.011244491674005985\n",
      "epoch: 3 step: 613, loss is 0.0037451351527124643\n",
      "epoch: 3 step: 614, loss is 0.01779642142355442\n",
      "epoch: 3 step: 615, loss is 0.008092551492154598\n",
      "epoch: 3 step: 616, loss is 0.10254526883363724\n",
      "epoch: 3 step: 617, loss is 0.0039449152536690235\n",
      "epoch: 3 step: 618, loss is 0.01912093721330166\n",
      "epoch: 3 step: 619, loss is 0.0006448701024055481\n",
      "epoch: 3 step: 620, loss is 0.0021904660388827324\n",
      "epoch: 3 step: 621, loss is 0.03324223309755325\n",
      "epoch: 3 step: 622, loss is 0.08272960036993027\n",
      "epoch: 3 step: 623, loss is 0.003775716293603182\n",
      "epoch: 3 step: 624, loss is 0.009847704321146011\n",
      "epoch: 3 step: 625, loss is 0.011193806305527687\n",
      "epoch: 3 step: 626, loss is 0.019029201939702034\n",
      "epoch: 3 step: 627, loss is 0.04030230641365051\n",
      "epoch: 3 step: 628, loss is 0.03156162425875664\n",
      "epoch: 3 step: 629, loss is 0.2068122774362564\n",
      "epoch: 3 step: 630, loss is 0.05065445974469185\n",
      "epoch: 3 step: 631, loss is 0.00121207139454782\n",
      "epoch: 3 step: 632, loss is 0.007045281585305929\n",
      "epoch: 3 step: 633, loss is 0.07353350520133972\n",
      "epoch: 3 step: 634, loss is 0.0019628352019935846\n",
      "epoch: 3 step: 635, loss is 0.16181136667728424\n",
      "epoch: 3 step: 636, loss is 0.005743160843849182\n",
      "epoch: 3 step: 637, loss is 0.013311481103301048\n",
      "epoch: 3 step: 638, loss is 0.0005391666200011969\n",
      "epoch: 3 step: 639, loss is 0.014180606231093407\n",
      "epoch: 3 step: 640, loss is 0.0026252237148582935\n",
      "epoch: 3 step: 641, loss is 0.00041065452387556434\n",
      "epoch: 3 step: 642, loss is 0.015257908031344414\n",
      "epoch: 3 step: 643, loss is 0.009393802843987942\n",
      "epoch: 3 step: 644, loss is 0.003359318943694234\n",
      "epoch: 3 step: 645, loss is 0.038588013499975204\n",
      "epoch: 3 step: 646, loss is 0.006291313096880913\n",
      "epoch: 3 step: 647, loss is 0.050640612840652466\n",
      "epoch: 3 step: 648, loss is 0.03111829236149788\n",
      "epoch: 3 step: 649, loss is 0.0030438643880188465\n",
      "epoch: 3 step: 650, loss is 0.012429923750460148\n",
      "epoch: 3 step: 651, loss is 0.07853180915117264\n",
      "epoch: 3 step: 652, loss is 0.000984757556580007\n",
      "epoch: 3 step: 653, loss is 0.1253540962934494\n",
      "epoch: 3 step: 654, loss is 0.15955105423927307\n",
      "epoch: 3 step: 655, loss is 0.22397685050964355\n",
      "epoch: 3 step: 656, loss is 0.0012681277003139257\n",
      "epoch: 3 step: 657, loss is 0.017628461122512817\n",
      "epoch: 3 step: 658, loss is 0.0992211401462555\n",
      "epoch: 3 step: 659, loss is 0.34692609310150146\n",
      "epoch: 3 step: 660, loss is 0.002108857035636902\n",
      "epoch: 3 step: 661, loss is 0.04907456040382385\n",
      "epoch: 3 step: 662, loss is 0.042032428085803986\n",
      "epoch: 3 step: 663, loss is 0.0032903084065765142\n",
      "epoch: 3 step: 664, loss is 0.03444679081439972\n",
      "epoch: 3 step: 665, loss is 0.0023522083647549152\n",
      "epoch: 3 step: 666, loss is 0.010819587856531143\n",
      "epoch: 3 step: 667, loss is 0.009115095250308514\n",
      "epoch: 3 step: 668, loss is 0.005108218640089035\n",
      "epoch: 3 step: 669, loss is 0.09733065217733383\n",
      "epoch: 3 step: 670, loss is 0.018917247653007507\n",
      "epoch: 3 step: 671, loss is 0.0025826762430369854\n",
      "epoch: 3 step: 672, loss is 0.173905149102211\n",
      "epoch: 3 step: 673, loss is 0.05896437540650368\n",
      "epoch: 3 step: 674, loss is 0.14703841507434845\n",
      "epoch: 3 step: 675, loss is 0.18907463550567627\n",
      "epoch: 3 step: 676, loss is 0.06282815337181091\n",
      "epoch: 3 step: 677, loss is 0.09607027471065521\n",
      "epoch: 3 step: 678, loss is 0.041278861463069916\n",
      "epoch: 3 step: 679, loss is 0.01506364531815052\n",
      "epoch: 3 step: 680, loss is 0.011494956910610199\n",
      "epoch: 3 step: 681, loss is 0.11466091871261597\n",
      "epoch: 3 step: 682, loss is 0.06752340495586395\n",
      "epoch: 3 step: 683, loss is 0.025883469730615616\n",
      "epoch: 3 step: 684, loss is 0.04585465043783188\n",
      "epoch: 3 step: 685, loss is 0.14248502254486084\n",
      "epoch: 3 step: 686, loss is 0.07132620364427567\n",
      "epoch: 3 step: 687, loss is 0.03055323287844658\n",
      "epoch: 3 step: 688, loss is 0.014292593114078045\n",
      "epoch: 3 step: 689, loss is 0.0355050154030323\n",
      "epoch: 3 step: 690, loss is 0.005748375318944454\n",
      "epoch: 3 step: 691, loss is 0.017670152708888054\n",
      "epoch: 3 step: 692, loss is 0.0005069251055829227\n",
      "epoch: 3 step: 693, loss is 0.007459322456270456\n",
      "epoch: 3 step: 694, loss is 0.006364087574183941\n",
      "epoch: 3 step: 695, loss is 0.0020321377087384462\n",
      "epoch: 3 step: 696, loss is 0.014825873076915741\n",
      "epoch: 3 step: 697, loss is 0.0008874578052200377\n",
      "epoch: 3 step: 698, loss is 0.006179466377943754\n",
      "epoch: 3 step: 699, loss is 0.0072072576731443405\n",
      "epoch: 3 step: 700, loss is 0.0021324718836694956\n",
      "epoch: 3 step: 701, loss is 0.056949228048324585\n",
      "epoch: 3 step: 702, loss is 0.004102605395019054\n",
      "epoch: 3 step: 703, loss is 0.007272064220160246\n",
      "epoch: 3 step: 704, loss is 0.13131937384605408\n",
      "epoch: 3 step: 705, loss is 0.00673671904951334\n",
      "epoch: 3 step: 706, loss is 0.00943432841449976\n",
      "epoch: 3 step: 707, loss is 0.08095618337392807\n",
      "epoch: 3 step: 708, loss is 0.07216352969408035\n",
      "epoch: 3 step: 709, loss is 0.0011649015359580517\n",
      "epoch: 3 step: 710, loss is 0.003496739314869046\n",
      "epoch: 3 step: 711, loss is 0.08873040974140167\n",
      "epoch: 3 step: 712, loss is 0.04577343165874481\n",
      "epoch: 3 step: 713, loss is 0.12666212022304535\n",
      "epoch: 3 step: 714, loss is 0.005862829275429249\n",
      "epoch: 3 step: 715, loss is 0.0024992148391902447\n",
      "epoch: 3 step: 716, loss is 0.18096786737442017\n",
      "epoch: 3 step: 717, loss is 0.007275510113686323\n",
      "epoch: 3 step: 718, loss is 0.15485155582427979\n",
      "epoch: 3 step: 719, loss is 0.0008834674954414368\n",
      "epoch: 3 step: 720, loss is 0.006879009772092104\n",
      "epoch: 3 step: 721, loss is 0.011805089190602303\n",
      "epoch: 3 step: 722, loss is 0.0034979525953531265\n",
      "epoch: 3 step: 723, loss is 0.08928273618221283\n",
      "epoch: 3 step: 724, loss is 0.03072330914437771\n",
      "epoch: 3 step: 725, loss is 0.09046527743339539\n",
      "epoch: 3 step: 726, loss is 0.05808234587311745\n",
      "epoch: 3 step: 727, loss is 0.012357988394796848\n",
      "epoch: 3 step: 728, loss is 0.02632281370460987\n",
      "epoch: 3 step: 729, loss is 0.024893159046769142\n",
      "epoch: 3 step: 730, loss is 0.004932904150336981\n",
      "epoch: 3 step: 731, loss is 0.025856386870145798\n",
      "epoch: 3 step: 732, loss is 0.024319911375641823\n",
      "epoch: 3 step: 733, loss is 0.008243514224886894\n",
      "epoch: 3 step: 734, loss is 0.004754274617880583\n",
      "epoch: 3 step: 735, loss is 0.030410733073949814\n",
      "epoch: 3 step: 736, loss is 0.009656576439738274\n",
      "epoch: 3 step: 737, loss is 0.14384061098098755\n",
      "epoch: 3 step: 738, loss is 0.1685451716184616\n",
      "epoch: 3 step: 739, loss is 0.04034983366727829\n",
      "epoch: 3 step: 740, loss is 0.11998642981052399\n",
      "epoch: 3 step: 741, loss is 0.139187753200531\n",
      "epoch: 3 step: 742, loss is 0.031018761917948723\n",
      "epoch: 3 step: 743, loss is 0.0424770712852478\n",
      "epoch: 3 step: 744, loss is 0.012882024981081486\n",
      "epoch: 3 step: 745, loss is 0.029168322682380676\n",
      "epoch: 3 step: 746, loss is 0.06658432632684708\n",
      "epoch: 3 step: 747, loss is 0.032594241201877594\n",
      "epoch: 3 step: 748, loss is 0.001141875865869224\n",
      "epoch: 3 step: 749, loss is 0.001507240696810186\n",
      "epoch: 3 step: 750, loss is 0.013039947487413883\n",
      "epoch: 3 step: 751, loss is 0.07572302222251892\n",
      "epoch: 3 step: 752, loss is 0.08600851148366928\n",
      "epoch: 3 step: 753, loss is 0.014311235398054123\n",
      "epoch: 3 step: 754, loss is 0.003732079640030861\n",
      "epoch: 3 step: 755, loss is 0.00614083232358098\n",
      "epoch: 3 step: 756, loss is 0.021449560299515724\n",
      "epoch: 3 step: 757, loss is 0.0013238617684692144\n",
      "epoch: 3 step: 758, loss is 0.030528325587511063\n",
      "epoch: 3 step: 759, loss is 0.01176300086081028\n",
      "epoch: 3 step: 760, loss is 0.15859687328338623\n",
      "epoch: 3 step: 761, loss is 0.29811224341392517\n",
      "epoch: 3 step: 762, loss is 0.0018151545664295554\n",
      "epoch: 3 step: 763, loss is 0.03437434881925583\n",
      "epoch: 3 step: 764, loss is 0.03729163855314255\n",
      "epoch: 3 step: 765, loss is 0.06421706080436707\n",
      "epoch: 3 step: 766, loss is 0.005596949253231287\n",
      "epoch: 3 step: 767, loss is 0.12559424340724945\n",
      "epoch: 3 step: 768, loss is 0.03769643232226372\n",
      "epoch: 3 step: 769, loss is 0.038708459585905075\n",
      "epoch: 3 step: 770, loss is 0.02630782127380371\n",
      "epoch: 3 step: 771, loss is 0.04444243758916855\n",
      "epoch: 3 step: 772, loss is 0.3069697320461273\n",
      "epoch: 3 step: 773, loss is 0.020198816433548927\n",
      "epoch: 3 step: 774, loss is 0.078099325299263\n",
      "epoch: 3 step: 775, loss is 0.010269313119351864\n",
      "epoch: 3 step: 776, loss is 0.013936890289187431\n",
      "epoch: 3 step: 777, loss is 0.018049059435725212\n",
      "epoch: 3 step: 778, loss is 0.24197451770305634\n",
      "epoch: 3 step: 779, loss is 0.07967120409011841\n",
      "epoch: 3 step: 780, loss is 0.0057940734550356865\n",
      "epoch: 3 step: 781, loss is 0.0037743239663541317\n",
      "epoch: 3 step: 782, loss is 0.02227690815925598\n",
      "epoch: 3 step: 783, loss is 0.00881767924875021\n",
      "epoch: 3 step: 784, loss is 0.09964639693498611\n",
      "epoch: 3 step: 785, loss is 0.04467763751745224\n",
      "epoch: 3 step: 786, loss is 0.004298374056816101\n",
      "epoch: 3 step: 787, loss is 0.03362381085753441\n",
      "epoch: 3 step: 788, loss is 0.05716399848461151\n",
      "epoch: 3 step: 789, loss is 0.058840561658144\n",
      "epoch: 3 step: 790, loss is 0.01495419442653656\n",
      "epoch: 3 step: 791, loss is 0.009562167339026928\n",
      "epoch: 3 step: 792, loss is 0.1073610782623291\n",
      "epoch: 3 step: 793, loss is 0.05006417632102966\n",
      "epoch: 3 step: 794, loss is 0.084910087287426\n",
      "epoch: 3 step: 795, loss is 0.007180045358836651\n",
      "epoch: 3 step: 796, loss is 0.003263558028265834\n",
      "epoch: 3 step: 797, loss is 0.08204639703035355\n",
      "epoch: 3 step: 798, loss is 0.13748031854629517\n",
      "epoch: 3 step: 799, loss is 0.0017553705256432295\n",
      "epoch: 3 step: 800, loss is 0.07898502051830292\n",
      "epoch: 3 step: 801, loss is 0.04512309283018112\n",
      "epoch: 3 step: 802, loss is 0.022797921672463417\n",
      "epoch: 3 step: 803, loss is 0.0508202463388443\n",
      "epoch: 3 step: 804, loss is 0.04135850444436073\n",
      "epoch: 3 step: 805, loss is 0.19165050983428955\n",
      "epoch: 3 step: 806, loss is 0.0292672086507082\n",
      "epoch: 3 step: 807, loss is 0.013414328917860985\n",
      "epoch: 3 step: 808, loss is 0.0030201482586562634\n",
      "epoch: 3 step: 809, loss is 0.24145492911338806\n",
      "epoch: 3 step: 810, loss is 0.022762710228562355\n",
      "epoch: 3 step: 811, loss is 0.033088475465774536\n",
      "epoch: 3 step: 812, loss is 0.03727731853723526\n",
      "epoch: 3 step: 813, loss is 0.022907715290784836\n",
      "epoch: 3 step: 814, loss is 0.007979588583111763\n",
      "epoch: 3 step: 815, loss is 0.030110910534858704\n",
      "epoch: 3 step: 816, loss is 0.0028576329350471497\n",
      "epoch: 3 step: 817, loss is 0.0009167456883005798\n",
      "epoch: 3 step: 818, loss is 0.00635698065161705\n",
      "epoch: 3 step: 819, loss is 0.04017475247383118\n",
      "epoch: 3 step: 820, loss is 0.24822373688220978\n",
      "epoch: 3 step: 821, loss is 0.0050918832421302795\n",
      "epoch: 3 step: 822, loss is 0.0018661005888134241\n",
      "epoch: 3 step: 823, loss is 0.075431689620018\n",
      "epoch: 3 step: 824, loss is 0.10537455230951309\n",
      "epoch: 3 step: 825, loss is 0.16402879357337952\n",
      "epoch: 3 step: 826, loss is 0.012802849523723125\n",
      "epoch: 3 step: 827, loss is 0.0018544142367318273\n",
      "epoch: 3 step: 828, loss is 0.17899954319000244\n",
      "epoch: 3 step: 829, loss is 0.2138839066028595\n",
      "epoch: 3 step: 830, loss is 0.006713684182614088\n",
      "epoch: 3 step: 831, loss is 0.016454927623271942\n",
      "epoch: 3 step: 832, loss is 0.02939654514193535\n",
      "epoch: 3 step: 833, loss is 0.014508070424199104\n",
      "epoch: 3 step: 834, loss is 0.1319330781698227\n",
      "epoch: 3 step: 835, loss is 0.17540915310382843\n",
      "epoch: 3 step: 836, loss is 0.1834067851305008\n",
      "epoch: 3 step: 837, loss is 0.15332013368606567\n",
      "epoch: 3 step: 838, loss is 0.008549712598323822\n",
      "epoch: 3 step: 839, loss is 0.010922310873866081\n",
      "epoch: 3 step: 840, loss is 0.004375820979475975\n",
      "epoch: 3 step: 841, loss is 0.1725076138973236\n",
      "epoch: 3 step: 842, loss is 0.03308141976594925\n",
      "epoch: 3 step: 843, loss is 0.05590023472905159\n",
      "epoch: 3 step: 844, loss is 0.04460085928440094\n",
      "epoch: 3 step: 845, loss is 0.01164195779711008\n",
      "epoch: 3 step: 846, loss is 0.09306227415800095\n",
      "epoch: 3 step: 847, loss is 0.02315465174615383\n",
      "epoch: 3 step: 848, loss is 0.03997446224093437\n",
      "epoch: 3 step: 849, loss is 0.01689436472952366\n",
      "epoch: 3 step: 850, loss is 0.010293494910001755\n",
      "epoch: 3 step: 851, loss is 0.118416428565979\n",
      "epoch: 3 step: 852, loss is 0.3231715261936188\n",
      "epoch: 3 step: 853, loss is 0.0065672993659973145\n",
      "epoch: 3 step: 854, loss is 0.08044928312301636\n",
      "epoch: 3 step: 855, loss is 0.011005327105522156\n",
      "epoch: 3 step: 856, loss is 0.002831972436979413\n",
      "epoch: 3 step: 857, loss is 0.0027672508731484413\n",
      "epoch: 3 step: 858, loss is 0.014988258481025696\n",
      "epoch: 3 step: 859, loss is 0.08101504296064377\n",
      "epoch: 3 step: 860, loss is 0.08993713557720184\n",
      "epoch: 3 step: 861, loss is 0.012292447499930859\n",
      "epoch: 3 step: 862, loss is 0.10644076019525528\n",
      "epoch: 3 step: 863, loss is 0.04204405099153519\n",
      "epoch: 3 step: 864, loss is 0.017937732860445976\n",
      "epoch: 3 step: 865, loss is 0.11434260755777359\n",
      "epoch: 3 step: 866, loss is 0.25826558470726013\n",
      "epoch: 3 step: 867, loss is 0.0025942076463252306\n",
      "epoch: 3 step: 868, loss is 0.09663703292608261\n",
      "epoch: 3 step: 869, loss is 0.018198996782302856\n",
      "epoch: 3 step: 870, loss is 0.01286562904715538\n",
      "epoch: 3 step: 871, loss is 0.0830146074295044\n",
      "epoch: 3 step: 872, loss is 0.06419044733047485\n",
      "epoch: 3 step: 873, loss is 0.014247897081077099\n",
      "epoch: 3 step: 874, loss is 0.011416054330766201\n",
      "epoch: 3 step: 875, loss is 0.03671397641301155\n",
      "epoch: 3 step: 876, loss is 0.07883577793836594\n",
      "epoch: 3 step: 877, loss is 0.049091510474681854\n",
      "epoch: 3 step: 878, loss is 0.010075993835926056\n",
      "epoch: 3 step: 879, loss is 0.04778445512056351\n",
      "epoch: 3 step: 880, loss is 0.10860113799571991\n",
      "epoch: 3 step: 881, loss is 0.013518043793737888\n",
      "epoch: 3 step: 882, loss is 0.010229725390672684\n",
      "epoch: 3 step: 883, loss is 0.12945939600467682\n",
      "epoch: 3 step: 884, loss is 0.007650325074791908\n",
      "epoch: 3 step: 885, loss is 0.13319174945354462\n",
      "epoch: 3 step: 886, loss is 0.014732921496033669\n",
      "epoch: 3 step: 887, loss is 0.005181792192161083\n",
      "epoch: 3 step: 888, loss is 0.04700004309415817\n",
      "epoch: 3 step: 889, loss is 0.05550401657819748\n",
      "epoch: 3 step: 890, loss is 0.06752585619688034\n",
      "epoch: 3 step: 891, loss is 0.016638444736599922\n",
      "epoch: 3 step: 892, loss is 0.030469289049506187\n",
      "epoch: 3 step: 893, loss is 0.2106674611568451\n",
      "epoch: 3 step: 894, loss is 0.011691958643496037\n",
      "epoch: 3 step: 895, loss is 0.05907082557678223\n",
      "epoch: 3 step: 896, loss is 0.05637718364596367\n",
      "epoch: 3 step: 897, loss is 0.0645158588886261\n",
      "epoch: 3 step: 898, loss is 0.0054182810708880424\n",
      "epoch: 3 step: 899, loss is 0.011337335221469402\n",
      "epoch: 3 step: 900, loss is 0.010612266138195992\n",
      "epoch: 3 step: 901, loss is 0.0260532908141613\n",
      "epoch: 3 step: 902, loss is 0.1360514760017395\n",
      "epoch: 3 step: 903, loss is 0.031061621382832527\n",
      "epoch: 3 step: 904, loss is 0.07751238346099854\n",
      "epoch: 3 step: 905, loss is 0.014969716779887676\n",
      "epoch: 3 step: 906, loss is 0.0034804637543857098\n",
      "epoch: 3 step: 907, loss is 0.011019156314432621\n",
      "epoch: 3 step: 908, loss is 0.0009673188906162977\n",
      "epoch: 3 step: 909, loss is 0.0008918429375626147\n",
      "epoch: 3 step: 910, loss is 0.0037890218663960695\n",
      "epoch: 3 step: 911, loss is 0.03341570124030113\n",
      "epoch: 3 step: 912, loss is 0.013138649053871632\n",
      "epoch: 3 step: 913, loss is 0.020266776904463768\n",
      "epoch: 3 step: 914, loss is 0.07306472212076187\n",
      "epoch: 3 step: 915, loss is 0.01943778805434704\n",
      "epoch: 3 step: 916, loss is 0.02557072788476944\n",
      "epoch: 3 step: 917, loss is 0.0007528988062404096\n",
      "epoch: 3 step: 918, loss is 0.17699092626571655\n",
      "epoch: 3 step: 919, loss is 0.004434309434145689\n",
      "epoch: 3 step: 920, loss is 0.016751056537032127\n",
      "epoch: 3 step: 921, loss is 0.025415144860744476\n",
      "epoch: 3 step: 922, loss is 0.11481598764657974\n",
      "epoch: 3 step: 923, loss is 0.002569201635196805\n",
      "epoch: 3 step: 924, loss is 0.06556417793035507\n",
      "epoch: 3 step: 925, loss is 0.00395372835919261\n",
      "epoch: 3 step: 926, loss is 0.014823777601122856\n",
      "epoch: 3 step: 927, loss is 0.10176297277212143\n",
      "epoch: 3 step: 928, loss is 0.08209818601608276\n",
      "epoch: 3 step: 929, loss is 0.053806863725185394\n",
      "epoch: 3 step: 930, loss is 0.0019567986018955708\n",
      "epoch: 3 step: 931, loss is 0.0128441471606493\n",
      "epoch: 3 step: 932, loss is 0.016924159601330757\n",
      "epoch: 3 step: 933, loss is 0.001258196891285479\n",
      "epoch: 3 step: 934, loss is 0.05406578257679939\n",
      "epoch: 3 step: 935, loss is 0.008646383881568909\n",
      "epoch: 3 step: 936, loss is 0.21143746376037598\n",
      "epoch: 3 step: 937, loss is 0.28628212213516235\n",
      "epoch: 3 step: 938, loss is 0.041551798582077026\n",
      "epoch: 3 step: 939, loss is 0.0065356288105249405\n",
      "epoch: 3 step: 940, loss is 0.10699541866779327\n",
      "epoch: 3 step: 941, loss is 0.02429194189608097\n",
      "epoch: 3 step: 942, loss is 0.0032995115034282207\n",
      "epoch: 3 step: 943, loss is 0.060201261192560196\n",
      "epoch: 3 step: 944, loss is 0.0034176737535744905\n",
      "epoch: 3 step: 945, loss is 0.034847602248191833\n",
      "epoch: 3 step: 946, loss is 0.031665951013565063\n",
      "epoch: 3 step: 947, loss is 0.020084412768483162\n",
      "epoch: 3 step: 948, loss is 0.0014363223453983665\n",
      "epoch: 3 step: 949, loss is 0.03194895014166832\n",
      "epoch: 3 step: 950, loss is 0.025466227903962135\n",
      "epoch: 3 step: 951, loss is 0.0028449767269194126\n",
      "epoch: 3 step: 952, loss is 0.0009521016618236899\n",
      "epoch: 3 step: 953, loss is 0.03260435163974762\n",
      "epoch: 3 step: 954, loss is 0.1428540199995041\n",
      "epoch: 3 step: 955, loss is 0.06231546401977539\n",
      "epoch: 3 step: 956, loss is 0.0038041339721530676\n",
      "epoch: 3 step: 957, loss is 0.009822276420891285\n",
      "epoch: 3 step: 958, loss is 0.07045119255781174\n",
      "epoch: 3 step: 959, loss is 0.0005050225299783051\n",
      "epoch: 3 step: 960, loss is 0.02100191079080105\n",
      "epoch: 3 step: 961, loss is 0.0033165402710437775\n",
      "epoch: 3 step: 962, loss is 0.1872604936361313\n",
      "epoch: 3 step: 963, loss is 0.0015593863790854812\n",
      "epoch: 3 step: 964, loss is 0.18069283664226532\n",
      "epoch: 3 step: 965, loss is 0.0026104459539055824\n",
      "epoch: 3 step: 966, loss is 0.018965711817145348\n",
      "epoch: 3 step: 967, loss is 0.004014299716800451\n",
      "epoch: 3 step: 968, loss is 0.10161110758781433\n",
      "epoch: 3 step: 969, loss is 0.01646261103451252\n",
      "epoch: 3 step: 970, loss is 0.09267507493495941\n",
      "epoch: 3 step: 971, loss is 0.023246919736266136\n",
      "epoch: 3 step: 972, loss is 0.17830316722393036\n",
      "epoch: 3 step: 973, loss is 0.0024700986687093973\n",
      "epoch: 3 step: 974, loss is 0.06169673055410385\n",
      "epoch: 3 step: 975, loss is 0.009924509562551975\n",
      "epoch: 3 step: 976, loss is 0.0012849027989432216\n",
      "epoch: 3 step: 977, loss is 0.23458147048950195\n",
      "epoch: 3 step: 978, loss is 0.03779704123735428\n",
      "epoch: 3 step: 979, loss is 0.00837614107877016\n",
      "epoch: 3 step: 980, loss is 0.17746812105178833\n",
      "epoch: 3 step: 981, loss is 0.005202211439609528\n",
      "epoch: 3 step: 982, loss is 0.0032881784718483686\n",
      "epoch: 3 step: 983, loss is 0.036210644990205765\n",
      "epoch: 3 step: 984, loss is 0.021842671558260918\n",
      "epoch: 3 step: 985, loss is 0.001012415741570294\n",
      "epoch: 3 step: 986, loss is 0.050141867250204086\n",
      "epoch: 3 step: 987, loss is 0.005368934478610754\n",
      "epoch: 3 step: 988, loss is 0.11409180611371994\n",
      "epoch: 3 step: 989, loss is 0.004423667676746845\n",
      "epoch: 3 step: 990, loss is 0.04730626195669174\n",
      "epoch: 3 step: 991, loss is 0.05182613432407379\n",
      "epoch: 3 step: 992, loss is 0.0061185103841125965\n",
      "epoch: 3 step: 993, loss is 0.004523854237049818\n",
      "epoch: 3 step: 994, loss is 0.04838919639587402\n",
      "epoch: 3 step: 995, loss is 0.0025748449843376875\n",
      "epoch: 3 step: 996, loss is 0.17138969898223877\n",
      "epoch: 3 step: 997, loss is 0.0041996147483587265\n",
      "epoch: 3 step: 998, loss is 0.2073056697845459\n",
      "epoch: 3 step: 999, loss is 0.0049799298867583275\n",
      "epoch: 3 step: 1000, loss is 0.02224988117814064\n",
      "epoch: 3 step: 1001, loss is 0.2695361375808716\n",
      "epoch: 3 step: 1002, loss is 0.008620593696832657\n",
      "epoch: 3 step: 1003, loss is 0.016422441229224205\n",
      "epoch: 3 step: 1004, loss is 0.010921675711870193\n",
      "epoch: 3 step: 1005, loss is 0.19633902609348297\n",
      "epoch: 3 step: 1006, loss is 0.029247619211673737\n",
      "epoch: 3 step: 1007, loss is 0.07762745022773743\n",
      "epoch: 3 step: 1008, loss is 0.009253204800188541\n",
      "epoch: 3 step: 1009, loss is 0.16173429787158966\n",
      "epoch: 3 step: 1010, loss is 0.0007895115413703024\n",
      "epoch: 3 step: 1011, loss is 0.0006024986505508423\n",
      "epoch: 3 step: 1012, loss is 0.004140474367886782\n",
      "epoch: 3 step: 1013, loss is 0.019829045981168747\n",
      "epoch: 3 step: 1014, loss is 0.14700661599636078\n",
      "epoch: 3 step: 1015, loss is 0.04179956391453743\n",
      "epoch: 3 step: 1016, loss is 0.05032018572092056\n",
      "epoch: 3 step: 1017, loss is 0.003406030824407935\n",
      "epoch: 3 step: 1018, loss is 0.03961973264813423\n",
      "epoch: 3 step: 1019, loss is 0.34646159410476685\n",
      "epoch: 3 step: 1020, loss is 0.032362040132284164\n",
      "epoch: 3 step: 1021, loss is 0.013666941784322262\n",
      "epoch: 3 step: 1022, loss is 0.08108958601951599\n",
      "epoch: 3 step: 1023, loss is 0.005998063366860151\n",
      "epoch: 3 step: 1024, loss is 0.035471826791763306\n",
      "epoch: 3 step: 1025, loss is 0.0018761431565508246\n",
      "epoch: 3 step: 1026, loss is 0.009777514263987541\n",
      "epoch: 3 step: 1027, loss is 0.013917820528149605\n",
      "epoch: 3 step: 1028, loss is 0.00761587405577302\n",
      "epoch: 3 step: 1029, loss is 0.006797301582992077\n",
      "epoch: 3 step: 1030, loss is 0.07214309275150299\n",
      "epoch: 3 step: 1031, loss is 0.023571178317070007\n",
      "epoch: 3 step: 1032, loss is 0.039449065923690796\n",
      "epoch: 3 step: 1033, loss is 0.04150718078017235\n",
      "epoch: 3 step: 1034, loss is 0.10576518625020981\n",
      "epoch: 3 step: 1035, loss is 0.24095655977725983\n",
      "epoch: 3 step: 1036, loss is 0.01508952584117651\n",
      "epoch: 3 step: 1037, loss is 0.04450412467122078\n",
      "epoch: 3 step: 1038, loss is 0.04185165464878082\n",
      "epoch: 3 step: 1039, loss is 0.0009188742842525244\n",
      "epoch: 3 step: 1040, loss is 0.09037883579730988\n",
      "epoch: 3 step: 1041, loss is 0.0016443125205114484\n",
      "epoch: 3 step: 1042, loss is 0.02011040598154068\n",
      "epoch: 3 step: 1043, loss is 0.002649861853569746\n",
      "epoch: 3 step: 1044, loss is 0.0667538121342659\n",
      "epoch: 3 step: 1045, loss is 0.0010219721589237452\n",
      "epoch: 3 step: 1046, loss is 0.006806952878832817\n",
      "epoch: 3 step: 1047, loss is 0.013629855588078499\n",
      "epoch: 3 step: 1048, loss is 0.08716060221195221\n",
      "epoch: 3 step: 1049, loss is 0.00841648131608963\n",
      "epoch: 3 step: 1050, loss is 0.03359317034482956\n",
      "epoch: 3 step: 1051, loss is 0.10928839445114136\n",
      "epoch: 3 step: 1052, loss is 0.01583324372768402\n",
      "epoch: 3 step: 1053, loss is 0.007242388091981411\n",
      "epoch: 3 step: 1054, loss is 0.0033032887149602175\n",
      "epoch: 3 step: 1055, loss is 0.0008639322477392852\n",
      "epoch: 3 step: 1056, loss is 0.11816896498203278\n",
      "epoch: 3 step: 1057, loss is 0.0013649372849613428\n",
      "epoch: 3 step: 1058, loss is 0.0042440686374902725\n",
      "epoch: 3 step: 1059, loss is 0.010619796812534332\n",
      "epoch: 3 step: 1060, loss is 0.0008282625349238515\n",
      "epoch: 3 step: 1061, loss is 0.002665746957063675\n",
      "epoch: 3 step: 1062, loss is 0.00465470040217042\n",
      "epoch: 3 step: 1063, loss is 0.006848206277936697\n",
      "epoch: 3 step: 1064, loss is 0.20526021718978882\n",
      "epoch: 3 step: 1065, loss is 0.16797161102294922\n",
      "epoch: 3 step: 1066, loss is 0.0030933127272874117\n",
      "epoch: 3 step: 1067, loss is 0.004518792033195496\n",
      "epoch: 3 step: 1068, loss is 0.037300266325473785\n",
      "epoch: 3 step: 1069, loss is 0.00852151494473219\n",
      "epoch: 3 step: 1070, loss is 0.026331890374422073\n",
      "epoch: 3 step: 1071, loss is 0.49993008375167847\n",
      "epoch: 3 step: 1072, loss is 0.008451346307992935\n",
      "epoch: 3 step: 1073, loss is 0.007342150900512934\n",
      "epoch: 3 step: 1074, loss is 0.04099351540207863\n",
      "epoch: 3 step: 1075, loss is 0.013950393535196781\n",
      "epoch: 3 step: 1076, loss is 0.14789244532585144\n",
      "epoch: 3 step: 1077, loss is 0.23121264576911926\n",
      "epoch: 3 step: 1078, loss is 0.20207178592681885\n",
      "epoch: 3 step: 1079, loss is 0.018204735592007637\n",
      "epoch: 3 step: 1080, loss is 0.003221596125513315\n",
      "epoch: 3 step: 1081, loss is 0.004306737333536148\n",
      "epoch: 3 step: 1082, loss is 0.018174894154071808\n",
      "epoch: 3 step: 1083, loss is 0.13637325167655945\n",
      "epoch: 3 step: 1084, loss is 0.06653214246034622\n",
      "epoch: 3 step: 1085, loss is 0.009681823663413525\n",
      "epoch: 3 step: 1086, loss is 0.03913168981671333\n",
      "epoch: 3 step: 1087, loss is 0.027041137218475342\n",
      "epoch: 3 step: 1088, loss is 0.044339872896671295\n",
      "epoch: 3 step: 1089, loss is 0.13181188702583313\n",
      "epoch: 3 step: 1090, loss is 0.007535639684647322\n",
      "epoch: 3 step: 1091, loss is 0.2379174828529358\n",
      "epoch: 3 step: 1092, loss is 0.027336008846759796\n",
      "epoch: 3 step: 1093, loss is 0.03944902867078781\n",
      "epoch: 3 step: 1094, loss is 0.039091333746910095\n",
      "epoch: 3 step: 1095, loss is 0.004613004624843597\n",
      "epoch: 3 step: 1096, loss is 0.010047337040305138\n",
      "epoch: 3 step: 1097, loss is 0.013486792333424091\n",
      "epoch: 3 step: 1098, loss is 0.026218706741929054\n",
      "epoch: 3 step: 1099, loss is 0.030598154291510582\n",
      "epoch: 3 step: 1100, loss is 0.021922709420323372\n",
      "epoch: 3 step: 1101, loss is 0.004294173792004585\n",
      "epoch: 3 step: 1102, loss is 0.009746363386511803\n",
      "epoch: 3 step: 1103, loss is 0.003385893302038312\n",
      "epoch: 3 step: 1104, loss is 0.11280286312103271\n",
      "epoch: 3 step: 1105, loss is 0.03959827125072479\n",
      "epoch: 3 step: 1106, loss is 0.017816245555877686\n",
      "epoch: 3 step: 1107, loss is 0.0025327522307634354\n",
      "epoch: 3 step: 1108, loss is 0.10968545079231262\n",
      "epoch: 3 step: 1109, loss is 0.004061555024236441\n",
      "epoch: 3 step: 1110, loss is 0.022398417815566063\n",
      "epoch: 3 step: 1111, loss is 0.011402557604014874\n",
      "epoch: 3 step: 1112, loss is 0.002297888742759824\n",
      "epoch: 3 step: 1113, loss is 0.03591805696487427\n",
      "epoch: 3 step: 1114, loss is 0.12760117650032043\n",
      "epoch: 3 step: 1115, loss is 0.2098970264196396\n",
      "epoch: 3 step: 1116, loss is 0.07513868063688278\n",
      "epoch: 3 step: 1117, loss is 0.03671036288142204\n",
      "epoch: 3 step: 1118, loss is 0.0037266425788402557\n",
      "epoch: 3 step: 1119, loss is 0.005983324721455574\n",
      "epoch: 3 step: 1120, loss is 0.022148331627249718\n",
      "epoch: 3 step: 1121, loss is 0.05201100558042526\n",
      "epoch: 3 step: 1122, loss is 0.033940959721803665\n",
      "epoch: 3 step: 1123, loss is 0.15798571705818176\n",
      "epoch: 3 step: 1124, loss is 0.2216029316186905\n",
      "epoch: 3 step: 1125, loss is 0.017062203958630562\n",
      "epoch: 3 step: 1126, loss is 0.07283274084329605\n",
      "epoch: 3 step: 1127, loss is 0.0036860413383692503\n",
      "epoch: 3 step: 1128, loss is 0.012937267310917377\n",
      "epoch: 3 step: 1129, loss is 0.25975531339645386\n",
      "epoch: 3 step: 1130, loss is 0.01408171746879816\n",
      "epoch: 3 step: 1131, loss is 0.038682516664266586\n",
      "epoch: 3 step: 1132, loss is 0.17852652072906494\n",
      "epoch: 3 step: 1133, loss is 0.0063216364942491055\n",
      "epoch: 3 step: 1134, loss is 0.005453453864902258\n",
      "epoch: 3 step: 1135, loss is 0.0041570174507796764\n",
      "epoch: 3 step: 1136, loss is 0.001838505151681602\n",
      "epoch: 3 step: 1137, loss is 0.13178817927837372\n",
      "epoch: 3 step: 1138, loss is 0.0008295424631796777\n",
      "epoch: 3 step: 1139, loss is 0.0032582059502601624\n",
      "epoch: 3 step: 1140, loss is 0.01708543673157692\n",
      "epoch: 3 step: 1141, loss is 0.003689731238409877\n",
      "epoch: 3 step: 1142, loss is 0.07824676483869553\n",
      "epoch: 3 step: 1143, loss is 0.005245290230959654\n",
      "epoch: 3 step: 1144, loss is 0.10139324516057968\n",
      "epoch: 3 step: 1145, loss is 0.010699987411499023\n",
      "epoch: 3 step: 1146, loss is 0.0027801210526376963\n",
      "epoch: 3 step: 1147, loss is 0.0130042415112257\n",
      "epoch: 3 step: 1148, loss is 0.019093452021479607\n",
      "epoch: 3 step: 1149, loss is 0.015315260738134384\n",
      "epoch: 3 step: 1150, loss is 0.0038698299322277308\n",
      "epoch: 3 step: 1151, loss is 0.015144238248467445\n",
      "epoch: 3 step: 1152, loss is 0.0886499285697937\n",
      "epoch: 3 step: 1153, loss is 0.009194191545248032\n",
      "epoch: 3 step: 1154, loss is 0.014194043353199959\n",
      "epoch: 3 step: 1155, loss is 0.25879573822021484\n",
      "epoch: 3 step: 1156, loss is 0.036861274391412735\n",
      "epoch: 3 step: 1157, loss is 0.044595081359148026\n",
      "epoch: 3 step: 1158, loss is 0.004054620396345854\n",
      "epoch: 3 step: 1159, loss is 0.018386565148830414\n",
      "epoch: 3 step: 1160, loss is 0.016411252319812775\n",
      "epoch: 3 step: 1161, loss is 0.025357097387313843\n",
      "epoch: 3 step: 1162, loss is 0.0013437554007396102\n",
      "epoch: 3 step: 1163, loss is 0.004705261904746294\n",
      "epoch: 3 step: 1164, loss is 0.017575252801179886\n",
      "epoch: 3 step: 1165, loss is 0.050386060029268265\n",
      "epoch: 3 step: 1166, loss is 0.01811131276190281\n",
      "epoch: 3 step: 1167, loss is 0.10881903022527695\n",
      "epoch: 3 step: 1168, loss is 0.06880497187376022\n",
      "epoch: 3 step: 1169, loss is 0.005515772383660078\n",
      "epoch: 3 step: 1170, loss is 0.04550430178642273\n",
      "epoch: 3 step: 1171, loss is 0.026185475289821625\n",
      "epoch: 3 step: 1172, loss is 0.06494487822055817\n",
      "epoch: 3 step: 1173, loss is 0.009873711504042149\n",
      "epoch: 3 step: 1174, loss is 0.11600027233362198\n",
      "epoch: 3 step: 1175, loss is 0.003176563186571002\n",
      "epoch: 3 step: 1176, loss is 0.003910801373422146\n",
      "epoch: 3 step: 1177, loss is 0.02317148819565773\n",
      "epoch: 3 step: 1178, loss is 0.13517169654369354\n",
      "epoch: 3 step: 1179, loss is 0.019800342619419098\n",
      "epoch: 3 step: 1180, loss is 0.009945959784090519\n",
      "epoch: 3 step: 1181, loss is 0.00611876230686903\n",
      "epoch: 3 step: 1182, loss is 0.000920912018045783\n",
      "epoch: 3 step: 1183, loss is 0.09993800520896912\n",
      "epoch: 3 step: 1184, loss is 0.02035694383084774\n",
      "epoch: 3 step: 1185, loss is 0.035889316350221634\n",
      "epoch: 3 step: 1186, loss is 0.020852571353316307\n",
      "epoch: 3 step: 1187, loss is 0.02335251122713089\n",
      "epoch: 3 step: 1188, loss is 0.01367129199206829\n",
      "epoch: 3 step: 1189, loss is 0.03288336843252182\n",
      "epoch: 3 step: 1190, loss is 0.38072142004966736\n",
      "epoch: 3 step: 1191, loss is 0.035456638783216476\n",
      "epoch: 3 step: 1192, loss is 0.004314310848712921\n",
      "epoch: 3 step: 1193, loss is 0.10341551154851913\n",
      "epoch: 3 step: 1194, loss is 0.06788258999586105\n",
      "epoch: 3 step: 1195, loss is 0.0060135130770504475\n",
      "epoch: 3 step: 1196, loss is 0.0011397764319553971\n",
      "epoch: 3 step: 1197, loss is 0.005002871621400118\n",
      "epoch: 3 step: 1198, loss is 0.01581609807908535\n",
      "epoch: 3 step: 1199, loss is 0.017503904178738594\n",
      "epoch: 3 step: 1200, loss is 0.0028299030382186174\n",
      "epoch: 3 step: 1201, loss is 0.0009323220583610237\n",
      "epoch: 3 step: 1202, loss is 0.02360391803085804\n",
      "epoch: 3 step: 1203, loss is 0.00042004763963632286\n",
      "epoch: 3 step: 1204, loss is 0.03974170237779617\n",
      "epoch: 3 step: 1205, loss is 0.05031529814004898\n",
      "epoch: 3 step: 1206, loss is 0.007938394322991371\n",
      "epoch: 3 step: 1207, loss is 0.0012479368597269058\n",
      "epoch: 3 step: 1208, loss is 0.1933935433626175\n",
      "epoch: 3 step: 1209, loss is 0.09045764803886414\n",
      "epoch: 3 step: 1210, loss is 0.11092528700828552\n",
      "epoch: 3 step: 1211, loss is 0.0034502327907830477\n",
      "epoch: 3 step: 1212, loss is 0.00525418296456337\n",
      "epoch: 3 step: 1213, loss is 0.005155810620635748\n",
      "epoch: 3 step: 1214, loss is 0.0998028814792633\n",
      "epoch: 3 step: 1215, loss is 0.09850380569696426\n",
      "epoch: 3 step: 1216, loss is 0.04756615683436394\n",
      "epoch: 3 step: 1217, loss is 0.03659021481871605\n",
      "epoch: 3 step: 1218, loss is 0.14060895144939423\n",
      "epoch: 3 step: 1219, loss is 0.005077781621366739\n",
      "epoch: 3 step: 1220, loss is 0.2903778851032257\n",
      "epoch: 3 step: 1221, loss is 0.0008210222003981471\n",
      "epoch: 3 step: 1222, loss is 0.01211183238774538\n",
      "epoch: 3 step: 1223, loss is 0.085164375603199\n",
      "epoch: 3 step: 1224, loss is 0.00199583126232028\n",
      "epoch: 3 step: 1225, loss is 0.005046983249485493\n",
      "epoch: 3 step: 1226, loss is 0.06706538796424866\n",
      "epoch: 3 step: 1227, loss is 0.0023890326265245676\n",
      "epoch: 3 step: 1228, loss is 0.0077627054415643215\n",
      "epoch: 3 step: 1229, loss is 0.0007256184471771121\n",
      "epoch: 3 step: 1230, loss is 0.0198145043104887\n",
      "epoch: 3 step: 1231, loss is 0.08134739100933075\n",
      "epoch: 3 step: 1232, loss is 0.05161624029278755\n",
      "epoch: 3 step: 1233, loss is 0.010894197970628738\n",
      "epoch: 3 step: 1234, loss is 0.0012619460467249155\n",
      "epoch: 3 step: 1235, loss is 0.28888243436813354\n",
      "epoch: 3 step: 1236, loss is 0.012516978196799755\n",
      "epoch: 3 step: 1237, loss is 0.030509354546666145\n",
      "epoch: 3 step: 1238, loss is 0.016906019300222397\n",
      "epoch: 3 step: 1239, loss is 0.18434084951877594\n",
      "epoch: 3 step: 1240, loss is 0.012892444618046284\n",
      "epoch: 3 step: 1241, loss is 0.004975075367838144\n",
      "epoch: 3 step: 1242, loss is 0.03215182200074196\n",
      "epoch: 3 step: 1243, loss is 0.007228294853121042\n",
      "epoch: 3 step: 1244, loss is 0.003621045034378767\n",
      "epoch: 3 step: 1245, loss is 0.10137733817100525\n",
      "epoch: 3 step: 1246, loss is 0.018934518098831177\n",
      "epoch: 3 step: 1247, loss is 0.006846165284514427\n",
      "epoch: 3 step: 1248, loss is 0.034676797688007355\n",
      "epoch: 3 step: 1249, loss is 0.0052350834012031555\n",
      "epoch: 3 step: 1250, loss is 0.02240440994501114\n",
      "epoch: 3 step: 1251, loss is 0.004126810934394598\n",
      "epoch: 3 step: 1252, loss is 0.007815956138074398\n",
      "epoch: 3 step: 1253, loss is 0.0016324648167937994\n",
      "epoch: 3 step: 1254, loss is 0.018127286806702614\n",
      "epoch: 3 step: 1255, loss is 0.16899223625659943\n",
      "epoch: 3 step: 1256, loss is 0.10003809630870819\n",
      "epoch: 3 step: 1257, loss is 0.021540658548474312\n",
      "epoch: 3 step: 1258, loss is 0.05559436231851578\n",
      "epoch: 3 step: 1259, loss is 0.04367212951183319\n",
      "epoch: 3 step: 1260, loss is 0.3226171135902405\n",
      "epoch: 3 step: 1261, loss is 0.007137664128094912\n",
      "epoch: 3 step: 1262, loss is 0.010847420431673527\n",
      "epoch: 3 step: 1263, loss is 0.02078014798462391\n",
      "epoch: 3 step: 1264, loss is 0.25741150975227356\n",
      "epoch: 3 step: 1265, loss is 0.02037046290934086\n",
      "epoch: 3 step: 1266, loss is 0.1379556804895401\n",
      "epoch: 3 step: 1267, loss is 0.009042037650942802\n",
      "epoch: 3 step: 1268, loss is 0.007123060990124941\n",
      "epoch: 3 step: 1269, loss is 0.003966117277741432\n",
      "epoch: 3 step: 1270, loss is 0.0770443007349968\n",
      "epoch: 3 step: 1271, loss is 0.043231599032878876\n",
      "epoch: 3 step: 1272, loss is 0.005492222961038351\n",
      "epoch: 3 step: 1273, loss is 0.03466352820396423\n",
      "epoch: 3 step: 1274, loss is 0.050948504358530045\n",
      "epoch: 3 step: 1275, loss is 0.007171469274908304\n",
      "epoch: 3 step: 1276, loss is 0.003290023421868682\n",
      "epoch: 3 step: 1277, loss is 0.1147511675953865\n",
      "epoch: 3 step: 1278, loss is 0.008584667928516865\n",
      "epoch: 3 step: 1279, loss is 0.002690303837880492\n",
      "epoch: 3 step: 1280, loss is 0.026554642245173454\n",
      "epoch: 3 step: 1281, loss is 0.15509727597236633\n",
      "epoch: 3 step: 1282, loss is 0.10633726418018341\n",
      "epoch: 3 step: 1283, loss is 0.002006182912737131\n",
      "epoch: 3 step: 1284, loss is 0.0010901177302002907\n",
      "epoch: 3 step: 1285, loss is 0.008024563081562519\n",
      "epoch: 3 step: 1286, loss is 0.0016870907275006175\n",
      "epoch: 3 step: 1287, loss is 0.10341387242078781\n",
      "epoch: 3 step: 1288, loss is 0.014056901447474957\n",
      "epoch: 3 step: 1289, loss is 0.052521154284477234\n",
      "epoch: 3 step: 1290, loss is 0.4114169776439667\n",
      "epoch: 3 step: 1291, loss is 0.01984318532049656\n",
      "epoch: 3 step: 1292, loss is 0.0010569782461971045\n",
      "epoch: 3 step: 1293, loss is 0.026212748140096664\n",
      "epoch: 3 step: 1294, loss is 0.0182527806609869\n",
      "epoch: 3 step: 1295, loss is 0.007714393083006144\n",
      "epoch: 3 step: 1296, loss is 0.12520767748355865\n",
      "epoch: 3 step: 1297, loss is 0.0033505773171782494\n",
      "epoch: 3 step: 1298, loss is 0.00253682816401124\n",
      "epoch: 3 step: 1299, loss is 0.1274624466896057\n",
      "epoch: 3 step: 1300, loss is 0.002188915153965354\n",
      "epoch: 3 step: 1301, loss is 0.02635793387889862\n",
      "epoch: 3 step: 1302, loss is 0.012414935044944286\n",
      "epoch: 3 step: 1303, loss is 0.025873666629195213\n",
      "epoch: 3 step: 1304, loss is 0.006893417332321405\n",
      "epoch: 3 step: 1305, loss is 0.09739048779010773\n",
      "epoch: 3 step: 1306, loss is 0.006356262136250734\n",
      "epoch: 3 step: 1307, loss is 0.000821890658698976\n",
      "epoch: 3 step: 1308, loss is 0.003174815559759736\n",
      "epoch: 3 step: 1309, loss is 0.004784409422427416\n",
      "epoch: 3 step: 1310, loss is 0.10684394091367722\n",
      "epoch: 3 step: 1311, loss is 0.002743801102042198\n",
      "epoch: 3 step: 1312, loss is 0.025367973372340202\n",
      "epoch: 3 step: 1313, loss is 0.010884477756917477\n",
      "epoch: 3 step: 1314, loss is 0.02367328107357025\n",
      "epoch: 3 step: 1315, loss is 0.09031976014375687\n",
      "epoch: 3 step: 1316, loss is 0.0008636199054308236\n",
      "epoch: 3 step: 1317, loss is 0.010597149841487408\n",
      "epoch: 3 step: 1318, loss is 0.0991722047328949\n",
      "epoch: 3 step: 1319, loss is 0.006265230476856232\n",
      "epoch: 3 step: 1320, loss is 0.001962585374712944\n",
      "epoch: 3 step: 1321, loss is 0.2565368711948395\n",
      "epoch: 3 step: 1322, loss is 0.07890129089355469\n",
      "epoch: 3 step: 1323, loss is 0.004329456947743893\n",
      "epoch: 3 step: 1324, loss is 0.061673976480960846\n",
      "epoch: 3 step: 1325, loss is 0.02503594569861889\n",
      "epoch: 3 step: 1326, loss is 0.0015250807628035545\n",
      "epoch: 3 step: 1327, loss is 0.08125950396060944\n",
      "epoch: 3 step: 1328, loss is 0.047417934983968735\n",
      "epoch: 3 step: 1329, loss is 0.15078569948673248\n",
      "epoch: 3 step: 1330, loss is 0.007195084821432829\n",
      "epoch: 3 step: 1331, loss is 0.013836933299899101\n",
      "epoch: 3 step: 1332, loss is 0.07709167152643204\n",
      "epoch: 3 step: 1333, loss is 0.08835076540708542\n",
      "epoch: 3 step: 1334, loss is 0.014986416324973106\n",
      "epoch: 3 step: 1335, loss is 0.014346116222441196\n",
      "epoch: 3 step: 1336, loss is 0.21909283101558685\n",
      "epoch: 3 step: 1337, loss is 0.057057566940784454\n",
      "epoch: 3 step: 1338, loss is 0.026898257434368134\n",
      "epoch: 3 step: 1339, loss is 0.011443127878010273\n",
      "epoch: 3 step: 1340, loss is 0.0009120801114477217\n",
      "epoch: 3 step: 1341, loss is 0.24709011614322662\n",
      "epoch: 3 step: 1342, loss is 0.026868034154176712\n",
      "epoch: 3 step: 1343, loss is 0.023168515413999557\n",
      "epoch: 3 step: 1344, loss is 0.1191941350698471\n",
      "epoch: 3 step: 1345, loss is 0.025888770818710327\n",
      "epoch: 3 step: 1346, loss is 0.11088009178638458\n",
      "epoch: 3 step: 1347, loss is 0.12672816216945648\n",
      "epoch: 3 step: 1348, loss is 0.004823741968721151\n",
      "epoch: 3 step: 1349, loss is 0.0024517192505300045\n",
      "epoch: 3 step: 1350, loss is 0.0016897442983463407\n",
      "epoch: 3 step: 1351, loss is 0.004273300990462303\n",
      "epoch: 3 step: 1352, loss is 0.33180996775627136\n",
      "epoch: 3 step: 1353, loss is 0.08891608566045761\n",
      "epoch: 3 step: 1354, loss is 0.12301158905029297\n",
      "epoch: 3 step: 1355, loss is 0.067858025431633\n",
      "epoch: 3 step: 1356, loss is 0.011928283609449863\n",
      "epoch: 3 step: 1357, loss is 0.007095557637512684\n",
      "epoch: 3 step: 1358, loss is 0.007179583422839642\n",
      "epoch: 3 step: 1359, loss is 0.024649543687701225\n",
      "epoch: 3 step: 1360, loss is 0.05213027447462082\n",
      "epoch: 3 step: 1361, loss is 0.03980303928256035\n",
      "epoch: 3 step: 1362, loss is 0.014748952351510525\n",
      "epoch: 3 step: 1363, loss is 0.018320441246032715\n",
      "epoch: 3 step: 1364, loss is 0.014261828735470772\n",
      "epoch: 3 step: 1365, loss is 0.11034680902957916\n",
      "epoch: 3 step: 1366, loss is 0.09798748046159744\n",
      "epoch: 3 step: 1367, loss is 0.012508093379437923\n",
      "epoch: 3 step: 1368, loss is 0.05593005195260048\n",
      "epoch: 3 step: 1369, loss is 0.03712042048573494\n",
      "epoch: 3 step: 1370, loss is 0.017514728009700775\n",
      "epoch: 3 step: 1371, loss is 0.09504693746566772\n",
      "epoch: 3 step: 1372, loss is 0.04230925813317299\n",
      "epoch: 3 step: 1373, loss is 0.023752886801958084\n",
      "epoch: 3 step: 1374, loss is 0.0008192273671738803\n",
      "epoch: 3 step: 1375, loss is 0.03741209954023361\n",
      "epoch: 3 step: 1376, loss is 0.0751480981707573\n",
      "epoch: 3 step: 1377, loss is 0.11179175227880478\n",
      "epoch: 3 step: 1378, loss is 0.007105570286512375\n",
      "epoch: 3 step: 1379, loss is 0.009884600527584553\n",
      "epoch: 3 step: 1380, loss is 0.003120096866041422\n",
      "epoch: 3 step: 1381, loss is 0.13070650398731232\n",
      "epoch: 3 step: 1382, loss is 0.0033302963711321354\n",
      "epoch: 3 step: 1383, loss is 0.06356121599674225\n",
      "epoch: 3 step: 1384, loss is 0.029623687267303467\n",
      "epoch: 3 step: 1385, loss is 0.02468523569405079\n",
      "epoch: 3 step: 1386, loss is 0.03125825151801109\n",
      "epoch: 3 step: 1387, loss is 0.007626053411513567\n",
      "epoch: 3 step: 1388, loss is 0.0022674959618598223\n",
      "epoch: 3 step: 1389, loss is 0.0021351969335228205\n",
      "epoch: 3 step: 1390, loss is 0.13004186749458313\n",
      "epoch: 3 step: 1391, loss is 0.244826540350914\n",
      "epoch: 3 step: 1392, loss is 0.002386254258453846\n",
      "epoch: 3 step: 1393, loss is 0.0379655659198761\n",
      "epoch: 3 step: 1394, loss is 0.10663899034261703\n",
      "epoch: 3 step: 1395, loss is 0.022794166579842567\n",
      "epoch: 3 step: 1396, loss is 0.048562899231910706\n",
      "epoch: 3 step: 1397, loss is 0.016712341457605362\n",
      "epoch: 3 step: 1398, loss is 0.00696187699213624\n",
      "epoch: 3 step: 1399, loss is 0.002647802699357271\n",
      "epoch: 3 step: 1400, loss is 0.0007068613194860518\n",
      "epoch: 3 step: 1401, loss is 0.1592554897069931\n",
      "epoch: 3 step: 1402, loss is 0.24003085494041443\n",
      "epoch: 3 step: 1403, loss is 0.01649155654013157\n",
      "epoch: 3 step: 1404, loss is 0.007135195191949606\n",
      "epoch: 3 step: 1405, loss is 0.0016860011965036392\n",
      "epoch: 3 step: 1406, loss is 0.0010291573125869036\n",
      "epoch: 3 step: 1407, loss is 0.03149527311325073\n",
      "epoch: 3 step: 1408, loss is 0.023745466023683548\n",
      "epoch: 3 step: 1409, loss is 0.17072607576847076\n",
      "epoch: 3 step: 1410, loss is 0.005582435522228479\n",
      "epoch: 3 step: 1411, loss is 0.013150908052921295\n",
      "epoch: 3 step: 1412, loss is 0.008009525015950203\n",
      "epoch: 3 step: 1413, loss is 0.017252879217267036\n",
      "epoch: 3 step: 1414, loss is 0.028671594336628914\n",
      "epoch: 3 step: 1415, loss is 0.28717777132987976\n",
      "epoch: 3 step: 1416, loss is 0.0007116651395335793\n",
      "epoch: 3 step: 1417, loss is 0.12022677063941956\n",
      "epoch: 3 step: 1418, loss is 0.010923564434051514\n",
      "epoch: 3 step: 1419, loss is 0.0005686669028364122\n",
      "epoch: 3 step: 1420, loss is 0.0015331240138038993\n",
      "epoch: 3 step: 1421, loss is 0.003379969857633114\n",
      "epoch: 3 step: 1422, loss is 0.01386906486004591\n",
      "epoch: 3 step: 1423, loss is 0.037886831909418106\n",
      "epoch: 3 step: 1424, loss is 0.011151807382702827\n",
      "epoch: 3 step: 1425, loss is 0.003426377894356847\n",
      "epoch: 3 step: 1426, loss is 0.006872509140521288\n",
      "epoch: 3 step: 1427, loss is 0.03162224218249321\n",
      "epoch: 3 step: 1428, loss is 0.007059508003294468\n",
      "epoch: 3 step: 1429, loss is 0.01583070680499077\n",
      "epoch: 3 step: 1430, loss is 0.02068309672176838\n",
      "epoch: 3 step: 1431, loss is 0.04736609384417534\n",
      "epoch: 3 step: 1432, loss is 0.03643116354942322\n",
      "epoch: 3 step: 1433, loss is 0.030566971749067307\n",
      "epoch: 3 step: 1434, loss is 0.00267742108553648\n",
      "epoch: 3 step: 1435, loss is 0.011248450726270676\n",
      "epoch: 3 step: 1436, loss is 0.021650701761245728\n",
      "epoch: 3 step: 1437, loss is 0.0019483675714582205\n",
      "epoch: 3 step: 1438, loss is 0.11648564040660858\n",
      "epoch: 3 step: 1439, loss is 0.015218177810311317\n",
      "epoch: 3 step: 1440, loss is 0.040123581886291504\n",
      "epoch: 3 step: 1441, loss is 0.0028113063890486956\n",
      "epoch: 3 step: 1442, loss is 0.07313694059848785\n",
      "epoch: 3 step: 1443, loss is 0.14905793964862823\n",
      "epoch: 3 step: 1444, loss is 0.15620875358581543\n",
      "epoch: 3 step: 1445, loss is 0.002743898192420602\n",
      "epoch: 3 step: 1446, loss is 0.054487090557813644\n",
      "epoch: 3 step: 1447, loss is 0.00432506063953042\n",
      "epoch: 3 step: 1448, loss is 0.013069706037640572\n",
      "epoch: 3 step: 1449, loss is 0.06347592920064926\n",
      "epoch: 3 step: 1450, loss is 0.0906841978430748\n",
      "epoch: 3 step: 1451, loss is 0.005716242827475071\n",
      "epoch: 3 step: 1452, loss is 0.043610088527202606\n",
      "epoch: 3 step: 1453, loss is 0.011313400231301785\n",
      "epoch: 3 step: 1454, loss is 0.011454877443611622\n",
      "epoch: 3 step: 1455, loss is 0.006763020996004343\n",
      "epoch: 3 step: 1456, loss is 0.004174911417067051\n",
      "epoch: 3 step: 1457, loss is 0.14070414006710052\n",
      "epoch: 3 step: 1458, loss is 0.038391660898923874\n",
      "epoch: 3 step: 1459, loss is 0.0018763316329568624\n",
      "epoch: 3 step: 1460, loss is 0.014376307837665081\n",
      "epoch: 3 step: 1461, loss is 0.012943162582814693\n",
      "epoch: 3 step: 1462, loss is 0.006695879623293877\n",
      "epoch: 3 step: 1463, loss is 0.03492993488907814\n",
      "epoch: 3 step: 1464, loss is 0.05644369497895241\n",
      "epoch: 3 step: 1465, loss is 0.006364016328006983\n",
      "epoch: 3 step: 1466, loss is 0.0029001932125538588\n",
      "epoch: 3 step: 1467, loss is 0.0205537136644125\n",
      "epoch: 3 step: 1468, loss is 0.0024596743751317263\n",
      "epoch: 3 step: 1469, loss is 0.000670609821099788\n",
      "epoch: 3 step: 1470, loss is 0.07921731472015381\n",
      "epoch: 3 step: 1471, loss is 0.10970144718885422\n",
      "epoch: 3 step: 1472, loss is 0.013050304725766182\n",
      "epoch: 3 step: 1473, loss is 0.0006342533743008971\n",
      "epoch: 3 step: 1474, loss is 0.0973304808139801\n",
      "epoch: 3 step: 1475, loss is 0.01909838244318962\n",
      "epoch: 3 step: 1476, loss is 0.004394764080643654\n",
      "epoch: 3 step: 1477, loss is 0.19027955830097198\n",
      "epoch: 3 step: 1478, loss is 0.0010374467819929123\n",
      "epoch: 3 step: 1479, loss is 0.0029099355451762676\n",
      "epoch: 3 step: 1480, loss is 0.035430215299129486\n",
      "epoch: 3 step: 1481, loss is 0.003615944879129529\n",
      "epoch: 3 step: 1482, loss is 0.033540189266204834\n",
      "epoch: 3 step: 1483, loss is 0.08832106739282608\n",
      "epoch: 3 step: 1484, loss is 0.00228800973854959\n",
      "epoch: 3 step: 1485, loss is 0.14786843955516815\n",
      "epoch: 3 step: 1486, loss is 0.1266210675239563\n",
      "epoch: 3 step: 1487, loss is 0.20048174262046814\n",
      "epoch: 3 step: 1488, loss is 0.005126800388097763\n",
      "epoch: 3 step: 1489, loss is 0.017143551260232925\n",
      "epoch: 3 step: 1490, loss is 0.006972868926823139\n",
      "epoch: 3 step: 1491, loss is 0.021680433303117752\n",
      "epoch: 3 step: 1492, loss is 0.015949105843901634\n",
      "epoch: 3 step: 1493, loss is 0.01760699599981308\n",
      "epoch: 3 step: 1494, loss is 0.29191797971725464\n",
      "epoch: 3 step: 1495, loss is 0.0016792913665995002\n",
      "epoch: 3 step: 1496, loss is 0.007499746046960354\n",
      "epoch: 3 step: 1497, loss is 0.006608919240534306\n",
      "epoch: 3 step: 1498, loss is 0.13976244628429413\n",
      "epoch: 3 step: 1499, loss is 0.22537751495838165\n",
      "epoch: 3 step: 1500, loss is 0.027343586087226868\n",
      "epoch: 3 step: 1501, loss is 0.22843238711357117\n",
      "epoch: 3 step: 1502, loss is 0.004528045654296875\n",
      "epoch: 3 step: 1503, loss is 0.010820714756846428\n",
      "epoch: 3 step: 1504, loss is 0.01950233243405819\n",
      "epoch: 3 step: 1505, loss is 0.17100553214550018\n",
      "epoch: 3 step: 1506, loss is 0.03526487946510315\n",
      "epoch: 3 step: 1507, loss is 0.2519949674606323\n",
      "epoch: 3 step: 1508, loss is 0.12605994939804077\n",
      "epoch: 3 step: 1509, loss is 0.0026326687075197697\n",
      "epoch: 3 step: 1510, loss is 0.014918933622539043\n",
      "epoch: 3 step: 1511, loss is 0.005647861864417791\n",
      "epoch: 3 step: 1512, loss is 0.12726321816444397\n",
      "epoch: 3 step: 1513, loss is 0.0750618502497673\n",
      "epoch: 3 step: 1514, loss is 0.01755281537771225\n",
      "epoch: 3 step: 1515, loss is 0.009775523096323013\n",
      "epoch: 3 step: 1516, loss is 0.029964666813611984\n",
      "epoch: 3 step: 1517, loss is 0.014843427576124668\n",
      "epoch: 3 step: 1518, loss is 0.0048921252600848675\n",
      "epoch: 3 step: 1519, loss is 0.03142867609858513\n",
      "epoch: 3 step: 1520, loss is 0.05745925009250641\n",
      "epoch: 3 step: 1521, loss is 0.12743385136127472\n",
      "epoch: 3 step: 1522, loss is 0.0025638651568442583\n",
      "epoch: 3 step: 1523, loss is 0.005047863814979792\n",
      "epoch: 3 step: 1524, loss is 0.0037750008050352335\n",
      "epoch: 3 step: 1525, loss is 0.07545103132724762\n",
      "epoch: 3 step: 1526, loss is 0.1810614913702011\n",
      "epoch: 3 step: 1527, loss is 0.020114991813898087\n",
      "epoch: 3 step: 1528, loss is 0.026173263788223267\n",
      "epoch: 3 step: 1529, loss is 0.10133734345436096\n",
      "epoch: 3 step: 1530, loss is 0.022492388263344765\n",
      "epoch: 3 step: 1531, loss is 0.018695149570703506\n",
      "epoch: 3 step: 1532, loss is 0.013656740076839924\n",
      "epoch: 3 step: 1533, loss is 0.008927546441555023\n",
      "epoch: 3 step: 1534, loss is 0.022875122725963593\n",
      "epoch: 3 step: 1535, loss is 0.14653481543064117\n",
      "epoch: 3 step: 1536, loss is 0.009616950526833534\n",
      "epoch: 3 step: 1537, loss is 0.0044937944039702415\n",
      "epoch: 3 step: 1538, loss is 0.17254038155078888\n",
      "epoch: 3 step: 1539, loss is 0.022550061345100403\n",
      "epoch: 3 step: 1540, loss is 0.12644152343273163\n",
      "epoch: 3 step: 1541, loss is 0.21541255712509155\n",
      "epoch: 3 step: 1542, loss is 0.05635529011487961\n",
      "epoch: 3 step: 1543, loss is 0.03191336244344711\n",
      "epoch: 3 step: 1544, loss is 0.004015493672341108\n",
      "epoch: 3 step: 1545, loss is 0.012123391963541508\n",
      "epoch: 3 step: 1546, loss is 0.09786960482597351\n",
      "epoch: 3 step: 1547, loss is 0.09227224439382553\n",
      "epoch: 3 step: 1548, loss is 0.026126720011234283\n",
      "epoch: 3 step: 1549, loss is 0.009680109098553658\n",
      "epoch: 3 step: 1550, loss is 0.05034028738737106\n",
      "epoch: 3 step: 1551, loss is 0.001263023354113102\n",
      "epoch: 3 step: 1552, loss is 0.07885892689228058\n",
      "epoch: 3 step: 1553, loss is 0.003746583592146635\n",
      "epoch: 3 step: 1554, loss is 0.009490746073424816\n",
      "epoch: 3 step: 1555, loss is 0.04637729004025459\n",
      "epoch: 3 step: 1556, loss is 0.06215750426054001\n",
      "epoch: 3 step: 1557, loss is 0.11939748376607895\n",
      "epoch: 3 step: 1558, loss is 0.010549189522862434\n",
      "epoch: 3 step: 1559, loss is 0.06744121015071869\n",
      "epoch: 3 step: 1560, loss is 0.005629139486700296\n",
      "epoch: 3 step: 1561, loss is 0.1526256799697876\n",
      "epoch: 3 step: 1562, loss is 0.004521884489804506\n",
      "epoch: 3 step: 1563, loss is 0.010670927353203297\n",
      "epoch: 3 step: 1564, loss is 0.0017369124107062817\n",
      "epoch: 3 step: 1565, loss is 0.11410122364759445\n",
      "epoch: 3 step: 1566, loss is 0.024240393191576004\n",
      "epoch: 3 step: 1567, loss is 0.01608118787407875\n",
      "epoch: 3 step: 1568, loss is 0.012564579024910927\n",
      "epoch: 3 step: 1569, loss is 0.09237965196371078\n",
      "epoch: 3 step: 1570, loss is 0.006362197455018759\n",
      "epoch: 3 step: 1571, loss is 0.07264962792396545\n",
      "epoch: 3 step: 1572, loss is 0.00209615845233202\n",
      "epoch: 3 step: 1573, loss is 0.007810971233993769\n",
      "epoch: 3 step: 1574, loss is 0.005246823187917471\n",
      "epoch: 3 step: 1575, loss is 0.0003234946052543819\n",
      "epoch: 3 step: 1576, loss is 0.0026519326493144035\n",
      "epoch: 3 step: 1577, loss is 0.04519373178482056\n",
      "epoch: 3 step: 1578, loss is 0.027657069265842438\n",
      "epoch: 3 step: 1579, loss is 0.01824910007417202\n",
      "epoch: 3 step: 1580, loss is 0.042491473257541656\n",
      "epoch: 3 step: 1581, loss is 0.02282853238284588\n",
      "epoch: 3 step: 1582, loss is 0.3417515158653259\n",
      "epoch: 3 step: 1583, loss is 0.0036336889024823904\n",
      "epoch: 3 step: 1584, loss is 0.08998358994722366\n",
      "epoch: 3 step: 1585, loss is 0.002562316134572029\n",
      "epoch: 3 step: 1586, loss is 0.16129672527313232\n",
      "epoch: 3 step: 1587, loss is 0.0056416792795062065\n",
      "epoch: 3 step: 1588, loss is 0.06263613700866699\n",
      "epoch: 3 step: 1589, loss is 0.019247613847255707\n",
      "epoch: 3 step: 1590, loss is 0.011725811287760735\n",
      "epoch: 3 step: 1591, loss is 0.002705668332055211\n",
      "epoch: 3 step: 1592, loss is 0.008935858495533466\n",
      "epoch: 3 step: 1593, loss is 0.1050049215555191\n",
      "epoch: 3 step: 1594, loss is 0.012768328189849854\n",
      "epoch: 3 step: 1595, loss is 0.009926817379891872\n",
      "epoch: 3 step: 1596, loss is 0.001165737514384091\n",
      "epoch: 3 step: 1597, loss is 0.11701058596372604\n",
      "epoch: 3 step: 1598, loss is 0.21717087924480438\n",
      "epoch: 3 step: 1599, loss is 0.0004870736738666892\n",
      "epoch: 3 step: 1600, loss is 0.010691408067941666\n",
      "epoch: 3 step: 1601, loss is 0.06773893535137177\n",
      "epoch: 3 step: 1602, loss is 0.11751118302345276\n",
      "epoch: 3 step: 1603, loss is 0.009246598929166794\n",
      "epoch: 3 step: 1604, loss is 0.02494664117693901\n",
      "epoch: 3 step: 1605, loss is 0.0031833858229219913\n",
      "epoch: 3 step: 1606, loss is 0.03931836783885956\n",
      "epoch: 3 step: 1607, loss is 0.03167084977030754\n",
      "epoch: 3 step: 1608, loss is 0.004642502870410681\n",
      "epoch: 3 step: 1609, loss is 0.0011524262372404337\n",
      "epoch: 3 step: 1610, loss is 0.02620147541165352\n",
      "epoch: 3 step: 1611, loss is 0.15547645092010498\n",
      "epoch: 3 step: 1612, loss is 0.00436402577906847\n",
      "epoch: 3 step: 1613, loss is 0.048554807901382446\n",
      "epoch: 3 step: 1614, loss is 0.0038003819063305855\n",
      "epoch: 3 step: 1615, loss is 0.005762311164289713\n",
      "epoch: 3 step: 1616, loss is 0.03696531802415848\n",
      "epoch: 3 step: 1617, loss is 0.21885159611701965\n",
      "epoch: 3 step: 1618, loss is 0.00735338032245636\n",
      "epoch: 3 step: 1619, loss is 0.046471916139125824\n",
      "epoch: 3 step: 1620, loss is 0.012294339016079903\n",
      "epoch: 3 step: 1621, loss is 0.018528195098042488\n",
      "epoch: 3 step: 1622, loss is 0.032887622714042664\n",
      "epoch: 3 step: 1623, loss is 0.005674508400261402\n",
      "epoch: 3 step: 1624, loss is 0.0035695878323167562\n",
      "epoch: 3 step: 1625, loss is 0.05765819177031517\n",
      "epoch: 3 step: 1626, loss is 0.06624644994735718\n",
      "epoch: 3 step: 1627, loss is 0.18129155039787292\n",
      "epoch: 3 step: 1628, loss is 0.039820555597543716\n",
      "epoch: 3 step: 1629, loss is 0.023370636627078056\n",
      "epoch: 3 step: 1630, loss is 0.19756628572940826\n",
      "epoch: 3 step: 1631, loss is 0.012188194319605827\n",
      "epoch: 3 step: 1632, loss is 0.004260230343788862\n",
      "epoch: 3 step: 1633, loss is 0.0056284223683178425\n",
      "epoch: 3 step: 1634, loss is 0.010659205727279186\n",
      "epoch: 3 step: 1635, loss is 0.029776060953736305\n",
      "epoch: 3 step: 1636, loss is 0.20876650512218475\n",
      "epoch: 3 step: 1637, loss is 0.010265907272696495\n",
      "epoch: 3 step: 1638, loss is 0.04502130299806595\n",
      "epoch: 3 step: 1639, loss is 0.08246982842683792\n",
      "epoch: 3 step: 1640, loss is 0.006948649417608976\n",
      "epoch: 3 step: 1641, loss is 0.005379405338317156\n",
      "epoch: 3 step: 1642, loss is 0.15212419629096985\n",
      "epoch: 3 step: 1643, loss is 0.007855735719203949\n",
      "epoch: 3 step: 1644, loss is 0.00536309415474534\n",
      "epoch: 3 step: 1645, loss is 0.004594797268509865\n",
      "epoch: 3 step: 1646, loss is 0.2663169503211975\n",
      "epoch: 3 step: 1647, loss is 0.08831089735031128\n",
      "epoch: 3 step: 1648, loss is 0.03384177014231682\n",
      "epoch: 3 step: 1649, loss is 0.03279943764209747\n",
      "epoch: 3 step: 1650, loss is 0.022427072748541832\n",
      "epoch: 3 step: 1651, loss is 0.020198902115225792\n",
      "epoch: 3 step: 1652, loss is 0.002082682680338621\n",
      "epoch: 3 step: 1653, loss is 0.029262755066156387\n",
      "epoch: 3 step: 1654, loss is 0.1656617820262909\n",
      "epoch: 3 step: 1655, loss is 0.006127289962023497\n",
      "epoch: 3 step: 1656, loss is 0.04087919369339943\n",
      "epoch: 3 step: 1657, loss is 0.007587729953229427\n",
      "epoch: 3 step: 1658, loss is 0.10540581494569778\n",
      "epoch: 3 step: 1659, loss is 0.01103570219129324\n",
      "epoch: 3 step: 1660, loss is 0.3014935553073883\n",
      "epoch: 3 step: 1661, loss is 0.015712786465883255\n",
      "epoch: 3 step: 1662, loss is 0.0038291863165795803\n",
      "epoch: 3 step: 1663, loss is 0.06331012398004532\n",
      "epoch: 3 step: 1664, loss is 0.011845549568533897\n",
      "epoch: 3 step: 1665, loss is 0.15355107188224792\n",
      "epoch: 3 step: 1666, loss is 0.04060922935605049\n",
      "epoch: 3 step: 1667, loss is 0.012020166032016277\n",
      "epoch: 3 step: 1668, loss is 0.052089907228946686\n",
      "epoch: 3 step: 1669, loss is 0.20523323118686676\n",
      "epoch: 3 step: 1670, loss is 0.018303096294403076\n",
      "epoch: 3 step: 1671, loss is 0.001993589336052537\n",
      "epoch: 3 step: 1672, loss is 0.03630897402763367\n",
      "epoch: 3 step: 1673, loss is 0.011565816588699818\n",
      "epoch: 3 step: 1674, loss is 0.03960269317030907\n",
      "epoch: 3 step: 1675, loss is 0.03424027934670448\n",
      "epoch: 3 step: 1676, loss is 0.0974302813410759\n",
      "epoch: 3 step: 1677, loss is 0.014504654332995415\n",
      "epoch: 3 step: 1678, loss is 0.08168654888868332\n",
      "epoch: 3 step: 1679, loss is 0.13496530055999756\n",
      "epoch: 3 step: 1680, loss is 0.0024062800221145153\n",
      "epoch: 3 step: 1681, loss is 0.0029856429900974035\n",
      "epoch: 3 step: 1682, loss is 0.058200690895318985\n",
      "epoch: 3 step: 1683, loss is 0.08771024644374847\n",
      "epoch: 3 step: 1684, loss is 0.006306100636720657\n",
      "epoch: 3 step: 1685, loss is 0.008902356028556824\n",
      "epoch: 3 step: 1686, loss is 0.11685707420110703\n",
      "epoch: 3 step: 1687, loss is 0.11509332060813904\n",
      "epoch: 3 step: 1688, loss is 0.012281825765967369\n",
      "epoch: 3 step: 1689, loss is 0.0011920322431251407\n",
      "epoch: 3 step: 1690, loss is 0.0059443083591759205\n",
      "epoch: 3 step: 1691, loss is 0.1117280051112175\n",
      "epoch: 3 step: 1692, loss is 0.004600939340889454\n",
      "epoch: 3 step: 1693, loss is 0.026282718405127525\n",
      "epoch: 3 step: 1694, loss is 0.037281595170497894\n",
      "epoch: 3 step: 1695, loss is 0.05794106423854828\n",
      "epoch: 3 step: 1696, loss is 0.15870724618434906\n",
      "epoch: 3 step: 1697, loss is 0.25427597761154175\n",
      "epoch: 3 step: 1698, loss is 0.020716603845357895\n",
      "epoch: 3 step: 1699, loss is 0.03267316892743111\n",
      "epoch: 3 step: 1700, loss is 0.003515869379043579\n",
      "epoch: 3 step: 1701, loss is 0.06521566212177277\n",
      "epoch: 3 step: 1702, loss is 0.1839471310377121\n",
      "epoch: 3 step: 1703, loss is 0.06784231960773468\n",
      "epoch: 3 step: 1704, loss is 0.029139472171664238\n",
      "epoch: 3 step: 1705, loss is 0.011776545085012913\n",
      "epoch: 3 step: 1706, loss is 0.22889824211597443\n",
      "epoch: 3 step: 1707, loss is 0.13740161061286926\n",
      "epoch: 3 step: 1708, loss is 0.2677406072616577\n",
      "epoch: 3 step: 1709, loss is 0.0018308956641703844\n",
      "epoch: 3 step: 1710, loss is 0.00899632927030325\n",
      "epoch: 3 step: 1711, loss is 0.010302764363586903\n",
      "epoch: 3 step: 1712, loss is 0.004896922968327999\n",
      "epoch: 3 step: 1713, loss is 0.06651521474123001\n",
      "epoch: 3 step: 1714, loss is 0.0017458063084632158\n",
      "epoch: 3 step: 1715, loss is 0.10887239128351212\n",
      "epoch: 3 step: 1716, loss is 0.01565849781036377\n",
      "epoch: 3 step: 1717, loss is 0.045409154146909714\n",
      "epoch: 3 step: 1718, loss is 0.048657480627298355\n",
      "epoch: 3 step: 1719, loss is 0.0216279998421669\n",
      "epoch: 3 step: 1720, loss is 0.0036856585647910833\n",
      "epoch: 3 step: 1721, loss is 0.04052850976586342\n",
      "epoch: 3 step: 1722, loss is 0.12956702709197998\n",
      "epoch: 3 step: 1723, loss is 0.13802874088287354\n",
      "epoch: 3 step: 1724, loss is 0.0036097047850489616\n",
      "epoch: 3 step: 1725, loss is 0.040381040424108505\n",
      "epoch: 3 step: 1726, loss is 0.08032196015119553\n",
      "epoch: 3 step: 1727, loss is 0.00783128198236227\n",
      "epoch: 3 step: 1728, loss is 0.01556712482124567\n",
      "epoch: 3 step: 1729, loss is 0.035612743347883224\n",
      "epoch: 3 step: 1730, loss is 0.008272520266473293\n",
      "epoch: 3 step: 1731, loss is 0.06258175522089005\n",
      "epoch: 3 step: 1732, loss is 0.00321637443266809\n",
      "epoch: 3 step: 1733, loss is 0.00046971728443168104\n",
      "epoch: 3 step: 1734, loss is 0.002853451296687126\n",
      "epoch: 3 step: 1735, loss is 0.016418687999248505\n",
      "epoch: 3 step: 1736, loss is 0.021655431017279625\n",
      "epoch: 3 step: 1737, loss is 0.0014091657940298319\n",
      "epoch: 3 step: 1738, loss is 0.004515127278864384\n",
      "epoch: 3 step: 1739, loss is 0.005625950638204813\n",
      "epoch: 3 step: 1740, loss is 0.010228009894490242\n",
      "epoch: 3 step: 1741, loss is 0.005675071384757757\n",
      "epoch: 3 step: 1742, loss is 0.010009922087192535\n",
      "epoch: 3 step: 1743, loss is 0.003929779399186373\n",
      "epoch: 3 step: 1744, loss is 0.0034134997986257076\n",
      "epoch: 3 step: 1745, loss is 0.01569395326077938\n",
      "epoch: 3 step: 1746, loss is 0.0923488512635231\n",
      "epoch: 3 step: 1747, loss is 0.005972809623926878\n",
      "epoch: 3 step: 1748, loss is 0.3166845738887787\n",
      "epoch: 3 step: 1749, loss is 0.04900788888335228\n",
      "epoch: 3 step: 1750, loss is 0.021559186279773712\n",
      "epoch: 3 step: 1751, loss is 0.0018568054074421525\n",
      "epoch: 3 step: 1752, loss is 0.0006766278529539704\n",
      "epoch: 3 step: 1753, loss is 0.12125471234321594\n",
      "epoch: 3 step: 1754, loss is 0.000893701973836869\n",
      "epoch: 3 step: 1755, loss is 0.0009457439882680774\n",
      "epoch: 3 step: 1756, loss is 0.0009196906466968358\n",
      "epoch: 3 step: 1757, loss is 0.014811056666076183\n",
      "epoch: 3 step: 1758, loss is 0.07838813960552216\n",
      "epoch: 3 step: 1759, loss is 0.12064012885093689\n",
      "epoch: 3 step: 1760, loss is 0.01153164729475975\n",
      "epoch: 3 step: 1761, loss is 0.07120172679424286\n",
      "epoch: 3 step: 1762, loss is 0.007547399029135704\n",
      "epoch: 3 step: 1763, loss is 0.19632801413536072\n",
      "epoch: 3 step: 1764, loss is 0.1880141943693161\n",
      "epoch: 3 step: 1765, loss is 0.04813946411013603\n",
      "epoch: 3 step: 1766, loss is 0.005668673198670149\n",
      "epoch: 3 step: 1767, loss is 0.022413210943341255\n",
      "epoch: 3 step: 1768, loss is 0.004532627761363983\n",
      "epoch: 3 step: 1769, loss is 0.015831127762794495\n",
      "epoch: 3 step: 1770, loss is 0.006280943751335144\n",
      "epoch: 3 step: 1771, loss is 0.02814142033457756\n",
      "epoch: 3 step: 1772, loss is 0.012347742915153503\n",
      "epoch: 3 step: 1773, loss is 0.0029137288220226765\n",
      "epoch: 3 step: 1774, loss is 0.018756095319986343\n",
      "epoch: 3 step: 1775, loss is 0.019106397405266762\n",
      "epoch: 3 step: 1776, loss is 0.003458282444626093\n",
      "epoch: 3 step: 1777, loss is 0.04716721177101135\n",
      "epoch: 3 step: 1778, loss is 0.03021826222538948\n",
      "epoch: 3 step: 1779, loss is 0.09307421743869781\n",
      "epoch: 3 step: 1780, loss is 0.0028629472944885492\n",
      "epoch: 3 step: 1781, loss is 0.012026477605104446\n",
      "epoch: 3 step: 1782, loss is 0.007143781520426273\n",
      "epoch: 3 step: 1783, loss is 0.0605100654065609\n",
      "epoch: 3 step: 1784, loss is 0.0005687314551323652\n",
      "epoch: 3 step: 1785, loss is 0.001048296457156539\n",
      "epoch: 3 step: 1786, loss is 0.003989429213106632\n",
      "epoch: 3 step: 1787, loss is 0.05668789520859718\n",
      "epoch: 3 step: 1788, loss is 0.00984680000692606\n",
      "epoch: 3 step: 1789, loss is 0.1946815401315689\n",
      "epoch: 3 step: 1790, loss is 0.01110707689076662\n",
      "epoch: 3 step: 1791, loss is 0.004400134552270174\n",
      "epoch: 3 step: 1792, loss is 0.021154889836907387\n",
      "epoch: 3 step: 1793, loss is 0.016727320849895477\n",
      "epoch: 3 step: 1794, loss is 0.06191013380885124\n",
      "epoch: 3 step: 1795, loss is 0.06636860966682434\n",
      "epoch: 3 step: 1796, loss is 0.011020299047231674\n",
      "epoch: 3 step: 1797, loss is 0.05344315245747566\n",
      "epoch: 3 step: 1798, loss is 0.007043621502816677\n",
      "epoch: 3 step: 1799, loss is 0.19865167140960693\n",
      "epoch: 3 step: 1800, loss is 0.003744112327694893\n",
      "epoch: 3 step: 1801, loss is 0.012446656823158264\n",
      "epoch: 3 step: 1802, loss is 0.0009099651360884309\n",
      "epoch: 3 step: 1803, loss is 0.013652942143380642\n",
      "epoch: 3 step: 1804, loss is 0.0725475624203682\n",
      "epoch: 3 step: 1805, loss is 0.00833427906036377\n",
      "epoch: 3 step: 1806, loss is 0.04128917306661606\n",
      "epoch: 3 step: 1807, loss is 0.0020937167573720217\n",
      "epoch: 3 step: 1808, loss is 0.002644547028467059\n",
      "epoch: 3 step: 1809, loss is 0.09949936717748642\n",
      "epoch: 3 step: 1810, loss is 0.0014871441526338458\n",
      "epoch: 3 step: 1811, loss is 0.009120028465986252\n",
      "epoch: 3 step: 1812, loss is 0.010777179151773453\n",
      "epoch: 3 step: 1813, loss is 0.008578531444072723\n",
      "epoch: 3 step: 1814, loss is 0.02334441989660263\n",
      "epoch: 3 step: 1815, loss is 0.06953589618206024\n",
      "epoch: 3 step: 1816, loss is 0.037130847573280334\n",
      "epoch: 3 step: 1817, loss is 0.3006143569946289\n",
      "epoch: 3 step: 1818, loss is 0.02695799618959427\n",
      "epoch: 3 step: 1819, loss is 0.0017853154568001628\n",
      "epoch: 3 step: 1820, loss is 0.010896778665482998\n",
      "epoch: 3 step: 1821, loss is 0.008005866780877113\n",
      "epoch: 3 step: 1822, loss is 0.16839417815208435\n",
      "epoch: 3 step: 1823, loss is 0.00670477282255888\n",
      "epoch: 3 step: 1824, loss is 0.01592624932527542\n",
      "epoch: 3 step: 1825, loss is 0.002552859950810671\n",
      "epoch: 3 step: 1826, loss is 0.00356586673296988\n",
      "epoch: 3 step: 1827, loss is 0.0746171623468399\n",
      "epoch: 3 step: 1828, loss is 0.013153520412743092\n",
      "epoch: 3 step: 1829, loss is 0.005115268286317587\n",
      "epoch: 3 step: 1830, loss is 0.0013012939598411322\n",
      "epoch: 3 step: 1831, loss is 0.01278904639184475\n",
      "epoch: 3 step: 1832, loss is 0.022357087582349777\n",
      "epoch: 3 step: 1833, loss is 0.10772557556629181\n",
      "epoch: 3 step: 1834, loss is 0.09041031450033188\n",
      "epoch: 3 step: 1835, loss is 0.004309322685003281\n",
      "epoch: 3 step: 1836, loss is 0.004583274479955435\n",
      "epoch: 3 step: 1837, loss is 0.03080720454454422\n",
      "epoch: 3 step: 1838, loss is 0.0043442463502287865\n",
      "epoch: 3 step: 1839, loss is 0.0014945614384487271\n",
      "epoch: 3 step: 1840, loss is 0.05110893398523331\n",
      "epoch: 3 step: 1841, loss is 0.011897544376552105\n",
      "epoch: 3 step: 1842, loss is 0.14591218531131744\n",
      "epoch: 3 step: 1843, loss is 0.07668084651231766\n",
      "epoch: 3 step: 1844, loss is 0.023047547787427902\n",
      "epoch: 3 step: 1845, loss is 0.01465552393347025\n",
      "epoch: 3 step: 1846, loss is 0.020941486582159996\n",
      "epoch: 3 step: 1847, loss is 0.0012507986975833774\n",
      "epoch: 3 step: 1848, loss is 0.005015682429075241\n",
      "epoch: 3 step: 1849, loss is 0.11793326586484909\n",
      "epoch: 3 step: 1850, loss is 0.35158050060272217\n",
      "epoch: 3 step: 1851, loss is 0.2579151391983032\n",
      "epoch: 3 step: 1852, loss is 0.017148060724139214\n",
      "epoch: 3 step: 1853, loss is 0.05539863184094429\n",
      "epoch: 3 step: 1854, loss is 0.20318175852298737\n",
      "epoch: 3 step: 1855, loss is 0.06265773624181747\n",
      "epoch: 3 step: 1856, loss is 0.007366325706243515\n",
      "epoch: 3 step: 1857, loss is 0.09906335175037384\n",
      "epoch: 3 step: 1858, loss is 0.0018757975194603205\n",
      "epoch: 3 step: 1859, loss is 0.050029456615448\n",
      "epoch: 3 step: 1860, loss is 0.0725666806101799\n",
      "epoch: 3 step: 1861, loss is 0.0009941651951521635\n",
      "epoch: 3 step: 1862, loss is 0.07670972496271133\n",
      "epoch: 3 step: 1863, loss is 0.018755905330181122\n",
      "epoch: 3 step: 1864, loss is 0.07191843539476395\n",
      "epoch: 3 step: 1865, loss is 0.02124042436480522\n",
      "epoch: 3 step: 1866, loss is 0.043957725167274475\n",
      "epoch: 3 step: 1867, loss is 0.015640517696738243\n",
      "epoch: 3 step: 1868, loss is 0.012891829013824463\n",
      "epoch: 3 step: 1869, loss is 0.026503155007958412\n",
      "epoch: 3 step: 1870, loss is 0.04449871927499771\n",
      "epoch: 3 step: 1871, loss is 0.09061207622289658\n",
      "epoch: 3 step: 1872, loss is 0.035874467343091965\n",
      "epoch: 3 step: 1873, loss is 0.006241414230316877\n",
      "epoch: 3 step: 1874, loss is 0.009391743689775467\n",
      "epoch: 3 step: 1875, loss is 0.03603249043226242\n",
      "epoch: 4 step: 1, loss is 0.0026422091759741306\n",
      "epoch: 4 step: 2, loss is 0.03384432941675186\n",
      "epoch: 4 step: 3, loss is 0.1025727167725563\n",
      "epoch: 4 step: 4, loss is 0.02388737164437771\n",
      "epoch: 4 step: 5, loss is 0.005629832856357098\n",
      "epoch: 4 step: 6, loss is 0.015169976279139519\n",
      "epoch: 4 step: 7, loss is 0.02442866750061512\n",
      "epoch: 4 step: 8, loss is 0.017514461651444435\n",
      "epoch: 4 step: 9, loss is 0.0001977057254407555\n",
      "epoch: 4 step: 10, loss is 0.007219964638352394\n",
      "epoch: 4 step: 11, loss is 0.0012660683132708073\n",
      "epoch: 4 step: 12, loss is 0.011424630880355835\n",
      "epoch: 4 step: 13, loss is 0.11663071066141129\n",
      "epoch: 4 step: 14, loss is 0.029251601547002792\n",
      "epoch: 4 step: 15, loss is 0.07018490880727768\n",
      "epoch: 4 step: 16, loss is 0.0032188964542001486\n",
      "epoch: 4 step: 17, loss is 0.010867368429899216\n",
      "epoch: 4 step: 18, loss is 0.00443546986207366\n",
      "epoch: 4 step: 19, loss is 0.0003409004129935056\n",
      "epoch: 4 step: 20, loss is 0.15217474102973938\n",
      "epoch: 4 step: 21, loss is 0.010418043471872807\n",
      "epoch: 4 step: 22, loss is 0.003746179398149252\n",
      "epoch: 4 step: 23, loss is 0.14221782982349396\n",
      "epoch: 4 step: 24, loss is 0.021587925031781197\n",
      "epoch: 4 step: 25, loss is 0.019923346117138863\n",
      "epoch: 4 step: 26, loss is 0.02137216180562973\n",
      "epoch: 4 step: 27, loss is 0.35103392601013184\n",
      "epoch: 4 step: 28, loss is 0.027997203171253204\n",
      "epoch: 4 step: 29, loss is 0.012744073756039143\n",
      "epoch: 4 step: 30, loss is 0.021162284538149834\n",
      "epoch: 4 step: 31, loss is 0.0006022740271873772\n",
      "epoch: 4 step: 32, loss is 0.0263887420296669\n",
      "epoch: 4 step: 33, loss is 0.0005477746017277241\n",
      "epoch: 4 step: 34, loss is 0.0007574588526040316\n",
      "epoch: 4 step: 35, loss is 0.10780619084835052\n",
      "epoch: 4 step: 36, loss is 0.00043696811189875007\n",
      "epoch: 4 step: 37, loss is 0.31062832474708557\n",
      "epoch: 4 step: 38, loss is 0.03264011815190315\n",
      "epoch: 4 step: 39, loss is 0.0051128980703651905\n",
      "epoch: 4 step: 40, loss is 0.13281874358654022\n",
      "epoch: 4 step: 41, loss is 0.003523250576108694\n",
      "epoch: 4 step: 42, loss is 0.027088861912488937\n",
      "epoch: 4 step: 43, loss is 0.013405277393758297\n",
      "epoch: 4 step: 44, loss is 0.009413192979991436\n",
      "epoch: 4 step: 45, loss is 0.04101318120956421\n",
      "epoch: 4 step: 46, loss is 0.1303020715713501\n",
      "epoch: 4 step: 47, loss is 0.006612737663090229\n",
      "epoch: 4 step: 48, loss is 0.025514209643006325\n",
      "epoch: 4 step: 49, loss is 0.014948183670639992\n",
      "epoch: 4 step: 50, loss is 0.060028865933418274\n",
      "epoch: 4 step: 51, loss is 0.04382415488362312\n",
      "epoch: 4 step: 52, loss is 0.015382572077214718\n",
      "epoch: 4 step: 53, loss is 0.007920549251139164\n",
      "epoch: 4 step: 54, loss is 0.022268356755375862\n",
      "epoch: 4 step: 55, loss is 0.00863648485392332\n",
      "epoch: 4 step: 56, loss is 0.006777467206120491\n",
      "epoch: 4 step: 57, loss is 0.04269726201891899\n",
      "epoch: 4 step: 58, loss is 0.07362014800310135\n",
      "epoch: 4 step: 59, loss is 0.0026833610609173775\n",
      "epoch: 4 step: 60, loss is 0.004190532490611076\n",
      "epoch: 4 step: 61, loss is 0.024374281987547874\n",
      "epoch: 4 step: 62, loss is 0.00044883028022013605\n",
      "epoch: 4 step: 63, loss is 0.04051114618778229\n",
      "epoch: 4 step: 64, loss is 0.13751710951328278\n",
      "epoch: 4 step: 65, loss is 0.15078061819076538\n",
      "epoch: 4 step: 66, loss is 0.15851140022277832\n",
      "epoch: 4 step: 67, loss is 0.001900436356663704\n",
      "epoch: 4 step: 68, loss is 0.0009060577722266316\n",
      "epoch: 4 step: 69, loss is 0.0009424634044989944\n",
      "epoch: 4 step: 70, loss is 0.050538644194602966\n",
      "epoch: 4 step: 71, loss is 0.0005842995014972985\n",
      "epoch: 4 step: 72, loss is 0.015332374721765518\n",
      "epoch: 4 step: 73, loss is 0.0030460425186902285\n",
      "epoch: 4 step: 74, loss is 0.010150598362088203\n",
      "epoch: 4 step: 75, loss is 0.024942180141806602\n",
      "epoch: 4 step: 76, loss is 0.02367391623556614\n",
      "epoch: 4 step: 77, loss is 0.0072209713980555534\n",
      "epoch: 4 step: 78, loss is 0.0042591099627316\n",
      "epoch: 4 step: 79, loss is 0.017969807609915733\n",
      "epoch: 4 step: 80, loss is 0.09245948493480682\n",
      "epoch: 4 step: 81, loss is 0.0014724434586241841\n",
      "epoch: 4 step: 82, loss is 0.004400275181978941\n",
      "epoch: 4 step: 83, loss is 0.03692830353975296\n",
      "epoch: 4 step: 84, loss is 0.08727982640266418\n",
      "epoch: 4 step: 85, loss is 0.0025360321160405874\n",
      "epoch: 4 step: 86, loss is 0.0011972358915954828\n",
      "epoch: 4 step: 87, loss is 0.023609263822436333\n",
      "epoch: 4 step: 88, loss is 0.0007909488631412387\n",
      "epoch: 4 step: 89, loss is 0.035766880959272385\n",
      "epoch: 4 step: 90, loss is 0.025745294988155365\n",
      "epoch: 4 step: 91, loss is 0.04421692714095116\n",
      "epoch: 4 step: 92, loss is 0.0008863839320838451\n",
      "epoch: 4 step: 93, loss is 0.15021133422851562\n",
      "epoch: 4 step: 94, loss is 0.009405580349266529\n",
      "epoch: 4 step: 95, loss is 0.023101521655917168\n",
      "epoch: 4 step: 96, loss is 0.08351220190525055\n",
      "epoch: 4 step: 97, loss is 0.04059255123138428\n",
      "epoch: 4 step: 98, loss is 0.0425979346036911\n",
      "epoch: 4 step: 99, loss is 0.0010898835025727749\n",
      "epoch: 4 step: 100, loss is 0.00706056971102953\n",
      "epoch: 4 step: 101, loss is 0.0028176673222333193\n",
      "epoch: 4 step: 102, loss is 0.05161775276064873\n",
      "epoch: 4 step: 103, loss is 0.10647612065076828\n",
      "epoch: 4 step: 104, loss is 0.08690598607063293\n",
      "epoch: 4 step: 105, loss is 0.18264687061309814\n",
      "epoch: 4 step: 106, loss is 0.00041229836642742157\n",
      "epoch: 4 step: 107, loss is 0.04433067888021469\n",
      "epoch: 4 step: 108, loss is 0.018227186053991318\n",
      "epoch: 4 step: 109, loss is 0.010554677806794643\n",
      "epoch: 4 step: 110, loss is 0.009144742041826248\n",
      "epoch: 4 step: 111, loss is 0.08594921976327896\n",
      "epoch: 4 step: 112, loss is 0.04128281772136688\n",
      "epoch: 4 step: 113, loss is 0.005933413747698069\n",
      "epoch: 4 step: 114, loss is 0.01663360372185707\n",
      "epoch: 4 step: 115, loss is 0.005867225583642721\n",
      "epoch: 4 step: 116, loss is 0.0018579402239993215\n",
      "epoch: 4 step: 117, loss is 0.0017681828467175364\n",
      "epoch: 4 step: 118, loss is 0.0003355603839736432\n",
      "epoch: 4 step: 119, loss is 0.0011750630801543593\n",
      "epoch: 4 step: 120, loss is 0.001710828160867095\n",
      "epoch: 4 step: 121, loss is 0.0014377565821632743\n",
      "epoch: 4 step: 122, loss is 0.02136746607720852\n",
      "epoch: 4 step: 123, loss is 0.024739094078540802\n",
      "epoch: 4 step: 124, loss is 0.0014238795265555382\n",
      "epoch: 4 step: 125, loss is 0.0022351841907948256\n",
      "epoch: 4 step: 126, loss is 0.012136599980294704\n",
      "epoch: 4 step: 127, loss is 0.024493422359228134\n",
      "epoch: 4 step: 128, loss is 0.049919068813323975\n",
      "epoch: 4 step: 129, loss is 0.00033015914959833026\n",
      "epoch: 4 step: 130, loss is 0.007336280308663845\n",
      "epoch: 4 step: 131, loss is 0.18644370138645172\n",
      "epoch: 4 step: 132, loss is 0.007003011181950569\n",
      "epoch: 4 step: 133, loss is 0.18171481788158417\n",
      "epoch: 4 step: 134, loss is 8.074032666627318e-05\n",
      "epoch: 4 step: 135, loss is 0.09419121593236923\n",
      "epoch: 4 step: 136, loss is 0.009272712282836437\n",
      "epoch: 4 step: 137, loss is 0.05061453580856323\n",
      "epoch: 4 step: 138, loss is 0.0004888040130026639\n",
      "epoch: 4 step: 139, loss is 0.0034062869381159544\n",
      "epoch: 4 step: 140, loss is 0.0036510813515633345\n",
      "epoch: 4 step: 141, loss is 0.005402097944170237\n",
      "epoch: 4 step: 142, loss is 0.014726490713655949\n",
      "epoch: 4 step: 143, loss is 0.003974013030529022\n",
      "epoch: 4 step: 144, loss is 0.015929922461509705\n",
      "epoch: 4 step: 145, loss is 0.10614615678787231\n",
      "epoch: 4 step: 146, loss is 0.08679766952991486\n",
      "epoch: 4 step: 147, loss is 0.13574086129665375\n",
      "epoch: 4 step: 148, loss is 0.007528411224484444\n",
      "epoch: 4 step: 149, loss is 0.0060799908824265\n",
      "epoch: 4 step: 150, loss is 0.012694133445620537\n",
      "epoch: 4 step: 151, loss is 0.058536820113658905\n",
      "epoch: 4 step: 152, loss is 0.0048270439729094505\n",
      "epoch: 4 step: 153, loss is 0.03305785730481148\n",
      "epoch: 4 step: 154, loss is 0.12510427832603455\n",
      "epoch: 4 step: 155, loss is 0.006812618114054203\n",
      "epoch: 4 step: 156, loss is 0.00034784956369549036\n",
      "epoch: 4 step: 157, loss is 0.0016128936549648643\n",
      "epoch: 4 step: 158, loss is 0.04199319705367088\n",
      "epoch: 4 step: 159, loss is 0.0022509205155074596\n",
      "epoch: 4 step: 160, loss is 0.0361391119658947\n",
      "epoch: 4 step: 161, loss is 0.014153624884784222\n",
      "epoch: 4 step: 162, loss is 0.054994307458400726\n",
      "epoch: 4 step: 163, loss is 0.08831058442592621\n",
      "epoch: 4 step: 164, loss is 0.03535937890410423\n",
      "epoch: 4 step: 165, loss is 0.0002779249334707856\n",
      "epoch: 4 step: 166, loss is 0.0033299680799245834\n",
      "epoch: 4 step: 167, loss is 0.00030059172422625124\n",
      "epoch: 4 step: 168, loss is 0.00039098510751500726\n",
      "epoch: 4 step: 169, loss is 0.18587197363376617\n",
      "epoch: 4 step: 170, loss is 0.00195596506819129\n",
      "epoch: 4 step: 171, loss is 0.0009101424366235733\n",
      "epoch: 4 step: 172, loss is 0.005929368082433939\n",
      "epoch: 4 step: 173, loss is 0.18709421157836914\n",
      "epoch: 4 step: 174, loss is 0.0012234629830345511\n",
      "epoch: 4 step: 175, loss is 0.0700564980506897\n",
      "epoch: 4 step: 176, loss is 0.002605346729978919\n",
      "epoch: 4 step: 177, loss is 0.0175597183406353\n",
      "epoch: 4 step: 178, loss is 0.012367159128189087\n",
      "epoch: 4 step: 179, loss is 0.005578943993896246\n",
      "epoch: 4 step: 180, loss is 0.0014142615254968405\n",
      "epoch: 4 step: 181, loss is 0.0010877818567678332\n",
      "epoch: 4 step: 182, loss is 0.02164609357714653\n",
      "epoch: 4 step: 183, loss is 0.010887840762734413\n",
      "epoch: 4 step: 184, loss is 0.0014087185263633728\n",
      "epoch: 4 step: 185, loss is 0.0018678868655115366\n",
      "epoch: 4 step: 186, loss is 0.17300410568714142\n",
      "epoch: 4 step: 187, loss is 0.25117167830467224\n",
      "epoch: 4 step: 188, loss is 0.04547714814543724\n",
      "epoch: 4 step: 189, loss is 0.010897936299443245\n",
      "epoch: 4 step: 190, loss is 0.14225266873836517\n",
      "epoch: 4 step: 191, loss is 0.0023870128206908703\n",
      "epoch: 4 step: 192, loss is 0.0032548324670642614\n",
      "epoch: 4 step: 193, loss is 0.012184507213532925\n",
      "epoch: 4 step: 194, loss is 0.0001954978797584772\n",
      "epoch: 4 step: 195, loss is 0.06902939081192017\n",
      "epoch: 4 step: 196, loss is 0.001012451946735382\n",
      "epoch: 4 step: 197, loss is 0.004858141764998436\n",
      "epoch: 4 step: 198, loss is 0.0901968851685524\n",
      "epoch: 4 step: 199, loss is 0.0012886120239272714\n",
      "epoch: 4 step: 200, loss is 0.006074293050915003\n",
      "epoch: 4 step: 201, loss is 0.0013100702781230211\n",
      "epoch: 4 step: 202, loss is 0.0015809492906555533\n",
      "epoch: 4 step: 203, loss is 0.16914068162441254\n",
      "epoch: 4 step: 204, loss is 0.00017844157991930842\n",
      "epoch: 4 step: 205, loss is 0.13094913959503174\n",
      "epoch: 4 step: 206, loss is 0.0203360915184021\n",
      "epoch: 4 step: 207, loss is 0.01082216389477253\n",
      "epoch: 4 step: 208, loss is 0.012881925329566002\n",
      "epoch: 4 step: 209, loss is 0.010160103440284729\n",
      "epoch: 4 step: 210, loss is 0.05018244683742523\n",
      "epoch: 4 step: 211, loss is 0.008881742134690285\n",
      "epoch: 4 step: 212, loss is 0.0018037606496363878\n",
      "epoch: 4 step: 213, loss is 0.005221622996032238\n",
      "epoch: 4 step: 214, loss is 0.045447491109371185\n",
      "epoch: 4 step: 215, loss is 0.002906331792473793\n",
      "epoch: 4 step: 216, loss is 0.01931818574666977\n",
      "epoch: 4 step: 217, loss is 0.03097960352897644\n",
      "epoch: 4 step: 218, loss is 0.014358085580170155\n",
      "epoch: 4 step: 219, loss is 0.0024678579065948725\n",
      "epoch: 4 step: 220, loss is 0.011342199519276619\n",
      "epoch: 4 step: 221, loss is 0.003545542946085334\n",
      "epoch: 4 step: 222, loss is 0.013205857947468758\n",
      "epoch: 4 step: 223, loss is 0.05573726072907448\n",
      "epoch: 4 step: 224, loss is 0.0218216460198164\n",
      "epoch: 4 step: 225, loss is 0.055898599326610565\n",
      "epoch: 4 step: 226, loss is 0.12394723296165466\n",
      "epoch: 4 step: 227, loss is 0.00031465632491745055\n",
      "epoch: 4 step: 228, loss is 0.012047347612679005\n",
      "epoch: 4 step: 229, loss is 0.012581173330545425\n",
      "epoch: 4 step: 230, loss is 0.1154056042432785\n",
      "epoch: 4 step: 231, loss is 0.037414174526929855\n",
      "epoch: 4 step: 232, loss is 0.002455715090036392\n",
      "epoch: 4 step: 233, loss is 0.0011008455185219646\n",
      "epoch: 4 step: 234, loss is 0.20178718864917755\n",
      "epoch: 4 step: 235, loss is 0.06679899990558624\n",
      "epoch: 4 step: 236, loss is 0.008269417099654675\n",
      "epoch: 4 step: 237, loss is 0.11335579305887222\n",
      "epoch: 4 step: 238, loss is 0.004141638986766338\n",
      "epoch: 4 step: 239, loss is 0.005096652079373598\n",
      "epoch: 4 step: 240, loss is 0.0011739719193428755\n",
      "epoch: 4 step: 241, loss is 0.0057865348644554615\n",
      "epoch: 4 step: 242, loss is 0.06712592393159866\n",
      "epoch: 4 step: 243, loss is 0.000498225970659405\n",
      "epoch: 4 step: 244, loss is 0.04573594778776169\n",
      "epoch: 4 step: 245, loss is 0.0003493543481454253\n",
      "epoch: 4 step: 246, loss is 0.00521149393171072\n",
      "epoch: 4 step: 247, loss is 0.19314038753509521\n",
      "epoch: 4 step: 248, loss is 0.0038423524238169193\n",
      "epoch: 4 step: 249, loss is 0.025933772325515747\n",
      "epoch: 4 step: 250, loss is 0.016030630096793175\n",
      "epoch: 4 step: 251, loss is 0.0010357970604673028\n",
      "epoch: 4 step: 252, loss is 0.03076561912894249\n",
      "epoch: 4 step: 253, loss is 0.018272962421178818\n",
      "epoch: 4 step: 254, loss is 0.01780259609222412\n",
      "epoch: 4 step: 255, loss is 0.025147536769509315\n",
      "epoch: 4 step: 256, loss is 0.0332641676068306\n",
      "epoch: 4 step: 257, loss is 0.0530511811375618\n",
      "epoch: 4 step: 258, loss is 0.2655276358127594\n",
      "epoch: 4 step: 259, loss is 0.021145856007933617\n",
      "epoch: 4 step: 260, loss is 0.0003573225694708526\n",
      "epoch: 4 step: 261, loss is 0.0037858053110539913\n",
      "epoch: 4 step: 262, loss is 0.010299458168447018\n",
      "epoch: 4 step: 263, loss is 0.01138579472899437\n",
      "epoch: 4 step: 264, loss is 0.004179845564067364\n",
      "epoch: 4 step: 265, loss is 0.01684572547674179\n",
      "epoch: 4 step: 266, loss is 0.005109866615384817\n",
      "epoch: 4 step: 267, loss is 0.03434382379055023\n",
      "epoch: 4 step: 268, loss is 0.013211915269494057\n",
      "epoch: 4 step: 269, loss is 0.02778673730790615\n",
      "epoch: 4 step: 270, loss is 0.001821886864490807\n",
      "epoch: 4 step: 271, loss is 0.0015214806189760566\n",
      "epoch: 4 step: 272, loss is 0.006680126301944256\n",
      "epoch: 4 step: 273, loss is 0.001867693499661982\n",
      "epoch: 4 step: 274, loss is 0.004758184775710106\n",
      "epoch: 4 step: 275, loss is 0.00468663964420557\n",
      "epoch: 4 step: 276, loss is 0.061787184327840805\n",
      "epoch: 4 step: 277, loss is 0.014146750792860985\n",
      "epoch: 4 step: 278, loss is 0.000785471114795655\n",
      "epoch: 4 step: 279, loss is 0.002104358747601509\n",
      "epoch: 4 step: 280, loss is 0.004642570856958628\n",
      "epoch: 4 step: 281, loss is 0.04571209475398064\n",
      "epoch: 4 step: 282, loss is 0.051789917051792145\n",
      "epoch: 4 step: 283, loss is 0.003151963697746396\n",
      "epoch: 4 step: 284, loss is 0.010631590150296688\n",
      "epoch: 4 step: 285, loss is 0.0310721043497324\n",
      "epoch: 4 step: 286, loss is 0.09608018398284912\n",
      "epoch: 4 step: 287, loss is 0.012320248410105705\n",
      "epoch: 4 step: 288, loss is 0.0028761804569512606\n",
      "epoch: 4 step: 289, loss is 0.0068286689929664135\n",
      "epoch: 4 step: 290, loss is 0.10186342149972916\n",
      "epoch: 4 step: 291, loss is 0.10721973329782486\n",
      "epoch: 4 step: 292, loss is 0.1431090533733368\n",
      "epoch: 4 step: 293, loss is 0.003550059162080288\n",
      "epoch: 4 step: 294, loss is 0.008086882531642914\n",
      "epoch: 4 step: 295, loss is 0.0008922826964408159\n",
      "epoch: 4 step: 296, loss is 0.00947646051645279\n",
      "epoch: 4 step: 297, loss is 0.004680709447711706\n",
      "epoch: 4 step: 298, loss is 0.011039466597139835\n",
      "epoch: 4 step: 299, loss is 0.0020918245427310467\n",
      "epoch: 4 step: 300, loss is 0.004969336092472076\n",
      "epoch: 4 step: 301, loss is 0.061645135283470154\n",
      "epoch: 4 step: 302, loss is 0.017202069982886314\n",
      "epoch: 4 step: 303, loss is 0.0008740288903936744\n",
      "epoch: 4 step: 304, loss is 0.0003832399088423699\n",
      "epoch: 4 step: 305, loss is 0.022716086357831955\n",
      "epoch: 4 step: 306, loss is 0.008116848766803741\n",
      "epoch: 4 step: 307, loss is 0.0056176758371293545\n",
      "epoch: 4 step: 308, loss is 0.12240444123744965\n",
      "epoch: 4 step: 309, loss is 0.10419168323278427\n",
      "epoch: 4 step: 310, loss is 0.07257188856601715\n",
      "epoch: 4 step: 311, loss is 0.11543639004230499\n",
      "epoch: 4 step: 312, loss is 0.0019880924373865128\n",
      "epoch: 4 step: 313, loss is 0.11867529153823853\n",
      "epoch: 4 step: 314, loss is 0.03659634664654732\n",
      "epoch: 4 step: 315, loss is 0.06717842817306519\n",
      "epoch: 4 step: 316, loss is 0.04249385744333267\n",
      "epoch: 4 step: 317, loss is 0.003983868286013603\n",
      "epoch: 4 step: 318, loss is 0.020658234134316444\n",
      "epoch: 4 step: 319, loss is 0.007666133344173431\n",
      "epoch: 4 step: 320, loss is 0.010983224026858807\n",
      "epoch: 4 step: 321, loss is 0.0008813096210360527\n",
      "epoch: 4 step: 322, loss is 0.017956165596842766\n",
      "epoch: 4 step: 323, loss is 0.015614686533808708\n",
      "epoch: 4 step: 324, loss is 0.00580381415784359\n",
      "epoch: 4 step: 325, loss is 0.0012569631217047572\n",
      "epoch: 4 step: 326, loss is 0.01191620621830225\n",
      "epoch: 4 step: 327, loss is 0.0022513947915285826\n",
      "epoch: 4 step: 328, loss is 0.002873316640034318\n",
      "epoch: 4 step: 329, loss is 0.007187007926404476\n",
      "epoch: 4 step: 330, loss is 0.011193893849849701\n",
      "epoch: 4 step: 331, loss is 0.1975124478340149\n",
      "epoch: 4 step: 332, loss is 0.007044641766697168\n",
      "epoch: 4 step: 333, loss is 0.0001552064932184294\n",
      "epoch: 4 step: 334, loss is 0.00748726911842823\n",
      "epoch: 4 step: 335, loss is 0.00829921942204237\n",
      "epoch: 4 step: 336, loss is 0.012227480299770832\n",
      "epoch: 4 step: 337, loss is 0.01203590165823698\n",
      "epoch: 4 step: 338, loss is 0.0030388152226805687\n",
      "epoch: 4 step: 339, loss is 0.07333391904830933\n",
      "epoch: 4 step: 340, loss is 0.10938779264688492\n",
      "epoch: 4 step: 341, loss is 0.00368184270337224\n",
      "epoch: 4 step: 342, loss is 0.10554066300392151\n",
      "epoch: 4 step: 343, loss is 0.05143830180168152\n",
      "epoch: 4 step: 344, loss is 0.0007079116767272353\n",
      "epoch: 4 step: 345, loss is 0.2353973537683487\n",
      "epoch: 4 step: 346, loss is 0.00045428916928358376\n",
      "epoch: 4 step: 347, loss is 0.18327213823795319\n",
      "epoch: 4 step: 348, loss is 0.0029550641775131226\n",
      "epoch: 4 step: 349, loss is 0.06280563026666641\n",
      "epoch: 4 step: 350, loss is 0.002504628850147128\n",
      "epoch: 4 step: 351, loss is 0.004766377620398998\n",
      "epoch: 4 step: 352, loss is 0.001362547161988914\n",
      "epoch: 4 step: 353, loss is 0.009874531999230385\n",
      "epoch: 4 step: 354, loss is 0.06806725263595581\n",
      "epoch: 4 step: 355, loss is 0.010572670958936214\n",
      "epoch: 4 step: 356, loss is 0.06594761461019516\n",
      "epoch: 4 step: 357, loss is 0.00964995939284563\n",
      "epoch: 4 step: 358, loss is 0.0023366280365735292\n",
      "epoch: 4 step: 359, loss is 0.11074145883321762\n",
      "epoch: 4 step: 360, loss is 0.0004411371483001858\n",
      "epoch: 4 step: 361, loss is 0.033878766000270844\n",
      "epoch: 4 step: 362, loss is 0.09655804187059402\n",
      "epoch: 4 step: 363, loss is 0.1004035621881485\n",
      "epoch: 4 step: 364, loss is 0.025668904185295105\n",
      "epoch: 4 step: 365, loss is 0.021637581288814545\n",
      "epoch: 4 step: 366, loss is 0.050511471927165985\n",
      "epoch: 4 step: 367, loss is 0.0006951314280740917\n",
      "epoch: 4 step: 368, loss is 0.008874376304447651\n",
      "epoch: 4 step: 369, loss is 0.003429640317335725\n",
      "epoch: 4 step: 370, loss is 0.0019809803925454617\n",
      "epoch: 4 step: 371, loss is 0.07846617698669434\n",
      "epoch: 4 step: 372, loss is 0.012729113921523094\n",
      "epoch: 4 step: 373, loss is 0.03464623540639877\n",
      "epoch: 4 step: 374, loss is 0.018419042229652405\n",
      "epoch: 4 step: 375, loss is 0.02241543121635914\n",
      "epoch: 4 step: 376, loss is 0.026609744876623154\n",
      "epoch: 4 step: 377, loss is 0.014255969785153866\n",
      "epoch: 4 step: 378, loss is 0.0013853323180228472\n",
      "epoch: 4 step: 379, loss is 0.003470935393124819\n",
      "epoch: 4 step: 380, loss is 0.001630370388738811\n",
      "epoch: 4 step: 381, loss is 0.02733566053211689\n",
      "epoch: 4 step: 382, loss is 0.15228693187236786\n",
      "epoch: 4 step: 383, loss is 0.09682901203632355\n",
      "epoch: 4 step: 384, loss is 0.05845392867922783\n",
      "epoch: 4 step: 385, loss is 0.029493212699890137\n",
      "epoch: 4 step: 386, loss is 0.027206849306821823\n",
      "epoch: 4 step: 387, loss is 0.02878732793033123\n",
      "epoch: 4 step: 388, loss is 0.0071662962436676025\n",
      "epoch: 4 step: 389, loss is 0.034125737845897675\n",
      "epoch: 4 step: 390, loss is 0.028504734858870506\n",
      "epoch: 4 step: 391, loss is 0.04937780648469925\n",
      "epoch: 4 step: 392, loss is 0.06940201669931412\n",
      "epoch: 4 step: 393, loss is 0.006518268957734108\n",
      "epoch: 4 step: 394, loss is 0.029844965785741806\n",
      "epoch: 4 step: 395, loss is 0.0037432685494422913\n",
      "epoch: 4 step: 396, loss is 0.003018800402060151\n",
      "epoch: 4 step: 397, loss is 0.025546828284859657\n",
      "epoch: 4 step: 398, loss is 0.007143556606024504\n",
      "epoch: 4 step: 399, loss is 0.004616040736436844\n",
      "epoch: 4 step: 400, loss is 0.006121781188994646\n",
      "epoch: 4 step: 401, loss is 0.0232902429997921\n",
      "epoch: 4 step: 402, loss is 0.0026627539191395044\n",
      "epoch: 4 step: 403, loss is 0.009395018219947815\n",
      "epoch: 4 step: 404, loss is 0.005994563922286034\n",
      "epoch: 4 step: 405, loss is 0.002475137123838067\n",
      "epoch: 4 step: 406, loss is 0.004359258804470301\n",
      "epoch: 4 step: 407, loss is 0.02245456725358963\n",
      "epoch: 4 step: 408, loss is 0.0037208320572972298\n",
      "epoch: 4 step: 409, loss is 0.0012155664153397083\n",
      "epoch: 4 step: 410, loss is 0.004572760313749313\n",
      "epoch: 4 step: 411, loss is 0.015164850279688835\n",
      "epoch: 4 step: 412, loss is 0.07599331438541412\n",
      "epoch: 4 step: 413, loss is 0.003074635285884142\n",
      "epoch: 4 step: 414, loss is 0.015293898060917854\n",
      "epoch: 4 step: 415, loss is 0.013656639494001865\n",
      "epoch: 4 step: 416, loss is 0.0024894687812775373\n",
      "epoch: 4 step: 417, loss is 0.08743824064731598\n",
      "epoch: 4 step: 418, loss is 0.00944509357213974\n",
      "epoch: 4 step: 419, loss is 0.002888420596718788\n",
      "epoch: 4 step: 420, loss is 0.002841852605342865\n",
      "epoch: 4 step: 421, loss is 0.002401323989033699\n",
      "epoch: 4 step: 422, loss is 0.008017164655029774\n",
      "epoch: 4 step: 423, loss is 0.10905303061008453\n",
      "epoch: 4 step: 424, loss is 0.03139876574277878\n",
      "epoch: 4 step: 425, loss is 0.004556067753583193\n",
      "epoch: 4 step: 426, loss is 0.04061337560415268\n",
      "epoch: 4 step: 427, loss is 0.04566936194896698\n",
      "epoch: 4 step: 428, loss is 0.14632436633110046\n",
      "epoch: 4 step: 429, loss is 0.002218940295279026\n",
      "epoch: 4 step: 430, loss is 0.004075089003890753\n",
      "epoch: 4 step: 431, loss is 0.023821912705898285\n",
      "epoch: 4 step: 432, loss is 0.0035402048379182816\n",
      "epoch: 4 step: 433, loss is 0.559741735458374\n",
      "epoch: 4 step: 434, loss is 0.004581205081194639\n",
      "epoch: 4 step: 435, loss is 0.0013458279427140951\n",
      "epoch: 4 step: 436, loss is 0.0038637963589280844\n",
      "epoch: 4 step: 437, loss is 0.07958988845348358\n",
      "epoch: 4 step: 438, loss is 0.004758638329803944\n",
      "epoch: 4 step: 439, loss is 0.00040393188828602433\n",
      "epoch: 4 step: 440, loss is 0.05340757593512535\n",
      "epoch: 4 step: 441, loss is 0.0053489129059016705\n",
      "epoch: 4 step: 442, loss is 0.16549862921237946\n",
      "epoch: 4 step: 443, loss is 0.08155980706214905\n",
      "epoch: 4 step: 444, loss is 0.029118994250893593\n",
      "epoch: 4 step: 445, loss is 0.02973952889442444\n",
      "epoch: 4 step: 446, loss is 0.04257883504033089\n",
      "epoch: 4 step: 447, loss is 0.010717673227190971\n",
      "epoch: 4 step: 448, loss is 0.002601602114737034\n",
      "epoch: 4 step: 449, loss is 0.005185315851122141\n",
      "epoch: 4 step: 450, loss is 0.14362096786499023\n",
      "epoch: 4 step: 451, loss is 0.05857333168387413\n",
      "epoch: 4 step: 452, loss is 0.06224524602293968\n",
      "epoch: 4 step: 453, loss is 0.010309437289834023\n",
      "epoch: 4 step: 454, loss is 0.05184277147054672\n",
      "epoch: 4 step: 455, loss is 0.08268001675605774\n",
      "epoch: 4 step: 456, loss is 0.05929037928581238\n",
      "epoch: 4 step: 457, loss is 0.004026354290544987\n",
      "epoch: 4 step: 458, loss is 0.03682679310441017\n",
      "epoch: 4 step: 459, loss is 0.005493145436048508\n",
      "epoch: 4 step: 460, loss is 0.02045772783458233\n",
      "epoch: 4 step: 461, loss is 0.0728977844119072\n",
      "epoch: 4 step: 462, loss is 0.007974891923367977\n",
      "epoch: 4 step: 463, loss is 0.00540138129144907\n",
      "epoch: 4 step: 464, loss is 0.011733666062355042\n",
      "epoch: 4 step: 465, loss is 0.01397283561527729\n",
      "epoch: 4 step: 466, loss is 0.006021587643772364\n",
      "epoch: 4 step: 467, loss is 0.021835269406437874\n",
      "epoch: 4 step: 468, loss is 0.001211809809319675\n",
      "epoch: 4 step: 469, loss is 0.0009535619174130261\n",
      "epoch: 4 step: 470, loss is 0.0015659879427403212\n",
      "epoch: 4 step: 471, loss is 0.12265664339065552\n",
      "epoch: 4 step: 472, loss is 0.002587071619927883\n",
      "epoch: 4 step: 473, loss is 0.3254748284816742\n",
      "epoch: 4 step: 474, loss is 0.022556331008672714\n",
      "epoch: 4 step: 475, loss is 0.3079589307308197\n",
      "epoch: 4 step: 476, loss is 0.028702624142169952\n",
      "epoch: 4 step: 477, loss is 0.0020871413871645927\n",
      "epoch: 4 step: 478, loss is 0.06496503204107285\n",
      "epoch: 4 step: 479, loss is 0.029723946005105972\n",
      "epoch: 4 step: 480, loss is 0.1829172670841217\n",
      "epoch: 4 step: 481, loss is 0.14887866377830505\n",
      "epoch: 4 step: 482, loss is 0.006083526182919741\n",
      "epoch: 4 step: 483, loss is 0.0378870889544487\n",
      "epoch: 4 step: 484, loss is 0.06299549341201782\n",
      "epoch: 4 step: 485, loss is 0.013072920963168144\n",
      "epoch: 4 step: 486, loss is 0.004898648709058762\n",
      "epoch: 4 step: 487, loss is 0.0071481987833976746\n",
      "epoch: 4 step: 488, loss is 0.0021525679621845484\n",
      "epoch: 4 step: 489, loss is 0.004331219010055065\n",
      "epoch: 4 step: 490, loss is 0.05289553105831146\n",
      "epoch: 4 step: 491, loss is 0.014358224347233772\n",
      "epoch: 4 step: 492, loss is 0.01595974899828434\n",
      "epoch: 4 step: 493, loss is 0.06269573420286179\n",
      "epoch: 4 step: 494, loss is 0.013279253616929054\n",
      "epoch: 4 step: 495, loss is 0.027743766084313393\n",
      "epoch: 4 step: 496, loss is 0.07198745012283325\n",
      "epoch: 4 step: 497, loss is 0.019286368042230606\n",
      "epoch: 4 step: 498, loss is 0.05247828736901283\n",
      "epoch: 4 step: 499, loss is 0.0029681557789444923\n",
      "epoch: 4 step: 500, loss is 0.12503185868263245\n",
      "epoch: 4 step: 501, loss is 0.008749234490096569\n",
      "epoch: 4 step: 502, loss is 0.0014099908294156194\n",
      "epoch: 4 step: 503, loss is 0.015130728483200073\n",
      "epoch: 4 step: 504, loss is 0.053389303386211395\n",
      "epoch: 4 step: 505, loss is 0.01679922640323639\n",
      "epoch: 4 step: 506, loss is 0.19307950139045715\n",
      "epoch: 4 step: 507, loss is 0.0016701753484085202\n",
      "epoch: 4 step: 508, loss is 0.08008398860692978\n",
      "epoch: 4 step: 509, loss is 0.05202743038535118\n",
      "epoch: 4 step: 510, loss is 0.008157510310411453\n",
      "epoch: 4 step: 511, loss is 0.04374166205525398\n",
      "epoch: 4 step: 512, loss is 0.219041109085083\n",
      "epoch: 4 step: 513, loss is 0.1418542116880417\n",
      "epoch: 4 step: 514, loss is 0.0037182001397013664\n",
      "epoch: 4 step: 515, loss is 0.06903742998838425\n",
      "epoch: 4 step: 516, loss is 0.05938534066081047\n",
      "epoch: 4 step: 517, loss is 0.009285759180784225\n",
      "epoch: 4 step: 518, loss is 0.03419267386198044\n",
      "epoch: 4 step: 519, loss is 0.080696240067482\n",
      "epoch: 4 step: 520, loss is 0.014073816128075123\n",
      "epoch: 4 step: 521, loss is 0.0005409553414210677\n",
      "epoch: 4 step: 522, loss is 0.06289424002170563\n",
      "epoch: 4 step: 523, loss is 0.0015240446664392948\n",
      "epoch: 4 step: 524, loss is 0.020975131541490555\n",
      "epoch: 4 step: 525, loss is 0.0027426360175013542\n",
      "epoch: 4 step: 526, loss is 0.2382606863975525\n",
      "epoch: 4 step: 527, loss is 0.004666478838771582\n",
      "epoch: 4 step: 528, loss is 0.0040611932054162025\n",
      "epoch: 4 step: 529, loss is 0.04428112879395485\n",
      "epoch: 4 step: 530, loss is 0.13413049280643463\n",
      "epoch: 4 step: 531, loss is 0.017919767647981644\n",
      "epoch: 4 step: 532, loss is 0.07039008289575577\n",
      "epoch: 4 step: 533, loss is 0.01685129664838314\n",
      "epoch: 4 step: 534, loss is 0.08584538847208023\n",
      "epoch: 4 step: 535, loss is 0.06436330080032349\n",
      "epoch: 4 step: 536, loss is 0.008688888512551785\n",
      "epoch: 4 step: 537, loss is 0.03589984402060509\n",
      "epoch: 4 step: 538, loss is 0.001700428081676364\n",
      "epoch: 4 step: 539, loss is 0.06553304940462112\n",
      "epoch: 4 step: 540, loss is 0.011060494929552078\n",
      "epoch: 4 step: 541, loss is 0.14961442351341248\n",
      "epoch: 4 step: 542, loss is 0.01225293055176735\n",
      "epoch: 4 step: 543, loss is 0.055179595947265625\n",
      "epoch: 4 step: 544, loss is 0.2163863331079483\n",
      "epoch: 4 step: 545, loss is 0.2770443558692932\n",
      "epoch: 4 step: 546, loss is 0.04980401694774628\n",
      "epoch: 4 step: 547, loss is 0.01339307613670826\n",
      "epoch: 4 step: 548, loss is 0.11885637789964676\n",
      "epoch: 4 step: 549, loss is 0.010924668982625008\n",
      "epoch: 4 step: 550, loss is 0.008176020346581936\n",
      "epoch: 4 step: 551, loss is 0.21195055544376373\n",
      "epoch: 4 step: 552, loss is 0.0049959514290094376\n",
      "epoch: 4 step: 553, loss is 0.006473264191299677\n",
      "epoch: 4 step: 554, loss is 0.009299509227275848\n",
      "epoch: 4 step: 555, loss is 0.001192341442219913\n",
      "epoch: 4 step: 556, loss is 0.1649196296930313\n",
      "epoch: 4 step: 557, loss is 0.02069147862493992\n",
      "epoch: 4 step: 558, loss is 0.013434607535600662\n",
      "epoch: 4 step: 559, loss is 0.006651359144598246\n",
      "epoch: 4 step: 560, loss is 0.06196078285574913\n",
      "epoch: 4 step: 561, loss is 0.012005029246211052\n",
      "epoch: 4 step: 562, loss is 0.07269773632287979\n",
      "epoch: 4 step: 563, loss is 0.03892112523317337\n",
      "epoch: 4 step: 564, loss is 0.010624895803630352\n",
      "epoch: 4 step: 565, loss is 0.02382485195994377\n",
      "epoch: 4 step: 566, loss is 0.007772571407258511\n",
      "epoch: 4 step: 567, loss is 0.025851542130112648\n",
      "epoch: 4 step: 568, loss is 0.0017904482083395123\n",
      "epoch: 4 step: 569, loss is 0.18497054278850555\n",
      "epoch: 4 step: 570, loss is 0.09498266875743866\n",
      "epoch: 4 step: 571, loss is 0.11635351926088333\n",
      "epoch: 4 step: 572, loss is 0.004290677607059479\n",
      "epoch: 4 step: 573, loss is 0.006805347744375467\n",
      "epoch: 4 step: 574, loss is 0.0012248075800016522\n",
      "epoch: 4 step: 575, loss is 0.017918167635798454\n",
      "epoch: 4 step: 576, loss is 0.04545245319604874\n",
      "epoch: 4 step: 577, loss is 0.0008828520076349378\n",
      "epoch: 4 step: 578, loss is 0.005301212426275015\n",
      "epoch: 4 step: 579, loss is 0.004690800327807665\n",
      "epoch: 4 step: 580, loss is 0.13074904680252075\n",
      "epoch: 4 step: 581, loss is 0.017341041937470436\n",
      "epoch: 4 step: 582, loss is 0.004512438550591469\n",
      "epoch: 4 step: 583, loss is 0.03176755830645561\n",
      "epoch: 4 step: 584, loss is 0.022659821435809135\n",
      "epoch: 4 step: 585, loss is 0.006865127943456173\n",
      "epoch: 4 step: 586, loss is 0.0025590788573026657\n",
      "epoch: 4 step: 587, loss is 0.003981238696724176\n",
      "epoch: 4 step: 588, loss is 0.031661417335271835\n",
      "epoch: 4 step: 589, loss is 0.009796963073313236\n",
      "epoch: 4 step: 590, loss is 0.0006615450838580728\n",
      "epoch: 4 step: 591, loss is 0.004053340759128332\n",
      "epoch: 4 step: 592, loss is 0.0009345283615402877\n",
      "epoch: 4 step: 593, loss is 0.0019919229671359062\n",
      "epoch: 4 step: 594, loss is 0.0010211473563686013\n",
      "epoch: 4 step: 595, loss is 0.011418153531849384\n",
      "epoch: 4 step: 596, loss is 0.001982560846954584\n",
      "epoch: 4 step: 597, loss is 0.10293212532997131\n",
      "epoch: 4 step: 598, loss is 0.0569005087018013\n",
      "epoch: 4 step: 599, loss is 0.001311129191890359\n",
      "epoch: 4 step: 600, loss is 0.0022410620003938675\n",
      "epoch: 4 step: 601, loss is 0.00015032727969810367\n",
      "epoch: 4 step: 602, loss is 0.006372193340212107\n",
      "epoch: 4 step: 603, loss is 0.010618360713124275\n",
      "epoch: 4 step: 604, loss is 0.002734885085374117\n",
      "epoch: 4 step: 605, loss is 0.0016415116842836142\n",
      "epoch: 4 step: 606, loss is 0.08674147725105286\n",
      "epoch: 4 step: 607, loss is 0.011740808375179768\n",
      "epoch: 4 step: 608, loss is 0.002728615887463093\n",
      "epoch: 4 step: 609, loss is 0.007561105769127607\n",
      "epoch: 4 step: 610, loss is 0.005675949156284332\n",
      "epoch: 4 step: 611, loss is 0.0232150387018919\n",
      "epoch: 4 step: 612, loss is 0.005458318628370762\n",
      "epoch: 4 step: 613, loss is 0.004445122089236975\n",
      "epoch: 4 step: 614, loss is 0.0010102676460519433\n",
      "epoch: 4 step: 615, loss is 0.001186715206131339\n",
      "epoch: 4 step: 616, loss is 0.0010508603882044554\n",
      "epoch: 4 step: 617, loss is 0.026537742465734482\n",
      "epoch: 4 step: 618, loss is 0.0005059499526396394\n",
      "epoch: 4 step: 619, loss is 0.0034987027756869793\n",
      "epoch: 4 step: 620, loss is 0.015753617510199547\n",
      "epoch: 4 step: 621, loss is 0.00032018241472542286\n",
      "epoch: 4 step: 622, loss is 0.000556141254492104\n",
      "epoch: 4 step: 623, loss is 0.004214343149214983\n",
      "epoch: 4 step: 624, loss is 0.026074469089508057\n",
      "epoch: 4 step: 625, loss is 0.0007776818820275366\n",
      "epoch: 4 step: 626, loss is 0.04630368947982788\n",
      "epoch: 4 step: 627, loss is 0.002003877656534314\n",
      "epoch: 4 step: 628, loss is 0.018249420449137688\n",
      "epoch: 4 step: 629, loss is 0.00647389143705368\n",
      "epoch: 4 step: 630, loss is 0.11202583461999893\n",
      "epoch: 4 step: 631, loss is 0.00341707281768322\n",
      "epoch: 4 step: 632, loss is 0.018846474587917328\n",
      "epoch: 4 step: 633, loss is 0.0003523707273416221\n",
      "epoch: 4 step: 634, loss is 0.0033128010109066963\n",
      "epoch: 4 step: 635, loss is 0.13052307069301605\n",
      "epoch: 4 step: 636, loss is 0.0768657773733139\n",
      "epoch: 4 step: 637, loss is 0.0007606111466884613\n",
      "epoch: 4 step: 638, loss is 0.0013386253267526627\n",
      "epoch: 4 step: 639, loss is 0.032670654356479645\n",
      "epoch: 4 step: 640, loss is 0.013978052884340286\n",
      "epoch: 4 step: 641, loss is 0.004883631598204374\n",
      "epoch: 4 step: 642, loss is 0.0006046030903235078\n",
      "epoch: 4 step: 643, loss is 0.002685032319277525\n",
      "epoch: 4 step: 644, loss is 0.0010221500415354967\n",
      "epoch: 4 step: 645, loss is 0.047021668404340744\n",
      "epoch: 4 step: 646, loss is 0.14599889516830444\n",
      "epoch: 4 step: 647, loss is 0.0016883626813068986\n",
      "epoch: 4 step: 648, loss is 0.18449093401432037\n",
      "epoch: 4 step: 649, loss is 0.007012805435806513\n",
      "epoch: 4 step: 650, loss is 0.0019641434773802757\n",
      "epoch: 4 step: 651, loss is 0.0014285006327554584\n",
      "epoch: 4 step: 652, loss is 0.0033323250245302916\n",
      "epoch: 4 step: 653, loss is 0.025988634675741196\n",
      "epoch: 4 step: 654, loss is 0.001648290315642953\n",
      "epoch: 4 step: 655, loss is 0.04320238158106804\n",
      "epoch: 4 step: 656, loss is 0.010956861078739166\n",
      "epoch: 4 step: 657, loss is 0.007956644520163536\n",
      "epoch: 4 step: 658, loss is 0.001957398373633623\n",
      "epoch: 4 step: 659, loss is 0.008605226874351501\n",
      "epoch: 4 step: 660, loss is 0.16982921957969666\n",
      "epoch: 4 step: 661, loss is 0.008501371368765831\n",
      "epoch: 4 step: 662, loss is 0.037753790616989136\n",
      "epoch: 4 step: 663, loss is 0.01057305559515953\n",
      "epoch: 4 step: 664, loss is 0.21615570783615112\n",
      "epoch: 4 step: 665, loss is 0.023381037637591362\n",
      "epoch: 4 step: 666, loss is 0.000604232947807759\n",
      "epoch: 4 step: 667, loss is 0.002936970442533493\n",
      "epoch: 4 step: 668, loss is 0.013724101707339287\n",
      "epoch: 4 step: 669, loss is 0.09131472557783127\n",
      "epoch: 4 step: 670, loss is 0.060680173337459564\n",
      "epoch: 4 step: 671, loss is 0.014511961489915848\n",
      "epoch: 4 step: 672, loss is 0.009026914834976196\n",
      "epoch: 4 step: 673, loss is 0.00397111289203167\n",
      "epoch: 4 step: 674, loss is 0.025260161608457565\n",
      "epoch: 4 step: 675, loss is 0.0015498073771595955\n",
      "epoch: 4 step: 676, loss is 0.008468191139400005\n",
      "epoch: 4 step: 677, loss is 0.006684624124318361\n",
      "epoch: 4 step: 678, loss is 0.0019517659675329924\n",
      "epoch: 4 step: 679, loss is 0.010971753858029842\n",
      "epoch: 4 step: 680, loss is 0.08329681307077408\n",
      "epoch: 4 step: 681, loss is 0.00260165031068027\n",
      "epoch: 4 step: 682, loss is 0.022154957056045532\n",
      "epoch: 4 step: 683, loss is 0.04456453397870064\n",
      "epoch: 4 step: 684, loss is 0.00648844288662076\n",
      "epoch: 4 step: 685, loss is 0.16012506186962128\n",
      "epoch: 4 step: 686, loss is 0.08791034668684006\n",
      "epoch: 4 step: 687, loss is 0.02020333893597126\n",
      "epoch: 4 step: 688, loss is 0.0010290381032973528\n",
      "epoch: 4 step: 689, loss is 0.012540347874164581\n",
      "epoch: 4 step: 690, loss is 0.116570845246315\n",
      "epoch: 4 step: 691, loss is 0.04454457387328148\n",
      "epoch: 4 step: 692, loss is 0.008820434100925922\n",
      "epoch: 4 step: 693, loss is 0.0027184125501662493\n",
      "epoch: 4 step: 694, loss is 0.0035678150597959757\n",
      "epoch: 4 step: 695, loss is 0.027005955576896667\n",
      "epoch: 4 step: 696, loss is 0.003452856093645096\n",
      "epoch: 4 step: 697, loss is 0.0031273802742362022\n",
      "epoch: 4 step: 698, loss is 0.002289622789248824\n",
      "epoch: 4 step: 699, loss is 0.061838991940021515\n",
      "epoch: 4 step: 700, loss is 0.007882886566221714\n",
      "epoch: 4 step: 701, loss is 0.033639196306467056\n",
      "epoch: 4 step: 702, loss is 0.028174428269267082\n",
      "epoch: 4 step: 703, loss is 0.06743697822093964\n",
      "epoch: 4 step: 704, loss is 0.004636190831661224\n",
      "epoch: 4 step: 705, loss is 0.008312409743666649\n",
      "epoch: 4 step: 706, loss is 0.20515473186969757\n",
      "epoch: 4 step: 707, loss is 0.008371247909963131\n",
      "epoch: 4 step: 708, loss is 0.032062750309705734\n",
      "epoch: 4 step: 709, loss is 0.003288483712822199\n",
      "epoch: 4 step: 710, loss is 0.020697150379419327\n",
      "epoch: 4 step: 711, loss is 0.002191971056163311\n",
      "epoch: 4 step: 712, loss is 0.00029504552367143333\n",
      "epoch: 4 step: 713, loss is 0.047759123146533966\n",
      "epoch: 4 step: 714, loss is 0.002492924453690648\n",
      "epoch: 4 step: 715, loss is 0.00047954151523299515\n",
      "epoch: 4 step: 716, loss is 0.24979376792907715\n",
      "epoch: 4 step: 717, loss is 0.016600586473941803\n",
      "epoch: 4 step: 718, loss is 0.01472175121307373\n",
      "epoch: 4 step: 719, loss is 0.07748779654502869\n",
      "epoch: 4 step: 720, loss is 0.0018820040859282017\n",
      "epoch: 4 step: 721, loss is 0.030830705538392067\n",
      "epoch: 4 step: 722, loss is 0.005488102789968252\n",
      "epoch: 4 step: 723, loss is 0.007399744354188442\n",
      "epoch: 4 step: 724, loss is 0.01667287014424801\n",
      "epoch: 4 step: 725, loss is 0.0018004087032750249\n",
      "epoch: 4 step: 726, loss is 0.03112885169684887\n",
      "epoch: 4 step: 727, loss is 0.003490071278065443\n",
      "epoch: 4 step: 728, loss is 0.0027881916612386703\n",
      "epoch: 4 step: 729, loss is 0.08725621551275253\n",
      "epoch: 4 step: 730, loss is 0.013777167536318302\n",
      "epoch: 4 step: 731, loss is 0.14210225641727448\n",
      "epoch: 4 step: 732, loss is 0.3324231803417206\n",
      "epoch: 4 step: 733, loss is 0.07118348777294159\n",
      "epoch: 4 step: 734, loss is 0.04410545527935028\n",
      "epoch: 4 step: 735, loss is 0.0021768591832369566\n",
      "epoch: 4 step: 736, loss is 0.021016597747802734\n",
      "epoch: 4 step: 737, loss is 0.0005870870663784444\n",
      "epoch: 4 step: 738, loss is 0.0022115360479801893\n",
      "epoch: 4 step: 739, loss is 0.0004242700815666467\n",
      "epoch: 4 step: 740, loss is 0.02929581329226494\n",
      "epoch: 4 step: 741, loss is 0.0012949667870998383\n",
      "epoch: 4 step: 742, loss is 0.0010118233039975166\n",
      "epoch: 4 step: 743, loss is 0.0037865228950977325\n",
      "epoch: 4 step: 744, loss is 0.002348699839785695\n",
      "epoch: 4 step: 745, loss is 0.10245808213949203\n",
      "epoch: 4 step: 746, loss is 0.04330875724554062\n",
      "epoch: 4 step: 747, loss is 0.05721587687730789\n",
      "epoch: 4 step: 748, loss is 0.00045891510671935976\n",
      "epoch: 4 step: 749, loss is 0.14098460972309113\n",
      "epoch: 4 step: 750, loss is 0.030308524146676064\n",
      "epoch: 4 step: 751, loss is 0.005121964029967785\n",
      "epoch: 4 step: 752, loss is 0.0040544504299759865\n",
      "epoch: 4 step: 753, loss is 0.011140433140099049\n",
      "epoch: 4 step: 754, loss is 0.13000932335853577\n",
      "epoch: 4 step: 755, loss is 0.0028616308700293303\n",
      "epoch: 4 step: 756, loss is 0.01661132462322712\n",
      "epoch: 4 step: 757, loss is 0.005289546679705381\n",
      "epoch: 4 step: 758, loss is 0.002956584794446826\n",
      "epoch: 4 step: 759, loss is 0.012119991704821587\n",
      "epoch: 4 step: 760, loss is 0.006799020804464817\n",
      "epoch: 4 step: 761, loss is 0.006373224779963493\n",
      "epoch: 4 step: 762, loss is 0.0020382723305374384\n",
      "epoch: 4 step: 763, loss is 0.0010961552616208792\n",
      "epoch: 4 step: 764, loss is 0.007167814299464226\n",
      "epoch: 4 step: 765, loss is 0.0048333993181586266\n",
      "epoch: 4 step: 766, loss is 0.1446179449558258\n",
      "epoch: 4 step: 767, loss is 0.04642388969659805\n",
      "epoch: 4 step: 768, loss is 0.0014185276813805103\n",
      "epoch: 4 step: 769, loss is 0.0006202610675245523\n",
      "epoch: 4 step: 770, loss is 0.00465084332972765\n",
      "epoch: 4 step: 771, loss is 0.00036071744398213923\n",
      "epoch: 4 step: 772, loss is 0.2070891559123993\n",
      "epoch: 4 step: 773, loss is 0.00017502414993941784\n",
      "epoch: 4 step: 774, loss is 0.2845187783241272\n",
      "epoch: 4 step: 775, loss is 0.010776557959616184\n",
      "epoch: 4 step: 776, loss is 0.10059786587953568\n",
      "epoch: 4 step: 777, loss is 0.12060851603746414\n",
      "epoch: 4 step: 778, loss is 0.042069606482982635\n",
      "epoch: 4 step: 779, loss is 0.005725913681089878\n",
      "epoch: 4 step: 780, loss is 0.14634832739830017\n",
      "epoch: 4 step: 781, loss is 0.001474146731197834\n",
      "epoch: 4 step: 782, loss is 0.06950471550226212\n",
      "epoch: 4 step: 783, loss is 0.016680343076586723\n",
      "epoch: 4 step: 784, loss is 0.002059651305899024\n",
      "epoch: 4 step: 785, loss is 0.009865600615739822\n",
      "epoch: 4 step: 786, loss is 0.07508891820907593\n",
      "epoch: 4 step: 787, loss is 0.1065407544374466\n",
      "epoch: 4 step: 788, loss is 0.021597014740109444\n",
      "epoch: 4 step: 789, loss is 0.021944118663668633\n",
      "epoch: 4 step: 790, loss is 0.0076988460496068\n",
      "epoch: 4 step: 791, loss is 0.1282859444618225\n",
      "epoch: 4 step: 792, loss is 0.019782382994890213\n",
      "epoch: 4 step: 793, loss is 0.0535123310983181\n",
      "epoch: 4 step: 794, loss is 0.036154378205537796\n",
      "epoch: 4 step: 795, loss is 0.0017547754105180502\n",
      "epoch: 4 step: 796, loss is 0.004094487521797419\n",
      "epoch: 4 step: 797, loss is 0.05207953602075577\n",
      "epoch: 4 step: 798, loss is 0.0018305669073015451\n",
      "epoch: 4 step: 799, loss is 0.03380857780575752\n",
      "epoch: 4 step: 800, loss is 0.03394760191440582\n",
      "epoch: 4 step: 801, loss is 0.019461242482066154\n",
      "epoch: 4 step: 802, loss is 0.032936252653598785\n",
      "epoch: 4 step: 803, loss is 0.06706006824970245\n",
      "epoch: 4 step: 804, loss is 0.012253805994987488\n",
      "epoch: 4 step: 805, loss is 0.0011881592217832804\n",
      "epoch: 4 step: 806, loss is 0.0023194088134914637\n",
      "epoch: 4 step: 807, loss is 0.015151350758969784\n",
      "epoch: 4 step: 808, loss is 0.14117667078971863\n",
      "epoch: 4 step: 809, loss is 0.00025931111304089427\n",
      "epoch: 4 step: 810, loss is 0.0008054016507230699\n",
      "epoch: 4 step: 811, loss is 0.0025224382989108562\n",
      "epoch: 4 step: 812, loss is 0.019282445311546326\n",
      "epoch: 4 step: 813, loss is 0.00935317575931549\n",
      "epoch: 4 step: 814, loss is 0.005033199209719896\n",
      "epoch: 4 step: 815, loss is 0.001597403665073216\n",
      "epoch: 4 step: 816, loss is 6.937178841326386e-05\n",
      "epoch: 4 step: 817, loss is 0.010120264254510403\n",
      "epoch: 4 step: 818, loss is 0.10547605901956558\n",
      "epoch: 4 step: 819, loss is 0.005463585257530212\n",
      "epoch: 4 step: 820, loss is 0.0020477210637181997\n",
      "epoch: 4 step: 821, loss is 0.005995438899844885\n",
      "epoch: 4 step: 822, loss is 0.06128936633467674\n",
      "epoch: 4 step: 823, loss is 0.04291288182139397\n",
      "epoch: 4 step: 824, loss is 0.012289483100175858\n",
      "epoch: 4 step: 825, loss is 0.05189034715294838\n",
      "epoch: 4 step: 826, loss is 0.14400091767311096\n",
      "epoch: 4 step: 827, loss is 0.020405232906341553\n",
      "epoch: 4 step: 828, loss is 0.27312061190605164\n",
      "epoch: 4 step: 829, loss is 0.00012995976430829614\n",
      "epoch: 4 step: 830, loss is 0.0011792685836553574\n",
      "epoch: 4 step: 831, loss is 0.006163408979773521\n",
      "epoch: 4 step: 832, loss is 0.0003048611106351018\n",
      "epoch: 4 step: 833, loss is 0.0275583453476429\n",
      "epoch: 4 step: 834, loss is 0.16052109003067017\n",
      "epoch: 4 step: 835, loss is 0.010262633673846722\n",
      "epoch: 4 step: 836, loss is 0.004035383462905884\n",
      "epoch: 4 step: 837, loss is 0.0013199226232245564\n",
      "epoch: 4 step: 838, loss is 0.051185257732868195\n",
      "epoch: 4 step: 839, loss is 0.016835186630487442\n",
      "epoch: 4 step: 840, loss is 0.0002211384562542662\n",
      "epoch: 4 step: 841, loss is 0.0011598592391237617\n",
      "epoch: 4 step: 842, loss is 0.21142803132534027\n",
      "epoch: 4 step: 843, loss is 0.07889821380376816\n",
      "epoch: 4 step: 844, loss is 0.011537088081240654\n",
      "epoch: 4 step: 845, loss is 0.0015785315772518516\n",
      "epoch: 4 step: 846, loss is 0.0025773472152650356\n",
      "epoch: 4 step: 847, loss is 0.0012963495682924986\n",
      "epoch: 4 step: 848, loss is 0.17611195147037506\n",
      "epoch: 4 step: 849, loss is 0.07795409858226776\n",
      "epoch: 4 step: 850, loss is 0.14344243705272675\n",
      "epoch: 4 step: 851, loss is 0.08315472304821014\n",
      "epoch: 4 step: 852, loss is 0.06820222735404968\n",
      "epoch: 4 step: 853, loss is 0.014894152991473675\n",
      "epoch: 4 step: 854, loss is 0.002785108517855406\n",
      "epoch: 4 step: 855, loss is 0.01879350282251835\n",
      "epoch: 4 step: 856, loss is 0.004196668043732643\n",
      "epoch: 4 step: 857, loss is 0.009741609916090965\n",
      "epoch: 4 step: 858, loss is 0.0011235357960686088\n",
      "epoch: 4 step: 859, loss is 0.041389334946870804\n",
      "epoch: 4 step: 860, loss is 0.13066858053207397\n",
      "epoch: 4 step: 861, loss is 0.0014627989148721099\n",
      "epoch: 4 step: 862, loss is 0.002802784088999033\n",
      "epoch: 4 step: 863, loss is 0.005006199236959219\n",
      "epoch: 4 step: 864, loss is 0.012540845200419426\n",
      "epoch: 4 step: 865, loss is 0.05364298075437546\n",
      "epoch: 4 step: 866, loss is 0.09038946777582169\n",
      "epoch: 4 step: 867, loss is 0.07594912499189377\n",
      "epoch: 4 step: 868, loss is 0.010523645207285881\n",
      "epoch: 4 step: 869, loss is 0.009959622286260128\n",
      "epoch: 4 step: 870, loss is 0.0053432900458574295\n",
      "epoch: 4 step: 871, loss is 0.004749676678329706\n",
      "epoch: 4 step: 872, loss is 0.012818224728107452\n",
      "epoch: 4 step: 873, loss is 0.03521277755498886\n",
      "epoch: 4 step: 874, loss is 0.08494891971349716\n",
      "epoch: 4 step: 875, loss is 0.007718218490481377\n",
      "epoch: 4 step: 876, loss is 0.003646223107352853\n",
      "epoch: 4 step: 877, loss is 0.006313929334282875\n",
      "epoch: 4 step: 878, loss is 0.010561839677393436\n",
      "epoch: 4 step: 879, loss is 0.004989487584680319\n",
      "epoch: 4 step: 880, loss is 0.041139401495456696\n",
      "epoch: 4 step: 881, loss is 0.007828615605831146\n",
      "epoch: 4 step: 882, loss is 0.14819659292697906\n",
      "epoch: 4 step: 883, loss is 0.004092933610081673\n",
      "epoch: 4 step: 884, loss is 0.006896248087286949\n",
      "epoch: 4 step: 885, loss is 0.0012461391743272543\n",
      "epoch: 4 step: 886, loss is 0.0008864289266057312\n",
      "epoch: 4 step: 887, loss is 0.009064029902219772\n",
      "epoch: 4 step: 888, loss is 0.0011473611230030656\n",
      "epoch: 4 step: 889, loss is 0.00791762862354517\n",
      "epoch: 4 step: 890, loss is 0.009763252921402454\n",
      "epoch: 4 step: 891, loss is 0.0257041584700346\n",
      "epoch: 4 step: 892, loss is 0.001560775563120842\n",
      "epoch: 4 step: 893, loss is 0.008506976068019867\n",
      "epoch: 4 step: 894, loss is 0.008739223703742027\n",
      "epoch: 4 step: 895, loss is 0.200991690158844\n",
      "epoch: 4 step: 896, loss is 0.0019135982729494572\n",
      "epoch: 4 step: 897, loss is 0.015128916129469872\n",
      "epoch: 4 step: 898, loss is 0.12465031445026398\n",
      "epoch: 4 step: 899, loss is 0.21700692176818848\n",
      "epoch: 4 step: 900, loss is 0.038111612200737\n",
      "epoch: 4 step: 901, loss is 0.03711621090769768\n",
      "epoch: 4 step: 902, loss is 0.0009259770158678293\n",
      "epoch: 4 step: 903, loss is 0.0406423956155777\n",
      "epoch: 4 step: 904, loss is 0.028882049024105072\n",
      "epoch: 4 step: 905, loss is 0.015836775302886963\n",
      "epoch: 4 step: 906, loss is 0.0026594947557896376\n",
      "epoch: 4 step: 907, loss is 0.005691820755600929\n",
      "epoch: 4 step: 908, loss is 0.0036991173401474953\n",
      "epoch: 4 step: 909, loss is 0.029428156092762947\n",
      "epoch: 4 step: 910, loss is 0.2002430558204651\n",
      "epoch: 4 step: 911, loss is 0.10271503031253815\n",
      "epoch: 4 step: 912, loss is 0.11814823746681213\n",
      "epoch: 4 step: 913, loss is 0.007215942721813917\n",
      "epoch: 4 step: 914, loss is 0.014318556524813175\n",
      "epoch: 4 step: 915, loss is 0.001596231246367097\n",
      "epoch: 4 step: 916, loss is 0.004679334349930286\n",
      "epoch: 4 step: 917, loss is 0.0015245195245370269\n",
      "epoch: 4 step: 918, loss is 0.004444111604243517\n",
      "epoch: 4 step: 919, loss is 0.051722172647714615\n",
      "epoch: 4 step: 920, loss is 0.04767240956425667\n",
      "epoch: 4 step: 921, loss is 0.06617923825979233\n",
      "epoch: 4 step: 922, loss is 0.05578586459159851\n",
      "epoch: 4 step: 923, loss is 0.00825293269008398\n",
      "epoch: 4 step: 924, loss is 0.16710111498832703\n",
      "epoch: 4 step: 925, loss is 0.059708938002586365\n",
      "epoch: 4 step: 926, loss is 0.0002797545166686177\n",
      "epoch: 4 step: 927, loss is 0.0570184700191021\n",
      "epoch: 4 step: 928, loss is 0.05592711269855499\n",
      "epoch: 4 step: 929, loss is 0.04251948371529579\n",
      "epoch: 4 step: 930, loss is 0.0013534388272091746\n",
      "epoch: 4 step: 931, loss is 0.005496342200785875\n",
      "epoch: 4 step: 932, loss is 0.0008589397766627371\n",
      "epoch: 4 step: 933, loss is 0.011782134883105755\n",
      "epoch: 4 step: 934, loss is 0.007024060934782028\n",
      "epoch: 4 step: 935, loss is 0.2737317383289337\n",
      "epoch: 4 step: 936, loss is 0.12559105455875397\n",
      "epoch: 4 step: 937, loss is 0.12507426738739014\n",
      "epoch: 4 step: 938, loss is 0.002771133091300726\n",
      "epoch: 4 step: 939, loss is 0.002577932085841894\n",
      "epoch: 4 step: 940, loss is 0.01355261355638504\n",
      "epoch: 4 step: 941, loss is 0.01767006516456604\n",
      "epoch: 4 step: 942, loss is 0.3322671949863434\n",
      "epoch: 4 step: 943, loss is 0.0017281851032748818\n",
      "epoch: 4 step: 944, loss is 0.005382098723202944\n",
      "epoch: 4 step: 945, loss is 0.0033430627081543207\n",
      "epoch: 4 step: 946, loss is 0.013053329661488533\n",
      "epoch: 4 step: 947, loss is 0.019140146672725677\n",
      "epoch: 4 step: 948, loss is 0.014895770698785782\n",
      "epoch: 4 step: 949, loss is 0.004092219285666943\n",
      "epoch: 4 step: 950, loss is 0.02856612764298916\n",
      "epoch: 4 step: 951, loss is 0.0034687079023569822\n",
      "epoch: 4 step: 952, loss is 0.018385235220193863\n",
      "epoch: 4 step: 953, loss is 0.011370163410902023\n",
      "epoch: 4 step: 954, loss is 0.040890783071517944\n",
      "epoch: 4 step: 955, loss is 0.1478419303894043\n",
      "epoch: 4 step: 956, loss is 0.02108866535127163\n",
      "epoch: 4 step: 957, loss is 0.021857399493455887\n",
      "epoch: 4 step: 958, loss is 0.0009755462524481118\n",
      "epoch: 4 step: 959, loss is 0.0026725097559392452\n",
      "epoch: 4 step: 960, loss is 0.002547618467360735\n",
      "epoch: 4 step: 961, loss is 0.0037050056271255016\n",
      "epoch: 4 step: 962, loss is 0.04906720295548439\n",
      "epoch: 4 step: 963, loss is 0.018274663016200066\n",
      "epoch: 4 step: 964, loss is 0.07052130997180939\n",
      "epoch: 4 step: 965, loss is 0.02481958270072937\n",
      "epoch: 4 step: 966, loss is 0.00260420935228467\n",
      "epoch: 4 step: 967, loss is 0.053930144757032394\n",
      "epoch: 4 step: 968, loss is 0.0018225141102448106\n",
      "epoch: 4 step: 969, loss is 0.009846299886703491\n",
      "epoch: 4 step: 970, loss is 0.00883564818650484\n",
      "epoch: 4 step: 971, loss is 0.007401815615594387\n",
      "epoch: 4 step: 972, loss is 0.24751517176628113\n",
      "epoch: 4 step: 973, loss is 0.000273869838565588\n",
      "epoch: 4 step: 974, loss is 0.016971999779343605\n",
      "epoch: 4 step: 975, loss is 0.008466595783829689\n",
      "epoch: 4 step: 976, loss is 0.0006357800448313355\n",
      "epoch: 4 step: 977, loss is 0.1850213259458542\n",
      "epoch: 4 step: 978, loss is 0.00427801301702857\n",
      "epoch: 4 step: 979, loss is 0.010957084596157074\n",
      "epoch: 4 step: 980, loss is 0.041489556431770325\n",
      "epoch: 4 step: 981, loss is 0.002731843153014779\n",
      "epoch: 4 step: 982, loss is 0.004728599451482296\n",
      "epoch: 4 step: 983, loss is 0.0013578432844951749\n",
      "epoch: 4 step: 984, loss is 0.006802988704293966\n",
      "epoch: 4 step: 985, loss is 0.00696810195222497\n",
      "epoch: 4 step: 986, loss is 0.007955620065331459\n",
      "epoch: 4 step: 987, loss is 0.24684418737888336\n",
      "epoch: 4 step: 988, loss is 0.01844053715467453\n",
      "epoch: 4 step: 989, loss is 0.021059654653072357\n",
      "epoch: 4 step: 990, loss is 0.0027597222942858934\n",
      "epoch: 4 step: 991, loss is 0.00793156772851944\n",
      "epoch: 4 step: 992, loss is 0.012398922815918922\n",
      "epoch: 4 step: 993, loss is 0.004646422807127237\n",
      "epoch: 4 step: 994, loss is 0.0034065549261868\n",
      "epoch: 4 step: 995, loss is 0.05648992210626602\n",
      "epoch: 4 step: 996, loss is 0.005161862354725599\n",
      "epoch: 4 step: 997, loss is 0.00431646266952157\n",
      "epoch: 4 step: 998, loss is 0.0012096097925677896\n",
      "epoch: 4 step: 999, loss is 0.007696559187024832\n",
      "epoch: 4 step: 1000, loss is 0.0083626015111804\n",
      "epoch: 4 step: 1001, loss is 0.012653184123337269\n",
      "epoch: 4 step: 1002, loss is 0.0027755347546190023\n",
      "epoch: 4 step: 1003, loss is 0.001342904637567699\n",
      "epoch: 4 step: 1004, loss is 0.021111510694026947\n",
      "epoch: 4 step: 1005, loss is 0.009438694454729557\n",
      "epoch: 4 step: 1006, loss is 0.0009680583607405424\n",
      "epoch: 4 step: 1007, loss is 0.0007955770706757903\n",
      "epoch: 4 step: 1008, loss is 0.04194992035627365\n",
      "epoch: 4 step: 1009, loss is 0.10605713725090027\n",
      "epoch: 4 step: 1010, loss is 0.007177221123129129\n",
      "epoch: 4 step: 1011, loss is 0.0010726689361035824\n",
      "epoch: 4 step: 1012, loss is 0.05923435464501381\n",
      "epoch: 4 step: 1013, loss is 0.00041864768718369305\n",
      "epoch: 4 step: 1014, loss is 0.0029036528430879116\n",
      "epoch: 4 step: 1015, loss is 0.025918666273355484\n",
      "epoch: 4 step: 1016, loss is 0.0017501040128991008\n",
      "epoch: 4 step: 1017, loss is 0.0029124789871275425\n",
      "epoch: 4 step: 1018, loss is 0.0011813014280050993\n",
      "epoch: 4 step: 1019, loss is 0.028164682909846306\n",
      "epoch: 4 step: 1020, loss is 0.15868094563484192\n",
      "epoch: 4 step: 1021, loss is 0.09783275425434113\n",
      "epoch: 4 step: 1022, loss is 0.0068667009472846985\n",
      "epoch: 4 step: 1023, loss is 0.0016040202463045716\n",
      "epoch: 4 step: 1024, loss is 0.0018489239737391472\n",
      "epoch: 4 step: 1025, loss is 0.11312787979841232\n",
      "epoch: 4 step: 1026, loss is 0.0008004315313883126\n",
      "epoch: 4 step: 1027, loss is 0.2472078800201416\n",
      "epoch: 4 step: 1028, loss is 0.14360693097114563\n",
      "epoch: 4 step: 1029, loss is 0.00015065126353874803\n",
      "epoch: 4 step: 1030, loss is 0.0005578788695856929\n",
      "epoch: 4 step: 1031, loss is 0.013482935726642609\n",
      "epoch: 4 step: 1032, loss is 0.02144894190132618\n",
      "epoch: 4 step: 1033, loss is 0.08002816140651703\n",
      "epoch: 4 step: 1034, loss is 0.344835489988327\n",
      "epoch: 4 step: 1035, loss is 0.34594547748565674\n",
      "epoch: 4 step: 1036, loss is 0.0012417073594406247\n",
      "epoch: 4 step: 1037, loss is 0.06630771607160568\n",
      "epoch: 4 step: 1038, loss is 0.06444460153579712\n",
      "epoch: 4 step: 1039, loss is 0.0005836213822476566\n",
      "epoch: 4 step: 1040, loss is 0.0010645219590514898\n",
      "epoch: 4 step: 1041, loss is 0.0010127457790076733\n",
      "epoch: 4 step: 1042, loss is 0.405030220746994\n",
      "epoch: 4 step: 1043, loss is 0.05380898714065552\n",
      "epoch: 4 step: 1044, loss is 0.06674515455961227\n",
      "epoch: 4 step: 1045, loss is 0.09571435302495956\n",
      "epoch: 4 step: 1046, loss is 0.04533261060714722\n",
      "epoch: 4 step: 1047, loss is 0.04042934626340866\n",
      "epoch: 4 step: 1048, loss is 0.00949916336685419\n",
      "epoch: 4 step: 1049, loss is 0.0063317399471998215\n",
      "epoch: 4 step: 1050, loss is 0.004915906116366386\n",
      "epoch: 4 step: 1051, loss is 0.010669603943824768\n",
      "epoch: 4 step: 1052, loss is 0.02832779660820961\n",
      "epoch: 4 step: 1053, loss is 0.02966141141951084\n",
      "epoch: 4 step: 1054, loss is 0.14924457669258118\n",
      "epoch: 4 step: 1055, loss is 0.066883385181427\n",
      "epoch: 4 step: 1056, loss is 0.04561762139201164\n",
      "epoch: 4 step: 1057, loss is 0.22565814852714539\n",
      "epoch: 4 step: 1058, loss is 0.005101256538182497\n",
      "epoch: 4 step: 1059, loss is 0.019855480641126633\n",
      "epoch: 4 step: 1060, loss is 0.07103393971920013\n",
      "epoch: 4 step: 1061, loss is 0.14002235233783722\n",
      "epoch: 4 step: 1062, loss is 0.0015156591543927789\n",
      "epoch: 4 step: 1063, loss is 0.005102723371237516\n",
      "epoch: 4 step: 1064, loss is 0.00479190843179822\n",
      "epoch: 4 step: 1065, loss is 0.26188889145851135\n",
      "epoch: 4 step: 1066, loss is 0.14260484278202057\n",
      "epoch: 4 step: 1067, loss is 0.0073366714641451836\n",
      "epoch: 4 step: 1068, loss is 0.010641107335686684\n",
      "epoch: 4 step: 1069, loss is 0.014233380556106567\n",
      "epoch: 4 step: 1070, loss is 0.11252560466527939\n",
      "epoch: 4 step: 1071, loss is 0.0007924943347461522\n",
      "epoch: 4 step: 1072, loss is 0.006677891127765179\n",
      "epoch: 4 step: 1073, loss is 0.004906358662992716\n",
      "epoch: 4 step: 1074, loss is 0.017606034874916077\n",
      "epoch: 4 step: 1075, loss is 0.0048360927030444145\n",
      "epoch: 4 step: 1076, loss is 0.4398873746395111\n",
      "epoch: 4 step: 1077, loss is 0.0033643089700490236\n",
      "epoch: 4 step: 1078, loss is 0.005564787890762091\n",
      "epoch: 4 step: 1079, loss is 0.034580063074827194\n",
      "epoch: 4 step: 1080, loss is 0.05703185126185417\n",
      "epoch: 4 step: 1081, loss is 0.334774374961853\n",
      "epoch: 4 step: 1082, loss is 0.028239954262971878\n",
      "epoch: 4 step: 1083, loss is 0.16194342076778412\n",
      "epoch: 4 step: 1084, loss is 0.07434257864952087\n",
      "epoch: 4 step: 1085, loss is 0.02446242980659008\n",
      "epoch: 4 step: 1086, loss is 0.008301617577672005\n",
      "epoch: 4 step: 1087, loss is 0.014359486289322376\n",
      "epoch: 4 step: 1088, loss is 0.026867635548114777\n",
      "epoch: 4 step: 1089, loss is 0.2172967940568924\n",
      "epoch: 4 step: 1090, loss is 0.13558803498744965\n",
      "epoch: 4 step: 1091, loss is 0.012985378503799438\n",
      "epoch: 4 step: 1092, loss is 0.03537566959857941\n",
      "epoch: 4 step: 1093, loss is 0.006062589585781097\n",
      "epoch: 4 step: 1094, loss is 0.028614746406674385\n",
      "epoch: 4 step: 1095, loss is 0.0032787243835628033\n",
      "epoch: 4 step: 1096, loss is 0.0008715057629160583\n",
      "epoch: 4 step: 1097, loss is 0.15235398709774017\n",
      "epoch: 4 step: 1098, loss is 0.01654522866010666\n",
      "epoch: 4 step: 1099, loss is 0.009044763632118702\n",
      "epoch: 4 step: 1100, loss is 0.0014483666745945811\n",
      "epoch: 4 step: 1101, loss is 0.006431132089346647\n",
      "epoch: 4 step: 1102, loss is 0.005018637049943209\n",
      "epoch: 4 step: 1103, loss is 0.02192317694425583\n",
      "epoch: 4 step: 1104, loss is 0.0062587084248661995\n",
      "epoch: 4 step: 1105, loss is 0.008834796957671642\n",
      "epoch: 4 step: 1106, loss is 0.012208407744765282\n",
      "epoch: 4 step: 1107, loss is 0.050638627260923386\n",
      "epoch: 4 step: 1108, loss is 0.023596983402967453\n",
      "epoch: 4 step: 1109, loss is 0.07445429265499115\n",
      "epoch: 4 step: 1110, loss is 0.06235610321164131\n",
      "epoch: 4 step: 1111, loss is 0.019472923129796982\n",
      "epoch: 4 step: 1112, loss is 0.00331510161049664\n",
      "epoch: 4 step: 1113, loss is 0.052597351372241974\n",
      "epoch: 4 step: 1114, loss is 0.039728567004203796\n",
      "epoch: 4 step: 1115, loss is 0.012390567921102047\n",
      "epoch: 4 step: 1116, loss is 0.05209540203213692\n",
      "epoch: 4 step: 1117, loss is 0.0050070746801793575\n",
      "epoch: 4 step: 1118, loss is 0.002955940319225192\n",
      "epoch: 4 step: 1119, loss is 0.09365760535001755\n",
      "epoch: 4 step: 1120, loss is 0.04180213436484337\n",
      "epoch: 4 step: 1121, loss is 0.0622362457215786\n",
      "epoch: 4 step: 1122, loss is 0.004040413536131382\n",
      "epoch: 4 step: 1123, loss is 0.002692263573408127\n",
      "epoch: 4 step: 1124, loss is 0.0028162419330328703\n",
      "epoch: 4 step: 1125, loss is 0.06950122863054276\n",
      "epoch: 4 step: 1126, loss is 0.004234284162521362\n",
      "epoch: 4 step: 1127, loss is 0.0012965723872184753\n",
      "epoch: 4 step: 1128, loss is 0.005049142520874739\n",
      "epoch: 4 step: 1129, loss is 0.023756345734000206\n",
      "epoch: 4 step: 1130, loss is 0.016216130927205086\n",
      "epoch: 4 step: 1131, loss is 0.018892502412199974\n",
      "epoch: 4 step: 1132, loss is 0.0021560294553637505\n",
      "epoch: 4 step: 1133, loss is 0.013264885172247887\n",
      "epoch: 4 step: 1134, loss is 0.06682659685611725\n",
      "epoch: 4 step: 1135, loss is 0.009197238832712173\n",
      "epoch: 4 step: 1136, loss is 0.027612168341875076\n",
      "epoch: 4 step: 1137, loss is 0.03772823512554169\n",
      "epoch: 4 step: 1138, loss is 0.013419942930340767\n",
      "epoch: 4 step: 1139, loss is 0.0037464189808815718\n",
      "epoch: 4 step: 1140, loss is 0.01644572615623474\n",
      "epoch: 4 step: 1141, loss is 0.00028281108825467527\n",
      "epoch: 4 step: 1142, loss is 0.037337593734264374\n",
      "epoch: 4 step: 1143, loss is 0.01532633975148201\n",
      "epoch: 4 step: 1144, loss is 0.03720124810934067\n",
      "epoch: 4 step: 1145, loss is 0.018053652718663216\n",
      "epoch: 4 step: 1146, loss is 0.0015952503308653831\n",
      "epoch: 4 step: 1147, loss is 0.01013875287026167\n",
      "epoch: 4 step: 1148, loss is 0.09891846776008606\n",
      "epoch: 4 step: 1149, loss is 0.015429830178618431\n",
      "epoch: 4 step: 1150, loss is 0.22507059574127197\n",
      "epoch: 4 step: 1151, loss is 0.035920675843954086\n",
      "epoch: 4 step: 1152, loss is 0.1628127247095108\n",
      "epoch: 4 step: 1153, loss is 0.006322311237454414\n",
      "epoch: 4 step: 1154, loss is 0.0032178282272070646\n",
      "epoch: 4 step: 1155, loss is 0.0038133191410452127\n",
      "epoch: 4 step: 1156, loss is 0.004347854293882847\n",
      "epoch: 4 step: 1157, loss is 0.030950743705034256\n",
      "epoch: 4 step: 1158, loss is 0.0006189366104081273\n",
      "epoch: 4 step: 1159, loss is 0.03578466549515724\n",
      "epoch: 4 step: 1160, loss is 0.029472727328538895\n",
      "epoch: 4 step: 1161, loss is 0.0029811637941747904\n",
      "epoch: 4 step: 1162, loss is 0.028837747871875763\n",
      "epoch: 4 step: 1163, loss is 0.007652630098164082\n",
      "epoch: 4 step: 1164, loss is 0.005970426835119724\n",
      "epoch: 4 step: 1165, loss is 0.0020772614516317844\n",
      "epoch: 4 step: 1166, loss is 0.0018745133420452476\n",
      "epoch: 4 step: 1167, loss is 0.08556582033634186\n",
      "epoch: 4 step: 1168, loss is 0.0004321498272474855\n",
      "epoch: 4 step: 1169, loss is 0.014025707729160786\n",
      "epoch: 4 step: 1170, loss is 0.04540550336241722\n",
      "epoch: 4 step: 1171, loss is 0.0033433774951845407\n",
      "epoch: 4 step: 1172, loss is 0.01119096577167511\n",
      "epoch: 4 step: 1173, loss is 0.03599352389574051\n",
      "epoch: 4 step: 1174, loss is 0.0037292586639523506\n",
      "epoch: 4 step: 1175, loss is 0.028024863451719284\n",
      "epoch: 4 step: 1176, loss is 0.019152389839291573\n",
      "epoch: 4 step: 1177, loss is 0.015763433650135994\n",
      "epoch: 4 step: 1178, loss is 0.16430923342704773\n",
      "epoch: 4 step: 1179, loss is 0.1744067668914795\n",
      "epoch: 4 step: 1180, loss is 0.0034158590715378523\n",
      "epoch: 4 step: 1181, loss is 0.007473770994693041\n",
      "epoch: 4 step: 1182, loss is 0.0022678065579384565\n",
      "epoch: 4 step: 1183, loss is 0.02737472578883171\n",
      "epoch: 4 step: 1184, loss is 0.16600127518177032\n",
      "epoch: 4 step: 1185, loss is 0.16465012729167938\n",
      "epoch: 4 step: 1186, loss is 0.11663012206554413\n",
      "epoch: 4 step: 1187, loss is 0.004534078296273947\n",
      "epoch: 4 step: 1188, loss is 0.0426744669675827\n",
      "epoch: 4 step: 1189, loss is 0.01623012125492096\n",
      "epoch: 4 step: 1190, loss is 0.12045225501060486\n",
      "epoch: 4 step: 1191, loss is 0.013196296989917755\n",
      "epoch: 4 step: 1192, loss is 0.0005870310124009848\n",
      "epoch: 4 step: 1193, loss is 0.23946097493171692\n",
      "epoch: 4 step: 1194, loss is 0.0038494286127388477\n",
      "epoch: 4 step: 1195, loss is 0.002369294175878167\n",
      "epoch: 4 step: 1196, loss is 0.01583871804177761\n",
      "epoch: 4 step: 1197, loss is 0.14604482054710388\n",
      "epoch: 4 step: 1198, loss is 0.0177061315625906\n",
      "epoch: 4 step: 1199, loss is 0.004830144811421633\n",
      "epoch: 4 step: 1200, loss is 0.004997438285499811\n",
      "epoch: 4 step: 1201, loss is 0.019594063982367516\n",
      "epoch: 4 step: 1202, loss is 0.0013543536188080907\n",
      "epoch: 4 step: 1203, loss is 0.20278051495552063\n",
      "epoch: 4 step: 1204, loss is 0.0811927542090416\n",
      "epoch: 4 step: 1205, loss is 0.002242019632831216\n",
      "epoch: 4 step: 1206, loss is 0.004560230765491724\n",
      "epoch: 4 step: 1207, loss is 0.09765946865081787\n",
      "epoch: 4 step: 1208, loss is 0.004211571998894215\n",
      "epoch: 4 step: 1209, loss is 0.001933908206410706\n",
      "epoch: 4 step: 1210, loss is 0.0090913912281394\n",
      "epoch: 4 step: 1211, loss is 0.04828859493136406\n",
      "epoch: 4 step: 1212, loss is 0.012654398567974567\n",
      "epoch: 4 step: 1213, loss is 0.1912398338317871\n",
      "epoch: 4 step: 1214, loss is 0.0018044449388980865\n",
      "epoch: 4 step: 1215, loss is 0.013605758547782898\n",
      "epoch: 4 step: 1216, loss is 0.10509936511516571\n",
      "epoch: 4 step: 1217, loss is 0.004235799424350262\n",
      "epoch: 4 step: 1218, loss is 0.022315680980682373\n",
      "epoch: 4 step: 1219, loss is 0.021864593029022217\n",
      "epoch: 4 step: 1220, loss is 0.11097549647092819\n",
      "epoch: 4 step: 1221, loss is 0.05971180647611618\n",
      "epoch: 4 step: 1222, loss is 0.09423387050628662\n",
      "epoch: 4 step: 1223, loss is 0.0009475883562117815\n",
      "epoch: 4 step: 1224, loss is 0.0021499719005078077\n",
      "epoch: 4 step: 1225, loss is 0.02316940203309059\n",
      "epoch: 4 step: 1226, loss is 0.002254282124340534\n",
      "epoch: 4 step: 1227, loss is 0.06831476837396622\n",
      "epoch: 4 step: 1228, loss is 0.000462948257336393\n",
      "epoch: 4 step: 1229, loss is 0.03209683671593666\n",
      "epoch: 4 step: 1230, loss is 0.08429361879825592\n",
      "epoch: 4 step: 1231, loss is 0.0034888568334281445\n",
      "epoch: 4 step: 1232, loss is 0.1189412996172905\n",
      "epoch: 4 step: 1233, loss is 0.03750245273113251\n",
      "epoch: 4 step: 1234, loss is 0.24134308099746704\n",
      "epoch: 4 step: 1235, loss is 0.014688028953969479\n",
      "epoch: 4 step: 1236, loss is 0.09820280224084854\n",
      "epoch: 4 step: 1237, loss is 0.030858421698212624\n",
      "epoch: 4 step: 1238, loss is 0.013097204267978668\n",
      "epoch: 4 step: 1239, loss is 0.00040878611616790295\n",
      "epoch: 4 step: 1240, loss is 0.00141687982250005\n",
      "epoch: 4 step: 1241, loss is 0.00017738319002091885\n",
      "epoch: 4 step: 1242, loss is 0.0006671181181445718\n",
      "epoch: 4 step: 1243, loss is 0.00413880031555891\n",
      "epoch: 4 step: 1244, loss is 0.005540488287806511\n",
      "epoch: 4 step: 1245, loss is 0.08785244077444077\n",
      "epoch: 4 step: 1246, loss is 0.002654670737683773\n",
      "epoch: 4 step: 1247, loss is 0.025504056364297867\n",
      "epoch: 4 step: 1248, loss is 0.012750206515192986\n",
      "epoch: 4 step: 1249, loss is 0.028723210096359253\n",
      "epoch: 4 step: 1250, loss is 0.0025344332680106163\n",
      "epoch: 4 step: 1251, loss is 0.02919555827975273\n",
      "epoch: 4 step: 1252, loss is 0.10333713889122009\n",
      "epoch: 4 step: 1253, loss is 0.01150198932737112\n",
      "epoch: 4 step: 1254, loss is 0.009781789034605026\n",
      "epoch: 4 step: 1255, loss is 0.045983415096998215\n",
      "epoch: 4 step: 1256, loss is 0.05726293474435806\n",
      "epoch: 4 step: 1257, loss is 0.015415458008646965\n",
      "epoch: 4 step: 1258, loss is 0.0010772536043077707\n",
      "epoch: 4 step: 1259, loss is 0.0022530958522111177\n",
      "epoch: 4 step: 1260, loss is 0.0036447211168706417\n",
      "epoch: 4 step: 1261, loss is 0.20312944054603577\n",
      "epoch: 4 step: 1262, loss is 0.005404227413237095\n",
      "epoch: 4 step: 1263, loss is 0.012107616290450096\n",
      "epoch: 4 step: 1264, loss is 0.04839550703763962\n",
      "epoch: 4 step: 1265, loss is 0.01391291618347168\n",
      "epoch: 4 step: 1266, loss is 0.021442294120788574\n",
      "epoch: 4 step: 1267, loss is 0.01298444252461195\n",
      "epoch: 4 step: 1268, loss is 0.0084192780777812\n",
      "epoch: 4 step: 1269, loss is 0.0878007560968399\n",
      "epoch: 4 step: 1270, loss is 0.07387945801019669\n",
      "epoch: 4 step: 1271, loss is 0.004299827851355076\n",
      "epoch: 4 step: 1272, loss is 0.005545175634324551\n",
      "epoch: 4 step: 1273, loss is 0.005260567180812359\n",
      "epoch: 4 step: 1274, loss is 0.011714335530996323\n",
      "epoch: 4 step: 1275, loss is 0.002911026356741786\n",
      "epoch: 4 step: 1276, loss is 0.0019706813618540764\n",
      "epoch: 4 step: 1277, loss is 0.11479969322681427\n",
      "epoch: 4 step: 1278, loss is 0.05681123957037926\n",
      "epoch: 4 step: 1279, loss is 0.0024190221447497606\n",
      "epoch: 4 step: 1280, loss is 0.02568655088543892\n",
      "epoch: 4 step: 1281, loss is 0.1618693470954895\n",
      "epoch: 4 step: 1282, loss is 0.005358802620321512\n",
      "epoch: 4 step: 1283, loss is 0.01576121151447296\n",
      "epoch: 4 step: 1284, loss is 0.026104312390089035\n",
      "epoch: 4 step: 1285, loss is 0.016001133248209953\n",
      "epoch: 4 step: 1286, loss is 0.11678135395050049\n",
      "epoch: 4 step: 1287, loss is 0.20356974005699158\n",
      "epoch: 4 step: 1288, loss is 0.03185570240020752\n",
      "epoch: 4 step: 1289, loss is 0.020269490778446198\n",
      "epoch: 4 step: 1290, loss is 0.00025219141389243305\n",
      "epoch: 4 step: 1291, loss is 0.002618746366351843\n",
      "epoch: 4 step: 1292, loss is 0.006007737945765257\n",
      "epoch: 4 step: 1293, loss is 0.03143339604139328\n",
      "epoch: 4 step: 1294, loss is 0.06954541802406311\n",
      "epoch: 4 step: 1295, loss is 0.004055062308907509\n",
      "epoch: 4 step: 1296, loss is 0.09034699946641922\n",
      "epoch: 4 step: 1297, loss is 0.04973889887332916\n",
      "epoch: 4 step: 1298, loss is 0.22748136520385742\n",
      "epoch: 4 step: 1299, loss is 0.011191538535058498\n",
      "epoch: 4 step: 1300, loss is 0.1683463305234909\n",
      "epoch: 4 step: 1301, loss is 0.017756495624780655\n",
      "epoch: 4 step: 1302, loss is 0.007394365034997463\n",
      "epoch: 4 step: 1303, loss is 0.15539151430130005\n",
      "epoch: 4 step: 1304, loss is 0.013434142805635929\n",
      "epoch: 4 step: 1305, loss is 0.0276474691927433\n",
      "epoch: 4 step: 1306, loss is 0.0034441801253706217\n",
      "epoch: 4 step: 1307, loss is 0.03820494934916496\n",
      "epoch: 4 step: 1308, loss is 0.025658586993813515\n",
      "epoch: 4 step: 1309, loss is 0.0035011691506952047\n",
      "epoch: 4 step: 1310, loss is 0.0061368574388325214\n",
      "epoch: 4 step: 1311, loss is 0.09657717496156693\n",
      "epoch: 4 step: 1312, loss is 0.0092216357588768\n",
      "epoch: 4 step: 1313, loss is 0.0015204173978418112\n",
      "epoch: 4 step: 1314, loss is 0.005815249867737293\n",
      "epoch: 4 step: 1315, loss is 0.01397508755326271\n",
      "epoch: 4 step: 1316, loss is 0.05610160157084465\n",
      "epoch: 4 step: 1317, loss is 0.001453335746191442\n",
      "epoch: 4 step: 1318, loss is 0.029173944145441055\n",
      "epoch: 4 step: 1319, loss is 0.03266530856490135\n",
      "epoch: 4 step: 1320, loss is 0.2142035812139511\n",
      "epoch: 4 step: 1321, loss is 0.131914421916008\n",
      "epoch: 4 step: 1322, loss is 0.03200380876660347\n",
      "epoch: 4 step: 1323, loss is 0.10211235284805298\n",
      "epoch: 4 step: 1324, loss is 0.05132730305194855\n",
      "epoch: 4 step: 1325, loss is 0.06074348837137222\n",
      "epoch: 4 step: 1326, loss is 0.0007396596483886242\n",
      "epoch: 4 step: 1327, loss is 0.017755983397364616\n",
      "epoch: 4 step: 1328, loss is 0.1598135530948639\n",
      "epoch: 4 step: 1329, loss is 0.008273385465145111\n",
      "epoch: 4 step: 1330, loss is 0.02730351686477661\n",
      "epoch: 4 step: 1331, loss is 0.047282874584198\n",
      "epoch: 4 step: 1332, loss is 0.006008713040500879\n",
      "epoch: 4 step: 1333, loss is 0.10811731964349747\n",
      "epoch: 4 step: 1334, loss is 0.03869171813130379\n",
      "epoch: 4 step: 1335, loss is 0.25984370708465576\n",
      "epoch: 4 step: 1336, loss is 0.007443526294082403\n",
      "epoch: 4 step: 1337, loss is 0.05534259229898453\n",
      "epoch: 4 step: 1338, loss is 0.09890630841255188\n",
      "epoch: 4 step: 1339, loss is 0.02353060618042946\n",
      "epoch: 4 step: 1340, loss is 0.0016551646403968334\n",
      "epoch: 4 step: 1341, loss is 0.03329801931977272\n",
      "epoch: 4 step: 1342, loss is 0.20920324325561523\n",
      "epoch: 4 step: 1343, loss is 0.008208017796278\n",
      "epoch: 4 step: 1344, loss is 0.0680335983633995\n",
      "epoch: 4 step: 1345, loss is 0.19818994402885437\n",
      "epoch: 4 step: 1346, loss is 0.06489866226911545\n",
      "epoch: 4 step: 1347, loss is 0.05322468280792236\n",
      "epoch: 4 step: 1348, loss is 0.008200623095035553\n",
      "epoch: 4 step: 1349, loss is 0.03789389878511429\n",
      "epoch: 4 step: 1350, loss is 0.021055301651358604\n",
      "epoch: 4 step: 1351, loss is 0.013005061075091362\n",
      "epoch: 4 step: 1352, loss is 0.09577316045761108\n",
      "epoch: 4 step: 1353, loss is 0.004990745335817337\n",
      "epoch: 4 step: 1354, loss is 0.03656192123889923\n",
      "epoch: 4 step: 1355, loss is 0.04454593360424042\n",
      "epoch: 4 step: 1356, loss is 0.0027608913369476795\n",
      "epoch: 4 step: 1357, loss is 0.04929986596107483\n",
      "epoch: 4 step: 1358, loss is 0.009925938211381435\n",
      "epoch: 4 step: 1359, loss is 0.017081402242183685\n",
      "epoch: 4 step: 1360, loss is 0.044616423547267914\n",
      "epoch: 4 step: 1361, loss is 0.03168277069926262\n",
      "epoch: 4 step: 1362, loss is 0.0012996902223676443\n",
      "epoch: 4 step: 1363, loss is 0.02318079024553299\n",
      "epoch: 4 step: 1364, loss is 0.006257988512516022\n",
      "epoch: 4 step: 1365, loss is 0.03466775640845299\n",
      "epoch: 4 step: 1366, loss is 0.002446145983412862\n",
      "epoch: 4 step: 1367, loss is 0.002074689604341984\n",
      "epoch: 4 step: 1368, loss is 0.0005565863102674484\n",
      "epoch: 4 step: 1369, loss is 0.011851537972688675\n",
      "epoch: 4 step: 1370, loss is 0.030521543696522713\n",
      "epoch: 4 step: 1371, loss is 0.0025536860339343548\n",
      "epoch: 4 step: 1372, loss is 0.023879321292042732\n",
      "epoch: 4 step: 1373, loss is 0.14735224843025208\n",
      "epoch: 4 step: 1374, loss is 0.029800735414028168\n",
      "epoch: 4 step: 1375, loss is 0.1189664751291275\n",
      "epoch: 4 step: 1376, loss is 0.01037402544170618\n",
      "epoch: 4 step: 1377, loss is 0.006793919950723648\n",
      "epoch: 4 step: 1378, loss is 0.05296024680137634\n",
      "epoch: 4 step: 1379, loss is 0.0019445524085313082\n",
      "epoch: 4 step: 1380, loss is 0.09178367257118225\n",
      "epoch: 4 step: 1381, loss is 0.05239671468734741\n",
      "epoch: 4 step: 1382, loss is 0.004424720071256161\n",
      "epoch: 4 step: 1383, loss is 0.034274592995643616\n",
      "epoch: 4 step: 1384, loss is 0.010732253082096577\n",
      "epoch: 4 step: 1385, loss is 0.0006922174943611026\n",
      "epoch: 4 step: 1386, loss is 0.011091284453868866\n",
      "epoch: 4 step: 1387, loss is 0.012031962163746357\n",
      "epoch: 4 step: 1388, loss is 0.0005437353393062949\n",
      "epoch: 4 step: 1389, loss is 0.0010673918295651674\n",
      "epoch: 4 step: 1390, loss is 0.0027847755700349808\n",
      "epoch: 4 step: 1391, loss is 0.010620217770338058\n",
      "epoch: 4 step: 1392, loss is 0.003786787623539567\n",
      "epoch: 4 step: 1393, loss is 0.009498780593276024\n",
      "epoch: 4 step: 1394, loss is 0.01035535242408514\n",
      "epoch: 4 step: 1395, loss is 0.0018727319547906518\n",
      "epoch: 4 step: 1396, loss is 0.07350027561187744\n",
      "epoch: 4 step: 1397, loss is 0.003974443767219782\n",
      "epoch: 4 step: 1398, loss is 0.2106800228357315\n",
      "epoch: 4 step: 1399, loss is 0.037670448422431946\n",
      "epoch: 4 step: 1400, loss is 0.0005589659558609128\n",
      "epoch: 4 step: 1401, loss is 0.0015052995877340436\n",
      "epoch: 4 step: 1402, loss is 0.19549164175987244\n",
      "epoch: 4 step: 1403, loss is 0.0010859942995011806\n",
      "epoch: 4 step: 1404, loss is 0.00334514700807631\n",
      "epoch: 4 step: 1405, loss is 0.003319364506751299\n",
      "epoch: 4 step: 1406, loss is 0.003270477754995227\n",
      "epoch: 4 step: 1407, loss is 0.20754922926425934\n",
      "epoch: 4 step: 1408, loss is 0.054000597447156906\n",
      "epoch: 4 step: 1409, loss is 0.010442778468132019\n",
      "epoch: 4 step: 1410, loss is 0.03534597158432007\n",
      "epoch: 4 step: 1411, loss is 0.014050575904548168\n",
      "epoch: 4 step: 1412, loss is 0.11787599325180054\n",
      "epoch: 4 step: 1413, loss is 0.008851272985339165\n",
      "epoch: 4 step: 1414, loss is 0.0015101103344932199\n",
      "epoch: 4 step: 1415, loss is 0.208377867937088\n",
      "epoch: 4 step: 1416, loss is 0.0021403746213763952\n",
      "epoch: 4 step: 1417, loss is 0.0753249004483223\n",
      "epoch: 4 step: 1418, loss is 0.07657838612794876\n",
      "epoch: 4 step: 1419, loss is 0.14383187890052795\n",
      "epoch: 4 step: 1420, loss is 0.025080237537622452\n",
      "epoch: 4 step: 1421, loss is 0.07517820596694946\n",
      "epoch: 4 step: 1422, loss is 0.036336664110422134\n",
      "epoch: 4 step: 1423, loss is 0.002972969086840749\n",
      "epoch: 4 step: 1424, loss is 0.028007417917251587\n",
      "epoch: 4 step: 1425, loss is 0.0020644189789891243\n",
      "epoch: 4 step: 1426, loss is 0.023032860830426216\n",
      "epoch: 4 step: 1427, loss is 0.002716367831453681\n",
      "epoch: 4 step: 1428, loss is 0.007977447472512722\n",
      "epoch: 4 step: 1429, loss is 0.0764579251408577\n",
      "epoch: 4 step: 1430, loss is 0.005895853042602539\n",
      "epoch: 4 step: 1431, loss is 0.059557124972343445\n",
      "epoch: 4 step: 1432, loss is 0.000790302874520421\n",
      "epoch: 4 step: 1433, loss is 0.14866331219673157\n",
      "epoch: 4 step: 1434, loss is 0.06900276243686676\n",
      "epoch: 4 step: 1435, loss is 0.03517881780862808\n",
      "epoch: 4 step: 1436, loss is 0.23467092216014862\n",
      "epoch: 4 step: 1437, loss is 0.006548403762280941\n",
      "epoch: 4 step: 1438, loss is 0.004733880050480366\n",
      "epoch: 4 step: 1439, loss is 0.0016711353091523051\n",
      "epoch: 4 step: 1440, loss is 0.02610507421195507\n",
      "epoch: 4 step: 1441, loss is 0.02623645029962063\n",
      "epoch: 4 step: 1442, loss is 0.10290414839982986\n",
      "epoch: 4 step: 1443, loss is 0.008702458813786507\n",
      "epoch: 4 step: 1444, loss is 0.07254382222890854\n",
      "epoch: 4 step: 1445, loss is 0.0030206842347979546\n",
      "epoch: 4 step: 1446, loss is 0.06463094800710678\n",
      "epoch: 4 step: 1447, loss is 0.025670137256383896\n",
      "epoch: 4 step: 1448, loss is 0.0920356959104538\n",
      "epoch: 4 step: 1449, loss is 0.16046980023384094\n",
      "epoch: 4 step: 1450, loss is 0.05812523514032364\n",
      "epoch: 4 step: 1451, loss is 0.05551585555076599\n",
      "epoch: 4 step: 1452, loss is 0.030634326860308647\n",
      "epoch: 4 step: 1453, loss is 0.0035787378437817097\n",
      "epoch: 4 step: 1454, loss is 0.004389600828289986\n",
      "epoch: 4 step: 1455, loss is 0.3374221622943878\n",
      "epoch: 4 step: 1456, loss is 0.18192186951637268\n",
      "epoch: 4 step: 1457, loss is 0.0018079241272062063\n",
      "epoch: 4 step: 1458, loss is 0.011258892714977264\n",
      "epoch: 4 step: 1459, loss is 0.011476028710603714\n",
      "epoch: 4 step: 1460, loss is 0.027192093431949615\n",
      "epoch: 4 step: 1461, loss is 0.014238283038139343\n",
      "epoch: 4 step: 1462, loss is 0.18670164048671722\n",
      "epoch: 4 step: 1463, loss is 0.11000480502843857\n",
      "epoch: 4 step: 1464, loss is 0.03903941437602043\n",
      "epoch: 4 step: 1465, loss is 0.03472088649868965\n",
      "epoch: 4 step: 1466, loss is 0.09336629509925842\n",
      "epoch: 4 step: 1467, loss is 0.04305284470319748\n",
      "epoch: 4 step: 1468, loss is 0.1667289137840271\n",
      "epoch: 4 step: 1469, loss is 0.005768092814832926\n",
      "epoch: 4 step: 1470, loss is 0.02924659475684166\n",
      "epoch: 4 step: 1471, loss is 0.004979644902050495\n",
      "epoch: 4 step: 1472, loss is 0.0015808563912287354\n",
      "epoch: 4 step: 1473, loss is 0.0026911327149719\n",
      "epoch: 4 step: 1474, loss is 0.04655097424983978\n",
      "epoch: 4 step: 1475, loss is 0.0038551974575966597\n",
      "epoch: 4 step: 1476, loss is 0.01907266303896904\n",
      "epoch: 4 step: 1477, loss is 0.11008979380130768\n",
      "epoch: 4 step: 1478, loss is 0.007955384440720081\n",
      "epoch: 4 step: 1479, loss is 0.0017828431446105242\n",
      "epoch: 4 step: 1480, loss is 0.018087416887283325\n",
      "epoch: 4 step: 1481, loss is 0.11744159460067749\n",
      "epoch: 4 step: 1482, loss is 0.07188398391008377\n",
      "epoch: 4 step: 1483, loss is 0.02556566521525383\n",
      "epoch: 4 step: 1484, loss is 0.16053080558776855\n",
      "epoch: 4 step: 1485, loss is 0.0029936842620372772\n",
      "epoch: 4 step: 1486, loss is 0.048622239381074905\n",
      "epoch: 4 step: 1487, loss is 0.0010835763532668352\n",
      "epoch: 4 step: 1488, loss is 0.006229693070054054\n",
      "epoch: 4 step: 1489, loss is 0.0023178409319370985\n",
      "epoch: 4 step: 1490, loss is 0.012872111052274704\n",
      "epoch: 4 step: 1491, loss is 0.0011324853403493762\n",
      "epoch: 4 step: 1492, loss is 0.11493288725614548\n",
      "epoch: 4 step: 1493, loss is 0.009082403965294361\n",
      "epoch: 4 step: 1494, loss is 0.0013973132008686662\n",
      "epoch: 4 step: 1495, loss is 0.6534595489501953\n",
      "epoch: 4 step: 1496, loss is 0.0041161938570439816\n",
      "epoch: 4 step: 1497, loss is 0.0027549767401069403\n",
      "epoch: 4 step: 1498, loss is 0.00702961441129446\n",
      "epoch: 4 step: 1499, loss is 0.048430539667606354\n",
      "epoch: 4 step: 1500, loss is 0.0019890815019607544\n",
      "epoch: 4 step: 1501, loss is 0.004455761052668095\n",
      "epoch: 4 step: 1502, loss is 0.0011180487927049398\n",
      "epoch: 4 step: 1503, loss is 0.005769945681095123\n",
      "epoch: 4 step: 1504, loss is 0.054416876286268234\n",
      "epoch: 4 step: 1505, loss is 0.0014200628502294421\n",
      "epoch: 4 step: 1506, loss is 0.031202832236886024\n",
      "epoch: 4 step: 1507, loss is 0.07008519768714905\n",
      "epoch: 4 step: 1508, loss is 0.0751751959323883\n",
      "epoch: 4 step: 1509, loss is 0.010713639669120312\n",
      "epoch: 4 step: 1510, loss is 0.0011603314196690917\n",
      "epoch: 4 step: 1511, loss is 0.10439618676900864\n",
      "epoch: 4 step: 1512, loss is 0.15223579108715057\n",
      "epoch: 4 step: 1513, loss is 0.03267894685268402\n",
      "epoch: 4 step: 1514, loss is 0.010192422196269035\n",
      "epoch: 4 step: 1515, loss is 0.0585334487259388\n",
      "epoch: 4 step: 1516, loss is 0.043115802109241486\n",
      "epoch: 4 step: 1517, loss is 0.04072375223040581\n",
      "epoch: 4 step: 1518, loss is 0.13069403171539307\n",
      "epoch: 4 step: 1519, loss is 0.05644290894269943\n",
      "epoch: 4 step: 1520, loss is 0.034730587154626846\n",
      "epoch: 4 step: 1521, loss is 0.048483993858098984\n",
      "epoch: 4 step: 1522, loss is 0.12108955532312393\n",
      "epoch: 4 step: 1523, loss is 0.005483116954565048\n",
      "epoch: 4 step: 1524, loss is 0.0037058184389024973\n",
      "epoch: 4 step: 1525, loss is 0.014719448052346706\n",
      "epoch: 4 step: 1526, loss is 0.01365493144840002\n",
      "epoch: 4 step: 1527, loss is 0.006124058272689581\n",
      "epoch: 4 step: 1528, loss is 0.002414165297523141\n",
      "epoch: 4 step: 1529, loss is 0.007551128510385752\n",
      "epoch: 4 step: 1530, loss is 0.0009360373369418085\n",
      "epoch: 4 step: 1531, loss is 0.01857692562043667\n",
      "epoch: 4 step: 1532, loss is 0.0007892405265010893\n",
      "epoch: 4 step: 1533, loss is 0.034617118537425995\n",
      "epoch: 4 step: 1534, loss is 0.007161301095038652\n",
      "epoch: 4 step: 1535, loss is 0.01645331084728241\n",
      "epoch: 4 step: 1536, loss is 0.06988997757434845\n",
      "epoch: 4 step: 1537, loss is 0.02901269868016243\n",
      "epoch: 4 step: 1538, loss is 0.008050340227782726\n",
      "epoch: 4 step: 1539, loss is 0.05293853580951691\n",
      "epoch: 4 step: 1540, loss is 0.009886525571346283\n",
      "epoch: 4 step: 1541, loss is 0.05978190153837204\n",
      "epoch: 4 step: 1542, loss is 0.006680003367364407\n",
      "epoch: 4 step: 1543, loss is 0.019303319975733757\n",
      "epoch: 4 step: 1544, loss is 0.007229757960885763\n",
      "epoch: 4 step: 1545, loss is 0.0010790337109938264\n",
      "epoch: 4 step: 1546, loss is 0.15178729593753815\n",
      "epoch: 4 step: 1547, loss is 0.01727202720940113\n",
      "epoch: 4 step: 1548, loss is 0.0017946896841749549\n",
      "epoch: 4 step: 1549, loss is 0.008059719577431679\n",
      "epoch: 4 step: 1550, loss is 0.0580902025103569\n",
      "epoch: 4 step: 1551, loss is 0.024808144196867943\n",
      "epoch: 4 step: 1552, loss is 0.006321390625089407\n",
      "epoch: 4 step: 1553, loss is 0.031021229922771454\n",
      "epoch: 4 step: 1554, loss is 0.009183636866509914\n",
      "epoch: 4 step: 1555, loss is 0.006206904537975788\n",
      "epoch: 4 step: 1556, loss is 0.09476687759160995\n",
      "epoch: 4 step: 1557, loss is 0.007325936108827591\n",
      "epoch: 4 step: 1558, loss is 0.10973607748746872\n",
      "epoch: 4 step: 1559, loss is 0.005360996350646019\n",
      "epoch: 4 step: 1560, loss is 0.011661785654723644\n",
      "epoch: 4 step: 1561, loss is 0.004158448427915573\n",
      "epoch: 4 step: 1562, loss is 0.00030452338978648186\n",
      "epoch: 4 step: 1563, loss is 0.1569824069738388\n",
      "epoch: 4 step: 1564, loss is 0.0675216019153595\n",
      "epoch: 4 step: 1565, loss is 0.0012000275310128927\n",
      "epoch: 4 step: 1566, loss is 0.05183446407318115\n",
      "epoch: 4 step: 1567, loss is 0.024508025497198105\n",
      "epoch: 4 step: 1568, loss is 0.028887100517749786\n",
      "epoch: 4 step: 1569, loss is 0.017830712720751762\n",
      "epoch: 4 step: 1570, loss is 0.00041193392826244235\n",
      "epoch: 4 step: 1571, loss is 0.0018322791438549757\n",
      "epoch: 4 step: 1572, loss is 0.0005982145667076111\n",
      "epoch: 4 step: 1573, loss is 0.033980827778577805\n",
      "epoch: 4 step: 1574, loss is 0.001269920845516026\n",
      "epoch: 4 step: 1575, loss is 0.1016901433467865\n",
      "epoch: 4 step: 1576, loss is 0.28980258107185364\n",
      "epoch: 4 step: 1577, loss is 0.06894858181476593\n",
      "epoch: 4 step: 1578, loss is 0.07494997978210449\n",
      "epoch: 4 step: 1579, loss is 0.0029915098566561937\n",
      "epoch: 4 step: 1580, loss is 0.0008612471865490079\n",
      "epoch: 4 step: 1581, loss is 0.0012181168422102928\n",
      "epoch: 4 step: 1582, loss is 0.008478729985654354\n",
      "epoch: 4 step: 1583, loss is 0.044802166521549225\n",
      "epoch: 4 step: 1584, loss is 0.0016021000919863582\n",
      "epoch: 4 step: 1585, loss is 0.008744221180677414\n",
      "epoch: 4 step: 1586, loss is 0.03750986605882645\n",
      "epoch: 4 step: 1587, loss is 0.0009562891791574657\n",
      "epoch: 4 step: 1588, loss is 0.004459599498659372\n",
      "epoch: 4 step: 1589, loss is 0.0056931194849312305\n",
      "epoch: 4 step: 1590, loss is 0.003997098654508591\n",
      "epoch: 4 step: 1591, loss is 0.021055880934000015\n",
      "epoch: 4 step: 1592, loss is 0.003070398699492216\n",
      "epoch: 4 step: 1593, loss is 0.0006653540767729282\n",
      "epoch: 4 step: 1594, loss is 0.034377094358205795\n",
      "epoch: 4 step: 1595, loss is 0.0018898092675954103\n",
      "epoch: 4 step: 1596, loss is 0.23352541029453278\n",
      "epoch: 4 step: 1597, loss is 0.0012618324253708124\n",
      "epoch: 4 step: 1598, loss is 0.005303244572132826\n",
      "epoch: 4 step: 1599, loss is 0.04598867893218994\n",
      "epoch: 4 step: 1600, loss is 0.06926172226667404\n",
      "epoch: 4 step: 1601, loss is 0.0010769780492410064\n",
      "epoch: 4 step: 1602, loss is 0.028206581249833107\n",
      "epoch: 4 step: 1603, loss is 0.06912313401699066\n",
      "epoch: 4 step: 1604, loss is 0.011722194030880928\n",
      "epoch: 4 step: 1605, loss is 0.014769256114959717\n",
      "epoch: 4 step: 1606, loss is 0.00879536010324955\n",
      "epoch: 4 step: 1607, loss is 0.013252648524940014\n",
      "epoch: 4 step: 1608, loss is 0.025402158498764038\n",
      "epoch: 4 step: 1609, loss is 0.0021838597021996975\n",
      "epoch: 4 step: 1610, loss is 0.042465973645448685\n",
      "epoch: 4 step: 1611, loss is 0.03401359170675278\n",
      "epoch: 4 step: 1612, loss is 0.025772126391530037\n",
      "epoch: 4 step: 1613, loss is 0.03869985044002533\n",
      "epoch: 4 step: 1614, loss is 0.021170714870095253\n",
      "epoch: 4 step: 1615, loss is 0.032401084899902344\n",
      "epoch: 4 step: 1616, loss is 0.09292712807655334\n",
      "epoch: 4 step: 1617, loss is 0.001364168943837285\n",
      "epoch: 4 step: 1618, loss is 0.023174313828349113\n",
      "epoch: 4 step: 1619, loss is 0.002054796554148197\n",
      "epoch: 4 step: 1620, loss is 0.027059417217969894\n",
      "epoch: 4 step: 1621, loss is 0.032951850444078445\n",
      "epoch: 4 step: 1622, loss is 0.0004567005962599069\n",
      "epoch: 4 step: 1623, loss is 0.32259202003479004\n",
      "epoch: 4 step: 1624, loss is 0.01501841563731432\n",
      "epoch: 4 step: 1625, loss is 0.0009346366277895868\n",
      "epoch: 4 step: 1626, loss is 0.005338327027857304\n",
      "epoch: 4 step: 1627, loss is 0.03636408969759941\n",
      "epoch: 4 step: 1628, loss is 0.003448187606409192\n",
      "epoch: 4 step: 1629, loss is 0.0014726098161190748\n",
      "epoch: 4 step: 1630, loss is 0.13319900631904602\n",
      "epoch: 4 step: 1631, loss is 0.0019721605349332094\n",
      "epoch: 4 step: 1632, loss is 0.0003027256461791694\n",
      "epoch: 4 step: 1633, loss is 0.2777256369590759\n",
      "epoch: 4 step: 1634, loss is 0.001476007280871272\n",
      "epoch: 4 step: 1635, loss is 0.010759510099887848\n",
      "epoch: 4 step: 1636, loss is 0.06962443888187408\n",
      "epoch: 4 step: 1637, loss is 0.0057357787154614925\n",
      "epoch: 4 step: 1638, loss is 0.020113036036491394\n",
      "epoch: 4 step: 1639, loss is 0.0007535833865404129\n",
      "epoch: 4 step: 1640, loss is 0.0007261864957399666\n",
      "epoch: 4 step: 1641, loss is 0.12701910734176636\n",
      "epoch: 4 step: 1642, loss is 0.009009738452732563\n",
      "epoch: 4 step: 1643, loss is 0.0019355214899405837\n",
      "epoch: 4 step: 1644, loss is 0.13284650444984436\n",
      "epoch: 4 step: 1645, loss is 0.07742999494075775\n",
      "epoch: 4 step: 1646, loss is 0.004027822054922581\n",
      "epoch: 4 step: 1647, loss is 0.012355201877653599\n",
      "epoch: 4 step: 1648, loss is 0.06380268931388855\n",
      "epoch: 4 step: 1649, loss is 0.02177024446427822\n",
      "epoch: 4 step: 1650, loss is 0.006340003572404385\n",
      "epoch: 4 step: 1651, loss is 0.01923910714685917\n",
      "epoch: 4 step: 1652, loss is 0.001172024174593389\n",
      "epoch: 4 step: 1653, loss is 0.006047979928553104\n",
      "epoch: 4 step: 1654, loss is 0.016895342618227005\n",
      "epoch: 4 step: 1655, loss is 0.04626163840293884\n",
      "epoch: 4 step: 1656, loss is 0.1690143644809723\n",
      "epoch: 4 step: 1657, loss is 0.010104882530868053\n",
      "epoch: 4 step: 1658, loss is 0.021671658381819725\n",
      "epoch: 4 step: 1659, loss is 0.009380556643009186\n",
      "epoch: 4 step: 1660, loss is 0.008228054270148277\n",
      "epoch: 4 step: 1661, loss is 0.003388670738786459\n",
      "epoch: 4 step: 1662, loss is 0.08064743876457214\n",
      "epoch: 4 step: 1663, loss is 0.14696726202964783\n",
      "epoch: 4 step: 1664, loss is 0.05079484358429909\n",
      "epoch: 4 step: 1665, loss is 0.0352686308324337\n",
      "epoch: 4 step: 1666, loss is 0.0037447644863277674\n",
      "epoch: 4 step: 1667, loss is 0.003786674700677395\n",
      "epoch: 4 step: 1668, loss is 0.028889905661344528\n",
      "epoch: 4 step: 1669, loss is 0.003029230982065201\n",
      "epoch: 4 step: 1670, loss is 0.014420521445572376\n",
      "epoch: 4 step: 1671, loss is 0.0017526139272376895\n",
      "epoch: 4 step: 1672, loss is 0.019551819190382957\n",
      "epoch: 4 step: 1673, loss is 0.09463611245155334\n",
      "epoch: 4 step: 1674, loss is 0.26233288645744324\n",
      "epoch: 4 step: 1675, loss is 0.002960114972665906\n",
      "epoch: 4 step: 1676, loss is 0.022424738854169846\n",
      "epoch: 4 step: 1677, loss is 0.030869176611304283\n",
      "epoch: 4 step: 1678, loss is 0.004266334231942892\n",
      "epoch: 4 step: 1679, loss is 0.008504309691488743\n",
      "epoch: 4 step: 1680, loss is 0.004036293365061283\n",
      "epoch: 4 step: 1681, loss is 0.06976833194494247\n",
      "epoch: 4 step: 1682, loss is 0.01199481263756752\n",
      "epoch: 4 step: 1683, loss is 0.014500570483505726\n",
      "epoch: 4 step: 1684, loss is 0.010942477732896805\n",
      "epoch: 4 step: 1685, loss is 0.0074557093903422356\n",
      "epoch: 4 step: 1686, loss is 0.0104756448417902\n",
      "epoch: 4 step: 1687, loss is 0.0018338392255827785\n",
      "epoch: 4 step: 1688, loss is 0.019428588449954987\n",
      "epoch: 4 step: 1689, loss is 0.000795528176240623\n",
      "epoch: 4 step: 1690, loss is 0.002529099816456437\n",
      "epoch: 4 step: 1691, loss is 0.037128984928131104\n",
      "epoch: 4 step: 1692, loss is 0.01301600318402052\n",
      "epoch: 4 step: 1693, loss is 0.0025591175071895123\n",
      "epoch: 4 step: 1694, loss is 0.005897685419768095\n",
      "epoch: 4 step: 1695, loss is 0.0028386088088154793\n",
      "epoch: 4 step: 1696, loss is 0.0037015401758253574\n",
      "epoch: 4 step: 1697, loss is 0.08953665941953659\n",
      "epoch: 4 step: 1698, loss is 0.006310146767646074\n",
      "epoch: 4 step: 1699, loss is 0.07206219434738159\n",
      "epoch: 4 step: 1700, loss is 0.0026995462831109762\n",
      "epoch: 4 step: 1701, loss is 0.0009257463389076293\n",
      "epoch: 4 step: 1702, loss is 0.08894906938076019\n",
      "epoch: 4 step: 1703, loss is 0.052423205226659775\n",
      "epoch: 4 step: 1704, loss is 0.007450744975358248\n",
      "epoch: 4 step: 1705, loss is 0.004154357127845287\n",
      "epoch: 4 step: 1706, loss is 0.23181605339050293\n",
      "epoch: 4 step: 1707, loss is 0.04049191251397133\n",
      "epoch: 4 step: 1708, loss is 0.05620340257883072\n",
      "epoch: 4 step: 1709, loss is 0.018148954957723618\n",
      "epoch: 4 step: 1710, loss is 0.027272937819361687\n",
      "epoch: 4 step: 1711, loss is 0.006522619631141424\n",
      "epoch: 4 step: 1712, loss is 0.0048692794516682625\n",
      "epoch: 4 step: 1713, loss is 0.02223963849246502\n",
      "epoch: 4 step: 1714, loss is 0.0010085813701152802\n",
      "epoch: 4 step: 1715, loss is 0.0038431973662227392\n",
      "epoch: 4 step: 1716, loss is 0.08571256697177887\n",
      "epoch: 4 step: 1717, loss is 0.004841221962124109\n",
      "epoch: 4 step: 1718, loss is 0.002172094536945224\n",
      "epoch: 4 step: 1719, loss is 0.00526597211137414\n",
      "epoch: 4 step: 1720, loss is 0.014467740431427956\n",
      "epoch: 4 step: 1721, loss is 0.0036154245026409626\n",
      "epoch: 4 step: 1722, loss is 0.025691375136375427\n",
      "epoch: 4 step: 1723, loss is 0.023418428376317024\n",
      "epoch: 4 step: 1724, loss is 0.003828407032415271\n",
      "epoch: 4 step: 1725, loss is 0.07547184824943542\n",
      "epoch: 4 step: 1726, loss is 0.03135432302951813\n",
      "epoch: 4 step: 1727, loss is 0.0017374242888763547\n",
      "epoch: 4 step: 1728, loss is 0.008252023719251156\n",
      "epoch: 4 step: 1729, loss is 0.00022473432181868702\n",
      "epoch: 4 step: 1730, loss is 0.0005936382221989334\n",
      "epoch: 4 step: 1731, loss is 0.0029299939051270485\n",
      "epoch: 4 step: 1732, loss is 0.004225661046802998\n",
      "epoch: 4 step: 1733, loss is 0.003682419890537858\n",
      "epoch: 4 step: 1734, loss is 0.009238929487764835\n",
      "epoch: 4 step: 1735, loss is 0.004202187526971102\n",
      "epoch: 4 step: 1736, loss is 0.08061534911394119\n",
      "epoch: 4 step: 1737, loss is 0.06095774099230766\n",
      "epoch: 4 step: 1738, loss is 0.0004441951750777662\n",
      "epoch: 4 step: 1739, loss is 0.25960618257522583\n",
      "epoch: 4 step: 1740, loss is 0.00030306572443805635\n",
      "epoch: 4 step: 1741, loss is 0.011066215112805367\n",
      "epoch: 4 step: 1742, loss is 0.0130534078925848\n",
      "epoch: 4 step: 1743, loss is 0.31168830394744873\n",
      "epoch: 4 step: 1744, loss is 0.0761173814535141\n",
      "epoch: 4 step: 1745, loss is 0.050245460122823715\n",
      "epoch: 4 step: 1746, loss is 0.0010430061956867576\n",
      "epoch: 4 step: 1747, loss is 0.06291251629590988\n",
      "epoch: 4 step: 1748, loss is 0.004538857843726873\n",
      "epoch: 4 step: 1749, loss is 0.00013163719268050045\n",
      "epoch: 4 step: 1750, loss is 0.01208033412694931\n",
      "epoch: 4 step: 1751, loss is 0.016120154410600662\n",
      "epoch: 4 step: 1752, loss is 0.002813714323565364\n",
      "epoch: 4 step: 1753, loss is 0.0016179525991901755\n",
      "epoch: 4 step: 1754, loss is 0.009520758874714375\n",
      "epoch: 4 step: 1755, loss is 0.04035583510994911\n",
      "epoch: 4 step: 1756, loss is 0.007917863316833973\n",
      "epoch: 4 step: 1757, loss is 0.043816789984703064\n",
      "epoch: 4 step: 1758, loss is 0.19579774141311646\n",
      "epoch: 4 step: 1759, loss is 0.012730974704027176\n",
      "epoch: 4 step: 1760, loss is 0.0008266601944342256\n",
      "epoch: 4 step: 1761, loss is 0.003798931837081909\n",
      "epoch: 4 step: 1762, loss is 0.006574244704097509\n",
      "epoch: 4 step: 1763, loss is 0.01853831857442856\n",
      "epoch: 4 step: 1764, loss is 0.0041772411204874516\n",
      "epoch: 4 step: 1765, loss is 0.01755998097360134\n",
      "epoch: 4 step: 1766, loss is 0.0020562021527439356\n",
      "epoch: 4 step: 1767, loss is 0.007493895478546619\n",
      "epoch: 4 step: 1768, loss is 0.10222813487052917\n",
      "epoch: 4 step: 1769, loss is 0.005971516016870737\n",
      "epoch: 4 step: 1770, loss is 0.016680415719747543\n",
      "epoch: 4 step: 1771, loss is 0.015954412519931793\n",
      "epoch: 4 step: 1772, loss is 0.003599487477913499\n",
      "epoch: 4 step: 1773, loss is 0.0008181541925296187\n",
      "epoch: 4 step: 1774, loss is 0.04516119137406349\n",
      "epoch: 4 step: 1775, loss is 0.005949453450739384\n",
      "epoch: 4 step: 1776, loss is 0.0019516469910740852\n",
      "epoch: 4 step: 1777, loss is 0.001343729323707521\n",
      "epoch: 4 step: 1778, loss is 0.0028837183490395546\n",
      "epoch: 4 step: 1779, loss is 0.0359344482421875\n",
      "epoch: 4 step: 1780, loss is 0.138003408908844\n",
      "epoch: 4 step: 1781, loss is 0.012083649635314941\n",
      "epoch: 4 step: 1782, loss is 0.23447999358177185\n",
      "epoch: 4 step: 1783, loss is 0.1834433227777481\n",
      "epoch: 4 step: 1784, loss is 0.046064894646406174\n",
      "epoch: 4 step: 1785, loss is 0.001284070429392159\n",
      "epoch: 4 step: 1786, loss is 0.002464638091623783\n",
      "epoch: 4 step: 1787, loss is 0.0011090940097346902\n",
      "epoch: 4 step: 1788, loss is 0.006348593160510063\n",
      "epoch: 4 step: 1789, loss is 0.03986272215843201\n",
      "epoch: 4 step: 1790, loss is 0.034226901829242706\n",
      "epoch: 4 step: 1791, loss is 0.01984856091439724\n",
      "epoch: 4 step: 1792, loss is 0.003869706764817238\n",
      "epoch: 4 step: 1793, loss is 0.0019391070818528533\n",
      "epoch: 4 step: 1794, loss is 0.007529590278863907\n",
      "epoch: 4 step: 1795, loss is 0.0598677396774292\n",
      "epoch: 4 step: 1796, loss is 0.01101771742105484\n",
      "epoch: 4 step: 1797, loss is 0.03252069652080536\n",
      "epoch: 4 step: 1798, loss is 0.004036769270896912\n",
      "epoch: 4 step: 1799, loss is 0.24606546759605408\n",
      "epoch: 4 step: 1800, loss is 0.0024594070855528116\n",
      "epoch: 4 step: 1801, loss is 0.007555022835731506\n",
      "epoch: 4 step: 1802, loss is 0.009666623547673225\n",
      "epoch: 4 step: 1803, loss is 0.030411845073103905\n",
      "epoch: 4 step: 1804, loss is 0.012180976569652557\n",
      "epoch: 4 step: 1805, loss is 0.0019864190835505724\n",
      "epoch: 4 step: 1806, loss is 0.00041652924846857786\n",
      "epoch: 4 step: 1807, loss is 0.0016431637341156602\n",
      "epoch: 4 step: 1808, loss is 0.05009088292717934\n",
      "epoch: 4 step: 1809, loss is 0.0001601752737769857\n",
      "epoch: 4 step: 1810, loss is 0.05811429023742676\n",
      "epoch: 4 step: 1811, loss is 0.0003681678790599108\n",
      "epoch: 4 step: 1812, loss is 0.010906190611422062\n",
      "epoch: 4 step: 1813, loss is 0.0010627523297443986\n",
      "epoch: 4 step: 1814, loss is 0.021822577342391014\n",
      "epoch: 4 step: 1815, loss is 0.14003042876720428\n",
      "epoch: 4 step: 1816, loss is 0.001638911315239966\n",
      "epoch: 4 step: 1817, loss is 0.034203700721263885\n",
      "epoch: 4 step: 1818, loss is 0.0038431694265455008\n",
      "epoch: 4 step: 1819, loss is 0.001519850455224514\n",
      "epoch: 4 step: 1820, loss is 0.016668517142534256\n",
      "epoch: 4 step: 1821, loss is 0.00046665192348882556\n",
      "epoch: 4 step: 1822, loss is 0.031958017498254776\n",
      "epoch: 4 step: 1823, loss is 0.030777351930737495\n",
      "epoch: 4 step: 1824, loss is 0.0007020020275376737\n",
      "epoch: 4 step: 1825, loss is 0.009294957853853703\n",
      "epoch: 4 step: 1826, loss is 0.0028054998256266117\n",
      "epoch: 4 step: 1827, loss is 0.004342939238995314\n",
      "epoch: 4 step: 1828, loss is 0.05983506888151169\n",
      "epoch: 4 step: 1829, loss is 0.0012927425559610128\n",
      "epoch: 4 step: 1830, loss is 0.010576824657619\n",
      "epoch: 4 step: 1831, loss is 0.14684733748435974\n",
      "epoch: 4 step: 1832, loss is 0.0010207033483311534\n",
      "epoch: 4 step: 1833, loss is 0.0543295294046402\n",
      "epoch: 4 step: 1834, loss is 0.20179928839206696\n",
      "epoch: 4 step: 1835, loss is 0.0004601585096679628\n",
      "epoch: 4 step: 1836, loss is 0.0257289856672287\n",
      "epoch: 4 step: 1837, loss is 0.012493232265114784\n",
      "epoch: 4 step: 1838, loss is 0.003343381453305483\n",
      "epoch: 4 step: 1839, loss is 0.006972902920097113\n",
      "epoch: 4 step: 1840, loss is 0.0037648652214556932\n",
      "epoch: 4 step: 1841, loss is 0.02702280879020691\n",
      "epoch: 4 step: 1842, loss is 0.034465573728084564\n",
      "epoch: 4 step: 1843, loss is 0.18828468024730682\n",
      "epoch: 4 step: 1844, loss is 0.0023968289606273174\n",
      "epoch: 4 step: 1845, loss is 0.0007165134884417057\n",
      "epoch: 4 step: 1846, loss is 0.16127583384513855\n",
      "epoch: 4 step: 1847, loss is 0.01310372818261385\n",
      "epoch: 4 step: 1848, loss is 0.004899420775473118\n",
      "epoch: 4 step: 1849, loss is 0.011887575499713421\n",
      "epoch: 4 step: 1850, loss is 0.0027851122431457043\n",
      "epoch: 4 step: 1851, loss is 0.005256779491901398\n",
      "epoch: 4 step: 1852, loss is 0.06156493350863457\n",
      "epoch: 4 step: 1853, loss is 0.0006157789030112326\n",
      "epoch: 4 step: 1854, loss is 0.001824590377509594\n",
      "epoch: 4 step: 1855, loss is 0.0031202894169837236\n",
      "epoch: 4 step: 1856, loss is 0.06760358065366745\n",
      "epoch: 4 step: 1857, loss is 0.009957288391888142\n",
      "epoch: 4 step: 1858, loss is 0.036834146827459335\n",
      "epoch: 4 step: 1859, loss is 0.0002544548478908837\n",
      "epoch: 4 step: 1860, loss is 0.017466487362980843\n",
      "epoch: 4 step: 1861, loss is 0.001715304097160697\n",
      "epoch: 4 step: 1862, loss is 0.15601994097232819\n",
      "epoch: 4 step: 1863, loss is 0.0018144608475267887\n",
      "epoch: 4 step: 1864, loss is 0.04285912215709686\n",
      "epoch: 4 step: 1865, loss is 0.011980276554822922\n",
      "epoch: 4 step: 1866, loss is 0.0004738832649309188\n",
      "epoch: 4 step: 1867, loss is 0.1339089274406433\n",
      "epoch: 4 step: 1868, loss is 0.0036995839327573776\n",
      "epoch: 4 step: 1869, loss is 0.011565122753381729\n",
      "epoch: 4 step: 1870, loss is 0.0031693761702626944\n",
      "epoch: 4 step: 1871, loss is 0.002772330306470394\n",
      "epoch: 4 step: 1872, loss is 0.00016729636990930885\n",
      "epoch: 4 step: 1873, loss is 0.010175483301281929\n",
      "epoch: 4 step: 1874, loss is 0.05406465008854866\n",
      "epoch: 4 step: 1875, loss is 0.032519470900297165\n",
      "epoch: 5 step: 1, loss is 0.0363827608525753\n",
      "epoch: 5 step: 2, loss is 0.18678104877471924\n",
      "epoch: 5 step: 3, loss is 0.005105387419462204\n",
      "epoch: 5 step: 4, loss is 0.017562290653586388\n",
      "epoch: 5 step: 5, loss is 0.002812637947499752\n",
      "epoch: 5 step: 6, loss is 0.01348300650715828\n",
      "epoch: 5 step: 7, loss is 0.28941968083381653\n",
      "epoch: 5 step: 8, loss is 0.028953172266483307\n",
      "epoch: 5 step: 9, loss is 0.029406975954771042\n",
      "epoch: 5 step: 10, loss is 0.05732977017760277\n",
      "epoch: 5 step: 11, loss is 0.1854405403137207\n",
      "epoch: 5 step: 12, loss is 0.004459982737898827\n",
      "epoch: 5 step: 13, loss is 0.0005233634728938341\n",
      "epoch: 5 step: 14, loss is 0.00423787347972393\n",
      "epoch: 5 step: 15, loss is 0.018444133922457695\n",
      "epoch: 5 step: 16, loss is 0.08885066211223602\n",
      "epoch: 5 step: 17, loss is 0.02624635212123394\n",
      "epoch: 5 step: 18, loss is 0.11508794128894806\n",
      "epoch: 5 step: 19, loss is 0.02892776019871235\n",
      "epoch: 5 step: 20, loss is 0.02786300517618656\n",
      "epoch: 5 step: 21, loss is 0.0966334342956543\n",
      "epoch: 5 step: 22, loss is 0.0025311412755399942\n",
      "epoch: 5 step: 23, loss is 0.0039227427914738655\n",
      "epoch: 5 step: 24, loss is 0.0010830846149474382\n",
      "epoch: 5 step: 25, loss is 0.022061118856072426\n",
      "epoch: 5 step: 26, loss is 0.08427266031503677\n",
      "epoch: 5 step: 27, loss is 0.0034450599923729897\n",
      "epoch: 5 step: 28, loss is 0.037432730197906494\n",
      "epoch: 5 step: 29, loss is 0.03382626920938492\n",
      "epoch: 5 step: 30, loss is 0.059385038912296295\n",
      "epoch: 5 step: 31, loss is 0.016810385510325432\n",
      "epoch: 5 step: 32, loss is 0.014178908430039883\n",
      "epoch: 5 step: 33, loss is 0.10501860082149506\n",
      "epoch: 5 step: 34, loss is 0.0003769884060602635\n",
      "epoch: 5 step: 35, loss is 0.005284566432237625\n",
      "epoch: 5 step: 36, loss is 0.0019927627872675657\n",
      "epoch: 5 step: 37, loss is 0.0003863086167257279\n",
      "epoch: 5 step: 38, loss is 0.039282795041799545\n",
      "epoch: 5 step: 39, loss is 0.002438905881717801\n",
      "epoch: 5 step: 40, loss is 0.029504597187042236\n",
      "epoch: 5 step: 41, loss is 0.006521859206259251\n",
      "epoch: 5 step: 42, loss is 0.0009497231221757829\n",
      "epoch: 5 step: 43, loss is 0.0008107034955173731\n",
      "epoch: 5 step: 44, loss is 0.003859614720568061\n",
      "epoch: 5 step: 45, loss is 0.026338163763284683\n",
      "epoch: 5 step: 46, loss is 0.0019557508639991283\n",
      "epoch: 5 step: 47, loss is 0.01839281991124153\n",
      "epoch: 5 step: 48, loss is 0.004336496815085411\n",
      "epoch: 5 step: 49, loss is 0.014279293827712536\n",
      "epoch: 5 step: 50, loss is 0.0007070322753861547\n",
      "epoch: 5 step: 51, loss is 0.033312972635030746\n",
      "epoch: 5 step: 52, loss is 0.0038727617356926203\n",
      "epoch: 5 step: 53, loss is 0.007855154573917389\n",
      "epoch: 5 step: 54, loss is 0.019952883943915367\n",
      "epoch: 5 step: 55, loss is 0.001037890324369073\n",
      "epoch: 5 step: 56, loss is 0.002585833892226219\n",
      "epoch: 5 step: 57, loss is 0.002880875254049897\n",
      "epoch: 5 step: 58, loss is 0.000938062381464988\n",
      "epoch: 5 step: 59, loss is 0.0013288770569488406\n",
      "epoch: 5 step: 60, loss is 0.00312665943056345\n",
      "epoch: 5 step: 61, loss is 0.1672254353761673\n",
      "epoch: 5 step: 62, loss is 0.01795048825442791\n",
      "epoch: 5 step: 63, loss is 0.004024842754006386\n",
      "epoch: 5 step: 64, loss is 0.022027678787708282\n",
      "epoch: 5 step: 65, loss is 0.0024197143502533436\n",
      "epoch: 5 step: 66, loss is 0.012006925418972969\n",
      "epoch: 5 step: 67, loss is 0.03839581459760666\n",
      "epoch: 5 step: 68, loss is 0.07888136059045792\n",
      "epoch: 5 step: 69, loss is 0.09439122676849365\n",
      "epoch: 5 step: 70, loss is 0.00045001652324572206\n",
      "epoch: 5 step: 71, loss is 0.00011097061360487714\n",
      "epoch: 5 step: 72, loss is 0.0063621350564062595\n",
      "epoch: 5 step: 73, loss is 0.001878676121123135\n",
      "epoch: 5 step: 74, loss is 0.0006385677261278033\n",
      "epoch: 5 step: 75, loss is 0.005469570402055979\n",
      "epoch: 5 step: 76, loss is 0.14581184089183807\n",
      "epoch: 5 step: 77, loss is 0.028944486752152443\n",
      "epoch: 5 step: 78, loss is 0.004458203446120024\n",
      "epoch: 5 step: 79, loss is 0.006364898290485144\n",
      "epoch: 5 step: 80, loss is 0.008024428971111774\n",
      "epoch: 5 step: 81, loss is 0.001730597112327814\n",
      "epoch: 5 step: 82, loss is 0.0007962827803567052\n",
      "epoch: 5 step: 83, loss is 0.031158262863755226\n",
      "epoch: 5 step: 84, loss is 0.00037140591302886605\n",
      "epoch: 5 step: 85, loss is 0.09675130993127823\n",
      "epoch: 5 step: 86, loss is 0.06280102580785751\n",
      "epoch: 5 step: 87, loss is 0.0326915867626667\n",
      "epoch: 5 step: 88, loss is 0.004628398455679417\n",
      "epoch: 5 step: 89, loss is 0.031888533383607864\n",
      "epoch: 5 step: 90, loss is 0.0007462636567652225\n",
      "epoch: 5 step: 91, loss is 0.005495465360581875\n",
      "epoch: 5 step: 92, loss is 0.03360937908291817\n",
      "epoch: 5 step: 93, loss is 0.0010698596015572548\n",
      "epoch: 5 step: 94, loss is 0.005302878096699715\n",
      "epoch: 5 step: 95, loss is 0.010165190324187279\n",
      "epoch: 5 step: 96, loss is 0.00012967613292858005\n",
      "epoch: 5 step: 97, loss is 0.0033114594407379627\n",
      "epoch: 5 step: 98, loss is 0.015798218548297882\n",
      "epoch: 5 step: 99, loss is 0.007235678378492594\n",
      "epoch: 5 step: 100, loss is 0.00038177776150405407\n",
      "epoch: 5 step: 101, loss is 0.01940159499645233\n",
      "epoch: 5 step: 102, loss is 0.0008277802262455225\n",
      "epoch: 5 step: 103, loss is 0.01945173740386963\n",
      "epoch: 5 step: 104, loss is 0.004280215594917536\n",
      "epoch: 5 step: 105, loss is 0.010482419282197952\n",
      "epoch: 5 step: 106, loss is 0.005703361239284277\n",
      "epoch: 5 step: 107, loss is 0.0002975271490868181\n",
      "epoch: 5 step: 108, loss is 0.04338245466351509\n",
      "epoch: 5 step: 109, loss is 0.02075004018843174\n",
      "epoch: 5 step: 110, loss is 0.037664271891117096\n",
      "epoch: 5 step: 111, loss is 0.014041945338249207\n",
      "epoch: 5 step: 112, loss is 0.005151794757694006\n",
      "epoch: 5 step: 113, loss is 0.0024953484535217285\n",
      "epoch: 5 step: 114, loss is 0.0037356573157012463\n",
      "epoch: 5 step: 115, loss is 0.06961029767990112\n",
      "epoch: 5 step: 116, loss is 0.0030864670407027006\n",
      "epoch: 5 step: 117, loss is 0.1612338125705719\n",
      "epoch: 5 step: 118, loss is 0.03239931911230087\n",
      "epoch: 5 step: 119, loss is 0.013058628886938095\n",
      "epoch: 5 step: 120, loss is 0.11294486373662949\n",
      "epoch: 5 step: 121, loss is 0.00021187558013480157\n",
      "epoch: 5 step: 122, loss is 0.007631268352270126\n",
      "epoch: 5 step: 123, loss is 0.01917337067425251\n",
      "epoch: 5 step: 124, loss is 0.004584203939884901\n",
      "epoch: 5 step: 125, loss is 0.11712858080863953\n",
      "epoch: 5 step: 126, loss is 0.03489672392606735\n",
      "epoch: 5 step: 127, loss is 0.002634712029248476\n",
      "epoch: 5 step: 128, loss is 0.0034270610194653273\n",
      "epoch: 5 step: 129, loss is 0.0014452148461714387\n",
      "epoch: 5 step: 130, loss is 0.0035191464703530073\n",
      "epoch: 5 step: 131, loss is 0.02406235598027706\n",
      "epoch: 5 step: 132, loss is 0.0054886192083358765\n",
      "epoch: 5 step: 133, loss is 0.031192542985081673\n",
      "epoch: 5 step: 134, loss is 0.0012171970447525382\n",
      "epoch: 5 step: 135, loss is 0.009127170778810978\n",
      "epoch: 5 step: 136, loss is 0.08879463374614716\n",
      "epoch: 5 step: 137, loss is 0.00021981050667818636\n",
      "epoch: 5 step: 138, loss is 0.10559751838445663\n",
      "epoch: 5 step: 139, loss is 0.06965688616037369\n",
      "epoch: 5 step: 140, loss is 0.025230271741747856\n",
      "epoch: 5 step: 141, loss is 0.0033484678715467453\n",
      "epoch: 5 step: 142, loss is 0.0005759779014624655\n",
      "epoch: 5 step: 143, loss is 0.004502851981669664\n",
      "epoch: 5 step: 144, loss is 0.009437082335352898\n",
      "epoch: 5 step: 145, loss is 0.0011032209731638432\n",
      "epoch: 5 step: 146, loss is 0.0037529896944761276\n",
      "epoch: 5 step: 147, loss is 0.0025826243218034506\n",
      "epoch: 5 step: 148, loss is 0.006415168754756451\n",
      "epoch: 5 step: 149, loss is 0.07053127884864807\n",
      "epoch: 5 step: 150, loss is 0.07561158388853073\n",
      "epoch: 5 step: 151, loss is 0.017141079530119896\n",
      "epoch: 5 step: 152, loss is 0.003936484456062317\n",
      "epoch: 5 step: 153, loss is 0.001705053262412548\n",
      "epoch: 5 step: 154, loss is 0.0012377111706882715\n",
      "epoch: 5 step: 155, loss is 0.002031687181442976\n",
      "epoch: 5 step: 156, loss is 0.0004297900013625622\n",
      "epoch: 5 step: 157, loss is 0.004841665271669626\n",
      "epoch: 5 step: 158, loss is 0.006188818719238043\n",
      "epoch: 5 step: 159, loss is 0.12003382295370102\n",
      "epoch: 5 step: 160, loss is 0.004738758318126202\n",
      "epoch: 5 step: 161, loss is 0.0011613334063440561\n",
      "epoch: 5 step: 162, loss is 0.016900433227419853\n",
      "epoch: 5 step: 163, loss is 0.0015782997943460941\n",
      "epoch: 5 step: 164, loss is 0.02455093525350094\n",
      "epoch: 5 step: 165, loss is 0.007083583157509565\n",
      "epoch: 5 step: 166, loss is 0.04812050610780716\n",
      "epoch: 5 step: 167, loss is 0.037050601094961166\n",
      "epoch: 5 step: 168, loss is 0.027958014979958534\n",
      "epoch: 5 step: 169, loss is 0.013393293134868145\n",
      "epoch: 5 step: 170, loss is 0.006714648101478815\n",
      "epoch: 5 step: 171, loss is 0.004732173401862383\n",
      "epoch: 5 step: 172, loss is 0.0030703661032021046\n",
      "epoch: 5 step: 173, loss is 0.004343593958765268\n",
      "epoch: 5 step: 174, loss is 0.0008998314733617008\n",
      "epoch: 5 step: 175, loss is 0.05309436470270157\n",
      "epoch: 5 step: 176, loss is 0.018132586032152176\n",
      "epoch: 5 step: 177, loss is 0.08798059821128845\n",
      "epoch: 5 step: 178, loss is 0.0024940671864897013\n",
      "epoch: 5 step: 179, loss is 0.0016575315967202187\n",
      "epoch: 5 step: 180, loss is 0.0022404221817851067\n",
      "epoch: 5 step: 181, loss is 0.10400624573230743\n",
      "epoch: 5 step: 182, loss is 0.001375300344079733\n",
      "epoch: 5 step: 183, loss is 0.0005718279862776399\n",
      "epoch: 5 step: 184, loss is 0.01134913694113493\n",
      "epoch: 5 step: 185, loss is 0.013624823652207851\n",
      "epoch: 5 step: 186, loss is 0.14052416384220123\n",
      "epoch: 5 step: 187, loss is 0.110509492456913\n",
      "epoch: 5 step: 188, loss is 0.29614487290382385\n",
      "epoch: 5 step: 189, loss is 0.10496843606233597\n",
      "epoch: 5 step: 190, loss is 0.06792071461677551\n",
      "epoch: 5 step: 191, loss is 0.006592984311282635\n",
      "epoch: 5 step: 192, loss is 0.1352110505104065\n",
      "epoch: 5 step: 193, loss is 0.002060796832665801\n",
      "epoch: 5 step: 194, loss is 0.0007548016728833318\n",
      "epoch: 5 step: 195, loss is 0.006212168838828802\n",
      "epoch: 5 step: 196, loss is 0.012644735164940357\n",
      "epoch: 5 step: 197, loss is 0.030677970498800278\n",
      "epoch: 5 step: 198, loss is 0.01834564283490181\n",
      "epoch: 5 step: 199, loss is 0.000821246241685003\n",
      "epoch: 5 step: 200, loss is 0.0007265364401973784\n",
      "epoch: 5 step: 201, loss is 0.057124681770801544\n",
      "epoch: 5 step: 202, loss is 0.006749879103153944\n",
      "epoch: 5 step: 203, loss is 0.005219405051320791\n",
      "epoch: 5 step: 204, loss is 0.03590694069862366\n",
      "epoch: 5 step: 205, loss is 0.0008306472445838153\n",
      "epoch: 5 step: 206, loss is 0.08354123681783676\n",
      "epoch: 5 step: 207, loss is 0.007511711213737726\n",
      "epoch: 5 step: 208, loss is 0.003310618456453085\n",
      "epoch: 5 step: 209, loss is 0.011721150949597359\n",
      "epoch: 5 step: 210, loss is 0.02118030935525894\n",
      "epoch: 5 step: 211, loss is 0.003916087560355663\n",
      "epoch: 5 step: 212, loss is 0.085786834359169\n",
      "epoch: 5 step: 213, loss is 0.06642813235521317\n",
      "epoch: 5 step: 214, loss is 0.017111897468566895\n",
      "epoch: 5 step: 215, loss is 0.003603527555242181\n",
      "epoch: 5 step: 216, loss is 0.007136069238185883\n",
      "epoch: 5 step: 217, loss is 0.019075028598308563\n",
      "epoch: 5 step: 218, loss is 0.007527654059231281\n",
      "epoch: 5 step: 219, loss is 0.0009508816292509437\n",
      "epoch: 5 step: 220, loss is 0.07757323235273361\n",
      "epoch: 5 step: 221, loss is 0.21883288025856018\n",
      "epoch: 5 step: 222, loss is 0.01656767539680004\n",
      "epoch: 5 step: 223, loss is 0.05865538492798805\n",
      "epoch: 5 step: 224, loss is 0.050277162343263626\n",
      "epoch: 5 step: 225, loss is 0.0257112979888916\n",
      "epoch: 5 step: 226, loss is 0.01304677315056324\n",
      "epoch: 5 step: 227, loss is 0.028277643024921417\n",
      "epoch: 5 step: 228, loss is 0.056462716311216354\n",
      "epoch: 5 step: 229, loss is 0.06281206011772156\n",
      "epoch: 5 step: 230, loss is 0.002111398382112384\n",
      "epoch: 5 step: 231, loss is 0.0007964865071699023\n",
      "epoch: 5 step: 232, loss is 0.0016253795474767685\n",
      "epoch: 5 step: 233, loss is 0.07953906804323196\n",
      "epoch: 5 step: 234, loss is 0.013873262330889702\n",
      "epoch: 5 step: 235, loss is 0.0013204094721004367\n",
      "epoch: 5 step: 236, loss is 0.002023605164140463\n",
      "epoch: 5 step: 237, loss is 0.0004222069401293993\n",
      "epoch: 5 step: 238, loss is 0.0006507361540570855\n",
      "epoch: 5 step: 239, loss is 0.037300314754247665\n",
      "epoch: 5 step: 240, loss is 0.0017713138367980719\n",
      "epoch: 5 step: 241, loss is 0.0034873972181230783\n",
      "epoch: 5 step: 242, loss is 0.012815926223993301\n",
      "epoch: 5 step: 243, loss is 0.0028080232441425323\n",
      "epoch: 5 step: 244, loss is 0.05739221349358559\n",
      "epoch: 5 step: 245, loss is 0.0010805780766531825\n",
      "epoch: 5 step: 246, loss is 0.0028637773357331753\n",
      "epoch: 5 step: 247, loss is 0.0005842526443302631\n",
      "epoch: 5 step: 248, loss is 0.0013763336464762688\n",
      "epoch: 5 step: 249, loss is 0.0019375319825485349\n",
      "epoch: 5 step: 250, loss is 0.0034083216451108456\n",
      "epoch: 5 step: 251, loss is 0.0069317324087023735\n",
      "epoch: 5 step: 252, loss is 6.761836993973702e-05\n",
      "epoch: 5 step: 253, loss is 0.1080295741558075\n",
      "epoch: 5 step: 254, loss is 0.04414084926247597\n",
      "epoch: 5 step: 255, loss is 0.01956787332892418\n",
      "epoch: 5 step: 256, loss is 0.010405347682535648\n",
      "epoch: 5 step: 257, loss is 0.13690753281116486\n",
      "epoch: 5 step: 258, loss is 0.06371589750051498\n",
      "epoch: 5 step: 259, loss is 0.03725221008062363\n",
      "epoch: 5 step: 260, loss is 0.14287206530570984\n",
      "epoch: 5 step: 261, loss is 0.000604809436481446\n",
      "epoch: 5 step: 262, loss is 0.21414442360401154\n",
      "epoch: 5 step: 263, loss is 0.038387469947338104\n",
      "epoch: 5 step: 264, loss is 0.0020041766110807657\n",
      "epoch: 5 step: 265, loss is 0.002227376913651824\n",
      "epoch: 5 step: 266, loss is 0.05323486775159836\n",
      "epoch: 5 step: 267, loss is 0.002274022903293371\n",
      "epoch: 5 step: 268, loss is 0.0009551860275678337\n",
      "epoch: 5 step: 269, loss is 0.005439442582428455\n",
      "epoch: 5 step: 270, loss is 0.04118522256612778\n",
      "epoch: 5 step: 271, loss is 0.001396020408719778\n",
      "epoch: 5 step: 272, loss is 0.010738514363765717\n",
      "epoch: 5 step: 273, loss is 0.020331330597400665\n",
      "epoch: 5 step: 274, loss is 0.12083641439676285\n",
      "epoch: 5 step: 275, loss is 0.008265381678938866\n",
      "epoch: 5 step: 276, loss is 0.013979910872876644\n",
      "epoch: 5 step: 277, loss is 0.004886980168521404\n",
      "epoch: 5 step: 278, loss is 0.005464697722345591\n",
      "epoch: 5 step: 279, loss is 0.002712565939873457\n",
      "epoch: 5 step: 280, loss is 0.002382761798799038\n",
      "epoch: 5 step: 281, loss is 0.0036173921544104815\n",
      "epoch: 5 step: 282, loss is 0.000386013271054253\n",
      "epoch: 5 step: 283, loss is 0.023260872811079025\n",
      "epoch: 5 step: 284, loss is 0.010155124589800835\n",
      "epoch: 5 step: 285, loss is 0.0017159113194793463\n",
      "epoch: 5 step: 286, loss is 0.0019618412479758263\n",
      "epoch: 5 step: 287, loss is 0.02335445210337639\n",
      "epoch: 5 step: 288, loss is 0.0028061585035175085\n",
      "epoch: 5 step: 289, loss is 0.03406643867492676\n",
      "epoch: 5 step: 290, loss is 0.021231092512607574\n",
      "epoch: 5 step: 291, loss is 0.0012827295577153563\n",
      "epoch: 5 step: 292, loss is 0.029095236212015152\n",
      "epoch: 5 step: 293, loss is 0.0727795660495758\n",
      "epoch: 5 step: 294, loss is 0.017525019124150276\n",
      "epoch: 5 step: 295, loss is 0.010386984795331955\n",
      "epoch: 5 step: 296, loss is 0.00581778772175312\n",
      "epoch: 5 step: 297, loss is 0.002509311307221651\n",
      "epoch: 5 step: 298, loss is 0.009315217845141888\n",
      "epoch: 5 step: 299, loss is 0.013701668940484524\n",
      "epoch: 5 step: 300, loss is 0.052283965051174164\n",
      "epoch: 5 step: 301, loss is 0.021670740097761154\n",
      "epoch: 5 step: 302, loss is 0.0008125987369567156\n",
      "epoch: 5 step: 303, loss is 0.002362038940191269\n",
      "epoch: 5 step: 304, loss is 0.17575141787528992\n",
      "epoch: 5 step: 305, loss is 0.006992504000663757\n",
      "epoch: 5 step: 306, loss is 0.008559363894164562\n",
      "epoch: 5 step: 307, loss is 0.0558796226978302\n",
      "epoch: 5 step: 308, loss is 0.020857684314250946\n",
      "epoch: 5 step: 309, loss is 0.004387333057820797\n",
      "epoch: 5 step: 310, loss is 0.056606926023960114\n",
      "epoch: 5 step: 311, loss is 0.00017363752704113722\n",
      "epoch: 5 step: 312, loss is 0.0005848306464031339\n",
      "epoch: 5 step: 313, loss is 0.0020158689003437757\n",
      "epoch: 5 step: 314, loss is 0.00393527140840888\n",
      "epoch: 5 step: 315, loss is 0.05513886362314224\n",
      "epoch: 5 step: 316, loss is 0.03998583182692528\n",
      "epoch: 5 step: 317, loss is 0.001366870361380279\n",
      "epoch: 5 step: 318, loss is 0.15801867842674255\n",
      "epoch: 5 step: 319, loss is 0.00132291903719306\n",
      "epoch: 5 step: 320, loss is 0.001927813165821135\n",
      "epoch: 5 step: 321, loss is 0.0009329747990705073\n",
      "epoch: 5 step: 322, loss is 0.02091643586754799\n",
      "epoch: 5 step: 323, loss is 0.00431592995300889\n",
      "epoch: 5 step: 324, loss is 0.044029198586940765\n",
      "epoch: 5 step: 325, loss is 0.0006200515199452639\n",
      "epoch: 5 step: 326, loss is 0.016713615506887436\n",
      "epoch: 5 step: 327, loss is 0.056255143135786057\n",
      "epoch: 5 step: 328, loss is 0.036315467208623886\n",
      "epoch: 5 step: 329, loss is 0.010263592936098576\n",
      "epoch: 5 step: 330, loss is 0.019836213439702988\n",
      "epoch: 5 step: 331, loss is 0.0609927736222744\n",
      "epoch: 5 step: 332, loss is 0.00021746873972006142\n",
      "epoch: 5 step: 333, loss is 0.005776474718004465\n",
      "epoch: 5 step: 334, loss is 0.00020858833158854395\n",
      "epoch: 5 step: 335, loss is 0.00025061782798729837\n",
      "epoch: 5 step: 336, loss is 0.0019778579007834196\n",
      "epoch: 5 step: 337, loss is 0.0003748832968994975\n",
      "epoch: 5 step: 338, loss is 0.08155018091201782\n",
      "epoch: 5 step: 339, loss is 0.001821397920139134\n",
      "epoch: 5 step: 340, loss is 0.005850679706782103\n",
      "epoch: 5 step: 341, loss is 0.001099595450796187\n",
      "epoch: 5 step: 342, loss is 0.0015219259075820446\n",
      "epoch: 5 step: 343, loss is 0.045737702399492264\n",
      "epoch: 5 step: 344, loss is 0.3343777358531952\n",
      "epoch: 5 step: 345, loss is 0.3941965401172638\n",
      "epoch: 5 step: 346, loss is 0.0026678338181227446\n",
      "epoch: 5 step: 347, loss is 0.07429729402065277\n",
      "epoch: 5 step: 348, loss is 0.009273179806768894\n",
      "epoch: 5 step: 349, loss is 0.020896395668387413\n",
      "epoch: 5 step: 350, loss is 0.00034694228088483214\n",
      "epoch: 5 step: 351, loss is 0.1812572181224823\n",
      "epoch: 5 step: 352, loss is 0.02822060137987137\n",
      "epoch: 5 step: 353, loss is 0.0020013870671391487\n",
      "epoch: 5 step: 354, loss is 0.01564072072505951\n",
      "epoch: 5 step: 355, loss is 0.016594113782048225\n",
      "epoch: 5 step: 356, loss is 0.04757099226117134\n",
      "epoch: 5 step: 357, loss is 0.03180588036775589\n",
      "epoch: 5 step: 358, loss is 0.008252427913248539\n",
      "epoch: 5 step: 359, loss is 0.010416828095912933\n",
      "epoch: 5 step: 360, loss is 0.07838091999292374\n",
      "epoch: 5 step: 361, loss is 0.010053692385554314\n",
      "epoch: 5 step: 362, loss is 0.04326235130429268\n",
      "epoch: 5 step: 363, loss is 0.025993986055254936\n",
      "epoch: 5 step: 364, loss is 0.017175277695059776\n",
      "epoch: 5 step: 365, loss is 0.026572054252028465\n",
      "epoch: 5 step: 366, loss is 0.001753922551870346\n",
      "epoch: 5 step: 367, loss is 0.04191816970705986\n",
      "epoch: 5 step: 368, loss is 0.006505983881652355\n",
      "epoch: 5 step: 369, loss is 0.02818525955080986\n",
      "epoch: 5 step: 370, loss is 0.012617161497473717\n",
      "epoch: 5 step: 371, loss is 0.009452620521187782\n",
      "epoch: 5 step: 372, loss is 0.06644803285598755\n",
      "epoch: 5 step: 373, loss is 0.002002716762945056\n",
      "epoch: 5 step: 374, loss is 0.037804074585437775\n",
      "epoch: 5 step: 375, loss is 0.0015600135084241629\n",
      "epoch: 5 step: 376, loss is 0.012001750990748405\n",
      "epoch: 5 step: 377, loss is 0.04845597967505455\n",
      "epoch: 5 step: 378, loss is 0.0008703608764335513\n",
      "epoch: 5 step: 379, loss is 0.002237081527709961\n",
      "epoch: 5 step: 380, loss is 0.008619892410933971\n",
      "epoch: 5 step: 381, loss is 0.0038048101123422384\n",
      "epoch: 5 step: 382, loss is 0.0023586039897054434\n",
      "epoch: 5 step: 383, loss is 0.015919646248221397\n",
      "epoch: 5 step: 384, loss is 0.03478969261050224\n",
      "epoch: 5 step: 385, loss is 0.0030719644855707884\n",
      "epoch: 5 step: 386, loss is 0.00013220452819950879\n",
      "epoch: 5 step: 387, loss is 0.02329244092106819\n",
      "epoch: 5 step: 388, loss is 0.10890325903892517\n",
      "epoch: 5 step: 389, loss is 0.0007749299984425306\n",
      "epoch: 5 step: 390, loss is 0.0007486514514312148\n",
      "epoch: 5 step: 391, loss is 0.00023130247427616268\n",
      "epoch: 5 step: 392, loss is 0.002081060316413641\n",
      "epoch: 5 step: 393, loss is 0.024845238775014877\n",
      "epoch: 5 step: 394, loss is 0.01285855658352375\n",
      "epoch: 5 step: 395, loss is 0.0003615799068938941\n",
      "epoch: 5 step: 396, loss is 0.008845350705087185\n",
      "epoch: 5 step: 397, loss is 0.0014397274935618043\n",
      "epoch: 5 step: 398, loss is 0.002804684918373823\n",
      "epoch: 5 step: 399, loss is 0.0005396784981712699\n",
      "epoch: 5 step: 400, loss is 0.08938204497098923\n",
      "epoch: 5 step: 401, loss is 0.03891065716743469\n",
      "epoch: 5 step: 402, loss is 0.06530723720788956\n",
      "epoch: 5 step: 403, loss is 0.0011803271481767297\n",
      "epoch: 5 step: 404, loss is 0.038930609822273254\n",
      "epoch: 5 step: 405, loss is 0.26956409215927124\n",
      "epoch: 5 step: 406, loss is 0.000725296966265887\n",
      "epoch: 5 step: 407, loss is 0.002196823013946414\n",
      "epoch: 5 step: 408, loss is 0.0011580284917727113\n",
      "epoch: 5 step: 409, loss is 0.06160284951329231\n",
      "epoch: 5 step: 410, loss is 0.001601576805114746\n",
      "epoch: 5 step: 411, loss is 0.00043301843106746674\n",
      "epoch: 5 step: 412, loss is 0.01435982808470726\n",
      "epoch: 5 step: 413, loss is 0.016793647781014442\n",
      "epoch: 5 step: 414, loss is 0.000894505821634084\n",
      "epoch: 5 step: 415, loss is 0.1570868045091629\n",
      "epoch: 5 step: 416, loss is 0.056112952530384064\n",
      "epoch: 5 step: 417, loss is 0.20662613213062286\n",
      "epoch: 5 step: 418, loss is 0.0424380749464035\n",
      "epoch: 5 step: 419, loss is 0.04620758816599846\n",
      "epoch: 5 step: 420, loss is 0.0650213360786438\n",
      "epoch: 5 step: 421, loss is 0.0023871525190770626\n",
      "epoch: 5 step: 422, loss is 0.002113680588081479\n",
      "epoch: 5 step: 423, loss is 0.0068095289170742035\n",
      "epoch: 5 step: 424, loss is 0.01618332788348198\n",
      "epoch: 5 step: 425, loss is 0.06133824959397316\n",
      "epoch: 5 step: 426, loss is 0.0480862557888031\n",
      "epoch: 5 step: 427, loss is 0.05205322429537773\n",
      "epoch: 5 step: 428, loss is 0.07718053460121155\n",
      "epoch: 5 step: 429, loss is 0.006919317878782749\n",
      "epoch: 5 step: 430, loss is 0.016304418444633484\n",
      "epoch: 5 step: 431, loss is 0.0010663139401003718\n",
      "epoch: 5 step: 432, loss is 0.03689814731478691\n",
      "epoch: 5 step: 433, loss is 0.007800376508384943\n",
      "epoch: 5 step: 434, loss is 0.0014032179024070501\n",
      "epoch: 5 step: 435, loss is 0.003803097177296877\n",
      "epoch: 5 step: 436, loss is 0.017137162387371063\n",
      "epoch: 5 step: 437, loss is 0.045799437910318375\n",
      "epoch: 5 step: 438, loss is 0.03837261348962784\n",
      "epoch: 5 step: 439, loss is 0.15868908166885376\n",
      "epoch: 5 step: 440, loss is 0.003757805097848177\n",
      "epoch: 5 step: 441, loss is 0.0011367984116077423\n",
      "epoch: 5 step: 442, loss is 0.004266238305717707\n",
      "epoch: 5 step: 443, loss is 0.009050365537405014\n",
      "epoch: 5 step: 444, loss is 0.0029107762966305017\n",
      "epoch: 5 step: 445, loss is 0.008823499083518982\n",
      "epoch: 5 step: 446, loss is 0.052933212369680405\n",
      "epoch: 5 step: 447, loss is 0.018598534166812897\n",
      "epoch: 5 step: 448, loss is 0.022434145212173462\n",
      "epoch: 5 step: 449, loss is 0.011348352767527103\n",
      "epoch: 5 step: 450, loss is 0.1218758225440979\n",
      "epoch: 5 step: 451, loss is 0.010837907902896404\n",
      "epoch: 5 step: 452, loss is 0.009535269811749458\n",
      "epoch: 5 step: 453, loss is 0.012521217577159405\n",
      "epoch: 5 step: 454, loss is 0.14103436470031738\n",
      "epoch: 5 step: 455, loss is 0.007333012297749519\n",
      "epoch: 5 step: 456, loss is 0.06356120854616165\n",
      "epoch: 5 step: 457, loss is 0.0013449580874294043\n",
      "epoch: 5 step: 458, loss is 0.006532835774123669\n",
      "epoch: 5 step: 459, loss is 0.06921250373125076\n",
      "epoch: 5 step: 460, loss is 0.00480543402954936\n",
      "epoch: 5 step: 461, loss is 0.0006952908006496727\n",
      "epoch: 5 step: 462, loss is 0.0009233485907316208\n",
      "epoch: 5 step: 463, loss is 0.018079323694109917\n",
      "epoch: 5 step: 464, loss is 0.0025449926033616066\n",
      "epoch: 5 step: 465, loss is 0.011311584152281284\n",
      "epoch: 5 step: 466, loss is 0.012783418409526348\n",
      "epoch: 5 step: 467, loss is 0.01473394874483347\n",
      "epoch: 5 step: 468, loss is 0.0058859349228441715\n",
      "epoch: 5 step: 469, loss is 0.11129215359687805\n",
      "epoch: 5 step: 470, loss is 0.06985173374414444\n",
      "epoch: 5 step: 471, loss is 0.005437730345875025\n",
      "epoch: 5 step: 472, loss is 0.00023436688934452832\n",
      "epoch: 5 step: 473, loss is 0.025042343884706497\n",
      "epoch: 5 step: 474, loss is 0.17428240180015564\n",
      "epoch: 5 step: 475, loss is 0.001194935292005539\n",
      "epoch: 5 step: 476, loss is 0.06839984655380249\n",
      "epoch: 5 step: 477, loss is 0.003398271044716239\n",
      "epoch: 5 step: 478, loss is 0.0071236006915569305\n",
      "epoch: 5 step: 479, loss is 0.035132139921188354\n",
      "epoch: 5 step: 480, loss is 0.09312134981155396\n",
      "epoch: 5 step: 481, loss is 0.0046524954959750175\n",
      "epoch: 5 step: 482, loss is 0.005967772100120783\n",
      "epoch: 5 step: 483, loss is 0.0006093883421272039\n",
      "epoch: 5 step: 484, loss is 0.0008543268777430058\n",
      "epoch: 5 step: 485, loss is 0.00033822370460256934\n",
      "epoch: 5 step: 486, loss is 0.012655436992645264\n",
      "epoch: 5 step: 487, loss is 0.017845815047621727\n",
      "epoch: 5 step: 488, loss is 0.025180935859680176\n",
      "epoch: 5 step: 489, loss is 0.002498659072443843\n",
      "epoch: 5 step: 490, loss is 0.09455375373363495\n",
      "epoch: 5 step: 491, loss is 0.02769392356276512\n",
      "epoch: 5 step: 492, loss is 0.11604853719472885\n",
      "epoch: 5 step: 493, loss is 0.013891579583287239\n",
      "epoch: 5 step: 494, loss is 0.03152281045913696\n",
      "epoch: 5 step: 495, loss is 0.07102376222610474\n",
      "epoch: 5 step: 496, loss is 0.001483251922763884\n",
      "epoch: 5 step: 497, loss is 0.00039848373853601515\n",
      "epoch: 5 step: 498, loss is 0.029631366953253746\n",
      "epoch: 5 step: 499, loss is 0.0015293258475139737\n",
      "epoch: 5 step: 500, loss is 0.0005070080514997244\n",
      "epoch: 5 step: 501, loss is 0.0019119682256132364\n",
      "epoch: 5 step: 502, loss is 0.002879475709050894\n",
      "epoch: 5 step: 503, loss is 0.013167082332074642\n",
      "epoch: 5 step: 504, loss is 0.017327364534139633\n",
      "epoch: 5 step: 505, loss is 0.015290538780391216\n",
      "epoch: 5 step: 506, loss is 0.01490162592381239\n",
      "epoch: 5 step: 507, loss is 0.006974755320698023\n",
      "epoch: 5 step: 508, loss is 0.028065679594874382\n",
      "epoch: 5 step: 509, loss is 0.04240637272596359\n",
      "epoch: 5 step: 510, loss is 0.009256256744265556\n",
      "epoch: 5 step: 511, loss is 0.0003904100158251822\n",
      "epoch: 5 step: 512, loss is 0.004486683290451765\n",
      "epoch: 5 step: 513, loss is 0.021014003083109856\n",
      "epoch: 5 step: 514, loss is 0.0015202333452180028\n",
      "epoch: 5 step: 515, loss is 0.04051339626312256\n",
      "epoch: 5 step: 516, loss is 0.0062491269782185555\n",
      "epoch: 5 step: 517, loss is 0.008472696878015995\n",
      "epoch: 5 step: 518, loss is 0.12483760714530945\n",
      "epoch: 5 step: 519, loss is 0.001992055680602789\n",
      "epoch: 5 step: 520, loss is 0.0018911967054009438\n",
      "epoch: 5 step: 521, loss is 2.4269105779239908e-05\n",
      "epoch: 5 step: 522, loss is 0.0007252490031532943\n",
      "epoch: 5 step: 523, loss is 0.001129702664911747\n",
      "epoch: 5 step: 524, loss is 0.006312499288469553\n",
      "epoch: 5 step: 525, loss is 0.05628344789147377\n",
      "epoch: 5 step: 526, loss is 0.00047720165457576513\n",
      "epoch: 5 step: 527, loss is 0.0026588202454149723\n",
      "epoch: 5 step: 528, loss is 0.007802057545632124\n",
      "epoch: 5 step: 529, loss is 0.0019931672140955925\n",
      "epoch: 5 step: 530, loss is 0.0261733029037714\n",
      "epoch: 5 step: 531, loss is 0.0007569838198833168\n",
      "epoch: 5 step: 532, loss is 0.0002907192683778703\n",
      "epoch: 5 step: 533, loss is 0.000240354347624816\n",
      "epoch: 5 step: 534, loss is 0.0014310124097391963\n",
      "epoch: 5 step: 535, loss is 0.00023573169892188162\n",
      "epoch: 5 step: 536, loss is 0.005133545491844416\n",
      "epoch: 5 step: 537, loss is 6.307146395556629e-05\n",
      "epoch: 5 step: 538, loss is 0.001548373606055975\n",
      "epoch: 5 step: 539, loss is 0.0005179340369068086\n",
      "epoch: 5 step: 540, loss is 0.00031424424378201365\n",
      "epoch: 5 step: 541, loss is 0.0005863943370059133\n",
      "epoch: 5 step: 542, loss is 0.00014445332635659724\n",
      "epoch: 5 step: 543, loss is 0.0013359434669837356\n",
      "epoch: 5 step: 544, loss is 0.0001179908576887101\n",
      "epoch: 5 step: 545, loss is 0.11910516023635864\n",
      "epoch: 5 step: 546, loss is 0.02075483836233616\n",
      "epoch: 5 step: 547, loss is 0.0014234151458367705\n",
      "epoch: 5 step: 548, loss is 0.0006649998249486089\n",
      "epoch: 5 step: 549, loss is 0.0006267345743253827\n",
      "epoch: 5 step: 550, loss is 0.0006881622248329222\n",
      "epoch: 5 step: 551, loss is 0.006556399632245302\n",
      "epoch: 5 step: 552, loss is 0.0013822679175063968\n",
      "epoch: 5 step: 553, loss is 0.00040730100590735674\n",
      "epoch: 5 step: 554, loss is 0.033734824508428574\n",
      "epoch: 5 step: 555, loss is 0.02072766423225403\n",
      "epoch: 5 step: 556, loss is 0.0032957743387669325\n",
      "epoch: 5 step: 557, loss is 0.0034634158946573734\n",
      "epoch: 5 step: 558, loss is 0.05690627172589302\n",
      "epoch: 5 step: 559, loss is 0.007730473298579454\n",
      "epoch: 5 step: 560, loss is 0.01174199115484953\n",
      "epoch: 5 step: 561, loss is 0.0004615113139152527\n",
      "epoch: 5 step: 562, loss is 0.01698399893939495\n",
      "epoch: 5 step: 563, loss is 0.08972743898630142\n",
      "epoch: 5 step: 564, loss is 0.0020411345176398754\n",
      "epoch: 5 step: 565, loss is 0.03501741588115692\n",
      "epoch: 5 step: 566, loss is 0.006825655233114958\n",
      "epoch: 5 step: 567, loss is 0.00061114935670048\n",
      "epoch: 5 step: 568, loss is 2.9712211471633054e-05\n",
      "epoch: 5 step: 569, loss is 0.0005764507804997265\n",
      "epoch: 5 step: 570, loss is 0.0007651467458344996\n",
      "epoch: 5 step: 571, loss is 0.002230552490800619\n",
      "epoch: 5 step: 572, loss is 0.0014366272371262312\n",
      "epoch: 5 step: 573, loss is 0.00019937308388762176\n",
      "epoch: 5 step: 574, loss is 0.0018673378508538008\n",
      "epoch: 5 step: 575, loss is 0.05504027381539345\n",
      "epoch: 5 step: 576, loss is 0.0001979617663891986\n",
      "epoch: 5 step: 577, loss is 0.0015196541789919138\n",
      "epoch: 5 step: 578, loss is 0.0005797443445771933\n",
      "epoch: 5 step: 579, loss is 0.00024332330212928355\n",
      "epoch: 5 step: 580, loss is 4.36242189607583e-05\n",
      "epoch: 5 step: 581, loss is 0.034899208694696426\n",
      "epoch: 5 step: 582, loss is 0.00300397165119648\n",
      "epoch: 5 step: 583, loss is 0.1534447818994522\n",
      "epoch: 5 step: 584, loss is 0.0015920456498861313\n",
      "epoch: 5 step: 585, loss is 0.0009625571547076106\n",
      "epoch: 5 step: 586, loss is 0.008742519654333591\n",
      "epoch: 5 step: 587, loss is 0.13648393750190735\n",
      "epoch: 5 step: 588, loss is 0.00011598678247537464\n",
      "epoch: 5 step: 589, loss is 0.0016049992991611362\n",
      "epoch: 5 step: 590, loss is 0.012035571038722992\n",
      "epoch: 5 step: 591, loss is 9.682653762865812e-05\n",
      "epoch: 5 step: 592, loss is 0.0018517684657126665\n",
      "epoch: 5 step: 593, loss is 0.01507091149687767\n",
      "epoch: 5 step: 594, loss is 0.015407650731503963\n",
      "epoch: 5 step: 595, loss is 0.0531744509935379\n",
      "epoch: 5 step: 596, loss is 0.0002724028308875859\n",
      "epoch: 5 step: 597, loss is 0.0002884342975448817\n",
      "epoch: 5 step: 598, loss is 0.01524331420660019\n",
      "epoch: 5 step: 599, loss is 0.0050741080194711685\n",
      "epoch: 5 step: 600, loss is 0.0011331309797242284\n",
      "epoch: 5 step: 601, loss is 0.3368806838989258\n",
      "epoch: 5 step: 602, loss is 0.2931548058986664\n",
      "epoch: 5 step: 603, loss is 0.011250516399741173\n",
      "epoch: 5 step: 604, loss is 0.0035367165692150593\n",
      "epoch: 5 step: 605, loss is 0.0010881295893341303\n",
      "epoch: 5 step: 606, loss is 0.00492937583476305\n",
      "epoch: 5 step: 607, loss is 0.04337037727236748\n",
      "epoch: 5 step: 608, loss is 0.013465390540659428\n",
      "epoch: 5 step: 609, loss is 0.11856793612241745\n",
      "epoch: 5 step: 610, loss is 0.025695009157061577\n",
      "epoch: 5 step: 611, loss is 0.06293976306915283\n",
      "epoch: 5 step: 612, loss is 0.003693218342959881\n",
      "epoch: 5 step: 613, loss is 0.029529087245464325\n",
      "epoch: 5 step: 614, loss is 0.11281301081180573\n",
      "epoch: 5 step: 615, loss is 0.021835261955857277\n",
      "epoch: 5 step: 616, loss is 0.013391365297138691\n",
      "epoch: 5 step: 617, loss is 0.004548035562038422\n",
      "epoch: 5 step: 618, loss is 0.0004191886109765619\n",
      "epoch: 5 step: 619, loss is 0.0013343995669856668\n",
      "epoch: 5 step: 620, loss is 0.14900948107242584\n",
      "epoch: 5 step: 621, loss is 0.03356695547699928\n",
      "epoch: 5 step: 622, loss is 0.04957805946469307\n",
      "epoch: 5 step: 623, loss is 0.002756539499387145\n",
      "epoch: 5 step: 624, loss is 0.001138791791163385\n",
      "epoch: 5 step: 625, loss is 0.01314026489853859\n",
      "epoch: 5 step: 626, loss is 0.011456307023763657\n",
      "epoch: 5 step: 627, loss is 0.0013401535106822848\n",
      "epoch: 5 step: 628, loss is 0.12092631310224533\n",
      "epoch: 5 step: 629, loss is 0.006796513218432665\n",
      "epoch: 5 step: 630, loss is 0.00048398549552075565\n",
      "epoch: 5 step: 631, loss is 0.0002530051569920033\n",
      "epoch: 5 step: 632, loss is 0.010805453173816204\n",
      "epoch: 5 step: 633, loss is 0.020374473184347153\n",
      "epoch: 5 step: 634, loss is 0.005436248611658812\n",
      "epoch: 5 step: 635, loss is 0.10477230697870255\n",
      "epoch: 5 step: 636, loss is 0.021106600761413574\n",
      "epoch: 5 step: 637, loss is 0.00687660276889801\n",
      "epoch: 5 step: 638, loss is 0.06597494333982468\n",
      "epoch: 5 step: 639, loss is 0.011382018215954304\n",
      "epoch: 5 step: 640, loss is 0.002807531738653779\n",
      "epoch: 5 step: 641, loss is 0.09699030220508575\n",
      "epoch: 5 step: 642, loss is 0.0035581537522375584\n",
      "epoch: 5 step: 643, loss is 0.011293936520814896\n",
      "epoch: 5 step: 644, loss is 0.0031704914290457964\n",
      "epoch: 5 step: 645, loss is 0.0052862572483718395\n",
      "epoch: 5 step: 646, loss is 0.002288493560627103\n",
      "epoch: 5 step: 647, loss is 0.004133245907723904\n",
      "epoch: 5 step: 648, loss is 0.01682000793516636\n",
      "epoch: 5 step: 649, loss is 0.15108905732631683\n",
      "epoch: 5 step: 650, loss is 0.17527888715267181\n",
      "epoch: 5 step: 651, loss is 0.07327649742364883\n",
      "epoch: 5 step: 652, loss is 0.004008973948657513\n",
      "epoch: 5 step: 653, loss is 0.0026874637696892023\n",
      "epoch: 5 step: 654, loss is 0.021316569298505783\n",
      "epoch: 5 step: 655, loss is 0.09094218909740448\n",
      "epoch: 5 step: 656, loss is 0.04014752060174942\n",
      "epoch: 5 step: 657, loss is 0.0007835784344933927\n",
      "epoch: 5 step: 658, loss is 0.01843256875872612\n",
      "epoch: 5 step: 659, loss is 0.01079944334924221\n",
      "epoch: 5 step: 660, loss is 0.04587865248322487\n",
      "epoch: 5 step: 661, loss is 0.004805676639080048\n",
      "epoch: 5 step: 662, loss is 0.014401335269212723\n",
      "epoch: 5 step: 663, loss is 0.00789353996515274\n",
      "epoch: 5 step: 664, loss is 0.07815204560756683\n",
      "epoch: 5 step: 665, loss is 0.002833073493093252\n",
      "epoch: 5 step: 666, loss is 0.00557977519929409\n",
      "epoch: 5 step: 667, loss is 0.06834232807159424\n",
      "epoch: 5 step: 668, loss is 0.014549680054187775\n",
      "epoch: 5 step: 669, loss is 0.002369884168729186\n",
      "epoch: 5 step: 670, loss is 0.016591036692261696\n",
      "epoch: 5 step: 671, loss is 0.003544989274814725\n",
      "epoch: 5 step: 672, loss is 0.011597548611462116\n",
      "epoch: 5 step: 673, loss is 0.022312268614768982\n",
      "epoch: 5 step: 674, loss is 0.013086202554404736\n",
      "epoch: 5 step: 675, loss is 0.008478811010718346\n",
      "epoch: 5 step: 676, loss is 0.005539766512811184\n",
      "epoch: 5 step: 677, loss is 0.0022168313153088093\n",
      "epoch: 5 step: 678, loss is 0.12777043879032135\n",
      "epoch: 5 step: 679, loss is 0.004665021784603596\n",
      "epoch: 5 step: 680, loss is 0.08675970882177353\n",
      "epoch: 5 step: 681, loss is 0.049954067915678024\n",
      "epoch: 5 step: 682, loss is 0.0007218515384010971\n",
      "epoch: 5 step: 683, loss is 0.00033703874214552343\n",
      "epoch: 5 step: 684, loss is 0.00014980952255427837\n",
      "epoch: 5 step: 685, loss is 0.0035345787182450294\n",
      "epoch: 5 step: 686, loss is 0.010990211740136147\n",
      "epoch: 5 step: 687, loss is 0.09616058319807053\n",
      "epoch: 5 step: 688, loss is 0.01156554277986288\n",
      "epoch: 5 step: 689, loss is 0.00020913044863846153\n",
      "epoch: 5 step: 690, loss is 0.01976929046213627\n",
      "epoch: 5 step: 691, loss is 0.014765346422791481\n",
      "epoch: 5 step: 692, loss is 0.016563737764954567\n",
      "epoch: 5 step: 693, loss is 0.11829616129398346\n",
      "epoch: 5 step: 694, loss is 0.04324318841099739\n",
      "epoch: 5 step: 695, loss is 0.20666565001010895\n",
      "epoch: 5 step: 696, loss is 0.11058541387319565\n",
      "epoch: 5 step: 697, loss is 0.0036714428570121527\n",
      "epoch: 5 step: 698, loss is 0.01564386859536171\n",
      "epoch: 5 step: 699, loss is 0.00041093601612374187\n",
      "epoch: 5 step: 700, loss is 0.004072447773069143\n",
      "epoch: 5 step: 701, loss is 0.0006975839496590197\n",
      "epoch: 5 step: 702, loss is 0.0023713866248726845\n",
      "epoch: 5 step: 703, loss is 0.012775582261383533\n",
      "epoch: 5 step: 704, loss is 0.03888658806681633\n",
      "epoch: 5 step: 705, loss is 0.00020394402963574976\n",
      "epoch: 5 step: 706, loss is 0.004594697151333094\n",
      "epoch: 5 step: 707, loss is 0.07683190703392029\n",
      "epoch: 5 step: 708, loss is 0.0002919177059084177\n",
      "epoch: 5 step: 709, loss is 0.00038872085860930383\n",
      "epoch: 5 step: 710, loss is 0.0025209079030901194\n",
      "epoch: 5 step: 711, loss is 0.08838040381669998\n",
      "epoch: 5 step: 712, loss is 0.024713927879929543\n",
      "epoch: 5 step: 713, loss is 0.1833762377500534\n",
      "epoch: 5 step: 714, loss is 0.00033098983112722635\n",
      "epoch: 5 step: 715, loss is 0.0005402673850767314\n",
      "epoch: 5 step: 716, loss is 0.17836114764213562\n",
      "epoch: 5 step: 717, loss is 0.04483828693628311\n",
      "epoch: 5 step: 718, loss is 0.0035169010516256094\n",
      "epoch: 5 step: 719, loss is 0.015821153298020363\n",
      "epoch: 5 step: 720, loss is 0.029180774465203285\n",
      "epoch: 5 step: 721, loss is 0.05968097597360611\n",
      "epoch: 5 step: 722, loss is 0.05671748146414757\n",
      "epoch: 5 step: 723, loss is 0.037893328815698624\n",
      "epoch: 5 step: 724, loss is 0.09471113979816437\n",
      "epoch: 5 step: 725, loss is 0.00415589427575469\n",
      "epoch: 5 step: 726, loss is 0.007016196846961975\n",
      "epoch: 5 step: 727, loss is 0.011529305018484592\n",
      "epoch: 5 step: 728, loss is 0.15815596282482147\n",
      "epoch: 5 step: 729, loss is 0.010363847948610783\n",
      "epoch: 5 step: 730, loss is 0.0006602135254070163\n",
      "epoch: 5 step: 731, loss is 0.001552609377540648\n",
      "epoch: 5 step: 732, loss is 0.08408792316913605\n",
      "epoch: 5 step: 733, loss is 0.023534376174211502\n",
      "epoch: 5 step: 734, loss is 0.011375950649380684\n",
      "epoch: 5 step: 735, loss is 0.0005537027609534562\n",
      "epoch: 5 step: 736, loss is 0.0017128009349107742\n",
      "epoch: 5 step: 737, loss is 0.058908894658088684\n",
      "epoch: 5 step: 738, loss is 0.042805224657058716\n",
      "epoch: 5 step: 739, loss is 0.011156484484672546\n",
      "epoch: 5 step: 740, loss is 0.011522977612912655\n",
      "epoch: 5 step: 741, loss is 0.0013548487331718206\n",
      "epoch: 5 step: 742, loss is 0.02991112321615219\n",
      "epoch: 5 step: 743, loss is 0.033251918852329254\n",
      "epoch: 5 step: 744, loss is 0.03236159682273865\n",
      "epoch: 5 step: 745, loss is 0.000933453906327486\n",
      "epoch: 5 step: 746, loss is 0.008746888488531113\n",
      "epoch: 5 step: 747, loss is 0.020985538139939308\n",
      "epoch: 5 step: 748, loss is 0.06318683177232742\n",
      "epoch: 5 step: 749, loss is 0.010470321401953697\n",
      "epoch: 5 step: 750, loss is 0.005101386923342943\n",
      "epoch: 5 step: 751, loss is 0.001154629630036652\n",
      "epoch: 5 step: 752, loss is 0.0003332308551762253\n",
      "epoch: 5 step: 753, loss is 0.017247306182980537\n",
      "epoch: 5 step: 754, loss is 0.024677405133843422\n",
      "epoch: 5 step: 755, loss is 0.0016891940031200647\n",
      "epoch: 5 step: 756, loss is 0.03945424035191536\n",
      "epoch: 5 step: 757, loss is 0.005676309112459421\n",
      "epoch: 5 step: 758, loss is 0.14272134006023407\n",
      "epoch: 5 step: 759, loss is 0.4043141007423401\n",
      "epoch: 5 step: 760, loss is 0.002514938358217478\n",
      "epoch: 5 step: 761, loss is 0.01454172283411026\n",
      "epoch: 5 step: 762, loss is 0.04209515452384949\n",
      "epoch: 5 step: 763, loss is 0.005320987198501825\n",
      "epoch: 5 step: 764, loss is 0.014213910326361656\n",
      "epoch: 5 step: 765, loss is 0.0022446371149271727\n",
      "epoch: 5 step: 766, loss is 0.00022910020197741687\n",
      "epoch: 5 step: 767, loss is 0.03185000643134117\n",
      "epoch: 5 step: 768, loss is 0.0021279030479490757\n",
      "epoch: 5 step: 769, loss is 0.05041204392910004\n",
      "epoch: 5 step: 770, loss is 0.007802966516464949\n",
      "epoch: 5 step: 771, loss is 0.00955780316144228\n",
      "epoch: 5 step: 772, loss is 0.0047978600487113\n",
      "epoch: 5 step: 773, loss is 0.013040117919445038\n",
      "epoch: 5 step: 774, loss is 0.018623707816004753\n",
      "epoch: 5 step: 775, loss is 0.001100344117730856\n",
      "epoch: 5 step: 776, loss is 0.27723923325538635\n",
      "epoch: 5 step: 777, loss is 0.004916410893201828\n",
      "epoch: 5 step: 778, loss is 0.0007976577617228031\n",
      "epoch: 5 step: 779, loss is 0.08925878256559372\n",
      "epoch: 5 step: 780, loss is 0.05303759127855301\n",
      "epoch: 5 step: 781, loss is 0.022627156227827072\n",
      "epoch: 5 step: 782, loss is 0.09922977536916733\n",
      "epoch: 5 step: 783, loss is 0.01472189836204052\n",
      "epoch: 5 step: 784, loss is 0.004056129604578018\n",
      "epoch: 5 step: 785, loss is 0.011258632875978947\n",
      "epoch: 5 step: 786, loss is 0.002716249320656061\n",
      "epoch: 5 step: 787, loss is 0.10585058480501175\n",
      "epoch: 5 step: 788, loss is 0.009539288468658924\n",
      "epoch: 5 step: 789, loss is 0.0010817240690812469\n",
      "epoch: 5 step: 790, loss is 0.00012139978935010731\n",
      "epoch: 5 step: 791, loss is 0.03919023647904396\n",
      "epoch: 5 step: 792, loss is 0.00307769444771111\n",
      "epoch: 5 step: 793, loss is 0.002345193410292268\n",
      "epoch: 5 step: 794, loss is 0.19674623012542725\n",
      "epoch: 5 step: 795, loss is 0.012614082545042038\n",
      "epoch: 5 step: 796, loss is 0.04124980419874191\n",
      "epoch: 5 step: 797, loss is 0.009225943125784397\n",
      "epoch: 5 step: 798, loss is 0.004253387451171875\n",
      "epoch: 5 step: 799, loss is 0.048486366868019104\n",
      "epoch: 5 step: 800, loss is 0.002383532002568245\n",
      "epoch: 5 step: 801, loss is 0.00466523040086031\n",
      "epoch: 5 step: 802, loss is 0.03693682327866554\n",
      "epoch: 5 step: 803, loss is 0.022118406370282173\n",
      "epoch: 5 step: 804, loss is 0.011135028675198555\n",
      "epoch: 5 step: 805, loss is 0.0016339350258931518\n",
      "epoch: 5 step: 806, loss is 0.000631945556961\n",
      "epoch: 5 step: 807, loss is 0.0076047927141189575\n",
      "epoch: 5 step: 808, loss is 0.08174071460962296\n",
      "epoch: 5 step: 809, loss is 0.07535553723573685\n",
      "epoch: 5 step: 810, loss is 0.0033616316504776478\n",
      "epoch: 5 step: 811, loss is 0.020060580223798752\n",
      "epoch: 5 step: 812, loss is 0.08186835050582886\n",
      "epoch: 5 step: 813, loss is 0.048373665660619736\n",
      "epoch: 5 step: 814, loss is 0.1045197993516922\n",
      "epoch: 5 step: 815, loss is 0.023108696565032005\n",
      "epoch: 5 step: 816, loss is 0.0011942342389374971\n",
      "epoch: 5 step: 817, loss is 0.0052377451211214066\n",
      "epoch: 5 step: 818, loss is 0.013741860166192055\n",
      "epoch: 5 step: 819, loss is 0.02230437658727169\n",
      "epoch: 5 step: 820, loss is 0.0006199547788128257\n",
      "epoch: 5 step: 821, loss is 0.010527195408940315\n",
      "epoch: 5 step: 822, loss is 0.10960724949836731\n",
      "epoch: 5 step: 823, loss is 0.14746960997581482\n",
      "epoch: 5 step: 824, loss is 0.024316605180501938\n",
      "epoch: 5 step: 825, loss is 0.008414425887167454\n",
      "epoch: 5 step: 826, loss is 0.006053184159100056\n",
      "epoch: 5 step: 827, loss is 0.0677894875407219\n",
      "epoch: 5 step: 828, loss is 0.06831534206867218\n",
      "epoch: 5 step: 829, loss is 0.00032972541521303356\n",
      "epoch: 5 step: 830, loss is 0.0013656020164489746\n",
      "epoch: 5 step: 831, loss is 0.0019757139962166548\n",
      "epoch: 5 step: 832, loss is 0.18490268290042877\n",
      "epoch: 5 step: 833, loss is 0.035607416182756424\n",
      "epoch: 5 step: 834, loss is 0.004269266966730356\n",
      "epoch: 5 step: 835, loss is 0.0019091570284217596\n",
      "epoch: 5 step: 836, loss is 0.07010643929243088\n",
      "epoch: 5 step: 837, loss is 0.0017456526402384043\n",
      "epoch: 5 step: 838, loss is 0.0010543285170570016\n",
      "epoch: 5 step: 839, loss is 0.002092843409627676\n",
      "epoch: 5 step: 840, loss is 0.0006259562214836478\n",
      "epoch: 5 step: 841, loss is 0.0006486300844699144\n",
      "epoch: 5 step: 842, loss is 0.0021446726750582457\n",
      "epoch: 5 step: 843, loss is 0.0038535355124622583\n",
      "epoch: 5 step: 844, loss is 0.0009428350604139268\n",
      "epoch: 5 step: 845, loss is 0.05565538629889488\n",
      "epoch: 5 step: 846, loss is 0.003265987616032362\n",
      "epoch: 5 step: 847, loss is 0.007582697086036205\n",
      "epoch: 5 step: 848, loss is 0.006801037583500147\n",
      "epoch: 5 step: 849, loss is 0.006683609914034605\n",
      "epoch: 5 step: 850, loss is 0.0031460884492844343\n",
      "epoch: 5 step: 851, loss is 8.248945960076526e-05\n",
      "epoch: 5 step: 852, loss is 0.0017474581254646182\n",
      "epoch: 5 step: 853, loss is 0.00042737610056065023\n",
      "epoch: 5 step: 854, loss is 0.0012009061174467206\n",
      "epoch: 5 step: 855, loss is 0.0005450551980175078\n",
      "epoch: 5 step: 856, loss is 0.003326571313664317\n",
      "epoch: 5 step: 857, loss is 0.007319328840821981\n",
      "epoch: 5 step: 858, loss is 0.00018670807185117155\n",
      "epoch: 5 step: 859, loss is 0.007384986151009798\n",
      "epoch: 5 step: 860, loss is 0.002209977013990283\n",
      "epoch: 5 step: 861, loss is 0.0011306052329018712\n",
      "epoch: 5 step: 862, loss is 0.0034004112239927053\n",
      "epoch: 5 step: 863, loss is 0.14228343963623047\n",
      "epoch: 5 step: 864, loss is 0.0024755930062383413\n",
      "epoch: 5 step: 865, loss is 0.2817901074886322\n",
      "epoch: 5 step: 866, loss is 0.04949384555220604\n",
      "epoch: 5 step: 867, loss is 0.04305553063750267\n",
      "epoch: 5 step: 868, loss is 0.0005466185393743217\n",
      "epoch: 5 step: 869, loss is 0.05274050682783127\n",
      "epoch: 5 step: 870, loss is 0.04972890764474869\n",
      "epoch: 5 step: 871, loss is 0.18000878393650055\n",
      "epoch: 5 step: 872, loss is 0.013797455467283726\n",
      "epoch: 5 step: 873, loss is 0.003683618502691388\n",
      "epoch: 5 step: 874, loss is 0.10237472504377365\n",
      "epoch: 5 step: 875, loss is 0.009084535762667656\n",
      "epoch: 5 step: 876, loss is 0.00225153099745512\n",
      "epoch: 5 step: 877, loss is 0.0039467285387218\n",
      "epoch: 5 step: 878, loss is 0.008647133596241474\n",
      "epoch: 5 step: 879, loss is 0.04767327755689621\n",
      "epoch: 5 step: 880, loss is 0.004107126034796238\n",
      "epoch: 5 step: 881, loss is 0.032642751932144165\n",
      "epoch: 5 step: 882, loss is 0.006122667342424393\n",
      "epoch: 5 step: 883, loss is 0.0032968036830425262\n",
      "epoch: 5 step: 884, loss is 0.0012005296302959323\n",
      "epoch: 5 step: 885, loss is 0.0601733922958374\n",
      "epoch: 5 step: 886, loss is 0.001697524101473391\n",
      "epoch: 5 step: 887, loss is 0.03319261223077774\n",
      "epoch: 5 step: 888, loss is 0.0031735619995743036\n",
      "epoch: 5 step: 889, loss is 0.046648453921079636\n",
      "epoch: 5 step: 890, loss is 0.010896355845034122\n",
      "epoch: 5 step: 891, loss is 0.035482484847307205\n",
      "epoch: 5 step: 892, loss is 0.0009910190710797906\n",
      "epoch: 5 step: 893, loss is 0.003173730568960309\n",
      "epoch: 5 step: 894, loss is 0.02912585251033306\n",
      "epoch: 5 step: 895, loss is 0.001266202423721552\n",
      "epoch: 5 step: 896, loss is 0.015700463205575943\n",
      "epoch: 5 step: 897, loss is 0.06762692332267761\n",
      "epoch: 5 step: 898, loss is 0.017840126529335976\n",
      "epoch: 5 step: 899, loss is 0.003431929973885417\n",
      "epoch: 5 step: 900, loss is 0.022373953834176064\n",
      "epoch: 5 step: 901, loss is 0.0017187751363962889\n",
      "epoch: 5 step: 902, loss is 0.10689388960599899\n",
      "epoch: 5 step: 903, loss is 0.005555706098675728\n",
      "epoch: 5 step: 904, loss is 0.033011604100465775\n",
      "epoch: 5 step: 905, loss is 0.0008113827207125723\n",
      "epoch: 5 step: 906, loss is 0.006664074957370758\n",
      "epoch: 5 step: 907, loss is 0.039583779871463776\n",
      "epoch: 5 step: 908, loss is 0.04274941235780716\n",
      "epoch: 5 step: 909, loss is 0.05005818232893944\n",
      "epoch: 5 step: 910, loss is 0.0014060059329494834\n",
      "epoch: 5 step: 911, loss is 0.00838361494243145\n",
      "epoch: 5 step: 912, loss is 0.0010217588860541582\n",
      "epoch: 5 step: 913, loss is 0.15528826415538788\n",
      "epoch: 5 step: 914, loss is 0.013889342546463013\n",
      "epoch: 5 step: 915, loss is 0.04897840693593025\n",
      "epoch: 5 step: 916, loss is 0.0035539499949663877\n",
      "epoch: 5 step: 917, loss is 0.008448800072073936\n",
      "epoch: 5 step: 918, loss is 0.004273262806236744\n",
      "epoch: 5 step: 919, loss is 0.028382999822497368\n",
      "epoch: 5 step: 920, loss is 0.009197990410029888\n",
      "epoch: 5 step: 921, loss is 0.13112221658229828\n",
      "epoch: 5 step: 922, loss is 0.033567849546670914\n",
      "epoch: 5 step: 923, loss is 0.02693868800997734\n",
      "epoch: 5 step: 924, loss is 0.07874174416065216\n",
      "epoch: 5 step: 925, loss is 0.010244281962513924\n",
      "epoch: 5 step: 926, loss is 0.0012080998858436942\n",
      "epoch: 5 step: 927, loss is 0.007645541336387396\n",
      "epoch: 5 step: 928, loss is 0.0014948101015761495\n",
      "epoch: 5 step: 929, loss is 0.0836627408862114\n",
      "epoch: 5 step: 930, loss is 0.0033435113728046417\n",
      "epoch: 5 step: 931, loss is 0.043791163712739944\n",
      "epoch: 5 step: 932, loss is 0.027841968461871147\n",
      "epoch: 5 step: 933, loss is 0.0135633684694767\n",
      "epoch: 5 step: 934, loss is 0.0012976727448403835\n",
      "epoch: 5 step: 935, loss is 0.007257014978677034\n",
      "epoch: 5 step: 936, loss is 0.003998295404016972\n",
      "epoch: 5 step: 937, loss is 0.012982258573174477\n",
      "epoch: 5 step: 938, loss is 0.09788013994693756\n",
      "epoch: 5 step: 939, loss is 0.02335892617702484\n",
      "epoch: 5 step: 940, loss is 0.0070317406207323074\n",
      "epoch: 5 step: 941, loss is 0.002099165227264166\n",
      "epoch: 5 step: 942, loss is 0.005043903831392527\n",
      "epoch: 5 step: 943, loss is 0.0017541859997436404\n",
      "epoch: 5 step: 944, loss is 0.0008556591928936541\n",
      "epoch: 5 step: 945, loss is 0.005523711442947388\n",
      "epoch: 5 step: 946, loss is 0.0070136780850589275\n",
      "epoch: 5 step: 947, loss is 0.019474048167467117\n",
      "epoch: 5 step: 948, loss is 0.033496737480163574\n",
      "epoch: 5 step: 949, loss is 0.034578945487737656\n",
      "epoch: 5 step: 950, loss is 0.0012258446076884866\n",
      "epoch: 5 step: 951, loss is 0.026452643796801567\n",
      "epoch: 5 step: 952, loss is 0.008536898531019688\n",
      "epoch: 5 step: 953, loss is 0.002468912396579981\n",
      "epoch: 5 step: 954, loss is 0.03657842054963112\n",
      "epoch: 5 step: 955, loss is 0.04326681047677994\n",
      "epoch: 5 step: 956, loss is 0.012230576016008854\n",
      "epoch: 5 step: 957, loss is 0.013027757406234741\n",
      "epoch: 5 step: 958, loss is 0.002557316329330206\n",
      "epoch: 5 step: 959, loss is 0.008494824171066284\n",
      "epoch: 5 step: 960, loss is 0.0024130716919898987\n",
      "epoch: 5 step: 961, loss is 0.13749445974826813\n",
      "epoch: 5 step: 962, loss is 0.010571641847491264\n",
      "epoch: 5 step: 963, loss is 0.033199358731508255\n",
      "epoch: 5 step: 964, loss is 0.0006265952251851559\n",
      "epoch: 5 step: 965, loss is 0.017468854784965515\n",
      "epoch: 5 step: 966, loss is 0.0007236902019940317\n",
      "epoch: 5 step: 967, loss is 0.1790870726108551\n",
      "epoch: 5 step: 968, loss is 0.018955465406179428\n",
      "epoch: 5 step: 969, loss is 0.001068101730197668\n",
      "epoch: 5 step: 970, loss is 0.3433782160282135\n",
      "epoch: 5 step: 971, loss is 0.011460548266768456\n",
      "epoch: 5 step: 972, loss is 0.1567794382572174\n",
      "epoch: 5 step: 973, loss is 0.00022618469665758312\n",
      "epoch: 5 step: 974, loss is 0.0036750854924321175\n",
      "epoch: 5 step: 975, loss is 0.0033641098998486996\n",
      "epoch: 5 step: 976, loss is 0.0292426235973835\n",
      "epoch: 5 step: 977, loss is 0.004313799552619457\n",
      "epoch: 5 step: 978, loss is 0.07971683889627457\n",
      "epoch: 5 step: 979, loss is 0.017554378136992455\n",
      "epoch: 5 step: 980, loss is 0.029264910146594048\n",
      "epoch: 5 step: 981, loss is 0.04094993695616722\n",
      "epoch: 5 step: 982, loss is 0.002249122131615877\n",
      "epoch: 5 step: 983, loss is 0.0586971789598465\n",
      "epoch: 5 step: 984, loss is 0.0886528417468071\n",
      "epoch: 5 step: 985, loss is 0.2236192375421524\n",
      "epoch: 5 step: 986, loss is 0.010320090688765049\n",
      "epoch: 5 step: 987, loss is 0.0012375878868624568\n",
      "epoch: 5 step: 988, loss is 0.013186738826334476\n",
      "epoch: 5 step: 989, loss is 0.013650406152009964\n",
      "epoch: 5 step: 990, loss is 0.011668018996715546\n",
      "epoch: 5 step: 991, loss is 0.002646382199600339\n",
      "epoch: 5 step: 992, loss is 0.0055871340446174145\n",
      "epoch: 5 step: 993, loss is 0.06569942086935043\n",
      "epoch: 5 step: 994, loss is 0.011274117976427078\n",
      "epoch: 5 step: 995, loss is 0.0037279915995895863\n",
      "epoch: 5 step: 996, loss is 0.009951479732990265\n",
      "epoch: 5 step: 997, loss is 0.014168203808367252\n",
      "epoch: 5 step: 998, loss is 0.000563893816433847\n",
      "epoch: 5 step: 999, loss is 0.006762533914297819\n",
      "epoch: 5 step: 1000, loss is 0.0035233483649790287\n",
      "epoch: 5 step: 1001, loss is 0.0003583365469239652\n",
      "epoch: 5 step: 1002, loss is 0.006884059868752956\n",
      "epoch: 5 step: 1003, loss is 0.1756865531206131\n",
      "epoch: 5 step: 1004, loss is 0.005746416747570038\n",
      "epoch: 5 step: 1005, loss is 0.00017858913633972406\n",
      "epoch: 5 step: 1006, loss is 0.015061954036355019\n",
      "epoch: 5 step: 1007, loss is 0.005189168266952038\n",
      "epoch: 5 step: 1008, loss is 0.006137075833976269\n",
      "epoch: 5 step: 1009, loss is 0.0018092355458065867\n",
      "epoch: 5 step: 1010, loss is 0.020634043961763382\n",
      "epoch: 5 step: 1011, loss is 0.0018850333290174603\n",
      "epoch: 5 step: 1012, loss is 0.00033066869946196675\n",
      "epoch: 5 step: 1013, loss is 0.04301626980304718\n",
      "epoch: 5 step: 1014, loss is 0.05956263095140457\n",
      "epoch: 5 step: 1015, loss is 0.007172912824898958\n",
      "epoch: 5 step: 1016, loss is 0.0018996380968019366\n",
      "epoch: 5 step: 1017, loss is 0.0008992526563815773\n",
      "epoch: 5 step: 1018, loss is 0.001519453595392406\n",
      "epoch: 5 step: 1019, loss is 0.016625870019197464\n",
      "epoch: 5 step: 1020, loss is 0.017525577917695045\n",
      "epoch: 5 step: 1021, loss is 0.005492941942065954\n",
      "epoch: 5 step: 1022, loss is 0.023453686386346817\n",
      "epoch: 5 step: 1023, loss is 0.0028482479974627495\n",
      "epoch: 5 step: 1024, loss is 0.11486388742923737\n",
      "epoch: 5 step: 1025, loss is 0.007611700799316168\n",
      "epoch: 5 step: 1026, loss is 0.0004067415138706565\n",
      "epoch: 5 step: 1027, loss is 0.00011846900451928377\n",
      "epoch: 5 step: 1028, loss is 0.004486457910388708\n",
      "epoch: 5 step: 1029, loss is 0.031633954495191574\n",
      "epoch: 5 step: 1030, loss is 0.011217674240469933\n",
      "epoch: 5 step: 1031, loss is 0.0360223650932312\n",
      "epoch: 5 step: 1032, loss is 0.0008568220655433834\n",
      "epoch: 5 step: 1033, loss is 0.0009692282765172422\n",
      "epoch: 5 step: 1034, loss is 0.00044303384493105114\n",
      "epoch: 5 step: 1035, loss is 0.0021704125683754683\n",
      "epoch: 5 step: 1036, loss is 0.0002165420155506581\n",
      "epoch: 5 step: 1037, loss is 0.0005916177178733051\n",
      "epoch: 5 step: 1038, loss is 0.003046862781047821\n",
      "epoch: 5 step: 1039, loss is 6.190435669850558e-05\n",
      "epoch: 5 step: 1040, loss is 0.00023591227363795042\n",
      "epoch: 5 step: 1041, loss is 0.0021880383137613535\n",
      "epoch: 5 step: 1042, loss is 0.0013284996384754777\n",
      "epoch: 5 step: 1043, loss is 0.001568357227370143\n",
      "epoch: 5 step: 1044, loss is 0.0036382991820573807\n",
      "epoch: 5 step: 1045, loss is 0.043557822704315186\n",
      "epoch: 5 step: 1046, loss is 0.0009579514153301716\n",
      "epoch: 5 step: 1047, loss is 0.007855549454689026\n",
      "epoch: 5 step: 1048, loss is 0.0007402449264191091\n",
      "epoch: 5 step: 1049, loss is 0.00398567970842123\n",
      "epoch: 5 step: 1050, loss is 0.0024810954928398132\n",
      "epoch: 5 step: 1051, loss is 0.0027974213007837534\n",
      "epoch: 5 step: 1052, loss is 0.007427721284329891\n",
      "epoch: 5 step: 1053, loss is 0.0016858732560649514\n",
      "epoch: 5 step: 1054, loss is 0.002366118598729372\n",
      "epoch: 5 step: 1055, loss is 0.0012118830345571041\n",
      "epoch: 5 step: 1056, loss is 0.04299280047416687\n",
      "epoch: 5 step: 1057, loss is 0.06266023218631744\n",
      "epoch: 5 step: 1058, loss is 0.003930989187210798\n",
      "epoch: 5 step: 1059, loss is 0.003783050226047635\n",
      "epoch: 5 step: 1060, loss is 0.005211861804127693\n",
      "epoch: 5 step: 1061, loss is 0.0014547660248354077\n",
      "epoch: 5 step: 1062, loss is 0.000401138182496652\n",
      "epoch: 5 step: 1063, loss is 0.03649346902966499\n",
      "epoch: 5 step: 1064, loss is 0.14559562504291534\n",
      "epoch: 5 step: 1065, loss is 0.010849532671272755\n",
      "epoch: 5 step: 1066, loss is 0.0033241943456232548\n",
      "epoch: 5 step: 1067, loss is 0.004976185038685799\n",
      "epoch: 5 step: 1068, loss is 0.0005872367764823139\n",
      "epoch: 5 step: 1069, loss is 0.0009221884538419545\n",
      "epoch: 5 step: 1070, loss is 0.000276527222013101\n",
      "epoch: 5 step: 1071, loss is 0.004531580489128828\n",
      "epoch: 5 step: 1072, loss is 0.00446136761456728\n",
      "epoch: 5 step: 1073, loss is 0.003320479765534401\n",
      "epoch: 5 step: 1074, loss is 0.01659933477640152\n",
      "epoch: 5 step: 1075, loss is 0.00322612002491951\n",
      "epoch: 5 step: 1076, loss is 0.125445157289505\n",
      "epoch: 5 step: 1077, loss is 0.00777626084163785\n",
      "epoch: 5 step: 1078, loss is 0.0025124014355242252\n",
      "epoch: 5 step: 1079, loss is 0.0021175716537982225\n",
      "epoch: 5 step: 1080, loss is 0.0003079791204072535\n",
      "epoch: 5 step: 1081, loss is 0.03582429513335228\n",
      "epoch: 5 step: 1082, loss is 0.0009231450967490673\n",
      "epoch: 5 step: 1083, loss is 0.00022455176804214716\n",
      "epoch: 5 step: 1084, loss is 0.19647417962551117\n",
      "epoch: 5 step: 1085, loss is 0.03947143629193306\n",
      "epoch: 5 step: 1086, loss is 0.0034729575272649527\n",
      "epoch: 5 step: 1087, loss is 0.00046852725790813565\n",
      "epoch: 5 step: 1088, loss is 0.054660141468048096\n",
      "epoch: 5 step: 1089, loss is 0.003577287308871746\n",
      "epoch: 5 step: 1090, loss is 0.20592175424098969\n",
      "epoch: 5 step: 1091, loss is 0.013102874159812927\n",
      "epoch: 5 step: 1092, loss is 0.0007742144516669214\n",
      "epoch: 5 step: 1093, loss is 0.004628150723874569\n",
      "epoch: 5 step: 1094, loss is 0.11513489484786987\n",
      "epoch: 5 step: 1095, loss is 0.0006520128226839006\n",
      "epoch: 5 step: 1096, loss is 0.11950374394655228\n",
      "epoch: 5 step: 1097, loss is 0.06110119819641113\n",
      "epoch: 5 step: 1098, loss is 0.017125168815255165\n",
      "epoch: 5 step: 1099, loss is 0.021264944225549698\n",
      "epoch: 5 step: 1100, loss is 0.13824522495269775\n",
      "epoch: 5 step: 1101, loss is 0.007716895081102848\n",
      "epoch: 5 step: 1102, loss is 0.006714316550642252\n",
      "epoch: 5 step: 1103, loss is 0.004274242091923952\n",
      "epoch: 5 step: 1104, loss is 0.00034638092620298266\n",
      "epoch: 5 step: 1105, loss is 0.2818916141986847\n",
      "epoch: 5 step: 1106, loss is 0.022185824811458588\n",
      "epoch: 5 step: 1107, loss is 0.000247572228545323\n",
      "epoch: 5 step: 1108, loss is 0.012661291286349297\n",
      "epoch: 5 step: 1109, loss is 0.07827641814947128\n",
      "epoch: 5 step: 1110, loss is 0.06335841119289398\n",
      "epoch: 5 step: 1111, loss is 0.014163035899400711\n",
      "epoch: 5 step: 1112, loss is 0.0002251910191262141\n",
      "epoch: 5 step: 1113, loss is 0.0007272581569850445\n",
      "epoch: 5 step: 1114, loss is 0.1006527692079544\n",
      "epoch: 5 step: 1115, loss is 0.011687922291457653\n",
      "epoch: 5 step: 1116, loss is 0.00015052914386615157\n",
      "epoch: 5 step: 1117, loss is 0.0024615658912807703\n",
      "epoch: 5 step: 1118, loss is 0.0012894970132037997\n",
      "epoch: 5 step: 1119, loss is 0.09302981942892075\n",
      "epoch: 5 step: 1120, loss is 0.004046780057251453\n",
      "epoch: 5 step: 1121, loss is 0.0793718472123146\n",
      "epoch: 5 step: 1122, loss is 0.13315241038799286\n",
      "epoch: 5 step: 1123, loss is 0.0008374826284125447\n",
      "epoch: 5 step: 1124, loss is 0.016500843688845634\n",
      "epoch: 5 step: 1125, loss is 0.0015336595242843032\n",
      "epoch: 5 step: 1126, loss is 9.241253428626806e-05\n",
      "epoch: 5 step: 1127, loss is 0.01550475973635912\n",
      "epoch: 5 step: 1128, loss is 0.014141269959509373\n",
      "epoch: 5 step: 1129, loss is 0.04663343355059624\n",
      "epoch: 5 step: 1130, loss is 0.03824067488312721\n",
      "epoch: 5 step: 1131, loss is 0.051130179315805435\n",
      "epoch: 5 step: 1132, loss is 0.013615591451525688\n",
      "epoch: 5 step: 1133, loss is 0.0012803502613678575\n",
      "epoch: 5 step: 1134, loss is 0.06692010164260864\n",
      "epoch: 5 step: 1135, loss is 0.06148655340075493\n",
      "epoch: 5 step: 1136, loss is 0.0003666762786451727\n",
      "epoch: 5 step: 1137, loss is 0.0002792440354824066\n",
      "epoch: 5 step: 1138, loss is 0.09982926398515701\n",
      "epoch: 5 step: 1139, loss is 0.030425535514950752\n",
      "epoch: 5 step: 1140, loss is 0.00030296226032078266\n",
      "epoch: 5 step: 1141, loss is 0.0024703482631593943\n",
      "epoch: 5 step: 1142, loss is 0.002871704287827015\n",
      "epoch: 5 step: 1143, loss is 0.0010159163502976298\n",
      "epoch: 5 step: 1144, loss is 0.010537720285356045\n",
      "epoch: 5 step: 1145, loss is 0.0007541209342889488\n",
      "epoch: 5 step: 1146, loss is 0.013945145532488823\n",
      "epoch: 5 step: 1147, loss is 0.0038258912973105907\n",
      "epoch: 5 step: 1148, loss is 0.17542223632335663\n",
      "epoch: 5 step: 1149, loss is 0.006377329584211111\n",
      "epoch: 5 step: 1150, loss is 0.13093416392803192\n",
      "epoch: 5 step: 1151, loss is 0.08876967430114746\n",
      "epoch: 5 step: 1152, loss is 0.038897592574357986\n",
      "epoch: 5 step: 1153, loss is 0.006458232179284096\n",
      "epoch: 5 step: 1154, loss is 0.0008908850140869617\n",
      "epoch: 5 step: 1155, loss is 0.0013117626076564193\n",
      "epoch: 5 step: 1156, loss is 0.00021406932501122355\n",
      "epoch: 5 step: 1157, loss is 0.09036104381084442\n",
      "epoch: 5 step: 1158, loss is 0.1332981288433075\n",
      "epoch: 5 step: 1159, loss is 0.00608565378934145\n",
      "epoch: 5 step: 1160, loss is 0.0006855693645775318\n",
      "epoch: 5 step: 1161, loss is 0.06819901615381241\n",
      "epoch: 5 step: 1162, loss is 0.020905636250972748\n",
      "epoch: 5 step: 1163, loss is 0.05488259345293045\n",
      "epoch: 5 step: 1164, loss is 0.0007140421657823026\n",
      "epoch: 5 step: 1165, loss is 0.011899800039827824\n",
      "epoch: 5 step: 1166, loss is 0.07195344567298889\n",
      "epoch: 5 step: 1167, loss is 0.03167475014925003\n",
      "epoch: 5 step: 1168, loss is 0.01074717566370964\n",
      "epoch: 5 step: 1169, loss is 0.0016842008335515857\n",
      "epoch: 5 step: 1170, loss is 0.036124337464571\n",
      "epoch: 5 step: 1171, loss is 0.07140732556581497\n",
      "epoch: 5 step: 1172, loss is 0.02387108840048313\n",
      "epoch: 5 step: 1173, loss is 0.0061675067991018295\n",
      "epoch: 5 step: 1174, loss is 0.005486452020704746\n",
      "epoch: 5 step: 1175, loss is 0.17356231808662415\n",
      "epoch: 5 step: 1176, loss is 0.1429782211780548\n",
      "epoch: 5 step: 1177, loss is 0.0017696797149255872\n",
      "epoch: 5 step: 1178, loss is 0.05855853855609894\n",
      "epoch: 5 step: 1179, loss is 0.0002568680210970342\n",
      "epoch: 5 step: 1180, loss is 0.00048039661487564445\n",
      "epoch: 5 step: 1181, loss is 0.0022812301758676767\n",
      "epoch: 5 step: 1182, loss is 0.0013370676897466183\n",
      "epoch: 5 step: 1183, loss is 0.07122541964054108\n",
      "epoch: 5 step: 1184, loss is 0.034213948994874954\n",
      "epoch: 5 step: 1185, loss is 0.05459202080965042\n",
      "epoch: 5 step: 1186, loss is 0.016151905059814453\n",
      "epoch: 5 step: 1187, loss is 0.0414884053170681\n",
      "epoch: 5 step: 1188, loss is 0.009507541544735432\n",
      "epoch: 5 step: 1189, loss is 0.0008248783997260034\n",
      "epoch: 5 step: 1190, loss is 0.007248620968312025\n",
      "epoch: 5 step: 1191, loss is 0.0018986029317602515\n",
      "epoch: 5 step: 1192, loss is 0.021191999316215515\n",
      "epoch: 5 step: 1193, loss is 0.021894728764891624\n",
      "epoch: 5 step: 1194, loss is 0.006842650938779116\n",
      "epoch: 5 step: 1195, loss is 0.07990800589323044\n",
      "epoch: 5 step: 1196, loss is 0.06530875712633133\n",
      "epoch: 5 step: 1197, loss is 0.02452651783823967\n",
      "epoch: 5 step: 1198, loss is 0.07058387249708176\n",
      "epoch: 5 step: 1199, loss is 0.004328099545091391\n",
      "epoch: 5 step: 1200, loss is 0.04164520651102066\n",
      "epoch: 5 step: 1201, loss is 0.013112079352140427\n",
      "epoch: 5 step: 1202, loss is 0.052284564822912216\n",
      "epoch: 5 step: 1203, loss is 0.00029074843041598797\n",
      "epoch: 5 step: 1204, loss is 0.030302878469228745\n",
      "epoch: 5 step: 1205, loss is 0.05223626270890236\n",
      "epoch: 5 step: 1206, loss is 0.0016928957775235176\n",
      "epoch: 5 step: 1207, loss is 0.0018333618063479662\n",
      "epoch: 5 step: 1208, loss is 0.0022157710045576096\n",
      "epoch: 5 step: 1209, loss is 0.012997886165976524\n",
      "epoch: 5 step: 1210, loss is 0.0010005704825744033\n",
      "epoch: 5 step: 1211, loss is 0.04028327390551567\n",
      "epoch: 5 step: 1212, loss is 0.00673341378569603\n",
      "epoch: 5 step: 1213, loss is 0.03653270751237869\n",
      "epoch: 5 step: 1214, loss is 0.0075780912302434444\n",
      "epoch: 5 step: 1215, loss is 0.0004348042421042919\n",
      "epoch: 5 step: 1216, loss is 0.002936330158263445\n",
      "epoch: 5 step: 1217, loss is 0.03101622872054577\n",
      "epoch: 5 step: 1218, loss is 0.12166957557201385\n",
      "epoch: 5 step: 1219, loss is 0.0002106255415128544\n",
      "epoch: 5 step: 1220, loss is 0.0018801905680447817\n",
      "epoch: 5 step: 1221, loss is 0.0014433348551392555\n",
      "epoch: 5 step: 1222, loss is 0.0008666987996548414\n",
      "epoch: 5 step: 1223, loss is 0.006662661209702492\n",
      "epoch: 5 step: 1224, loss is 0.011768776923418045\n",
      "epoch: 5 step: 1225, loss is 0.009812532924115658\n",
      "epoch: 5 step: 1226, loss is 0.03163425996899605\n",
      "epoch: 5 step: 1227, loss is 0.012890071608126163\n",
      "epoch: 5 step: 1228, loss is 0.0001685746683506295\n",
      "epoch: 5 step: 1229, loss is 0.0015706050908192992\n",
      "epoch: 5 step: 1230, loss is 0.0022664847783744335\n",
      "epoch: 5 step: 1231, loss is 0.09517476707696915\n",
      "epoch: 5 step: 1232, loss is 0.01970677450299263\n",
      "epoch: 5 step: 1233, loss is 0.02091004140675068\n",
      "epoch: 5 step: 1234, loss is 0.020298918709158897\n",
      "epoch: 5 step: 1235, loss is 0.0022825701162219048\n",
      "epoch: 5 step: 1236, loss is 0.022922875359654427\n",
      "epoch: 5 step: 1237, loss is 0.0002063922001980245\n",
      "epoch: 5 step: 1238, loss is 0.009260071441531181\n",
      "epoch: 5 step: 1239, loss is 0.0024443138390779495\n",
      "epoch: 5 step: 1240, loss is 0.002476250985637307\n",
      "epoch: 5 step: 1241, loss is 0.0006949258968234062\n",
      "epoch: 5 step: 1242, loss is 0.0003976109728682786\n",
      "epoch: 5 step: 1243, loss is 0.0003696094499900937\n",
      "epoch: 5 step: 1244, loss is 0.01571877859532833\n",
      "epoch: 5 step: 1245, loss is 0.08410932868719101\n",
      "epoch: 5 step: 1246, loss is 0.00023671676171943545\n",
      "epoch: 5 step: 1247, loss is 0.03916482999920845\n",
      "epoch: 5 step: 1248, loss is 0.4482145309448242\n",
      "epoch: 5 step: 1249, loss is 0.0031837373971939087\n",
      "epoch: 5 step: 1250, loss is 0.0051642172038555145\n",
      "epoch: 5 step: 1251, loss is 0.0013039493933320045\n",
      "epoch: 5 step: 1252, loss is 0.0817127600312233\n",
      "epoch: 5 step: 1253, loss is 0.003868607571348548\n",
      "epoch: 5 step: 1254, loss is 0.0003598436596803367\n",
      "epoch: 5 step: 1255, loss is 0.0037533417344093323\n",
      "epoch: 5 step: 1256, loss is 0.006815626285970211\n",
      "epoch: 5 step: 1257, loss is 0.09412435442209244\n",
      "epoch: 5 step: 1258, loss is 0.0016636435175314546\n",
      "epoch: 5 step: 1259, loss is 0.10874443501234055\n",
      "epoch: 5 step: 1260, loss is 0.0033728647977113724\n",
      "epoch: 5 step: 1261, loss is 0.0011114689987152815\n",
      "epoch: 5 step: 1262, loss is 0.0001809836394386366\n",
      "epoch: 5 step: 1263, loss is 0.04892939701676369\n",
      "epoch: 5 step: 1264, loss is 0.02216838300228119\n",
      "epoch: 5 step: 1265, loss is 0.406133770942688\n",
      "epoch: 5 step: 1266, loss is 0.018605541437864304\n",
      "epoch: 5 step: 1267, loss is 0.0397605374455452\n",
      "epoch: 5 step: 1268, loss is 0.08390818536281586\n",
      "epoch: 5 step: 1269, loss is 0.08565297722816467\n",
      "epoch: 5 step: 1270, loss is 0.38750308752059937\n",
      "epoch: 5 step: 1271, loss is 0.002403530990704894\n",
      "epoch: 5 step: 1272, loss is 0.00738014979287982\n",
      "epoch: 5 step: 1273, loss is 0.03302393853664398\n",
      "epoch: 5 step: 1274, loss is 0.008532540872693062\n",
      "epoch: 5 step: 1275, loss is 0.01281034667044878\n",
      "epoch: 5 step: 1276, loss is 0.005859956610947847\n",
      "epoch: 5 step: 1277, loss is 0.029996300116181374\n",
      "epoch: 5 step: 1278, loss is 0.1492995023727417\n",
      "epoch: 5 step: 1279, loss is 0.0021576399449259043\n",
      "epoch: 5 step: 1280, loss is 0.04362063482403755\n",
      "epoch: 5 step: 1281, loss is 0.005593110341578722\n",
      "epoch: 5 step: 1282, loss is 0.020032888278365135\n",
      "epoch: 5 step: 1283, loss is 0.004739530850201845\n",
      "epoch: 5 step: 1284, loss is 0.023581339046359062\n",
      "epoch: 5 step: 1285, loss is 0.0066074710339307785\n",
      "epoch: 5 step: 1286, loss is 0.02907404862344265\n",
      "epoch: 5 step: 1287, loss is 0.06623239815235138\n",
      "epoch: 5 step: 1288, loss is 0.010743454098701477\n",
      "epoch: 5 step: 1289, loss is 0.3290635943412781\n",
      "epoch: 5 step: 1290, loss is 0.0014551891945302486\n",
      "epoch: 5 step: 1291, loss is 0.0025305040180683136\n",
      "epoch: 5 step: 1292, loss is 0.04720878601074219\n",
      "epoch: 5 step: 1293, loss is 0.009877546690404415\n",
      "epoch: 5 step: 1294, loss is 0.21675866842269897\n",
      "epoch: 5 step: 1295, loss is 0.0014943369897082448\n",
      "epoch: 5 step: 1296, loss is 0.0003961475449614227\n",
      "epoch: 5 step: 1297, loss is 0.022434022277593613\n",
      "epoch: 5 step: 1298, loss is 0.011716154403984547\n",
      "epoch: 5 step: 1299, loss is 0.08467423170804977\n",
      "epoch: 5 step: 1300, loss is 0.16969531774520874\n",
      "epoch: 5 step: 1301, loss is 0.0711132287979126\n",
      "epoch: 5 step: 1302, loss is 0.0027787110302597284\n",
      "epoch: 5 step: 1303, loss is 0.0022369541693478823\n",
      "epoch: 5 step: 1304, loss is 0.023631464689970016\n",
      "epoch: 5 step: 1305, loss is 0.003331588115543127\n",
      "epoch: 5 step: 1306, loss is 0.07565022259950638\n",
      "epoch: 5 step: 1307, loss is 0.0030064641032367945\n",
      "epoch: 5 step: 1308, loss is 0.0020939779933542013\n",
      "epoch: 5 step: 1309, loss is 0.0012511088280007243\n",
      "epoch: 5 step: 1310, loss is 0.0010890290141105652\n",
      "epoch: 5 step: 1311, loss is 0.0027668860275298357\n",
      "epoch: 5 step: 1312, loss is 0.0029301494359970093\n",
      "epoch: 5 step: 1313, loss is 0.1757279634475708\n",
      "epoch: 5 step: 1314, loss is 0.0006118893506936729\n",
      "epoch: 5 step: 1315, loss is 0.023617275059223175\n",
      "epoch: 5 step: 1316, loss is 0.004677090793848038\n",
      "epoch: 5 step: 1317, loss is 0.0020010885782539845\n",
      "epoch: 5 step: 1318, loss is 0.0008906510192900896\n",
      "epoch: 5 step: 1319, loss is 0.006643304601311684\n",
      "epoch: 5 step: 1320, loss is 0.004782532341778278\n",
      "epoch: 5 step: 1321, loss is 0.09364061057567596\n",
      "epoch: 5 step: 1322, loss is 0.004322286229580641\n",
      "epoch: 5 step: 1323, loss is 0.01698925718665123\n",
      "epoch: 5 step: 1324, loss is 0.0007715414394624531\n",
      "epoch: 5 step: 1325, loss is 0.007849614135921001\n",
      "epoch: 5 step: 1326, loss is 0.06715139001607895\n",
      "epoch: 5 step: 1327, loss is 0.034927885979413986\n",
      "epoch: 5 step: 1328, loss is 0.009663938544690609\n",
      "epoch: 5 step: 1329, loss is 0.019705653190612793\n",
      "epoch: 5 step: 1330, loss is 0.006648709066212177\n",
      "epoch: 5 step: 1331, loss is 0.015324970707297325\n",
      "epoch: 5 step: 1332, loss is 0.003147087525576353\n",
      "epoch: 5 step: 1333, loss is 0.002963837468996644\n",
      "epoch: 5 step: 1334, loss is 0.0009527248330414295\n",
      "epoch: 5 step: 1335, loss is 0.004689019639045\n",
      "epoch: 5 step: 1336, loss is 0.1222672089934349\n",
      "epoch: 5 step: 1337, loss is 0.030792338773608208\n",
      "epoch: 5 step: 1338, loss is 0.01082559023052454\n",
      "epoch: 5 step: 1339, loss is 0.07232540845870972\n",
      "epoch: 5 step: 1340, loss is 0.03862510994076729\n",
      "epoch: 5 step: 1341, loss is 0.0020898624788969755\n",
      "epoch: 5 step: 1342, loss is 0.004727459046989679\n",
      "epoch: 5 step: 1343, loss is 0.011343022808432579\n",
      "epoch: 5 step: 1344, loss is 0.008454753085970879\n",
      "epoch: 5 step: 1345, loss is 0.009704937227070332\n",
      "epoch: 5 step: 1346, loss is 0.0005046101287007332\n",
      "epoch: 5 step: 1347, loss is 0.0313645638525486\n",
      "epoch: 5 step: 1348, loss is 0.005205905064940453\n",
      "epoch: 5 step: 1349, loss is 0.000687559018842876\n",
      "epoch: 5 step: 1350, loss is 0.0808236375451088\n",
      "epoch: 5 step: 1351, loss is 0.0013421777402982116\n",
      "epoch: 5 step: 1352, loss is 0.04657401144504547\n",
      "epoch: 5 step: 1353, loss is 0.005801911000162363\n",
      "epoch: 5 step: 1354, loss is 0.002286395989358425\n",
      "epoch: 5 step: 1355, loss is 0.0003109496901743114\n",
      "epoch: 5 step: 1356, loss is 0.16528823971748352\n",
      "epoch: 5 step: 1357, loss is 0.03136450797319412\n",
      "epoch: 5 step: 1358, loss is 0.07821130752563477\n",
      "epoch: 5 step: 1359, loss is 0.00017740338807925582\n",
      "epoch: 5 step: 1360, loss is 0.0525498129427433\n",
      "epoch: 5 step: 1361, loss is 0.0036757446359843016\n",
      "epoch: 5 step: 1362, loss is 0.0015160944312810898\n",
      "epoch: 5 step: 1363, loss is 0.0316290557384491\n",
      "epoch: 5 step: 1364, loss is 0.0063708871603012085\n",
      "epoch: 5 step: 1365, loss is 0.001693548634648323\n",
      "epoch: 5 step: 1366, loss is 0.00947551429271698\n",
      "epoch: 5 step: 1367, loss is 0.0035520188976079226\n",
      "epoch: 5 step: 1368, loss is 0.001796269090846181\n",
      "epoch: 5 step: 1369, loss is 0.003625174518674612\n",
      "epoch: 5 step: 1370, loss is 0.007303789723664522\n",
      "epoch: 5 step: 1371, loss is 0.002790263621136546\n",
      "epoch: 5 step: 1372, loss is 0.006789542268961668\n",
      "epoch: 5 step: 1373, loss is 0.0002958553668577224\n",
      "epoch: 5 step: 1374, loss is 0.006986264605075121\n",
      "epoch: 5 step: 1375, loss is 0.019409891217947006\n",
      "epoch: 5 step: 1376, loss is 0.0005895709618926048\n",
      "epoch: 5 step: 1377, loss is 0.00019629154121503234\n",
      "epoch: 5 step: 1378, loss is 0.0097251171246171\n",
      "epoch: 5 step: 1379, loss is 0.0007923122611828148\n",
      "epoch: 5 step: 1380, loss is 0.0017848038114607334\n",
      "epoch: 5 step: 1381, loss is 0.0001935078762471676\n",
      "epoch: 5 step: 1382, loss is 0.03199681267142296\n",
      "epoch: 5 step: 1383, loss is 0.0040001459419727325\n",
      "epoch: 5 step: 1384, loss is 0.0003129260439891368\n",
      "epoch: 5 step: 1385, loss is 0.001971873687580228\n",
      "epoch: 5 step: 1386, loss is 0.000746839796192944\n",
      "epoch: 5 step: 1387, loss is 0.0024650890845805407\n",
      "epoch: 5 step: 1388, loss is 0.009289199486374855\n",
      "epoch: 5 step: 1389, loss is 0.0027514328248798847\n",
      "epoch: 5 step: 1390, loss is 0.002243156312033534\n",
      "epoch: 5 step: 1391, loss is 0.0014542910503223538\n",
      "epoch: 5 step: 1392, loss is 0.005780383013188839\n",
      "epoch: 5 step: 1393, loss is 0.001530015142634511\n",
      "epoch: 5 step: 1394, loss is 0.023847369477152824\n",
      "epoch: 5 step: 1395, loss is 0.026158170774579048\n",
      "epoch: 5 step: 1396, loss is 0.11678023636341095\n",
      "epoch: 5 step: 1397, loss is 0.14453120529651642\n",
      "epoch: 5 step: 1398, loss is 0.16404010355472565\n",
      "epoch: 5 step: 1399, loss is 0.11010398715734482\n",
      "epoch: 5 step: 1400, loss is 0.00018004793673753738\n",
      "epoch: 5 step: 1401, loss is 0.020542560145258904\n",
      "epoch: 5 step: 1402, loss is 0.00022777175763621926\n",
      "epoch: 5 step: 1403, loss is 0.17982740700244904\n",
      "epoch: 5 step: 1404, loss is 0.008014705032110214\n",
      "epoch: 5 step: 1405, loss is 0.0021608443930745125\n",
      "epoch: 5 step: 1406, loss is 0.001986265415325761\n",
      "epoch: 5 step: 1407, loss is 0.05006352439522743\n",
      "epoch: 5 step: 1408, loss is 0.0002968988846987486\n",
      "epoch: 5 step: 1409, loss is 0.10175178945064545\n",
      "epoch: 5 step: 1410, loss is 0.0006993371061980724\n",
      "epoch: 5 step: 1411, loss is 0.09120215475559235\n",
      "epoch: 5 step: 1412, loss is 0.000906158413272351\n",
      "epoch: 5 step: 1413, loss is 0.0001960091176442802\n",
      "epoch: 5 step: 1414, loss is 0.06933598220348358\n",
      "epoch: 5 step: 1415, loss is 0.01173713244497776\n",
      "epoch: 5 step: 1416, loss is 0.060081254690885544\n",
      "epoch: 5 step: 1417, loss is 0.0023284917697310448\n",
      "epoch: 5 step: 1418, loss is 0.027620738372206688\n",
      "epoch: 5 step: 1419, loss is 0.028436269611120224\n",
      "epoch: 5 step: 1420, loss is 0.0011451218742877245\n",
      "epoch: 5 step: 1421, loss is 0.12432403862476349\n",
      "epoch: 5 step: 1422, loss is 0.19349312782287598\n",
      "epoch: 5 step: 1423, loss is 0.09934064000844955\n",
      "epoch: 5 step: 1424, loss is 0.04658902809023857\n",
      "epoch: 5 step: 1425, loss is 0.025809403508901596\n",
      "epoch: 5 step: 1426, loss is 0.005868878215551376\n",
      "epoch: 5 step: 1427, loss is 0.022029371932148933\n",
      "epoch: 5 step: 1428, loss is 0.011981931515038013\n",
      "epoch: 5 step: 1429, loss is 0.009438639506697655\n",
      "epoch: 5 step: 1430, loss is 0.002123480197042227\n",
      "epoch: 5 step: 1431, loss is 0.1202293410897255\n",
      "epoch: 5 step: 1432, loss is 0.14236174523830414\n",
      "epoch: 5 step: 1433, loss is 0.0012405886081978679\n",
      "epoch: 5 step: 1434, loss is 0.023079613223671913\n",
      "epoch: 5 step: 1435, loss is 0.0026074235793203115\n",
      "epoch: 5 step: 1436, loss is 0.0008955849334597588\n",
      "epoch: 5 step: 1437, loss is 0.16479474306106567\n",
      "epoch: 5 step: 1438, loss is 0.015402046963572502\n",
      "epoch: 5 step: 1439, loss is 0.013961984775960445\n",
      "epoch: 5 step: 1440, loss is 0.0014316485030576587\n",
      "epoch: 5 step: 1441, loss is 0.040885839611291885\n",
      "epoch: 5 step: 1442, loss is 0.03697170689702034\n",
      "epoch: 5 step: 1443, loss is 0.012539567425847054\n",
      "epoch: 5 step: 1444, loss is 0.03007371351122856\n",
      "epoch: 5 step: 1445, loss is 0.0352977029979229\n",
      "epoch: 5 step: 1446, loss is 0.00598814757540822\n",
      "epoch: 5 step: 1447, loss is 0.10720132291316986\n",
      "epoch: 5 step: 1448, loss is 0.03014962747693062\n",
      "epoch: 5 step: 1449, loss is 0.09979942440986633\n",
      "epoch: 5 step: 1450, loss is 0.018254950642585754\n",
      "epoch: 5 step: 1451, loss is 0.0022863331250846386\n",
      "epoch: 5 step: 1452, loss is 0.12761808931827545\n",
      "epoch: 5 step: 1453, loss is 0.02465759590268135\n",
      "epoch: 5 step: 1454, loss is 0.001078076078556478\n",
      "epoch: 5 step: 1455, loss is 0.06924337893724442\n",
      "epoch: 5 step: 1456, loss is 0.0017426044214516878\n",
      "epoch: 5 step: 1457, loss is 0.0016329593490809202\n",
      "epoch: 5 step: 1458, loss is 0.0014632577076554298\n",
      "epoch: 5 step: 1459, loss is 0.0704464539885521\n",
      "epoch: 5 step: 1460, loss is 0.0038592831697314978\n",
      "epoch: 5 step: 1461, loss is 0.014494155533611774\n",
      "epoch: 5 step: 1462, loss is 0.002534841885790229\n",
      "epoch: 5 step: 1463, loss is 0.10788141936063766\n",
      "epoch: 5 step: 1464, loss is 0.028151486068964005\n",
      "epoch: 5 step: 1465, loss is 0.035125140100717545\n",
      "epoch: 5 step: 1466, loss is 0.11758790910243988\n",
      "epoch: 5 step: 1467, loss is 0.144435316324234\n",
      "epoch: 5 step: 1468, loss is 0.0024017435498535633\n",
      "epoch: 5 step: 1469, loss is 0.004314785823225975\n",
      "epoch: 5 step: 1470, loss is 0.0005704555078409612\n",
      "epoch: 5 step: 1471, loss is 0.022941650822758675\n",
      "epoch: 5 step: 1472, loss is 0.003180763917043805\n",
      "epoch: 5 step: 1473, loss is 0.0037928856909275055\n",
      "epoch: 5 step: 1474, loss is 0.15662206709384918\n",
      "epoch: 5 step: 1475, loss is 0.001315841800533235\n",
      "epoch: 5 step: 1476, loss is 0.0014113967772573233\n",
      "epoch: 5 step: 1477, loss is 0.06785064935684204\n",
      "epoch: 5 step: 1478, loss is 0.00046234490582719445\n",
      "epoch: 5 step: 1479, loss is 0.01595199480652809\n",
      "epoch: 5 step: 1480, loss is 0.0008464945713058114\n",
      "epoch: 5 step: 1481, loss is 0.0009785305010154843\n",
      "epoch: 5 step: 1482, loss is 0.0011940052499994636\n",
      "epoch: 5 step: 1483, loss is 0.017969492822885513\n",
      "epoch: 5 step: 1484, loss is 0.0008974958909675479\n",
      "epoch: 5 step: 1485, loss is 0.01821153052151203\n",
      "epoch: 5 step: 1486, loss is 0.019934052601456642\n",
      "epoch: 5 step: 1487, loss is 0.17981697618961334\n",
      "epoch: 5 step: 1488, loss is 0.0019300539279356599\n",
      "epoch: 5 step: 1489, loss is 0.013583865016698837\n",
      "epoch: 5 step: 1490, loss is 0.0006643489468842745\n",
      "epoch: 5 step: 1491, loss is 0.006968917790800333\n",
      "epoch: 5 step: 1492, loss is 0.0027578421868383884\n",
      "epoch: 5 step: 1493, loss is 0.05749300867319107\n",
      "epoch: 5 step: 1494, loss is 0.00014938367530703545\n",
      "epoch: 5 step: 1495, loss is 0.001013891538605094\n",
      "epoch: 5 step: 1496, loss is 0.0050394474528729916\n",
      "epoch: 5 step: 1497, loss is 0.32788175344467163\n",
      "epoch: 5 step: 1498, loss is 0.1614082306623459\n",
      "epoch: 5 step: 1499, loss is 0.005180071108043194\n",
      "epoch: 5 step: 1500, loss is 0.007293070666491985\n",
      "epoch: 5 step: 1501, loss is 0.0023061519023030996\n",
      "epoch: 5 step: 1502, loss is 0.003273953916504979\n",
      "epoch: 5 step: 1503, loss is 0.002829025499522686\n",
      "epoch: 5 step: 1504, loss is 0.0012580521870404482\n",
      "epoch: 5 step: 1505, loss is 0.002753936918452382\n",
      "epoch: 5 step: 1506, loss is 0.020669925957918167\n",
      "epoch: 5 step: 1507, loss is 0.017257388681173325\n",
      "epoch: 5 step: 1508, loss is 0.3577041029930115\n",
      "epoch: 5 step: 1509, loss is 0.040794678032398224\n",
      "epoch: 5 step: 1510, loss is 0.32013407349586487\n",
      "epoch: 5 step: 1511, loss is 0.0008335517486557364\n",
      "epoch: 5 step: 1512, loss is 0.010803792625665665\n",
      "epoch: 5 step: 1513, loss is 0.013888020068407059\n",
      "epoch: 5 step: 1514, loss is 0.1964360475540161\n",
      "epoch: 5 step: 1515, loss is 0.02527264505624771\n",
      "epoch: 5 step: 1516, loss is 0.005864120554178953\n",
      "epoch: 5 step: 1517, loss is 0.006880759261548519\n",
      "epoch: 5 step: 1518, loss is 0.002226336160674691\n",
      "epoch: 5 step: 1519, loss is 0.00417284993454814\n",
      "epoch: 5 step: 1520, loss is 0.018896568566560745\n",
      "epoch: 5 step: 1521, loss is 0.0038858288899064064\n",
      "epoch: 5 step: 1522, loss is 0.03395593538880348\n",
      "epoch: 5 step: 1523, loss is 0.06180277839303017\n",
      "epoch: 5 step: 1524, loss is 0.07304377853870392\n",
      "epoch: 5 step: 1525, loss is 0.004551511723548174\n",
      "epoch: 5 step: 1526, loss is 0.005316169932484627\n",
      "epoch: 5 step: 1527, loss is 0.02165054716169834\n",
      "epoch: 5 step: 1528, loss is 0.0003611317661125213\n",
      "epoch: 5 step: 1529, loss is 0.03291389346122742\n",
      "epoch: 5 step: 1530, loss is 0.27297478914260864\n",
      "epoch: 5 step: 1531, loss is 0.003297236980870366\n",
      "epoch: 5 step: 1532, loss is 0.0010653947247192264\n",
      "epoch: 5 step: 1533, loss is 0.004167595878243446\n",
      "epoch: 5 step: 1534, loss is 0.007725215051323175\n",
      "epoch: 5 step: 1535, loss is 0.09082716703414917\n",
      "epoch: 5 step: 1536, loss is 0.02452068403363228\n",
      "epoch: 5 step: 1537, loss is 0.02004622481763363\n",
      "epoch: 5 step: 1538, loss is 0.001569263869896531\n",
      "epoch: 5 step: 1539, loss is 0.013830507174134254\n",
      "epoch: 5 step: 1540, loss is 0.14641624689102173\n",
      "epoch: 5 step: 1541, loss is 0.010793933644890785\n",
      "epoch: 5 step: 1542, loss is 0.01247716136276722\n",
      "epoch: 5 step: 1543, loss is 0.0071955714374780655\n",
      "epoch: 5 step: 1544, loss is 0.01791144348680973\n",
      "epoch: 5 step: 1545, loss is 0.0357496552169323\n",
      "epoch: 5 step: 1546, loss is 0.0005812917370349169\n",
      "epoch: 5 step: 1547, loss is 0.009355184622108936\n",
      "epoch: 5 step: 1548, loss is 0.0031326578464359045\n",
      "epoch: 5 step: 1549, loss is 0.0012026738841086626\n",
      "epoch: 5 step: 1550, loss is 0.0004920898354612291\n",
      "epoch: 5 step: 1551, loss is 0.08290344476699829\n",
      "epoch: 5 step: 1552, loss is 0.0025034400168806314\n",
      "epoch: 5 step: 1553, loss is 0.1224479228258133\n",
      "epoch: 5 step: 1554, loss is 0.13144773244857788\n",
      "epoch: 5 step: 1555, loss is 0.04914141818881035\n",
      "epoch: 5 step: 1556, loss is 0.0019123228266835213\n",
      "epoch: 5 step: 1557, loss is 0.0015210021520033479\n",
      "epoch: 5 step: 1558, loss is 0.01768176257610321\n",
      "epoch: 5 step: 1559, loss is 0.00023430604778695852\n",
      "epoch: 5 step: 1560, loss is 0.02865796536207199\n",
      "epoch: 5 step: 1561, loss is 0.16349638998508453\n",
      "epoch: 5 step: 1562, loss is 0.0004657462995965034\n",
      "epoch: 5 step: 1563, loss is 0.052786219865083694\n",
      "epoch: 5 step: 1564, loss is 0.15123069286346436\n",
      "epoch: 5 step: 1565, loss is 0.2354772537946701\n",
      "epoch: 5 step: 1566, loss is 0.0005603524623438716\n",
      "epoch: 5 step: 1567, loss is 0.0011501139961183071\n",
      "epoch: 5 step: 1568, loss is 0.0034596219193190336\n",
      "epoch: 5 step: 1569, loss is 0.0035498840734362602\n",
      "epoch: 5 step: 1570, loss is 0.07162606716156006\n",
      "epoch: 5 step: 1571, loss is 0.004283546004444361\n",
      "epoch: 5 step: 1572, loss is 0.11168361455202103\n",
      "epoch: 5 step: 1573, loss is 0.001914175576530397\n",
      "epoch: 5 step: 1574, loss is 0.17542430758476257\n",
      "epoch: 5 step: 1575, loss is 0.04504234716296196\n",
      "epoch: 5 step: 1576, loss is 0.012500646524131298\n",
      "epoch: 5 step: 1577, loss is 0.001880347146652639\n",
      "epoch: 5 step: 1578, loss is 0.001971449702978134\n",
      "epoch: 5 step: 1579, loss is 0.04108717665076256\n",
      "epoch: 5 step: 1580, loss is 0.029083123430609703\n",
      "epoch: 5 step: 1581, loss is 0.07295247912406921\n",
      "epoch: 5 step: 1582, loss is 0.12685714662075043\n",
      "epoch: 5 step: 1583, loss is 0.04757088050246239\n",
      "epoch: 5 step: 1584, loss is 0.01023756805807352\n",
      "epoch: 5 step: 1585, loss is 0.010782054625451565\n",
      "epoch: 5 step: 1586, loss is 0.3618238568305969\n",
      "epoch: 5 step: 1587, loss is 0.01667691394686699\n",
      "epoch: 5 step: 1588, loss is 0.04909725859761238\n",
      "epoch: 5 step: 1589, loss is 0.009235593490302563\n",
      "epoch: 5 step: 1590, loss is 0.007524616084992886\n",
      "epoch: 5 step: 1591, loss is 0.0282822884619236\n",
      "epoch: 5 step: 1592, loss is 0.05515041947364807\n",
      "epoch: 5 step: 1593, loss is 0.0027899453416466713\n",
      "epoch: 5 step: 1594, loss is 0.01792476698756218\n",
      "epoch: 5 step: 1595, loss is 0.14309924840927124\n",
      "epoch: 5 step: 1596, loss is 0.04578227549791336\n",
      "epoch: 5 step: 1597, loss is 0.003287911880761385\n",
      "epoch: 5 step: 1598, loss is 0.012848851270973682\n",
      "epoch: 5 step: 1599, loss is 0.023733654990792274\n",
      "epoch: 5 step: 1600, loss is 0.023231780156493187\n",
      "epoch: 5 step: 1601, loss is 0.018208106979727745\n",
      "epoch: 5 step: 1602, loss is 0.02010391838848591\n",
      "epoch: 5 step: 1603, loss is 0.008617536164820194\n",
      "epoch: 5 step: 1604, loss is 0.08750993013381958\n",
      "epoch: 5 step: 1605, loss is 0.03173048049211502\n",
      "epoch: 5 step: 1606, loss is 0.01506008394062519\n",
      "epoch: 5 step: 1607, loss is 0.026676952838897705\n",
      "epoch: 5 step: 1608, loss is 0.16314400732517242\n",
      "epoch: 5 step: 1609, loss is 0.013648252934217453\n",
      "epoch: 5 step: 1610, loss is 0.005669695790857077\n",
      "epoch: 5 step: 1611, loss is 0.003962201997637749\n",
      "epoch: 5 step: 1612, loss is 0.00787073653191328\n",
      "epoch: 5 step: 1613, loss is 0.01037885807454586\n",
      "epoch: 5 step: 1614, loss is 0.08947496116161346\n",
      "epoch: 5 step: 1615, loss is 0.002225058851763606\n",
      "epoch: 5 step: 1616, loss is 0.01315033994615078\n",
      "epoch: 5 step: 1617, loss is 0.001642586663365364\n",
      "epoch: 5 step: 1618, loss is 0.11726200580596924\n",
      "epoch: 5 step: 1619, loss is 0.18994860351085663\n",
      "epoch: 5 step: 1620, loss is 0.0015495725674554706\n",
      "epoch: 5 step: 1621, loss is 0.031925641000270844\n",
      "epoch: 5 step: 1622, loss is 0.0013676236849278212\n",
      "epoch: 5 step: 1623, loss is 0.009077105671167374\n",
      "epoch: 5 step: 1624, loss is 0.0027894035447388887\n",
      "epoch: 5 step: 1625, loss is 0.0005346332327462733\n",
      "epoch: 5 step: 1626, loss is 0.012903054244816303\n",
      "epoch: 5 step: 1627, loss is 0.19584473967552185\n",
      "epoch: 5 step: 1628, loss is 0.003759537823498249\n",
      "epoch: 5 step: 1629, loss is 0.006801106501370668\n",
      "epoch: 5 step: 1630, loss is 0.027112523093819618\n",
      "epoch: 5 step: 1631, loss is 0.02214575558900833\n",
      "epoch: 5 step: 1632, loss is 0.05578509718179703\n",
      "epoch: 5 step: 1633, loss is 0.07199278473854065\n",
      "epoch: 5 step: 1634, loss is 0.020404670387506485\n",
      "epoch: 5 step: 1635, loss is 0.0031385100446641445\n",
      "epoch: 5 step: 1636, loss is 0.010158992372453213\n",
      "epoch: 5 step: 1637, loss is 0.015513451769948006\n",
      "epoch: 5 step: 1638, loss is 0.03428402915596962\n",
      "epoch: 5 step: 1639, loss is 0.00034144893288612366\n",
      "epoch: 5 step: 1640, loss is 0.019957108423113823\n",
      "epoch: 5 step: 1641, loss is 0.0010849721729755402\n",
      "epoch: 5 step: 1642, loss is 0.004420811776071787\n",
      "epoch: 5 step: 1643, loss is 0.015546794049441814\n",
      "epoch: 5 step: 1644, loss is 0.014300797134637833\n",
      "epoch: 5 step: 1645, loss is 0.15378975868225098\n",
      "epoch: 5 step: 1646, loss is 0.0006469294312410057\n",
      "epoch: 5 step: 1647, loss is 0.0017152848886325955\n",
      "epoch: 5 step: 1648, loss is 0.0015595993027091026\n",
      "epoch: 5 step: 1649, loss is 0.014192682690918446\n",
      "epoch: 5 step: 1650, loss is 0.0019003095803782344\n",
      "epoch: 5 step: 1651, loss is 0.00650888541713357\n",
      "epoch: 5 step: 1652, loss is 0.034790247678756714\n",
      "epoch: 5 step: 1653, loss is 0.07344497740268707\n",
      "epoch: 5 step: 1654, loss is 0.10884470492601395\n",
      "epoch: 5 step: 1655, loss is 0.15644334256649017\n",
      "epoch: 5 step: 1656, loss is 0.023051461204886436\n",
      "epoch: 5 step: 1657, loss is 0.06783477216959\n",
      "epoch: 5 step: 1658, loss is 0.011796444654464722\n",
      "epoch: 5 step: 1659, loss is 0.014121482148766518\n",
      "epoch: 5 step: 1660, loss is 0.0006321908440440893\n",
      "epoch: 5 step: 1661, loss is 0.0041811708360910416\n",
      "epoch: 5 step: 1662, loss is 0.03982669860124588\n",
      "epoch: 5 step: 1663, loss is 0.007418699096888304\n",
      "epoch: 5 step: 1664, loss is 0.010634416714310646\n",
      "epoch: 5 step: 1665, loss is 0.0595535933971405\n",
      "epoch: 5 step: 1666, loss is 0.04225722700357437\n",
      "epoch: 5 step: 1667, loss is 0.013692116364836693\n",
      "epoch: 5 step: 1668, loss is 0.04542405530810356\n",
      "epoch: 5 step: 1669, loss is 0.005245023872703314\n",
      "epoch: 5 step: 1670, loss is 0.02519303746521473\n",
      "epoch: 5 step: 1671, loss is 0.012739398516714573\n",
      "epoch: 5 step: 1672, loss is 0.030033955350518227\n",
      "epoch: 5 step: 1673, loss is 0.07272452116012573\n",
      "epoch: 5 step: 1674, loss is 0.014290808700025082\n",
      "epoch: 5 step: 1675, loss is 0.07048770040273666\n",
      "epoch: 5 step: 1676, loss is 0.0014475735370069742\n",
      "epoch: 5 step: 1677, loss is 0.0008989318157546222\n",
      "epoch: 5 step: 1678, loss is 0.21393240988254547\n",
      "epoch: 5 step: 1679, loss is 0.00047194064245559275\n",
      "epoch: 5 step: 1680, loss is 0.0020829641725867987\n",
      "epoch: 5 step: 1681, loss is 0.000534090562723577\n",
      "epoch: 5 step: 1682, loss is 0.11904450505971909\n",
      "epoch: 5 step: 1683, loss is 0.00861010979861021\n",
      "epoch: 5 step: 1684, loss is 0.000991914072073996\n",
      "epoch: 5 step: 1685, loss is 0.13278785347938538\n",
      "epoch: 5 step: 1686, loss is 0.00139979412779212\n",
      "epoch: 5 step: 1687, loss is 0.0080607570707798\n",
      "epoch: 5 step: 1688, loss is 0.0979674831032753\n",
      "epoch: 5 step: 1689, loss is 0.09560887515544891\n",
      "epoch: 5 step: 1690, loss is 0.14082637429237366\n",
      "epoch: 5 step: 1691, loss is 0.0014221336459740996\n",
      "epoch: 5 step: 1692, loss is 0.01781502179801464\n",
      "epoch: 5 step: 1693, loss is 0.21628427505493164\n",
      "epoch: 5 step: 1694, loss is 0.0015556362923234701\n",
      "epoch: 5 step: 1695, loss is 0.014968172647058964\n",
      "epoch: 5 step: 1696, loss is 0.00748083833605051\n",
      "epoch: 5 step: 1697, loss is 0.09868472814559937\n",
      "epoch: 5 step: 1698, loss is 0.03403739631175995\n",
      "epoch: 5 step: 1699, loss is 0.06236337497830391\n",
      "epoch: 5 step: 1700, loss is 0.07292679697275162\n",
      "epoch: 5 step: 1701, loss is 0.08424605429172516\n",
      "epoch: 5 step: 1702, loss is 0.004754702560603619\n",
      "epoch: 5 step: 1703, loss is 0.005732956808060408\n",
      "epoch: 5 step: 1704, loss is 0.16453389823436737\n",
      "epoch: 5 step: 1705, loss is 0.038222137838602066\n",
      "epoch: 5 step: 1706, loss is 0.006544020958244801\n",
      "epoch: 5 step: 1707, loss is 0.0024845662992447615\n",
      "epoch: 5 step: 1708, loss is 0.024343358352780342\n",
      "epoch: 5 step: 1709, loss is 0.0160081684589386\n",
      "epoch: 5 step: 1710, loss is 0.0043682558462023735\n",
      "epoch: 5 step: 1711, loss is 0.015264236368238926\n",
      "epoch: 5 step: 1712, loss is 0.002939933678135276\n",
      "epoch: 5 step: 1713, loss is 0.0037267422303557396\n",
      "epoch: 5 step: 1714, loss is 0.018669551238417625\n",
      "epoch: 5 step: 1715, loss is 0.001235587173141539\n",
      "epoch: 5 step: 1716, loss is 0.005986073054373264\n",
      "epoch: 5 step: 1717, loss is 0.0011629045475274324\n",
      "epoch: 5 step: 1718, loss is 0.0025495020672678947\n",
      "epoch: 5 step: 1719, loss is 0.000901832478120923\n",
      "epoch: 5 step: 1720, loss is 0.001757555641233921\n",
      "epoch: 5 step: 1721, loss is 0.013033182360231876\n",
      "epoch: 5 step: 1722, loss is 0.04067203775048256\n",
      "epoch: 5 step: 1723, loss is 0.004390005022287369\n",
      "epoch: 5 step: 1724, loss is 0.0026408275589346886\n",
      "epoch: 5 step: 1725, loss is 0.034669429063797\n",
      "epoch: 5 step: 1726, loss is 0.013080024160444736\n",
      "epoch: 5 step: 1727, loss is 0.006802455056458712\n",
      "epoch: 5 step: 1728, loss is 0.0007208110764622688\n",
      "epoch: 5 step: 1729, loss is 0.0019643609412014484\n",
      "epoch: 5 step: 1730, loss is 0.0038406101521104574\n",
      "epoch: 5 step: 1731, loss is 0.08206944167613983\n",
      "epoch: 5 step: 1732, loss is 0.002152435015887022\n",
      "epoch: 5 step: 1733, loss is 0.005633815191686153\n",
      "epoch: 5 step: 1734, loss is 0.014240340329706669\n",
      "epoch: 5 step: 1735, loss is 0.02668924815952778\n",
      "epoch: 5 step: 1736, loss is 0.009283792227506638\n",
      "epoch: 5 step: 1737, loss is 0.0027827934827655554\n",
      "epoch: 5 step: 1738, loss is 0.00011472022742964327\n",
      "epoch: 5 step: 1739, loss is 0.031709153205156326\n",
      "epoch: 5 step: 1740, loss is 0.009330161847174168\n",
      "epoch: 5 step: 1741, loss is 0.13581417500972748\n",
      "epoch: 5 step: 1742, loss is 0.01855343207716942\n",
      "epoch: 5 step: 1743, loss is 0.0019886200316250324\n",
      "epoch: 5 step: 1744, loss is 0.024601101875305176\n",
      "epoch: 5 step: 1745, loss is 0.0016234718495979905\n",
      "epoch: 5 step: 1746, loss is 0.0074949003756046295\n",
      "epoch: 5 step: 1747, loss is 0.17201347649097443\n",
      "epoch: 5 step: 1748, loss is 0.013823197223246098\n",
      "epoch: 5 step: 1749, loss is 0.05617193505167961\n",
      "epoch: 5 step: 1750, loss is 0.1553685963153839\n",
      "epoch: 5 step: 1751, loss is 0.004952133167535067\n",
      "epoch: 5 step: 1752, loss is 0.002510279417037964\n",
      "epoch: 5 step: 1753, loss is 0.046952053904533386\n",
      "epoch: 5 step: 1754, loss is 0.2231599986553192\n",
      "epoch: 5 step: 1755, loss is 0.0034340880811214447\n",
      "epoch: 5 step: 1756, loss is 0.014130462892353535\n",
      "epoch: 5 step: 1757, loss is 0.056884415447711945\n",
      "epoch: 5 step: 1758, loss is 0.03858347237110138\n",
      "epoch: 5 step: 1759, loss is 0.018974000588059425\n",
      "epoch: 5 step: 1760, loss is 0.04255310073494911\n",
      "epoch: 5 step: 1761, loss is 0.01442277617752552\n",
      "epoch: 5 step: 1762, loss is 0.08715853840112686\n",
      "epoch: 5 step: 1763, loss is 0.035540927201509476\n",
      "epoch: 5 step: 1764, loss is 0.00799593236297369\n",
      "epoch: 5 step: 1765, loss is 0.007622112054377794\n",
      "epoch: 5 step: 1766, loss is 0.004578211344778538\n",
      "epoch: 5 step: 1767, loss is 0.018498124554753304\n",
      "epoch: 5 step: 1768, loss is 0.039099328219890594\n",
      "epoch: 5 step: 1769, loss is 0.005126069765537977\n",
      "epoch: 5 step: 1770, loss is 0.14607910811901093\n",
      "epoch: 5 step: 1771, loss is 0.3364607095718384\n",
      "epoch: 5 step: 1772, loss is 0.13396893441677094\n",
      "epoch: 5 step: 1773, loss is 0.07590234279632568\n",
      "epoch: 5 step: 1774, loss is 0.0965314731001854\n",
      "epoch: 5 step: 1775, loss is 0.006576888263225555\n",
      "epoch: 5 step: 1776, loss is 0.025496019050478935\n",
      "epoch: 5 step: 1777, loss is 0.0028261952102184296\n",
      "epoch: 5 step: 1778, loss is 0.0013428365346044302\n",
      "epoch: 5 step: 1779, loss is 0.06681320816278458\n",
      "epoch: 5 step: 1780, loss is 0.0030429584439843893\n",
      "epoch: 5 step: 1781, loss is 0.03835751861333847\n",
      "epoch: 5 step: 1782, loss is 0.003736558835953474\n",
      "epoch: 5 step: 1783, loss is 0.005373367108404636\n",
      "epoch: 5 step: 1784, loss is 0.003285365179181099\n",
      "epoch: 5 step: 1785, loss is 0.00805649533867836\n",
      "epoch: 5 step: 1786, loss is 0.0055712321773171425\n",
      "epoch: 5 step: 1787, loss is 0.051217466592788696\n",
      "epoch: 5 step: 1788, loss is 0.01438110414892435\n",
      "epoch: 5 step: 1789, loss is 0.0014493543421849608\n",
      "epoch: 5 step: 1790, loss is 0.20094937086105347\n",
      "epoch: 5 step: 1791, loss is 0.0022369956132024527\n",
      "epoch: 5 step: 1792, loss is 0.0031675631180405617\n",
      "epoch: 5 step: 1793, loss is 0.00853690505027771\n",
      "epoch: 5 step: 1794, loss is 0.0037894006818532944\n",
      "epoch: 5 step: 1795, loss is 0.1463324874639511\n",
      "epoch: 5 step: 1796, loss is 0.036404967308044434\n",
      "epoch: 5 step: 1797, loss is 0.05899279564619064\n",
      "epoch: 5 step: 1798, loss is 0.07851319015026093\n",
      "epoch: 5 step: 1799, loss is 0.008418386802077293\n",
      "epoch: 5 step: 1800, loss is 0.028019899502396584\n",
      "epoch: 5 step: 1801, loss is 0.0204203762114048\n",
      "epoch: 5 step: 1802, loss is 0.01789318583905697\n",
      "epoch: 5 step: 1803, loss is 0.0031185224652290344\n",
      "epoch: 5 step: 1804, loss is 0.002066186396405101\n",
      "epoch: 5 step: 1805, loss is 0.136426642537117\n",
      "epoch: 5 step: 1806, loss is 0.03844432905316353\n",
      "epoch: 5 step: 1807, loss is 0.005502454005181789\n",
      "epoch: 5 step: 1808, loss is 0.09283057600259781\n",
      "epoch: 5 step: 1809, loss is 0.012432226911187172\n",
      "epoch: 5 step: 1810, loss is 0.001484943088144064\n",
      "epoch: 5 step: 1811, loss is 0.005835248623043299\n",
      "epoch: 5 step: 1812, loss is 0.014925720170140266\n",
      "epoch: 5 step: 1813, loss is 0.0012761123944073915\n",
      "epoch: 5 step: 1814, loss is 0.029084263369441032\n",
      "epoch: 5 step: 1815, loss is 0.06207945570349693\n",
      "epoch: 5 step: 1816, loss is 0.00804001186043024\n",
      "epoch: 5 step: 1817, loss is 0.0017816124018281698\n",
      "epoch: 5 step: 1818, loss is 0.00042900117114186287\n",
      "epoch: 5 step: 1819, loss is 0.007620298303663731\n",
      "epoch: 5 step: 1820, loss is 0.01257119607180357\n",
      "epoch: 5 step: 1821, loss is 0.010935047641396523\n",
      "epoch: 5 step: 1822, loss is 0.016602784395217896\n",
      "epoch: 5 step: 1823, loss is 0.00010805878264363855\n",
      "epoch: 5 step: 1824, loss is 0.007396331988275051\n",
      "epoch: 5 step: 1825, loss is 0.008361103013157845\n",
      "epoch: 5 step: 1826, loss is 0.000634635507594794\n",
      "epoch: 5 step: 1827, loss is 0.17247413098812103\n",
      "epoch: 5 step: 1828, loss is 0.006107649765908718\n",
      "epoch: 5 step: 1829, loss is 0.008400497026741505\n",
      "epoch: 5 step: 1830, loss is 0.16397875547409058\n",
      "epoch: 5 step: 1831, loss is 0.0033484098967164755\n",
      "epoch: 5 step: 1832, loss is 0.022592518478631973\n",
      "epoch: 5 step: 1833, loss is 0.00512400409206748\n",
      "epoch: 5 step: 1834, loss is 0.2005831003189087\n",
      "epoch: 5 step: 1835, loss is 0.06289618462324142\n",
      "epoch: 5 step: 1836, loss is 0.007574590854346752\n",
      "epoch: 5 step: 1837, loss is 0.001679818145930767\n",
      "epoch: 5 step: 1838, loss is 0.02242191694676876\n",
      "epoch: 5 step: 1839, loss is 0.021423880010843277\n",
      "epoch: 5 step: 1840, loss is 0.0009149559773504734\n",
      "epoch: 5 step: 1841, loss is 0.004229761194437742\n",
      "epoch: 5 step: 1842, loss is 0.0017206397606059909\n",
      "epoch: 5 step: 1843, loss is 0.01305107120424509\n",
      "epoch: 5 step: 1844, loss is 0.021756580099463463\n",
      "epoch: 5 step: 1845, loss is 0.025731604546308517\n",
      "epoch: 5 step: 1846, loss is 0.0016755749238654971\n",
      "epoch: 5 step: 1847, loss is 0.000975252129137516\n",
      "epoch: 5 step: 1848, loss is 0.006879887077957392\n",
      "epoch: 5 step: 1849, loss is 0.007476589642465115\n",
      "epoch: 5 step: 1850, loss is 0.2214815765619278\n",
      "epoch: 5 step: 1851, loss is 0.0021067564375698566\n",
      "epoch: 5 step: 1852, loss is 0.03722291439771652\n",
      "epoch: 5 step: 1853, loss is 0.010603955015540123\n",
      "epoch: 5 step: 1854, loss is 0.01265399158000946\n",
      "epoch: 5 step: 1855, loss is 0.14566673338413239\n",
      "epoch: 5 step: 1856, loss is 0.0014371129218488932\n",
      "epoch: 5 step: 1857, loss is 0.0434909462928772\n",
      "epoch: 5 step: 1858, loss is 0.02739708311855793\n",
      "epoch: 5 step: 1859, loss is 0.0016427289228886366\n",
      "epoch: 5 step: 1860, loss is 0.13793613016605377\n",
      "epoch: 5 step: 1861, loss is 0.0035423797089606524\n",
      "epoch: 5 step: 1862, loss is 0.08054803311824799\n",
      "epoch: 5 step: 1863, loss is 0.13201004266738892\n",
      "epoch: 5 step: 1864, loss is 0.005653372034430504\n",
      "epoch: 5 step: 1865, loss is 0.01059981994330883\n",
      "epoch: 5 step: 1866, loss is 0.013414978981018066\n",
      "epoch: 5 step: 1867, loss is 0.00631711445748806\n",
      "epoch: 5 step: 1868, loss is 0.0021183108910918236\n",
      "epoch: 5 step: 1869, loss is 0.00732145132496953\n",
      "epoch: 5 step: 1870, loss is 0.0028419510927051306\n",
      "epoch: 5 step: 1871, loss is 0.0013925558887422085\n",
      "epoch: 5 step: 1872, loss is 0.0061462935991585255\n",
      "epoch: 5 step: 1873, loss is 0.0048072622157633305\n",
      "epoch: 5 step: 1874, loss is 0.07714395225048065\n",
      "epoch: 5 step: 1875, loss is 0.01974017173051834\n",
      "epoch: 6 step: 1, loss is 0.027452846989035606\n",
      "epoch: 6 step: 2, loss is 0.0027538773138076067\n",
      "epoch: 6 step: 3, loss is 0.003561940509825945\n",
      "epoch: 6 step: 4, loss is 0.005571945570409298\n",
      "epoch: 6 step: 5, loss is 0.0027156879659742117\n",
      "epoch: 6 step: 6, loss is 0.0005097378161735833\n",
      "epoch: 6 step: 7, loss is 0.023973962292075157\n",
      "epoch: 6 step: 8, loss is 0.0014581882860511541\n",
      "epoch: 6 step: 9, loss is 0.00038239319110289216\n",
      "epoch: 6 step: 10, loss is 0.00412343442440033\n",
      "epoch: 6 step: 11, loss is 0.030044227838516235\n",
      "epoch: 6 step: 12, loss is 0.0018644898664206266\n",
      "epoch: 6 step: 13, loss is 0.017294645309448242\n",
      "epoch: 6 step: 14, loss is 0.001222117803990841\n",
      "epoch: 6 step: 15, loss is 0.0013683519791811705\n",
      "epoch: 6 step: 16, loss is 0.030526960268616676\n",
      "epoch: 6 step: 17, loss is 0.0002555245300754905\n",
      "epoch: 6 step: 18, loss is 0.0011763236252591014\n",
      "epoch: 6 step: 19, loss is 0.03989703953266144\n",
      "epoch: 6 step: 20, loss is 0.053380049765110016\n",
      "epoch: 6 step: 21, loss is 0.017780456691980362\n",
      "epoch: 6 step: 22, loss is 0.10001932829618454\n",
      "epoch: 6 step: 23, loss is 0.005122779868543148\n",
      "epoch: 6 step: 24, loss is 0.0025552846491336823\n",
      "epoch: 6 step: 25, loss is 0.007791380863636732\n",
      "epoch: 6 step: 26, loss is 0.0019073582952842116\n",
      "epoch: 6 step: 27, loss is 0.0002897360536735505\n",
      "epoch: 6 step: 28, loss is 0.010074285790324211\n",
      "epoch: 6 step: 29, loss is 0.0012840053532272577\n",
      "epoch: 6 step: 30, loss is 0.023819128051400185\n",
      "epoch: 6 step: 31, loss is 0.0005951927741989493\n",
      "epoch: 6 step: 32, loss is 0.00032330123940482736\n",
      "epoch: 6 step: 33, loss is 0.01897292397916317\n",
      "epoch: 6 step: 34, loss is 0.1806831657886505\n",
      "epoch: 6 step: 35, loss is 0.004178073722869158\n",
      "epoch: 6 step: 36, loss is 0.007968498393893242\n",
      "epoch: 6 step: 37, loss is 0.0012480694567784667\n",
      "epoch: 6 step: 38, loss is 0.001068766345269978\n",
      "epoch: 6 step: 39, loss is 0.0016378015279769897\n",
      "epoch: 6 step: 40, loss is 0.010693242773413658\n",
      "epoch: 6 step: 41, loss is 0.03373979032039642\n",
      "epoch: 6 step: 42, loss is 0.010958424769341946\n",
      "epoch: 6 step: 43, loss is 0.059905145317316055\n",
      "epoch: 6 step: 44, loss is 0.0019708978943526745\n",
      "epoch: 6 step: 45, loss is 0.009679634124040604\n",
      "epoch: 6 step: 46, loss is 0.004772468935698271\n",
      "epoch: 6 step: 47, loss is 0.009818410500884056\n",
      "epoch: 6 step: 48, loss is 0.0054193902760744095\n",
      "epoch: 6 step: 49, loss is 0.001550437300466001\n",
      "epoch: 6 step: 50, loss is 0.0181304719299078\n",
      "epoch: 6 step: 51, loss is 0.01730411872267723\n",
      "epoch: 6 step: 52, loss is 0.06269210577011108\n",
      "epoch: 6 step: 53, loss is 0.01865703985095024\n",
      "epoch: 6 step: 54, loss is 0.062331605702638626\n",
      "epoch: 6 step: 55, loss is 0.0007376670837402344\n",
      "epoch: 6 step: 56, loss is 0.0021922218147665262\n",
      "epoch: 6 step: 57, loss is 0.0006641162326559424\n",
      "epoch: 6 step: 58, loss is 0.0005874155322089791\n",
      "epoch: 6 step: 59, loss is 0.00032048544380813837\n",
      "epoch: 6 step: 60, loss is 0.12830746173858643\n",
      "epoch: 6 step: 61, loss is 0.00029078879742883146\n",
      "epoch: 6 step: 62, loss is 0.05484623461961746\n",
      "epoch: 6 step: 63, loss is 0.010501202195882797\n",
      "epoch: 6 step: 64, loss is 0.004163919482380152\n",
      "epoch: 6 step: 65, loss is 0.04311342537403107\n",
      "epoch: 6 step: 66, loss is 0.005464473739266396\n",
      "epoch: 6 step: 67, loss is 0.002180651994422078\n",
      "epoch: 6 step: 68, loss is 0.050473522394895554\n",
      "epoch: 6 step: 69, loss is 0.0002625855850055814\n",
      "epoch: 6 step: 70, loss is 0.14925618469715118\n",
      "epoch: 6 step: 71, loss is 0.026011819019913673\n",
      "epoch: 6 step: 72, loss is 0.037024568766355515\n",
      "epoch: 6 step: 73, loss is 0.030144870281219482\n",
      "epoch: 6 step: 74, loss is 0.0016835982678458095\n",
      "epoch: 6 step: 75, loss is 0.08436009287834167\n",
      "epoch: 6 step: 76, loss is 0.005404741503298283\n",
      "epoch: 6 step: 77, loss is 0.003490332281216979\n",
      "epoch: 6 step: 78, loss is 0.0022967227268964052\n",
      "epoch: 6 step: 79, loss is 0.0003735501377377659\n",
      "epoch: 6 step: 80, loss is 0.0006728647276759148\n",
      "epoch: 6 step: 81, loss is 0.019008101895451546\n",
      "epoch: 6 step: 82, loss is 0.0027039393316954374\n",
      "epoch: 6 step: 83, loss is 0.03606562688946724\n",
      "epoch: 6 step: 84, loss is 0.0015188349643722177\n",
      "epoch: 6 step: 85, loss is 0.000742803851608187\n",
      "epoch: 6 step: 86, loss is 0.006384911015629768\n",
      "epoch: 6 step: 87, loss is 0.0028324525337666273\n",
      "epoch: 6 step: 88, loss is 0.058300890028476715\n",
      "epoch: 6 step: 89, loss is 0.01691042073071003\n",
      "epoch: 6 step: 90, loss is 0.03690916299819946\n",
      "epoch: 6 step: 91, loss is 0.00018174105207435787\n",
      "epoch: 6 step: 92, loss is 0.028270099312067032\n",
      "epoch: 6 step: 93, loss is 0.0006179967895150185\n",
      "epoch: 6 step: 94, loss is 0.10463593900203705\n",
      "epoch: 6 step: 95, loss is 0.009807675145566463\n",
      "epoch: 6 step: 96, loss is 0.0014086236478760839\n",
      "epoch: 6 step: 97, loss is 0.03305887430906296\n",
      "epoch: 6 step: 98, loss is 0.005808116868138313\n",
      "epoch: 6 step: 99, loss is 0.06376591324806213\n",
      "epoch: 6 step: 100, loss is 0.001826467807404697\n",
      "epoch: 6 step: 101, loss is 0.01200965978205204\n",
      "epoch: 6 step: 102, loss is 0.004329593852162361\n",
      "epoch: 6 step: 103, loss is 0.06896831840276718\n",
      "epoch: 6 step: 104, loss is 0.008819010108709335\n",
      "epoch: 6 step: 105, loss is 0.004513692110776901\n",
      "epoch: 6 step: 106, loss is 0.10443802922964096\n",
      "epoch: 6 step: 107, loss is 0.007132596801966429\n",
      "epoch: 6 step: 108, loss is 0.09813894331455231\n",
      "epoch: 6 step: 109, loss is 0.2119068056344986\n",
      "epoch: 6 step: 110, loss is 7.063808152452111e-05\n",
      "epoch: 6 step: 111, loss is 0.019211675971746445\n",
      "epoch: 6 step: 112, loss is 0.0015028218040242791\n",
      "epoch: 6 step: 113, loss is 0.05786888301372528\n",
      "epoch: 6 step: 114, loss is 0.1317043900489807\n",
      "epoch: 6 step: 115, loss is 0.1691891849040985\n",
      "epoch: 6 step: 116, loss is 0.0028245924040675163\n",
      "epoch: 6 step: 117, loss is 0.215575709939003\n",
      "epoch: 6 step: 118, loss is 0.018588071689009666\n",
      "epoch: 6 step: 119, loss is 0.039666514843702316\n",
      "epoch: 6 step: 120, loss is 0.0011804206296801567\n",
      "epoch: 6 step: 121, loss is 0.0025625079870224\n",
      "epoch: 6 step: 122, loss is 0.011267210356891155\n",
      "epoch: 6 step: 123, loss is 0.059421539306640625\n",
      "epoch: 6 step: 124, loss is 0.004513924941420555\n",
      "epoch: 6 step: 125, loss is 0.006085530389100313\n",
      "epoch: 6 step: 126, loss is 0.021650753915309906\n",
      "epoch: 6 step: 127, loss is 0.0006887362105771899\n",
      "epoch: 6 step: 128, loss is 0.014322426170110703\n",
      "epoch: 6 step: 129, loss is 0.0038715116679668427\n",
      "epoch: 6 step: 130, loss is 0.0006436652620323002\n",
      "epoch: 6 step: 131, loss is 0.24681290984153748\n",
      "epoch: 6 step: 132, loss is 0.004595744423568249\n",
      "epoch: 6 step: 133, loss is 0.058389633893966675\n",
      "epoch: 6 step: 134, loss is 0.006941529922187328\n",
      "epoch: 6 step: 135, loss is 0.017165672034025192\n",
      "epoch: 6 step: 136, loss is 0.009783806279301643\n",
      "epoch: 6 step: 137, loss is 0.04377074912190437\n",
      "epoch: 6 step: 138, loss is 0.0020750139374285936\n",
      "epoch: 6 step: 139, loss is 0.0013635718496516347\n",
      "epoch: 6 step: 140, loss is 0.001050231629051268\n",
      "epoch: 6 step: 141, loss is 0.007239248137921095\n",
      "epoch: 6 step: 142, loss is 0.010739831253886223\n",
      "epoch: 6 step: 143, loss is 0.03318816050887108\n",
      "epoch: 6 step: 144, loss is 0.0006433380767703056\n",
      "epoch: 6 step: 145, loss is 0.0004930173163302243\n",
      "epoch: 6 step: 146, loss is 0.03034193627536297\n",
      "epoch: 6 step: 147, loss is 0.07238699495792389\n",
      "epoch: 6 step: 148, loss is 0.05023874342441559\n",
      "epoch: 6 step: 149, loss is 0.030736669898033142\n",
      "epoch: 6 step: 150, loss is 0.0025727113243192434\n",
      "epoch: 6 step: 151, loss is 0.0005125249736011028\n",
      "epoch: 6 step: 152, loss is 0.012848760932683945\n",
      "epoch: 6 step: 153, loss is 0.009921328164637089\n",
      "epoch: 6 step: 154, loss is 0.02507946267724037\n",
      "epoch: 6 step: 155, loss is 0.002575960475951433\n",
      "epoch: 6 step: 156, loss is 0.0008434316841885448\n",
      "epoch: 6 step: 157, loss is 0.0004110693989787251\n",
      "epoch: 6 step: 158, loss is 0.0007600495591759682\n",
      "epoch: 6 step: 159, loss is 0.0033725574612617493\n",
      "epoch: 6 step: 160, loss is 0.0002972765068989247\n",
      "epoch: 6 step: 161, loss is 0.057280369102954865\n",
      "epoch: 6 step: 162, loss is 0.0003772389027290046\n",
      "epoch: 6 step: 163, loss is 0.011082278564572334\n",
      "epoch: 6 step: 164, loss is 8.196815906558186e-05\n",
      "epoch: 6 step: 165, loss is 3.454031320870854e-05\n",
      "epoch: 6 step: 166, loss is 0.009521354921162128\n",
      "epoch: 6 step: 167, loss is 0.02775440365076065\n",
      "epoch: 6 step: 168, loss is 0.00016640273679513484\n",
      "epoch: 6 step: 169, loss is 0.03298043832182884\n",
      "epoch: 6 step: 170, loss is 0.004743189550936222\n",
      "epoch: 6 step: 171, loss is 0.00681510241702199\n",
      "epoch: 6 step: 172, loss is 0.0005014023627154529\n",
      "epoch: 6 step: 173, loss is 0.00021957109856884927\n",
      "epoch: 6 step: 174, loss is 0.06219233572483063\n",
      "epoch: 6 step: 175, loss is 0.0013589394511654973\n",
      "epoch: 6 step: 176, loss is 0.00600988045334816\n",
      "epoch: 6 step: 177, loss is 0.12417319416999817\n",
      "epoch: 6 step: 178, loss is 0.0010760822333395481\n",
      "epoch: 6 step: 179, loss is 0.011503287591040134\n",
      "epoch: 6 step: 180, loss is 0.15253174304962158\n",
      "epoch: 6 step: 181, loss is 0.01551232859492302\n",
      "epoch: 6 step: 182, loss is 0.06164218485355377\n",
      "epoch: 6 step: 183, loss is 0.0005749083356931806\n",
      "epoch: 6 step: 184, loss is 0.0936988964676857\n",
      "epoch: 6 step: 185, loss is 0.08381597697734833\n",
      "epoch: 6 step: 186, loss is 0.0008754380396567285\n",
      "epoch: 6 step: 187, loss is 0.0016972703160718083\n",
      "epoch: 6 step: 188, loss is 0.05747511237859726\n",
      "epoch: 6 step: 189, loss is 0.000272798934020102\n",
      "epoch: 6 step: 190, loss is 0.0012747403234243393\n",
      "epoch: 6 step: 191, loss is 0.00017170360661111772\n",
      "epoch: 6 step: 192, loss is 0.0005132718360982835\n",
      "epoch: 6 step: 193, loss is 0.0009106174111366272\n",
      "epoch: 6 step: 194, loss is 0.015363911166787148\n",
      "epoch: 6 step: 195, loss is 0.08507119119167328\n",
      "epoch: 6 step: 196, loss is 0.00016108891577459872\n",
      "epoch: 6 step: 197, loss is 0.002563328482210636\n",
      "epoch: 6 step: 198, loss is 0.02467544749379158\n",
      "epoch: 6 step: 199, loss is 0.0016076951287686825\n",
      "epoch: 6 step: 200, loss is 0.000303551962133497\n",
      "epoch: 6 step: 201, loss is 0.0010594723280519247\n",
      "epoch: 6 step: 202, loss is 0.00018372814520262182\n",
      "epoch: 6 step: 203, loss is 0.0004485436948016286\n",
      "epoch: 6 step: 204, loss is 0.011004199273884296\n",
      "epoch: 6 step: 205, loss is 0.011430056765675545\n",
      "epoch: 6 step: 206, loss is 0.11648883670568466\n",
      "epoch: 6 step: 207, loss is 0.022820377722382545\n",
      "epoch: 6 step: 208, loss is 0.003608295228332281\n",
      "epoch: 6 step: 209, loss is 0.00332626118324697\n",
      "epoch: 6 step: 210, loss is 0.008225667290389538\n",
      "epoch: 6 step: 211, loss is 0.01407572440803051\n",
      "epoch: 6 step: 212, loss is 0.043873902410268784\n",
      "epoch: 6 step: 213, loss is 0.012258412316441536\n",
      "epoch: 6 step: 214, loss is 0.004033614415675402\n",
      "epoch: 6 step: 215, loss is 0.06590620428323746\n",
      "epoch: 6 step: 216, loss is 0.0005290395929478109\n",
      "epoch: 6 step: 217, loss is 0.0050431424751877785\n",
      "epoch: 6 step: 218, loss is 0.0007840731414034963\n",
      "epoch: 6 step: 219, loss is 0.01224865298718214\n",
      "epoch: 6 step: 220, loss is 0.1415327489376068\n",
      "epoch: 6 step: 221, loss is 0.009904664009809494\n",
      "epoch: 6 step: 222, loss is 0.006199838127940893\n",
      "epoch: 6 step: 223, loss is 0.0003704121627379209\n",
      "epoch: 6 step: 224, loss is 0.0033355732448399067\n",
      "epoch: 6 step: 225, loss is 0.002474700566381216\n",
      "epoch: 6 step: 226, loss is 0.0014678138541057706\n",
      "epoch: 6 step: 227, loss is 0.0033938447013497353\n",
      "epoch: 6 step: 228, loss is 0.0036794389598071575\n",
      "epoch: 6 step: 229, loss is 0.024809429422020912\n",
      "epoch: 6 step: 230, loss is 0.0010651367483660579\n",
      "epoch: 6 step: 231, loss is 0.0023719877935945988\n",
      "epoch: 6 step: 232, loss is 0.0013214277569204569\n",
      "epoch: 6 step: 233, loss is 0.06504058092832565\n",
      "epoch: 6 step: 234, loss is 0.00465644896030426\n",
      "epoch: 6 step: 235, loss is 0.02271329052746296\n",
      "epoch: 6 step: 236, loss is 0.0011396349873393774\n",
      "epoch: 6 step: 237, loss is 0.014860628172755241\n",
      "epoch: 6 step: 238, loss is 0.03712954744696617\n",
      "epoch: 6 step: 239, loss is 0.0017827239353209734\n",
      "epoch: 6 step: 240, loss is 0.002259312430396676\n",
      "epoch: 6 step: 241, loss is 0.13037605583667755\n",
      "epoch: 6 step: 242, loss is 0.0006577679305337369\n",
      "epoch: 6 step: 243, loss is 0.008428696542978287\n",
      "epoch: 6 step: 244, loss is 0.000830159755423665\n",
      "epoch: 6 step: 245, loss is 0.02707580104470253\n",
      "epoch: 6 step: 246, loss is 0.01319889910519123\n",
      "epoch: 6 step: 247, loss is 0.0075079756788909435\n",
      "epoch: 6 step: 248, loss is 9.259567741537467e-05\n",
      "epoch: 6 step: 249, loss is 0.0010766282211989164\n",
      "epoch: 6 step: 250, loss is 0.001278489944525063\n",
      "epoch: 6 step: 251, loss is 0.0005047623999416828\n",
      "epoch: 6 step: 252, loss is 0.20840732753276825\n",
      "epoch: 6 step: 253, loss is 8.879566303221509e-05\n",
      "epoch: 6 step: 254, loss is 0.0051603601314127445\n",
      "epoch: 6 step: 255, loss is 0.00195265281945467\n",
      "epoch: 6 step: 256, loss is 0.00010713401570683345\n",
      "epoch: 6 step: 257, loss is 0.08000636845827103\n",
      "epoch: 6 step: 258, loss is 0.0023828602861613035\n",
      "epoch: 6 step: 259, loss is 0.009619898162782192\n",
      "epoch: 6 step: 260, loss is 0.0002726594393607229\n",
      "epoch: 6 step: 261, loss is 0.006018929649144411\n",
      "epoch: 6 step: 262, loss is 0.005393152125179768\n",
      "epoch: 6 step: 263, loss is 0.0012647120747715235\n",
      "epoch: 6 step: 264, loss is 0.001931772567331791\n",
      "epoch: 6 step: 265, loss is 0.017046593129634857\n",
      "epoch: 6 step: 266, loss is 0.0005161509616300464\n",
      "epoch: 6 step: 267, loss is 0.002946028485894203\n",
      "epoch: 6 step: 268, loss is 0.0012732327450066805\n",
      "epoch: 6 step: 269, loss is 0.010376198217272758\n",
      "epoch: 6 step: 270, loss is 0.0010862493654713035\n",
      "epoch: 6 step: 271, loss is 0.0007218790706247091\n",
      "epoch: 6 step: 272, loss is 0.0005153149249963462\n",
      "epoch: 6 step: 273, loss is 0.03577938303351402\n",
      "epoch: 6 step: 274, loss is 0.04931509867310524\n",
      "epoch: 6 step: 275, loss is 0.005432148929685354\n",
      "epoch: 6 step: 276, loss is 0.0011569850612431765\n",
      "epoch: 6 step: 277, loss is 0.07470207661390305\n",
      "epoch: 6 step: 278, loss is 0.08194494992494583\n",
      "epoch: 6 step: 279, loss is 0.13031911849975586\n",
      "epoch: 6 step: 280, loss is 0.0012859920971095562\n",
      "epoch: 6 step: 281, loss is 0.004160403739660978\n",
      "epoch: 6 step: 282, loss is 0.00037324888398870826\n",
      "epoch: 6 step: 283, loss is 0.0004343418695498258\n",
      "epoch: 6 step: 284, loss is 0.0017501716502010822\n",
      "epoch: 6 step: 285, loss is 0.0008210360538214445\n",
      "epoch: 6 step: 286, loss is 0.0007968726567924023\n",
      "epoch: 6 step: 287, loss is 0.0011406431440263987\n",
      "epoch: 6 step: 288, loss is 6.598861364182085e-05\n",
      "epoch: 6 step: 289, loss is 0.007612844463437796\n",
      "epoch: 6 step: 290, loss is 0.03367321193218231\n",
      "epoch: 6 step: 291, loss is 0.0010998665820807219\n",
      "epoch: 6 step: 292, loss is 0.00499624153599143\n",
      "epoch: 6 step: 293, loss is 0.00031651643803343177\n",
      "epoch: 6 step: 294, loss is 0.0002787667035590857\n",
      "epoch: 6 step: 295, loss is 0.04308537766337395\n",
      "epoch: 6 step: 296, loss is 0.004334148019552231\n",
      "epoch: 6 step: 297, loss is 0.01906304433941841\n",
      "epoch: 6 step: 298, loss is 0.00015461839211639017\n",
      "epoch: 6 step: 299, loss is 0.0007656610105186701\n",
      "epoch: 6 step: 300, loss is 0.008077244274318218\n",
      "epoch: 6 step: 301, loss is 0.005518533289432526\n",
      "epoch: 6 step: 302, loss is 0.04261202737689018\n",
      "epoch: 6 step: 303, loss is 0.0015902294544503093\n",
      "epoch: 6 step: 304, loss is 0.0435330756008625\n",
      "epoch: 6 step: 305, loss is 0.0005013519548811018\n",
      "epoch: 6 step: 306, loss is 0.09692148119211197\n",
      "epoch: 6 step: 307, loss is 3.860682045342401e-05\n",
      "epoch: 6 step: 308, loss is 0.0039413427002727985\n",
      "epoch: 6 step: 309, loss is 0.05828629061579704\n",
      "epoch: 6 step: 310, loss is 0.00023157063696999103\n",
      "epoch: 6 step: 311, loss is 0.0006604821537621319\n",
      "epoch: 6 step: 312, loss is 0.031408004462718964\n",
      "epoch: 6 step: 313, loss is 0.0007588209700770676\n",
      "epoch: 6 step: 314, loss is 0.013226458802819252\n",
      "epoch: 6 step: 315, loss is 0.0035312732215970755\n",
      "epoch: 6 step: 316, loss is 0.014768969267606735\n",
      "epoch: 6 step: 317, loss is 0.002243074355646968\n",
      "epoch: 6 step: 318, loss is 0.031452327966690063\n",
      "epoch: 6 step: 319, loss is 0.08944699913263321\n",
      "epoch: 6 step: 320, loss is 0.0003769342147279531\n",
      "epoch: 6 step: 321, loss is 0.002393085742369294\n",
      "epoch: 6 step: 322, loss is 0.004391116090118885\n",
      "epoch: 6 step: 323, loss is 0.008319765329360962\n",
      "epoch: 6 step: 324, loss is 0.029815591871738434\n",
      "epoch: 6 step: 325, loss is 0.0032605247106403112\n",
      "epoch: 6 step: 326, loss is 0.03578317537903786\n",
      "epoch: 6 step: 327, loss is 0.0004093932220712304\n",
      "epoch: 6 step: 328, loss is 0.003871095133945346\n",
      "epoch: 6 step: 329, loss is 0.08266939222812653\n",
      "epoch: 6 step: 330, loss is 0.020657090470194817\n",
      "epoch: 6 step: 331, loss is 0.04597748443484306\n",
      "epoch: 6 step: 332, loss is 0.0009628139086998999\n",
      "epoch: 6 step: 333, loss is 0.06532982736825943\n",
      "epoch: 6 step: 334, loss is 0.03803137689828873\n",
      "epoch: 6 step: 335, loss is 0.005530837457627058\n",
      "epoch: 6 step: 336, loss is 0.009267382323741913\n",
      "epoch: 6 step: 337, loss is 0.055122632533311844\n",
      "epoch: 6 step: 338, loss is 9.322934783995152e-05\n",
      "epoch: 6 step: 339, loss is 0.0036573042161762714\n",
      "epoch: 6 step: 340, loss is 0.001127799740061164\n",
      "epoch: 6 step: 341, loss is 0.0001899480848805979\n",
      "epoch: 6 step: 342, loss is 0.0007701314752921462\n",
      "epoch: 6 step: 343, loss is 0.04122859612107277\n",
      "epoch: 6 step: 344, loss is 0.00044935630285181105\n",
      "epoch: 6 step: 345, loss is 0.027126669883728027\n",
      "epoch: 6 step: 346, loss is 0.00039266390376724303\n",
      "epoch: 6 step: 347, loss is 0.015772489830851555\n",
      "epoch: 6 step: 348, loss is 0.0017421621596440673\n",
      "epoch: 6 step: 349, loss is 0.013614318333566189\n",
      "epoch: 6 step: 350, loss is 0.061547502875328064\n",
      "epoch: 6 step: 351, loss is 0.0005871832254342735\n",
      "epoch: 6 step: 352, loss is 0.07919663935899734\n",
      "epoch: 6 step: 353, loss is 2.5402940082130954e-05\n",
      "epoch: 6 step: 354, loss is 0.013093449175357819\n",
      "epoch: 6 step: 355, loss is 0.006920681335031986\n",
      "epoch: 6 step: 356, loss is 0.0025548662524670362\n",
      "epoch: 6 step: 357, loss is 0.00018070887017529458\n",
      "epoch: 6 step: 358, loss is 0.004905165638774633\n",
      "epoch: 6 step: 359, loss is 0.00045083125587552786\n",
      "epoch: 6 step: 360, loss is 8.071956835919991e-05\n",
      "epoch: 6 step: 361, loss is 0.1548839956521988\n",
      "epoch: 6 step: 362, loss is 0.0007212084019556642\n",
      "epoch: 6 step: 363, loss is 0.0018528418149799109\n",
      "epoch: 6 step: 364, loss is 0.0072620888240635395\n",
      "epoch: 6 step: 365, loss is 0.1233971044421196\n",
      "epoch: 6 step: 366, loss is 0.0035688146017491817\n",
      "epoch: 6 step: 367, loss is 0.04040209949016571\n",
      "epoch: 6 step: 368, loss is 0.00022803462343290448\n",
      "epoch: 6 step: 369, loss is 0.0009184723603539169\n",
      "epoch: 6 step: 370, loss is 0.02337237261235714\n",
      "epoch: 6 step: 371, loss is 0.14635880291461945\n",
      "epoch: 6 step: 372, loss is 0.0004231800849083811\n",
      "epoch: 6 step: 373, loss is 0.20627610385417938\n",
      "epoch: 6 step: 374, loss is 0.009177612140774727\n",
      "epoch: 6 step: 375, loss is 0.001702018897049129\n",
      "epoch: 6 step: 376, loss is 0.004019929561764002\n",
      "epoch: 6 step: 377, loss is 0.024155844002962112\n",
      "epoch: 6 step: 378, loss is 0.021192796528339386\n",
      "epoch: 6 step: 379, loss is 0.0008819495560601354\n",
      "epoch: 6 step: 380, loss is 0.0034677700605243444\n",
      "epoch: 6 step: 381, loss is 0.002406452316790819\n",
      "epoch: 6 step: 382, loss is 0.06337421387434006\n",
      "epoch: 6 step: 383, loss is 0.008219925686717033\n",
      "epoch: 6 step: 384, loss is 0.12197865545749664\n",
      "epoch: 6 step: 385, loss is 0.0006002215668559074\n",
      "epoch: 6 step: 386, loss is 0.04601534083485603\n",
      "epoch: 6 step: 387, loss is 0.0111228683963418\n",
      "epoch: 6 step: 388, loss is 0.00042335677426308393\n",
      "epoch: 6 step: 389, loss is 0.06959231942892075\n",
      "epoch: 6 step: 390, loss is 0.005603324621915817\n",
      "epoch: 6 step: 391, loss is 0.0017039584927260876\n",
      "epoch: 6 step: 392, loss is 0.004249977879226208\n",
      "epoch: 6 step: 393, loss is 0.0067686839029192924\n",
      "epoch: 6 step: 394, loss is 0.08795152604579926\n",
      "epoch: 6 step: 395, loss is 0.0006734582129865885\n",
      "epoch: 6 step: 396, loss is 0.0008794291643425822\n",
      "epoch: 6 step: 397, loss is 0.0019055010052397847\n",
      "epoch: 6 step: 398, loss is 0.024969223886728287\n",
      "epoch: 6 step: 399, loss is 0.004246553406119347\n",
      "epoch: 6 step: 400, loss is 0.0008889817399904132\n",
      "epoch: 6 step: 401, loss is 0.00011743397044483572\n",
      "epoch: 6 step: 402, loss is 0.0013931677676737309\n",
      "epoch: 6 step: 403, loss is 0.0015391879715025425\n",
      "epoch: 6 step: 404, loss is 0.0020310550462454557\n",
      "epoch: 6 step: 405, loss is 0.0010562443640083075\n",
      "epoch: 6 step: 406, loss is 0.014899222180247307\n",
      "epoch: 6 step: 407, loss is 0.0031990520656108856\n",
      "epoch: 6 step: 408, loss is 0.04207847639918327\n",
      "epoch: 6 step: 409, loss is 0.04162832722067833\n",
      "epoch: 6 step: 410, loss is 0.004358711186796427\n",
      "epoch: 6 step: 411, loss is 0.0014763819053769112\n",
      "epoch: 6 step: 412, loss is 0.002456728368997574\n",
      "epoch: 6 step: 413, loss is 0.03839395195245743\n",
      "epoch: 6 step: 414, loss is 0.0006765122525393963\n",
      "epoch: 6 step: 415, loss is 0.05616569146513939\n",
      "epoch: 6 step: 416, loss is 0.029721809551119804\n",
      "epoch: 6 step: 417, loss is 0.00011569523485377431\n",
      "epoch: 6 step: 418, loss is 0.015621775761246681\n",
      "epoch: 6 step: 419, loss is 0.11286754906177521\n",
      "epoch: 6 step: 420, loss is 0.008507628925144672\n",
      "epoch: 6 step: 421, loss is 0.09223820269107819\n",
      "epoch: 6 step: 422, loss is 0.0014409614959731698\n",
      "epoch: 6 step: 423, loss is 0.0020013635512441397\n",
      "epoch: 6 step: 424, loss is 0.002054973505437374\n",
      "epoch: 6 step: 425, loss is 0.00037261185934767127\n",
      "epoch: 6 step: 426, loss is 0.0741485208272934\n",
      "epoch: 6 step: 427, loss is 0.015284652821719646\n",
      "epoch: 6 step: 428, loss is 0.00014362683577928692\n",
      "epoch: 6 step: 429, loss is 0.004338911734521389\n",
      "epoch: 6 step: 430, loss is 0.002406905870884657\n",
      "epoch: 6 step: 431, loss is 0.06364597380161285\n",
      "epoch: 6 step: 432, loss is 0.0006005709292367101\n",
      "epoch: 6 step: 433, loss is 0.003120503155514598\n",
      "epoch: 6 step: 434, loss is 0.0026665281038731337\n",
      "epoch: 6 step: 435, loss is 0.009984354488551617\n",
      "epoch: 6 step: 436, loss is 0.023500563576817513\n",
      "epoch: 6 step: 437, loss is 0.00015972914115991443\n",
      "epoch: 6 step: 438, loss is 0.11404203623533249\n",
      "epoch: 6 step: 439, loss is 0.0039673857390880585\n",
      "epoch: 6 step: 440, loss is 0.01117531768977642\n",
      "epoch: 6 step: 441, loss is 0.0037197896745055914\n",
      "epoch: 6 step: 442, loss is 0.025254854932427406\n",
      "epoch: 6 step: 443, loss is 0.1438637375831604\n",
      "epoch: 6 step: 444, loss is 0.0009376761736348271\n",
      "epoch: 6 step: 445, loss is 0.012081604450941086\n",
      "epoch: 6 step: 446, loss is 0.008120961487293243\n",
      "epoch: 6 step: 447, loss is 0.037120308727025986\n",
      "epoch: 6 step: 448, loss is 0.001825556973926723\n",
      "epoch: 6 step: 449, loss is 0.0019766914192587137\n",
      "epoch: 6 step: 450, loss is 0.00016935018356889486\n",
      "epoch: 6 step: 451, loss is 0.11965715885162354\n",
      "epoch: 6 step: 452, loss is 0.0029956584330648184\n",
      "epoch: 6 step: 453, loss is 0.0009002878214232624\n",
      "epoch: 6 step: 454, loss is 0.14689511060714722\n",
      "epoch: 6 step: 455, loss is 0.022228611633181572\n",
      "epoch: 6 step: 456, loss is 0.0006981041515246034\n",
      "epoch: 6 step: 457, loss is 0.0005532684735953808\n",
      "epoch: 6 step: 458, loss is 0.008291623555123806\n",
      "epoch: 6 step: 459, loss is 0.016587942838668823\n",
      "epoch: 6 step: 460, loss is 0.004047070164233446\n",
      "epoch: 6 step: 461, loss is 0.14436563849449158\n",
      "epoch: 6 step: 462, loss is 0.010733733884990215\n",
      "epoch: 6 step: 463, loss is 0.013564016669988632\n",
      "epoch: 6 step: 464, loss is 0.00036794025800190866\n",
      "epoch: 6 step: 465, loss is 0.0008234607521444559\n",
      "epoch: 6 step: 466, loss is 0.020587611943483353\n",
      "epoch: 6 step: 467, loss is 0.003673757892102003\n",
      "epoch: 6 step: 468, loss is 0.027837971225380898\n",
      "epoch: 6 step: 469, loss is 0.001738017424941063\n",
      "epoch: 6 step: 470, loss is 0.017161477357149124\n",
      "epoch: 6 step: 471, loss is 0.033083103597164154\n",
      "epoch: 6 step: 472, loss is 0.01942283846437931\n",
      "epoch: 6 step: 473, loss is 3.5340308386366814e-05\n",
      "epoch: 6 step: 474, loss is 0.0004273573576938361\n",
      "epoch: 6 step: 475, loss is 0.000432004890171811\n",
      "epoch: 6 step: 476, loss is 0.0002720640623010695\n",
      "epoch: 6 step: 477, loss is 0.09888971596956253\n",
      "epoch: 6 step: 478, loss is 0.0789233148097992\n",
      "epoch: 6 step: 479, loss is 0.12495319545269012\n",
      "epoch: 6 step: 480, loss is 0.0003625520912464708\n",
      "epoch: 6 step: 481, loss is 0.0003360960981808603\n",
      "epoch: 6 step: 482, loss is 0.00013776871492154896\n",
      "epoch: 6 step: 483, loss is 0.0017159590497612953\n",
      "epoch: 6 step: 484, loss is 0.009669274091720581\n",
      "epoch: 6 step: 485, loss is 0.016742659732699394\n",
      "epoch: 6 step: 486, loss is 0.0028272029012441635\n",
      "epoch: 6 step: 487, loss is 0.0024529180955141783\n",
      "epoch: 6 step: 488, loss is 0.0009004849125631154\n",
      "epoch: 6 step: 489, loss is 0.00023129636247176677\n",
      "epoch: 6 step: 490, loss is 0.07608489692211151\n",
      "epoch: 6 step: 491, loss is 0.0015919636934995651\n",
      "epoch: 6 step: 492, loss is 0.007831373251974583\n",
      "epoch: 6 step: 493, loss is 0.004726969636976719\n",
      "epoch: 6 step: 494, loss is 0.0004998756339773536\n",
      "epoch: 6 step: 495, loss is 0.002446457976475358\n",
      "epoch: 6 step: 496, loss is 0.006552268285304308\n",
      "epoch: 6 step: 497, loss is 0.004347614943981171\n",
      "epoch: 6 step: 498, loss is 0.16983844339847565\n",
      "epoch: 6 step: 499, loss is 0.13057953119277954\n",
      "epoch: 6 step: 500, loss is 0.0007117475615814328\n",
      "epoch: 6 step: 501, loss is 0.008537737652659416\n",
      "epoch: 6 step: 502, loss is 0.020268820226192474\n",
      "epoch: 6 step: 503, loss is 0.13201741874217987\n",
      "epoch: 6 step: 504, loss is 0.04273683950304985\n",
      "epoch: 6 step: 505, loss is 0.00023354617587756366\n",
      "epoch: 6 step: 506, loss is 0.04448568448424339\n",
      "epoch: 6 step: 507, loss is 0.0006089538801461458\n",
      "epoch: 6 step: 508, loss is 0.00044043094385415316\n",
      "epoch: 6 step: 509, loss is 0.009652643464505672\n",
      "epoch: 6 step: 510, loss is 0.00021477334666997194\n",
      "epoch: 6 step: 511, loss is 0.006879253312945366\n",
      "epoch: 6 step: 512, loss is 0.10581112653017044\n",
      "epoch: 6 step: 513, loss is 0.0009232328156940639\n",
      "epoch: 6 step: 514, loss is 0.15052387118339539\n",
      "epoch: 6 step: 515, loss is 0.003133139805868268\n",
      "epoch: 6 step: 516, loss is 0.0020393114537000656\n",
      "epoch: 6 step: 517, loss is 0.0034863706678152084\n",
      "epoch: 6 step: 518, loss is 0.007264852058142424\n",
      "epoch: 6 step: 519, loss is 0.004084347747266293\n",
      "epoch: 6 step: 520, loss is 0.002683487720787525\n",
      "epoch: 6 step: 521, loss is 0.001462198793888092\n",
      "epoch: 6 step: 522, loss is 0.00261112442240119\n",
      "epoch: 6 step: 523, loss is 0.019946308806538582\n",
      "epoch: 6 step: 524, loss is 0.014164648950099945\n",
      "epoch: 6 step: 525, loss is 0.013019447214901447\n",
      "epoch: 6 step: 526, loss is 0.0003040888113901019\n",
      "epoch: 6 step: 527, loss is 0.008786925114691257\n",
      "epoch: 6 step: 528, loss is 0.03771859034895897\n",
      "epoch: 6 step: 529, loss is 0.010618031956255436\n",
      "epoch: 6 step: 530, loss is 0.08758321404457092\n",
      "epoch: 6 step: 531, loss is 0.04809308797121048\n",
      "epoch: 6 step: 532, loss is 0.0011793625308200717\n",
      "epoch: 6 step: 533, loss is 0.006828805897384882\n",
      "epoch: 6 step: 534, loss is 0.0008546484168618917\n",
      "epoch: 6 step: 535, loss is 0.052606239914894104\n",
      "epoch: 6 step: 536, loss is 0.028180764988064766\n",
      "epoch: 6 step: 537, loss is 0.0009052291861735284\n",
      "epoch: 6 step: 538, loss is 0.004779063165187836\n",
      "epoch: 6 step: 539, loss is 0.006094146519899368\n",
      "epoch: 6 step: 540, loss is 0.00014711436233483255\n",
      "epoch: 6 step: 541, loss is 0.003951516468077898\n",
      "epoch: 6 step: 542, loss is 0.015410334803164005\n",
      "epoch: 6 step: 543, loss is 0.3189068138599396\n",
      "epoch: 6 step: 544, loss is 0.0007302768062800169\n",
      "epoch: 6 step: 545, loss is 0.00022220004757400602\n",
      "epoch: 6 step: 546, loss is 0.00954353716224432\n",
      "epoch: 6 step: 547, loss is 0.001401955378241837\n",
      "epoch: 6 step: 548, loss is 0.001764047658070922\n",
      "epoch: 6 step: 549, loss is 0.0011268921662122011\n",
      "epoch: 6 step: 550, loss is 0.006572210695594549\n",
      "epoch: 6 step: 551, loss is 0.010853919200599194\n",
      "epoch: 6 step: 552, loss is 0.0037168539129197598\n",
      "epoch: 6 step: 553, loss is 0.0019130476284772158\n",
      "epoch: 6 step: 554, loss is 0.0002131061046384275\n",
      "epoch: 6 step: 555, loss is 0.0005332138971425593\n",
      "epoch: 6 step: 556, loss is 0.048664890229701996\n",
      "epoch: 6 step: 557, loss is 0.0011179738212376833\n",
      "epoch: 6 step: 558, loss is 0.007379502058029175\n",
      "epoch: 6 step: 559, loss is 0.004377510864287615\n",
      "epoch: 6 step: 560, loss is 0.027379537001252174\n",
      "epoch: 6 step: 561, loss is 0.03592447564005852\n",
      "epoch: 6 step: 562, loss is 0.020223861560225487\n",
      "epoch: 6 step: 563, loss is 0.022860364988446236\n",
      "epoch: 6 step: 564, loss is 0.010452422313392162\n",
      "epoch: 6 step: 565, loss is 0.03323087841272354\n",
      "epoch: 6 step: 566, loss is 0.011520697735249996\n",
      "epoch: 6 step: 567, loss is 0.0026587420143187046\n",
      "epoch: 6 step: 568, loss is 0.005002074874937534\n",
      "epoch: 6 step: 569, loss is 0.0012064216425642371\n",
      "epoch: 6 step: 570, loss is 0.0029978295788168907\n",
      "epoch: 6 step: 571, loss is 0.02472139336168766\n",
      "epoch: 6 step: 572, loss is 0.08649422228336334\n",
      "epoch: 6 step: 573, loss is 0.0009042634628713131\n",
      "epoch: 6 step: 574, loss is 0.00841317605227232\n",
      "epoch: 6 step: 575, loss is 0.0060134949162602425\n",
      "epoch: 6 step: 576, loss is 0.0003998003958258778\n",
      "epoch: 6 step: 577, loss is 0.26614663004875183\n",
      "epoch: 6 step: 578, loss is 0.0021369527094066143\n",
      "epoch: 6 step: 579, loss is 0.0030261471401900053\n",
      "epoch: 6 step: 580, loss is 0.016854597255587578\n",
      "epoch: 6 step: 581, loss is 0.004050750285387039\n",
      "epoch: 6 step: 582, loss is 0.0002803174720611423\n",
      "epoch: 6 step: 583, loss is 0.002583714434877038\n",
      "epoch: 6 step: 584, loss is 0.0037326267920434475\n",
      "epoch: 6 step: 585, loss is 0.00038599406252615154\n",
      "epoch: 6 step: 586, loss is 0.0024199658073484898\n",
      "epoch: 6 step: 587, loss is 0.0027806470170617104\n",
      "epoch: 6 step: 588, loss is 0.07037259638309479\n",
      "epoch: 6 step: 589, loss is 0.0014593569794669747\n",
      "epoch: 6 step: 590, loss is 0.11296810209751129\n",
      "epoch: 6 step: 591, loss is 0.06725304573774338\n",
      "epoch: 6 step: 592, loss is 0.09606543928384781\n",
      "epoch: 6 step: 593, loss is 0.020896712318062782\n",
      "epoch: 6 step: 594, loss is 0.0007290077046491206\n",
      "epoch: 6 step: 595, loss is 0.0022957208566367626\n",
      "epoch: 6 step: 596, loss is 0.02850937843322754\n",
      "epoch: 6 step: 597, loss is 0.03516462445259094\n",
      "epoch: 6 step: 598, loss is 0.08334279805421829\n",
      "epoch: 6 step: 599, loss is 0.0003941341128665954\n",
      "epoch: 6 step: 600, loss is 0.0018366820877417922\n",
      "epoch: 6 step: 601, loss is 0.00155240623280406\n",
      "epoch: 6 step: 602, loss is 0.00011236916907364503\n",
      "epoch: 6 step: 603, loss is 0.18122778832912445\n",
      "epoch: 6 step: 604, loss is 0.0011323185171931982\n",
      "epoch: 6 step: 605, loss is 0.0005928633618168533\n",
      "epoch: 6 step: 606, loss is 0.02899600937962532\n",
      "epoch: 6 step: 607, loss is 0.002399531891569495\n",
      "epoch: 6 step: 608, loss is 0.0025630963500589132\n",
      "epoch: 6 step: 609, loss is 0.008253732696175575\n",
      "epoch: 6 step: 610, loss is 0.007381947711110115\n",
      "epoch: 6 step: 611, loss is 0.0021721378434449434\n",
      "epoch: 6 step: 612, loss is 0.0007248810725286603\n",
      "epoch: 6 step: 613, loss is 0.08564431965351105\n",
      "epoch: 6 step: 614, loss is 0.04614265635609627\n",
      "epoch: 6 step: 615, loss is 0.006294195540249348\n",
      "epoch: 6 step: 616, loss is 0.00039586651837453246\n",
      "epoch: 6 step: 617, loss is 0.06503687053918839\n",
      "epoch: 6 step: 618, loss is 0.0032379610929638147\n",
      "epoch: 6 step: 619, loss is 0.0028040558099746704\n",
      "epoch: 6 step: 620, loss is 0.001475050114095211\n",
      "epoch: 6 step: 621, loss is 0.07488241046667099\n",
      "epoch: 6 step: 622, loss is 0.028585314750671387\n",
      "epoch: 6 step: 623, loss is 0.0047506834380328655\n",
      "epoch: 6 step: 624, loss is 0.00210561603307724\n",
      "epoch: 6 step: 625, loss is 0.017617356032133102\n",
      "epoch: 6 step: 626, loss is 0.00028031031251884997\n",
      "epoch: 6 step: 627, loss is 0.012822045013308525\n",
      "epoch: 6 step: 628, loss is 0.01376487035304308\n",
      "epoch: 6 step: 629, loss is 0.006152925081551075\n",
      "epoch: 6 step: 630, loss is 0.0002694855211302638\n",
      "epoch: 6 step: 631, loss is 0.007906529121100903\n",
      "epoch: 6 step: 632, loss is 0.00808150228112936\n",
      "epoch: 6 step: 633, loss is 0.00035220838617533445\n",
      "epoch: 6 step: 634, loss is 0.005499657243490219\n",
      "epoch: 6 step: 635, loss is 0.0007092578453011811\n",
      "epoch: 6 step: 636, loss is 0.0031445291824638844\n",
      "epoch: 6 step: 637, loss is 0.02152620069682598\n",
      "epoch: 6 step: 638, loss is 0.010958175174891949\n",
      "epoch: 6 step: 639, loss is 0.003112142439931631\n",
      "epoch: 6 step: 640, loss is 0.02117418870329857\n",
      "epoch: 6 step: 641, loss is 0.005000061355531216\n",
      "epoch: 6 step: 642, loss is 0.007248384412378073\n",
      "epoch: 6 step: 643, loss is 0.0048286388628184795\n",
      "epoch: 6 step: 644, loss is 0.007568805478513241\n",
      "epoch: 6 step: 645, loss is 0.001737332670018077\n",
      "epoch: 6 step: 646, loss is 0.011755690909922123\n",
      "epoch: 6 step: 647, loss is 0.01946290396153927\n",
      "epoch: 6 step: 648, loss is 0.016484975814819336\n",
      "epoch: 6 step: 649, loss is 0.05833332613110542\n",
      "epoch: 6 step: 650, loss is 0.0028800053987652063\n",
      "epoch: 6 step: 651, loss is 0.0011546311434358358\n",
      "epoch: 6 step: 652, loss is 0.005617409944534302\n",
      "epoch: 6 step: 653, loss is 0.017438339069485664\n",
      "epoch: 6 step: 654, loss is 2.9371693017310463e-05\n",
      "epoch: 6 step: 655, loss is 0.017772674560546875\n",
      "epoch: 6 step: 656, loss is 0.00011487231677165255\n",
      "epoch: 6 step: 657, loss is 0.0017961207777261734\n",
      "epoch: 6 step: 658, loss is 0.002966243075206876\n",
      "epoch: 6 step: 659, loss is 0.000305246067000553\n",
      "epoch: 6 step: 660, loss is 0.0013755898689851165\n",
      "epoch: 6 step: 661, loss is 0.005757448263466358\n",
      "epoch: 6 step: 662, loss is 0.0007383341435343027\n",
      "epoch: 6 step: 663, loss is 0.0004787214857060462\n",
      "epoch: 6 step: 664, loss is 0.0032338460441678762\n",
      "epoch: 6 step: 665, loss is 0.13456299901008606\n",
      "epoch: 6 step: 666, loss is 0.020742349326610565\n",
      "epoch: 6 step: 667, loss is 0.014832375571131706\n",
      "epoch: 6 step: 668, loss is 0.0026355956215411425\n",
      "epoch: 6 step: 669, loss is 0.0006000726716592908\n",
      "epoch: 6 step: 670, loss is 0.009321891702711582\n",
      "epoch: 6 step: 671, loss is 0.0005285850493237376\n",
      "epoch: 6 step: 672, loss is 0.0007742384332232177\n",
      "epoch: 6 step: 673, loss is 0.03974407911300659\n",
      "epoch: 6 step: 674, loss is 0.1307424008846283\n",
      "epoch: 6 step: 675, loss is 0.007529223337769508\n",
      "epoch: 6 step: 676, loss is 0.004546503070741892\n",
      "epoch: 6 step: 677, loss is 0.0005587637424468994\n",
      "epoch: 6 step: 678, loss is 0.0051711201667785645\n",
      "epoch: 6 step: 679, loss is 9.880237485049292e-05\n",
      "epoch: 6 step: 680, loss is 0.0028792747762054205\n",
      "epoch: 6 step: 681, loss is 0.04110240936279297\n",
      "epoch: 6 step: 682, loss is 0.0002811848826240748\n",
      "epoch: 6 step: 683, loss is 0.0007081252406351268\n",
      "epoch: 6 step: 684, loss is 0.003268720116466284\n",
      "epoch: 6 step: 685, loss is 0.009736347943544388\n",
      "epoch: 6 step: 686, loss is 0.018849067389965057\n",
      "epoch: 6 step: 687, loss is 0.052672211080789566\n",
      "epoch: 6 step: 688, loss is 0.000278831081232056\n",
      "epoch: 6 step: 689, loss is 0.002608714858070016\n",
      "epoch: 6 step: 690, loss is 0.007537009660154581\n",
      "epoch: 6 step: 691, loss is 0.0007270898204296827\n",
      "epoch: 6 step: 692, loss is 0.0007255985983647406\n",
      "epoch: 6 step: 693, loss is 0.014603926800191402\n",
      "epoch: 6 step: 694, loss is 0.03314732015132904\n",
      "epoch: 6 step: 695, loss is 0.0004999517113901675\n",
      "epoch: 6 step: 696, loss is 0.0039405301213264465\n",
      "epoch: 6 step: 697, loss is 0.00955484714359045\n",
      "epoch: 6 step: 698, loss is 0.03757374733686447\n",
      "epoch: 6 step: 699, loss is 0.004220464266836643\n",
      "epoch: 6 step: 700, loss is 2.6749708922579885e-05\n",
      "epoch: 6 step: 701, loss is 0.12573863565921783\n",
      "epoch: 6 step: 702, loss is 0.0005367151461541653\n",
      "epoch: 6 step: 703, loss is 0.0002575100807007402\n",
      "epoch: 6 step: 704, loss is 0.00036731467116624117\n",
      "epoch: 6 step: 705, loss is 0.005546686705201864\n",
      "epoch: 6 step: 706, loss is 0.014792215079069138\n",
      "epoch: 6 step: 707, loss is 4.863262438448146e-05\n",
      "epoch: 6 step: 708, loss is 0.004639030899852514\n",
      "epoch: 6 step: 709, loss is 0.04387865960597992\n",
      "epoch: 6 step: 710, loss is 0.00022314504894893616\n",
      "epoch: 6 step: 711, loss is 0.0023950408212840557\n",
      "epoch: 6 step: 712, loss is 0.0010744965402409434\n",
      "epoch: 6 step: 713, loss is 0.0010246160672977567\n",
      "epoch: 6 step: 714, loss is 0.003363438881933689\n",
      "epoch: 6 step: 715, loss is 0.036139097064733505\n",
      "epoch: 6 step: 716, loss is 0.07147998362779617\n",
      "epoch: 6 step: 717, loss is 0.0726352334022522\n",
      "epoch: 6 step: 718, loss is 0.004790517035871744\n",
      "epoch: 6 step: 719, loss is 0.003947402350604534\n",
      "epoch: 6 step: 720, loss is 0.0714273452758789\n",
      "epoch: 6 step: 721, loss is 0.001157052582129836\n",
      "epoch: 6 step: 722, loss is 0.00012576246808748692\n",
      "epoch: 6 step: 723, loss is 0.033409178256988525\n",
      "epoch: 6 step: 724, loss is 0.00026697019347921014\n",
      "epoch: 6 step: 725, loss is 0.0004981944221071899\n",
      "epoch: 6 step: 726, loss is 0.0007546244887635112\n",
      "epoch: 6 step: 727, loss is 0.0006122160702943802\n",
      "epoch: 6 step: 728, loss is 0.0002832789905369282\n",
      "epoch: 6 step: 729, loss is 2.9621201974805444e-05\n",
      "epoch: 6 step: 730, loss is 0.006136751268059015\n",
      "epoch: 6 step: 731, loss is 2.1303088942659087e-05\n",
      "epoch: 6 step: 732, loss is 0.00010091796139022335\n",
      "epoch: 6 step: 733, loss is 0.006897378712892532\n",
      "epoch: 6 step: 734, loss is 0.0006651804433204234\n",
      "epoch: 6 step: 735, loss is 0.07980875670909882\n",
      "epoch: 6 step: 736, loss is 0.001457270118407905\n",
      "epoch: 6 step: 737, loss is 0.09610636532306671\n",
      "epoch: 6 step: 738, loss is 0.00021057826234027743\n",
      "epoch: 6 step: 739, loss is 0.0029641410801559687\n",
      "epoch: 6 step: 740, loss is 0.004117593634873629\n",
      "epoch: 6 step: 741, loss is 0.0013617341173812747\n",
      "epoch: 6 step: 742, loss is 0.0008055265061557293\n",
      "epoch: 6 step: 743, loss is 0.003616123693063855\n",
      "epoch: 6 step: 744, loss is 0.0015444005839526653\n",
      "epoch: 6 step: 745, loss is 0.009806625545024872\n",
      "epoch: 6 step: 746, loss is 0.01100607868283987\n",
      "epoch: 6 step: 747, loss is 0.0011231107637286186\n",
      "epoch: 6 step: 748, loss is 0.042314693331718445\n",
      "epoch: 6 step: 749, loss is 0.033681388944387436\n",
      "epoch: 6 step: 750, loss is 0.003749322611838579\n",
      "epoch: 6 step: 751, loss is 0.04969150945544243\n",
      "epoch: 6 step: 752, loss is 0.0024333440233021975\n",
      "epoch: 6 step: 753, loss is 0.00014569306222256273\n",
      "epoch: 6 step: 754, loss is 0.05180663987994194\n",
      "epoch: 6 step: 755, loss is 0.005165009293705225\n",
      "epoch: 6 step: 756, loss is 0.0018136012367904186\n",
      "epoch: 6 step: 757, loss is 0.0010291815269738436\n",
      "epoch: 6 step: 758, loss is 0.013851936906576157\n",
      "epoch: 6 step: 759, loss is 0.08525170385837555\n",
      "epoch: 6 step: 760, loss is 5.106745447847061e-05\n",
      "epoch: 6 step: 761, loss is 8.736785093788058e-05\n",
      "epoch: 6 step: 762, loss is 0.026111848652362823\n",
      "epoch: 6 step: 763, loss is 0.003943504299968481\n",
      "epoch: 6 step: 764, loss is 0.0007056322647258639\n",
      "epoch: 6 step: 765, loss is 0.05440840125083923\n",
      "epoch: 6 step: 766, loss is 0.004384350031614304\n",
      "epoch: 6 step: 767, loss is 0.007980920374393463\n",
      "epoch: 6 step: 768, loss is 0.0006649181013926864\n",
      "epoch: 6 step: 769, loss is 0.008509524166584015\n",
      "epoch: 6 step: 770, loss is 0.06616342812776566\n",
      "epoch: 6 step: 771, loss is 3.234122777939774e-05\n",
      "epoch: 6 step: 772, loss is 0.00040698295924812555\n",
      "epoch: 6 step: 773, loss is 0.0007694422965869308\n",
      "epoch: 6 step: 774, loss is 0.0003656089829746634\n",
      "epoch: 6 step: 775, loss is 0.004000912420451641\n",
      "epoch: 6 step: 776, loss is 0.01825779862701893\n",
      "epoch: 6 step: 777, loss is 0.001168321818113327\n",
      "epoch: 6 step: 778, loss is 0.0022964656818658113\n",
      "epoch: 6 step: 779, loss is 0.00011933779751416296\n",
      "epoch: 6 step: 780, loss is 0.0029373245779424906\n",
      "epoch: 6 step: 781, loss is 0.0002279664040543139\n",
      "epoch: 6 step: 782, loss is 0.02247118577361107\n",
      "epoch: 6 step: 783, loss is 0.0003305770514998585\n",
      "epoch: 6 step: 784, loss is 0.04734589532017708\n",
      "epoch: 6 step: 785, loss is 0.2893780469894409\n",
      "epoch: 6 step: 786, loss is 0.31925007700920105\n",
      "epoch: 6 step: 787, loss is 0.014760264195501804\n",
      "epoch: 6 step: 788, loss is 0.04444722458720207\n",
      "epoch: 6 step: 789, loss is 0.09893521666526794\n",
      "epoch: 6 step: 790, loss is 0.004387488588690758\n",
      "epoch: 6 step: 791, loss is 0.18735463917255402\n",
      "epoch: 6 step: 792, loss is 0.00913158617913723\n",
      "epoch: 6 step: 793, loss is 0.04007411375641823\n",
      "epoch: 6 step: 794, loss is 0.00044144876301288605\n",
      "epoch: 6 step: 795, loss is 0.016066595911979675\n",
      "epoch: 6 step: 796, loss is 0.0005008370499126613\n",
      "epoch: 6 step: 797, loss is 0.0602988637983799\n",
      "epoch: 6 step: 798, loss is 0.02566726692020893\n",
      "epoch: 6 step: 799, loss is 0.004133595619350672\n",
      "epoch: 6 step: 800, loss is 0.04224808141589165\n",
      "epoch: 6 step: 801, loss is 0.004112579394131899\n",
      "epoch: 6 step: 802, loss is 0.0007370359380729496\n",
      "epoch: 6 step: 803, loss is 0.035932544618844986\n",
      "epoch: 6 step: 804, loss is 0.0058702765963971615\n",
      "epoch: 6 step: 805, loss is 0.005994775798171759\n",
      "epoch: 6 step: 806, loss is 0.00593789154663682\n",
      "epoch: 6 step: 807, loss is 0.11635289341211319\n",
      "epoch: 6 step: 808, loss is 0.0033859005197882652\n",
      "epoch: 6 step: 809, loss is 0.04779697582125664\n",
      "epoch: 6 step: 810, loss is 0.0004883757792413235\n",
      "epoch: 6 step: 811, loss is 0.138876810669899\n",
      "epoch: 6 step: 812, loss is 0.0002826617856044322\n",
      "epoch: 6 step: 813, loss is 0.007852593436837196\n",
      "epoch: 6 step: 814, loss is 0.004294070415198803\n",
      "epoch: 6 step: 815, loss is 0.02534347213804722\n",
      "epoch: 6 step: 816, loss is 0.021412499248981476\n",
      "epoch: 6 step: 817, loss is 0.0006518797599710524\n",
      "epoch: 6 step: 818, loss is 0.0008043504785746336\n",
      "epoch: 6 step: 819, loss is 0.05293187499046326\n",
      "epoch: 6 step: 820, loss is 0.016713842749595642\n",
      "epoch: 6 step: 821, loss is 0.0003745797148440033\n",
      "epoch: 6 step: 822, loss is 0.07933470606803894\n",
      "epoch: 6 step: 823, loss is 0.004186942707747221\n",
      "epoch: 6 step: 824, loss is 0.05182588845491409\n",
      "epoch: 6 step: 825, loss is 0.0007948644924908876\n",
      "epoch: 6 step: 826, loss is 0.0009261580416932702\n",
      "epoch: 6 step: 827, loss is 0.0004993517068214715\n",
      "epoch: 6 step: 828, loss is 0.0024730998557060957\n",
      "epoch: 6 step: 829, loss is 0.008281945250928402\n",
      "epoch: 6 step: 830, loss is 0.0010257307440042496\n",
      "epoch: 6 step: 831, loss is 0.0007204300491139293\n",
      "epoch: 6 step: 832, loss is 0.02140888012945652\n",
      "epoch: 6 step: 833, loss is 0.01491782907396555\n",
      "epoch: 6 step: 834, loss is 0.002999810967594385\n",
      "epoch: 6 step: 835, loss is 0.04008585959672928\n",
      "epoch: 6 step: 836, loss is 0.008592173457145691\n",
      "epoch: 6 step: 837, loss is 0.05413049831986427\n",
      "epoch: 6 step: 838, loss is 0.006673046853393316\n",
      "epoch: 6 step: 839, loss is 0.008419260382652283\n",
      "epoch: 6 step: 840, loss is 0.005394330248236656\n",
      "epoch: 6 step: 841, loss is 8.989007619675249e-05\n",
      "epoch: 6 step: 842, loss is 0.013230535201728344\n",
      "epoch: 6 step: 843, loss is 0.21029001474380493\n",
      "epoch: 6 step: 844, loss is 0.004803949501365423\n",
      "epoch: 6 step: 845, loss is 0.0008812960586510599\n",
      "epoch: 6 step: 846, loss is 0.02181321755051613\n",
      "epoch: 6 step: 847, loss is 0.005319149233400822\n",
      "epoch: 6 step: 848, loss is 0.011128462851047516\n",
      "epoch: 6 step: 849, loss is 0.007538112811744213\n",
      "epoch: 6 step: 850, loss is 0.019349634647369385\n",
      "epoch: 6 step: 851, loss is 0.003278797958046198\n",
      "epoch: 6 step: 852, loss is 0.08072193711996078\n",
      "epoch: 6 step: 853, loss is 7.18264200259e-05\n",
      "epoch: 6 step: 854, loss is 0.03616785258054733\n",
      "epoch: 6 step: 855, loss is 0.0028754947707057\n",
      "epoch: 6 step: 856, loss is 0.0031059186439961195\n",
      "epoch: 6 step: 857, loss is 0.008820722810924053\n",
      "epoch: 6 step: 858, loss is 0.012214604765176773\n",
      "epoch: 6 step: 859, loss is 0.0007681483984924853\n",
      "epoch: 6 step: 860, loss is 0.0010658320970833302\n",
      "epoch: 6 step: 861, loss is 0.031034275889396667\n",
      "epoch: 6 step: 862, loss is 0.004380326252430677\n",
      "epoch: 6 step: 863, loss is 0.0015345150604844093\n",
      "epoch: 6 step: 864, loss is 0.03680972754955292\n",
      "epoch: 6 step: 865, loss is 0.00443818187341094\n",
      "epoch: 6 step: 866, loss is 0.0040139551274478436\n",
      "epoch: 6 step: 867, loss is 0.0037607839331030846\n",
      "epoch: 6 step: 868, loss is 0.0005813778843730688\n",
      "epoch: 6 step: 869, loss is 0.06465791910886765\n",
      "epoch: 6 step: 870, loss is 0.00047010311391204596\n",
      "epoch: 6 step: 871, loss is 0.0006717088399454951\n",
      "epoch: 6 step: 872, loss is 0.0032981305848807096\n",
      "epoch: 6 step: 873, loss is 0.003703592112287879\n",
      "epoch: 6 step: 874, loss is 0.005229430738836527\n",
      "epoch: 6 step: 875, loss is 0.06338264793157578\n",
      "epoch: 6 step: 876, loss is 6.759432289982215e-05\n",
      "epoch: 6 step: 877, loss is 0.0666775330901146\n",
      "epoch: 6 step: 878, loss is 0.1323705017566681\n",
      "epoch: 6 step: 879, loss is 0.0005979955312795937\n",
      "epoch: 6 step: 880, loss is 0.0588005855679512\n",
      "epoch: 6 step: 881, loss is 0.0030942976009100676\n",
      "epoch: 6 step: 882, loss is 0.030268754810094833\n",
      "epoch: 6 step: 883, loss is 0.00017212089733220637\n",
      "epoch: 6 step: 884, loss is 0.0009065093472599983\n",
      "epoch: 6 step: 885, loss is 0.012581497430801392\n",
      "epoch: 6 step: 886, loss is 0.08745861798524857\n",
      "epoch: 6 step: 887, loss is 0.034717313945293427\n",
      "epoch: 6 step: 888, loss is 0.0025542438961565495\n",
      "epoch: 6 step: 889, loss is 0.013094954192638397\n",
      "epoch: 6 step: 890, loss is 0.0006399265839718282\n",
      "epoch: 6 step: 891, loss is 0.0006638625636696815\n",
      "epoch: 6 step: 892, loss is 0.00022326996258925647\n",
      "epoch: 6 step: 893, loss is 0.0005909389583393931\n",
      "epoch: 6 step: 894, loss is 0.0002575941034592688\n",
      "epoch: 6 step: 895, loss is 0.04782160744071007\n",
      "epoch: 6 step: 896, loss is 0.0005724381189793348\n",
      "epoch: 6 step: 897, loss is 0.0046312930062413216\n",
      "epoch: 6 step: 898, loss is 0.009922097437083721\n",
      "epoch: 6 step: 899, loss is 0.003127428935840726\n",
      "epoch: 6 step: 900, loss is 0.004617435857653618\n",
      "epoch: 6 step: 901, loss is 0.003958035726100206\n",
      "epoch: 6 step: 902, loss is 0.005068412981927395\n",
      "epoch: 6 step: 903, loss is 0.18178389966487885\n",
      "epoch: 6 step: 904, loss is 0.002894562901929021\n",
      "epoch: 6 step: 905, loss is 0.1316641867160797\n",
      "epoch: 6 step: 906, loss is 0.10135642439126968\n",
      "epoch: 6 step: 907, loss is 0.004927157890051603\n",
      "epoch: 6 step: 908, loss is 0.0016280388226732612\n",
      "epoch: 6 step: 909, loss is 0.004856320098042488\n",
      "epoch: 6 step: 910, loss is 0.015524237416684628\n",
      "epoch: 6 step: 911, loss is 0.1447933465242386\n",
      "epoch: 6 step: 912, loss is 0.004091895651072264\n",
      "epoch: 6 step: 913, loss is 0.0062597328796982765\n",
      "epoch: 6 step: 914, loss is 0.0300113745033741\n",
      "epoch: 6 step: 915, loss is 0.16586504876613617\n",
      "epoch: 6 step: 916, loss is 0.0007447447278536856\n",
      "epoch: 6 step: 917, loss is 0.007819104008376598\n",
      "epoch: 6 step: 918, loss is 0.09638965129852295\n",
      "epoch: 6 step: 919, loss is 0.05401666834950447\n",
      "epoch: 6 step: 920, loss is 0.15468527376651764\n",
      "epoch: 6 step: 921, loss is 0.031774215400218964\n",
      "epoch: 6 step: 922, loss is 0.023653311654925346\n",
      "epoch: 6 step: 923, loss is 0.016651425510644913\n",
      "epoch: 6 step: 924, loss is 0.0032816757448017597\n",
      "epoch: 6 step: 925, loss is 0.014929654076695442\n",
      "epoch: 6 step: 926, loss is 0.005386527627706528\n",
      "epoch: 6 step: 927, loss is 0.018678942695260048\n",
      "epoch: 6 step: 928, loss is 0.12747400999069214\n",
      "epoch: 6 step: 929, loss is 0.11022596806287766\n",
      "epoch: 6 step: 930, loss is 0.02571260929107666\n",
      "epoch: 6 step: 931, loss is 0.055753275752067566\n",
      "epoch: 6 step: 932, loss is 0.00556156737729907\n",
      "epoch: 6 step: 933, loss is 0.0009039733558893204\n",
      "epoch: 6 step: 934, loss is 0.011515568010509014\n",
      "epoch: 6 step: 935, loss is 0.0009072553948499262\n",
      "epoch: 6 step: 936, loss is 0.0014693585690110922\n",
      "epoch: 6 step: 937, loss is 0.0012881549773737788\n",
      "epoch: 6 step: 938, loss is 0.008678709156811237\n",
      "epoch: 6 step: 939, loss is 0.029178772121667862\n",
      "epoch: 6 step: 940, loss is 0.0036080803256481886\n",
      "epoch: 6 step: 941, loss is 0.02648320235311985\n",
      "epoch: 6 step: 942, loss is 0.0008308818796649575\n",
      "epoch: 6 step: 943, loss is 0.004313732497394085\n",
      "epoch: 6 step: 944, loss is 0.01721956767141819\n",
      "epoch: 6 step: 945, loss is 0.0012213747249916196\n",
      "epoch: 6 step: 946, loss is 0.027095064520835876\n",
      "epoch: 6 step: 947, loss is 0.023158442229032516\n",
      "epoch: 6 step: 948, loss is 0.00991071667522192\n",
      "epoch: 6 step: 949, loss is 0.011027405969798565\n",
      "epoch: 6 step: 950, loss is 0.001436620601452887\n",
      "epoch: 6 step: 951, loss is 0.004375686403363943\n",
      "epoch: 6 step: 952, loss is 0.002198379021137953\n",
      "epoch: 6 step: 953, loss is 0.0009288603323511779\n",
      "epoch: 6 step: 954, loss is 0.0003964497009292245\n",
      "epoch: 6 step: 955, loss is 0.03587440773844719\n",
      "epoch: 6 step: 956, loss is 0.0031931172125041485\n",
      "epoch: 6 step: 957, loss is 0.030960768461227417\n",
      "epoch: 6 step: 958, loss is 0.0008959349943324924\n",
      "epoch: 6 step: 959, loss is 0.04887193441390991\n",
      "epoch: 6 step: 960, loss is 0.0011043472914025187\n",
      "epoch: 6 step: 961, loss is 0.0029473653994500637\n",
      "epoch: 6 step: 962, loss is 0.0008817452471703291\n",
      "epoch: 6 step: 963, loss is 0.04349619150161743\n",
      "epoch: 6 step: 964, loss is 0.004366720095276833\n",
      "epoch: 6 step: 965, loss is 0.002194334752857685\n",
      "epoch: 6 step: 966, loss is 0.003211787436157465\n",
      "epoch: 6 step: 967, loss is 0.022960301488637924\n",
      "epoch: 6 step: 968, loss is 2.1684350940631703e-05\n",
      "epoch: 6 step: 969, loss is 0.0001236582756973803\n",
      "epoch: 6 step: 970, loss is 0.0009334454662166536\n",
      "epoch: 6 step: 971, loss is 0.0011505363509058952\n",
      "epoch: 6 step: 972, loss is 0.010336898267269135\n",
      "epoch: 6 step: 973, loss is 0.001796059892512858\n",
      "epoch: 6 step: 974, loss is 0.0011818843195214868\n",
      "epoch: 6 step: 975, loss is 0.020161984488368034\n",
      "epoch: 6 step: 976, loss is 0.003740184474736452\n",
      "epoch: 6 step: 977, loss is 0.004270271863788366\n",
      "epoch: 6 step: 978, loss is 0.001219540718011558\n",
      "epoch: 6 step: 979, loss is 0.03848174586892128\n",
      "epoch: 6 step: 980, loss is 0.001133510610088706\n",
      "epoch: 6 step: 981, loss is 0.0008049864554777741\n",
      "epoch: 6 step: 982, loss is 0.0012971600517630577\n",
      "epoch: 6 step: 983, loss is 0.08283306658267975\n",
      "epoch: 6 step: 984, loss is 0.01230835635215044\n",
      "epoch: 6 step: 985, loss is 0.00134034245274961\n",
      "epoch: 6 step: 986, loss is 0.004771007690578699\n",
      "epoch: 6 step: 987, loss is 0.00036770207225345075\n",
      "epoch: 6 step: 988, loss is 0.00026519098901189864\n",
      "epoch: 6 step: 989, loss is 0.01693427748978138\n",
      "epoch: 6 step: 990, loss is 0.0005146889016032219\n",
      "epoch: 6 step: 991, loss is 0.0001280613651033491\n",
      "epoch: 6 step: 992, loss is 0.000312390475301072\n",
      "epoch: 6 step: 993, loss is 0.0002072035858873278\n",
      "epoch: 6 step: 994, loss is 0.011739547364413738\n",
      "epoch: 6 step: 995, loss is 0.1665756106376648\n",
      "epoch: 6 step: 996, loss is 0.13121673464775085\n",
      "epoch: 6 step: 997, loss is 0.0003707513678818941\n",
      "epoch: 6 step: 998, loss is 0.0014192642411217093\n",
      "epoch: 6 step: 999, loss is 0.08392588049173355\n",
      "epoch: 6 step: 1000, loss is 0.00026196593535132706\n",
      "epoch: 6 step: 1001, loss is 0.023963436484336853\n",
      "epoch: 6 step: 1002, loss is 0.009027127176523209\n",
      "epoch: 6 step: 1003, loss is 0.003327988786622882\n",
      "epoch: 6 step: 1004, loss is 0.004025039728730917\n",
      "epoch: 6 step: 1005, loss is 0.006231129169464111\n",
      "epoch: 6 step: 1006, loss is 0.0005101378192193806\n",
      "epoch: 6 step: 1007, loss is 0.004347885958850384\n",
      "epoch: 6 step: 1008, loss is 0.007141569629311562\n",
      "epoch: 6 step: 1009, loss is 0.01745288074016571\n",
      "epoch: 6 step: 1010, loss is 0.019276054576039314\n",
      "epoch: 6 step: 1011, loss is 0.002843997674062848\n",
      "epoch: 6 step: 1012, loss is 0.016010979190468788\n",
      "epoch: 6 step: 1013, loss is 0.00026393908774480224\n",
      "epoch: 6 step: 1014, loss is 0.0006624793750233948\n",
      "epoch: 6 step: 1015, loss is 0.003710165387019515\n",
      "epoch: 6 step: 1016, loss is 0.0023595101665705442\n",
      "epoch: 6 step: 1017, loss is 0.0036005645524710417\n",
      "epoch: 6 step: 1018, loss is 5.169619544176385e-05\n",
      "epoch: 6 step: 1019, loss is 0.000314352655550465\n",
      "epoch: 6 step: 1020, loss is 0.000508637516759336\n",
      "epoch: 6 step: 1021, loss is 0.07674785703420639\n",
      "epoch: 6 step: 1022, loss is 0.00659254239872098\n",
      "epoch: 6 step: 1023, loss is 0.00014200877922121435\n",
      "epoch: 6 step: 1024, loss is 0.08247978985309601\n",
      "epoch: 6 step: 1025, loss is 0.022048871964216232\n",
      "epoch: 6 step: 1026, loss is 0.0011600475991144776\n",
      "epoch: 6 step: 1027, loss is 0.002912982599809766\n",
      "epoch: 6 step: 1028, loss is 0.0002542262081988156\n",
      "epoch: 6 step: 1029, loss is 0.09422972053289413\n",
      "epoch: 6 step: 1030, loss is 0.0460067093372345\n",
      "epoch: 6 step: 1031, loss is 0.015050666406750679\n",
      "epoch: 6 step: 1032, loss is 0.0005171910161152482\n",
      "epoch: 6 step: 1033, loss is 0.00024341666721738875\n",
      "epoch: 6 step: 1034, loss is 0.00024393976491410285\n",
      "epoch: 6 step: 1035, loss is 0.0010568585712462664\n",
      "epoch: 6 step: 1036, loss is 0.00040714911301620305\n",
      "epoch: 6 step: 1037, loss is 0.0006560177425853908\n",
      "epoch: 6 step: 1038, loss is 0.02223017066717148\n",
      "epoch: 6 step: 1039, loss is 0.008284241892397404\n",
      "epoch: 6 step: 1040, loss is 0.014450033195316792\n",
      "epoch: 6 step: 1041, loss is 0.001899332506582141\n",
      "epoch: 6 step: 1042, loss is 2.9541666663135402e-05\n",
      "epoch: 6 step: 1043, loss is 0.0007230500341393054\n",
      "epoch: 6 step: 1044, loss is 0.0003377384855411947\n",
      "epoch: 6 step: 1045, loss is 0.0010557897621765733\n",
      "epoch: 6 step: 1046, loss is 9.689838043414056e-05\n",
      "epoch: 6 step: 1047, loss is 0.0001622088166186586\n",
      "epoch: 6 step: 1048, loss is 0.010652462020516396\n",
      "epoch: 6 step: 1049, loss is 0.0016723863082006574\n",
      "epoch: 6 step: 1050, loss is 0.0934399962425232\n",
      "epoch: 6 step: 1051, loss is 0.010066241025924683\n",
      "epoch: 6 step: 1052, loss is 0.00752416905015707\n",
      "epoch: 6 step: 1053, loss is 0.00021177891176193953\n",
      "epoch: 6 step: 1054, loss is 0.002343201544135809\n",
      "epoch: 6 step: 1055, loss is 6.802637653890997e-05\n",
      "epoch: 6 step: 1056, loss is 0.01196861732751131\n",
      "epoch: 6 step: 1057, loss is 0.04991978779435158\n",
      "epoch: 6 step: 1058, loss is 0.0018657222390174866\n",
      "epoch: 6 step: 1059, loss is 0.007341954857110977\n",
      "epoch: 6 step: 1060, loss is 0.05100563168525696\n",
      "epoch: 6 step: 1061, loss is 1.10979772216524e-05\n",
      "epoch: 6 step: 1062, loss is 0.05493536591529846\n",
      "epoch: 6 step: 1063, loss is 6.0030502936569974e-05\n",
      "epoch: 6 step: 1064, loss is 0.006258349400013685\n",
      "epoch: 6 step: 1065, loss is 5.1874128985218704e-05\n",
      "epoch: 6 step: 1066, loss is 0.013646012172102928\n",
      "epoch: 6 step: 1067, loss is 0.006697893142700195\n",
      "epoch: 6 step: 1068, loss is 9.068159852176905e-05\n",
      "epoch: 6 step: 1069, loss is 0.007157138083130121\n",
      "epoch: 6 step: 1070, loss is 0.023497257381677628\n",
      "epoch: 6 step: 1071, loss is 0.05612579360604286\n",
      "epoch: 6 step: 1072, loss is 0.0017650827066972852\n",
      "epoch: 6 step: 1073, loss is 0.0018355092033743858\n",
      "epoch: 6 step: 1074, loss is 0.04713898152112961\n",
      "epoch: 6 step: 1075, loss is 0.04428790882229805\n",
      "epoch: 6 step: 1076, loss is 0.003483721986413002\n",
      "epoch: 6 step: 1077, loss is 0.005677655339241028\n",
      "epoch: 6 step: 1078, loss is 0.36875563859939575\n",
      "epoch: 6 step: 1079, loss is 0.09140890091657639\n",
      "epoch: 6 step: 1080, loss is 0.21853278577327728\n",
      "epoch: 6 step: 1081, loss is 6.229289283510298e-05\n",
      "epoch: 6 step: 1082, loss is 0.0005110273486934602\n",
      "epoch: 6 step: 1083, loss is 0.00014602650480810553\n",
      "epoch: 6 step: 1084, loss is 0.019530966877937317\n",
      "epoch: 6 step: 1085, loss is 0.003298562252894044\n",
      "epoch: 6 step: 1086, loss is 0.001286463113501668\n",
      "epoch: 6 step: 1087, loss is 0.011566105298697948\n",
      "epoch: 6 step: 1088, loss is 0.00311349262483418\n",
      "epoch: 6 step: 1089, loss is 0.0012654641177505255\n",
      "epoch: 6 step: 1090, loss is 0.01495632529258728\n",
      "epoch: 6 step: 1091, loss is 0.001752162235789001\n",
      "epoch: 6 step: 1092, loss is 0.001703985151834786\n",
      "epoch: 6 step: 1093, loss is 0.012598752975463867\n",
      "epoch: 6 step: 1094, loss is 0.0013889383990317583\n",
      "epoch: 6 step: 1095, loss is 0.0007354571134783328\n",
      "epoch: 6 step: 1096, loss is 0.0005495797959156334\n",
      "epoch: 6 step: 1097, loss is 0.0602090060710907\n",
      "epoch: 6 step: 1098, loss is 0.0017264584312215447\n",
      "epoch: 6 step: 1099, loss is 4.697785698226653e-05\n",
      "epoch: 6 step: 1100, loss is 0.2780154049396515\n",
      "epoch: 6 step: 1101, loss is 0.04418318718671799\n",
      "epoch: 6 step: 1102, loss is 0.00992025900632143\n",
      "epoch: 6 step: 1103, loss is 0.0002297907485626638\n",
      "epoch: 6 step: 1104, loss is 0.007882347330451012\n",
      "epoch: 6 step: 1105, loss is 0.009661384858191013\n",
      "epoch: 6 step: 1106, loss is 0.042535677552223206\n",
      "epoch: 6 step: 1107, loss is 0.0038405500818043947\n",
      "epoch: 6 step: 1108, loss is 0.001233444083482027\n",
      "epoch: 6 step: 1109, loss is 0.0011297487653791904\n",
      "epoch: 6 step: 1110, loss is 0.0016557627823203802\n",
      "epoch: 6 step: 1111, loss is 0.00643489696085453\n",
      "epoch: 6 step: 1112, loss is 0.06522723287343979\n",
      "epoch: 6 step: 1113, loss is 0.14169058203697205\n",
      "epoch: 6 step: 1114, loss is 0.045741140842437744\n",
      "epoch: 6 step: 1115, loss is 0.017243744805455208\n",
      "epoch: 6 step: 1116, loss is 0.020762747153639793\n",
      "epoch: 6 step: 1117, loss is 0.0053553455509245396\n",
      "epoch: 6 step: 1118, loss is 0.0010104613611474633\n",
      "epoch: 6 step: 1119, loss is 0.019735921174287796\n",
      "epoch: 6 step: 1120, loss is 9.42784536164254e-05\n",
      "epoch: 6 step: 1121, loss is 0.008819347247481346\n",
      "epoch: 6 step: 1122, loss is 0.009350670501589775\n",
      "epoch: 6 step: 1123, loss is 0.0016996543854475021\n",
      "epoch: 6 step: 1124, loss is 0.013092654757201672\n",
      "epoch: 6 step: 1125, loss is 0.1651185303926468\n",
      "epoch: 6 step: 1126, loss is 0.010762573219835758\n",
      "epoch: 6 step: 1127, loss is 0.00025323149748146534\n",
      "epoch: 6 step: 1128, loss is 0.0012151659466326237\n",
      "epoch: 6 step: 1129, loss is 0.0009754809434525669\n",
      "epoch: 6 step: 1130, loss is 0.0441577173769474\n",
      "epoch: 6 step: 1131, loss is 0.020637400448322296\n",
      "epoch: 6 step: 1132, loss is 0.002778806257992983\n",
      "epoch: 6 step: 1133, loss is 0.0006015875842422247\n",
      "epoch: 6 step: 1134, loss is 0.07875650376081467\n",
      "epoch: 6 step: 1135, loss is 0.027202190831303596\n",
      "epoch: 6 step: 1136, loss is 0.13619598746299744\n",
      "epoch: 6 step: 1137, loss is 0.002433022251352668\n",
      "epoch: 6 step: 1138, loss is 0.009233579970896244\n",
      "epoch: 6 step: 1139, loss is 0.002576599596068263\n",
      "epoch: 6 step: 1140, loss is 0.00037800255813635886\n",
      "epoch: 6 step: 1141, loss is 0.0012970486423000693\n",
      "epoch: 6 step: 1142, loss is 0.0023434541653841734\n",
      "epoch: 6 step: 1143, loss is 0.003336318302899599\n",
      "epoch: 6 step: 1144, loss is 0.001860845717601478\n",
      "epoch: 6 step: 1145, loss is 0.00015979607996996492\n",
      "epoch: 6 step: 1146, loss is 0.005707898642867804\n",
      "epoch: 6 step: 1147, loss is 0.0011303049977868795\n",
      "epoch: 6 step: 1148, loss is 0.00877927802503109\n",
      "epoch: 6 step: 1149, loss is 0.0015516169369220734\n",
      "epoch: 6 step: 1150, loss is 0.001012916793115437\n",
      "epoch: 6 step: 1151, loss is 0.07417371869087219\n",
      "epoch: 6 step: 1152, loss is 0.041979849338531494\n",
      "epoch: 6 step: 1153, loss is 0.0006675524637103081\n",
      "epoch: 6 step: 1154, loss is 0.00042833160841837525\n",
      "epoch: 6 step: 1155, loss is 0.0002846939314622432\n",
      "epoch: 6 step: 1156, loss is 0.010052300989627838\n",
      "epoch: 6 step: 1157, loss is 0.008438090793788433\n",
      "epoch: 6 step: 1158, loss is 0.004163912497460842\n",
      "epoch: 6 step: 1159, loss is 0.001970698358491063\n",
      "epoch: 6 step: 1160, loss is 0.0008782697841525078\n",
      "epoch: 6 step: 1161, loss is 0.001444492256268859\n",
      "epoch: 6 step: 1162, loss is 0.0027194807771593332\n",
      "epoch: 6 step: 1163, loss is 0.0040777456015348434\n",
      "epoch: 6 step: 1164, loss is 0.02072940394282341\n",
      "epoch: 6 step: 1165, loss is 0.003518703393638134\n",
      "epoch: 6 step: 1166, loss is 0.014128475449979305\n",
      "epoch: 6 step: 1167, loss is 0.003313910448923707\n",
      "epoch: 6 step: 1168, loss is 0.0029585754964500666\n",
      "epoch: 6 step: 1169, loss is 0.019897349178791046\n",
      "epoch: 6 step: 1170, loss is 0.18161964416503906\n",
      "epoch: 6 step: 1171, loss is 0.0005761510692536831\n",
      "epoch: 6 step: 1172, loss is 0.00029236034606583416\n",
      "epoch: 6 step: 1173, loss is 0.043589577078819275\n",
      "epoch: 6 step: 1174, loss is 0.00716229947283864\n",
      "epoch: 6 step: 1175, loss is 0.006064800079911947\n",
      "epoch: 6 step: 1176, loss is 0.0015055392868816853\n",
      "epoch: 6 step: 1177, loss is 0.05282417684793472\n",
      "epoch: 6 step: 1178, loss is 0.0013234337093308568\n",
      "epoch: 6 step: 1179, loss is 0.03900875523686409\n",
      "epoch: 6 step: 1180, loss is 0.0005646966164931655\n",
      "epoch: 6 step: 1181, loss is 0.012221398763358593\n",
      "epoch: 6 step: 1182, loss is 0.06399665027856827\n",
      "epoch: 6 step: 1183, loss is 0.2989550530910492\n",
      "epoch: 6 step: 1184, loss is 0.01925783045589924\n",
      "epoch: 6 step: 1185, loss is 0.05464382842183113\n",
      "epoch: 6 step: 1186, loss is 0.00046202962403185666\n",
      "epoch: 6 step: 1187, loss is 0.03226340562105179\n",
      "epoch: 6 step: 1188, loss is 0.000100417310022749\n",
      "epoch: 6 step: 1189, loss is 0.010994893498718739\n",
      "epoch: 6 step: 1190, loss is 0.03109518252313137\n",
      "epoch: 6 step: 1191, loss is 0.13240675628185272\n",
      "epoch: 6 step: 1192, loss is 0.001845466555096209\n",
      "epoch: 6 step: 1193, loss is 0.0040872483514249325\n",
      "epoch: 6 step: 1194, loss is 0.0005712406127713621\n",
      "epoch: 6 step: 1195, loss is 0.0020440330263227224\n",
      "epoch: 6 step: 1196, loss is 0.0021765553392469883\n",
      "epoch: 6 step: 1197, loss is 0.07574087381362915\n",
      "epoch: 6 step: 1198, loss is 0.006874178536236286\n",
      "epoch: 6 step: 1199, loss is 0.10603099316358566\n",
      "epoch: 6 step: 1200, loss is 0.11347604542970657\n",
      "epoch: 6 step: 1201, loss is 0.06660400331020355\n",
      "epoch: 6 step: 1202, loss is 0.0014480185927823186\n",
      "epoch: 6 step: 1203, loss is 0.0007640203111805022\n",
      "epoch: 6 step: 1204, loss is 0.0013379077427089214\n",
      "epoch: 6 step: 1205, loss is 0.00439527677372098\n",
      "epoch: 6 step: 1206, loss is 0.03297547250986099\n",
      "epoch: 6 step: 1207, loss is 0.011082068085670471\n",
      "epoch: 6 step: 1208, loss is 0.002661277074366808\n",
      "epoch: 6 step: 1209, loss is 0.010964443907141685\n",
      "epoch: 6 step: 1210, loss is 0.17718376219272614\n",
      "epoch: 6 step: 1211, loss is 0.002064571250230074\n",
      "epoch: 6 step: 1212, loss is 0.0022943723015487194\n",
      "epoch: 6 step: 1213, loss is 0.05446266382932663\n",
      "epoch: 6 step: 1214, loss is 0.1746140867471695\n",
      "epoch: 6 step: 1215, loss is 0.003212276380509138\n",
      "epoch: 6 step: 1216, loss is 0.0284710805863142\n",
      "epoch: 6 step: 1217, loss is 0.0013490578858181834\n",
      "epoch: 6 step: 1218, loss is 0.0019017281010746956\n",
      "epoch: 6 step: 1219, loss is 0.00320314709097147\n",
      "epoch: 6 step: 1220, loss is 0.0035341549664735794\n",
      "epoch: 6 step: 1221, loss is 0.004764799494296312\n",
      "epoch: 6 step: 1222, loss is 0.008708052337169647\n",
      "epoch: 6 step: 1223, loss is 0.017851144075393677\n",
      "epoch: 6 step: 1224, loss is 0.160599485039711\n",
      "epoch: 6 step: 1225, loss is 0.09161406755447388\n",
      "epoch: 6 step: 1226, loss is 0.23797206580638885\n",
      "epoch: 6 step: 1227, loss is 0.012897608801722527\n",
      "epoch: 6 step: 1228, loss is 0.10121552646160126\n",
      "epoch: 6 step: 1229, loss is 0.0020130740012973547\n",
      "epoch: 6 step: 1230, loss is 0.13575497269630432\n",
      "epoch: 6 step: 1231, loss is 0.002893420634791255\n",
      "epoch: 6 step: 1232, loss is 0.07207325100898743\n",
      "epoch: 6 step: 1233, loss is 0.003374792169779539\n",
      "epoch: 6 step: 1234, loss is 0.014465102925896645\n",
      "epoch: 6 step: 1235, loss is 0.001653901650570333\n",
      "epoch: 6 step: 1236, loss is 0.0442563034594059\n",
      "epoch: 6 step: 1237, loss is 0.014726925641298294\n",
      "epoch: 6 step: 1238, loss is 0.001067891251295805\n",
      "epoch: 6 step: 1239, loss is 0.005568206775933504\n",
      "epoch: 6 step: 1240, loss is 8.350731513928622e-05\n",
      "epoch: 6 step: 1241, loss is 0.017358873039484024\n",
      "epoch: 6 step: 1242, loss is 0.09004674851894379\n",
      "epoch: 6 step: 1243, loss is 0.005599023774266243\n",
      "epoch: 6 step: 1244, loss is 0.1764889508485794\n",
      "epoch: 6 step: 1245, loss is 0.013445780612528324\n",
      "epoch: 6 step: 1246, loss is 0.07106731086969376\n",
      "epoch: 6 step: 1247, loss is 0.05361387878656387\n",
      "epoch: 6 step: 1248, loss is 0.0990792065858841\n",
      "epoch: 6 step: 1249, loss is 0.02359476499259472\n",
      "epoch: 6 step: 1250, loss is 0.11843094229698181\n",
      "epoch: 6 step: 1251, loss is 0.0054306830279529095\n",
      "epoch: 6 step: 1252, loss is 0.1300801932811737\n",
      "epoch: 6 step: 1253, loss is 0.0006446337793022394\n",
      "epoch: 6 step: 1254, loss is 0.026565570384263992\n",
      "epoch: 6 step: 1255, loss is 0.0007696160464547575\n",
      "epoch: 6 step: 1256, loss is 0.0020626788027584553\n",
      "epoch: 6 step: 1257, loss is 0.1405222862958908\n",
      "epoch: 6 step: 1258, loss is 0.0061269900761544704\n",
      "epoch: 6 step: 1259, loss is 0.01086563989520073\n",
      "epoch: 6 step: 1260, loss is 0.001058749738149345\n",
      "epoch: 6 step: 1261, loss is 0.10180746018886566\n",
      "epoch: 6 step: 1262, loss is 0.049660325050354004\n",
      "epoch: 6 step: 1263, loss is 0.042040903121232986\n",
      "epoch: 6 step: 1264, loss is 0.01333182118833065\n",
      "epoch: 6 step: 1265, loss is 0.2525401711463928\n",
      "epoch: 6 step: 1266, loss is 0.00505703454837203\n",
      "epoch: 6 step: 1267, loss is 0.06544474512338638\n",
      "epoch: 6 step: 1268, loss is 0.04511309415102005\n",
      "epoch: 6 step: 1269, loss is 0.006917415652424097\n",
      "epoch: 6 step: 1270, loss is 0.001295682042837143\n",
      "epoch: 6 step: 1271, loss is 0.1375085711479187\n",
      "epoch: 6 step: 1272, loss is 0.23369739949703217\n",
      "epoch: 6 step: 1273, loss is 0.00040147415711544454\n",
      "epoch: 6 step: 1274, loss is 0.014160470105707645\n",
      "epoch: 6 step: 1275, loss is 0.004363114479929209\n",
      "epoch: 6 step: 1276, loss is 0.0032266248017549515\n",
      "epoch: 6 step: 1277, loss is 0.0027306906413286924\n",
      "epoch: 6 step: 1278, loss is 0.011756565421819687\n",
      "epoch: 6 step: 1279, loss is 0.011428849771618843\n",
      "epoch: 6 step: 1280, loss is 0.03342688828706741\n",
      "epoch: 6 step: 1281, loss is 0.004851910751312971\n",
      "epoch: 6 step: 1282, loss is 0.05044674128293991\n",
      "epoch: 6 step: 1283, loss is 0.06573227792978287\n",
      "epoch: 6 step: 1284, loss is 0.13572604954242706\n",
      "epoch: 6 step: 1285, loss is 0.0170914139598608\n",
      "epoch: 6 step: 1286, loss is 0.0061449394561350346\n",
      "epoch: 6 step: 1287, loss is 0.023041335865855217\n",
      "epoch: 6 step: 1288, loss is 0.0011351918801665306\n",
      "epoch: 6 step: 1289, loss is 0.0014027970610186458\n",
      "epoch: 6 step: 1290, loss is 0.001174526521936059\n",
      "epoch: 6 step: 1291, loss is 0.07290872931480408\n",
      "epoch: 6 step: 1292, loss is 0.11966803669929504\n",
      "epoch: 6 step: 1293, loss is 0.02153497189283371\n",
      "epoch: 6 step: 1294, loss is 0.0031770598143339157\n",
      "epoch: 6 step: 1295, loss is 0.10952255129814148\n",
      "epoch: 6 step: 1296, loss is 0.0032036183401942253\n",
      "epoch: 6 step: 1297, loss is 0.013606226071715355\n",
      "epoch: 6 step: 1298, loss is 0.05295362323522568\n",
      "epoch: 6 step: 1299, loss is 0.09498576074838638\n",
      "epoch: 6 step: 1300, loss is 0.0029445800464600325\n",
      "epoch: 6 step: 1301, loss is 0.0064042615704238415\n",
      "epoch: 6 step: 1302, loss is 0.0006164989317767322\n",
      "epoch: 6 step: 1303, loss is 0.019363166764378548\n",
      "epoch: 6 step: 1304, loss is 0.007196179591119289\n",
      "epoch: 6 step: 1305, loss is 0.008839190937578678\n",
      "epoch: 6 step: 1306, loss is 0.015498414635658264\n",
      "epoch: 6 step: 1307, loss is 0.00344359059818089\n",
      "epoch: 6 step: 1308, loss is 0.0021886881440877914\n",
      "epoch: 6 step: 1309, loss is 0.0019285677699372172\n",
      "epoch: 6 step: 1310, loss is 0.005070332903414965\n",
      "epoch: 6 step: 1311, loss is 0.06142862141132355\n",
      "epoch: 6 step: 1312, loss is 0.013896054588258266\n",
      "epoch: 6 step: 1313, loss is 0.010674075223505497\n",
      "epoch: 6 step: 1314, loss is 0.05365743860602379\n",
      "epoch: 6 step: 1315, loss is 0.007736844941973686\n",
      "epoch: 6 step: 1316, loss is 0.03824376314878464\n",
      "epoch: 6 step: 1317, loss is 0.0056378492154181\n",
      "epoch: 6 step: 1318, loss is 0.0015916371485218406\n",
      "epoch: 6 step: 1319, loss is 0.0007571841706521809\n",
      "epoch: 6 step: 1320, loss is 0.01496929582208395\n",
      "epoch: 6 step: 1321, loss is 0.017222946509718895\n",
      "epoch: 6 step: 1322, loss is 0.0006038942956365645\n",
      "epoch: 6 step: 1323, loss is 0.05806047096848488\n",
      "epoch: 6 step: 1324, loss is 0.0030823401175439358\n",
      "epoch: 6 step: 1325, loss is 0.03733401373028755\n",
      "epoch: 6 step: 1326, loss is 0.0022317622788250446\n",
      "epoch: 6 step: 1327, loss is 0.0012765100691467524\n",
      "epoch: 6 step: 1328, loss is 0.018715713173151016\n",
      "epoch: 6 step: 1329, loss is 0.022270221263170242\n",
      "epoch: 6 step: 1330, loss is 0.013914160430431366\n",
      "epoch: 6 step: 1331, loss is 0.003022791352123022\n",
      "epoch: 6 step: 1332, loss is 0.02289832942187786\n",
      "epoch: 6 step: 1333, loss is 6.191492866491899e-05\n",
      "epoch: 6 step: 1334, loss is 0.002999700838699937\n",
      "epoch: 6 step: 1335, loss is 0.0005944871227256954\n",
      "epoch: 6 step: 1336, loss is 0.013366403058171272\n",
      "epoch: 6 step: 1337, loss is 0.03139448165893555\n",
      "epoch: 6 step: 1338, loss is 0.0007159949163906276\n",
      "epoch: 6 step: 1339, loss is 0.02010306529700756\n",
      "epoch: 6 step: 1340, loss is 0.00033595654531382024\n",
      "epoch: 6 step: 1341, loss is 0.03189468756318092\n",
      "epoch: 6 step: 1342, loss is 0.01455731876194477\n",
      "epoch: 6 step: 1343, loss is 0.002760824514552951\n",
      "epoch: 6 step: 1344, loss is 0.00046749215107411146\n",
      "epoch: 6 step: 1345, loss is 0.008552485145628452\n",
      "epoch: 6 step: 1346, loss is 0.05491931363940239\n",
      "epoch: 6 step: 1347, loss is 0.00658692792057991\n",
      "epoch: 6 step: 1348, loss is 8.108797919703647e-05\n",
      "epoch: 6 step: 1349, loss is 0.0240151509642601\n",
      "epoch: 6 step: 1350, loss is 0.00659326883032918\n",
      "epoch: 6 step: 1351, loss is 0.0003395434468984604\n",
      "epoch: 6 step: 1352, loss is 0.001867297338321805\n",
      "epoch: 6 step: 1353, loss is 0.000726673926692456\n",
      "epoch: 6 step: 1354, loss is 0.034599240869283676\n",
      "epoch: 6 step: 1355, loss is 0.0006294646300375462\n",
      "epoch: 6 step: 1356, loss is 0.007086456753313541\n",
      "epoch: 6 step: 1357, loss is 0.0003539487370289862\n",
      "epoch: 6 step: 1358, loss is 0.0001568599254824221\n",
      "epoch: 6 step: 1359, loss is 0.0015448573976755142\n",
      "epoch: 6 step: 1360, loss is 0.08837377279996872\n",
      "epoch: 6 step: 1361, loss is 0.06560537964105606\n",
      "epoch: 6 step: 1362, loss is 0.0010662205750122666\n",
      "epoch: 6 step: 1363, loss is 0.027379117906093597\n",
      "epoch: 6 step: 1364, loss is 0.00011362455552443862\n",
      "epoch: 6 step: 1365, loss is 0.0006519785965792835\n",
      "epoch: 6 step: 1366, loss is 0.0013087015831843019\n",
      "epoch: 6 step: 1367, loss is 0.005692001897841692\n",
      "epoch: 6 step: 1368, loss is 0.018874265253543854\n",
      "epoch: 6 step: 1369, loss is 0.0035098008811473846\n",
      "epoch: 6 step: 1370, loss is 0.0004859649925492704\n",
      "epoch: 6 step: 1371, loss is 0.13184823095798492\n",
      "epoch: 6 step: 1372, loss is 0.12337305396795273\n",
      "epoch: 6 step: 1373, loss is 0.00020517493248917162\n",
      "epoch: 6 step: 1374, loss is 0.03603832423686981\n",
      "epoch: 6 step: 1375, loss is 0.0020346331875771284\n",
      "epoch: 6 step: 1376, loss is 2.5836077838903293e-05\n",
      "epoch: 6 step: 1377, loss is 0.00017195592226926237\n",
      "epoch: 6 step: 1378, loss is 0.0004165687132626772\n",
      "epoch: 6 step: 1379, loss is 0.0034882170148193836\n",
      "epoch: 6 step: 1380, loss is 0.013231891207396984\n",
      "epoch: 6 step: 1381, loss is 0.0022071804851293564\n",
      "epoch: 6 step: 1382, loss is 0.051817238330841064\n",
      "epoch: 6 step: 1383, loss is 0.030124038457870483\n",
      "epoch: 6 step: 1384, loss is 0.00027120221056975424\n",
      "epoch: 6 step: 1385, loss is 0.06544338911771774\n",
      "epoch: 6 step: 1386, loss is 0.0006606572424061596\n",
      "epoch: 6 step: 1387, loss is 0.03446701541543007\n",
      "epoch: 6 step: 1388, loss is 0.03992443159222603\n",
      "epoch: 6 step: 1389, loss is 0.003476472804322839\n",
      "epoch: 6 step: 1390, loss is 0.0037551498971879482\n",
      "epoch: 6 step: 1391, loss is 0.0015542861074209213\n",
      "epoch: 6 step: 1392, loss is 0.0004872740537393838\n",
      "epoch: 6 step: 1393, loss is 0.009137927554547787\n",
      "epoch: 6 step: 1394, loss is 0.009657774120569229\n",
      "epoch: 6 step: 1395, loss is 0.03239062801003456\n",
      "epoch: 6 step: 1396, loss is 0.001480567385442555\n",
      "epoch: 6 step: 1397, loss is 0.029143076390028\n",
      "epoch: 6 step: 1398, loss is 0.020076002925634384\n",
      "epoch: 6 step: 1399, loss is 0.04235072433948517\n",
      "epoch: 6 step: 1400, loss is 0.0005199821898713708\n",
      "epoch: 6 step: 1401, loss is 0.00044290523510426283\n",
      "epoch: 6 step: 1402, loss is 0.025544270873069763\n",
      "epoch: 6 step: 1403, loss is 0.0014061470283195376\n",
      "epoch: 6 step: 1404, loss is 0.00034944823710247874\n",
      "epoch: 6 step: 1405, loss is 0.003641402581706643\n",
      "epoch: 6 step: 1406, loss is 0.007451951503753662\n",
      "epoch: 6 step: 1407, loss is 0.05525893718004227\n",
      "epoch: 6 step: 1408, loss is 0.013362227939069271\n",
      "epoch: 6 step: 1409, loss is 0.0016494246665388346\n",
      "epoch: 6 step: 1410, loss is 0.005586089566349983\n",
      "epoch: 6 step: 1411, loss is 0.1629110723733902\n",
      "epoch: 6 step: 1412, loss is 0.007690778933465481\n",
      "epoch: 6 step: 1413, loss is 0.0002476476947776973\n",
      "epoch: 6 step: 1414, loss is 0.0010694039519876242\n",
      "epoch: 6 step: 1415, loss is 0.006765780039131641\n",
      "epoch: 6 step: 1416, loss is 0.001613173633813858\n",
      "epoch: 6 step: 1417, loss is 0.020312512293457985\n",
      "epoch: 6 step: 1418, loss is 0.012989259324967861\n",
      "epoch: 6 step: 1419, loss is 0.0734141618013382\n",
      "epoch: 6 step: 1420, loss is 0.000482821196783334\n",
      "epoch: 6 step: 1421, loss is 0.0007904073572717607\n",
      "epoch: 6 step: 1422, loss is 0.0011875402415171266\n",
      "epoch: 6 step: 1423, loss is 0.0014793702866882086\n",
      "epoch: 6 step: 1424, loss is 0.007128978613764048\n",
      "epoch: 6 step: 1425, loss is 0.010532733052968979\n",
      "epoch: 6 step: 1426, loss is 0.009041103534400463\n",
      "epoch: 6 step: 1427, loss is 0.002764836186543107\n",
      "epoch: 6 step: 1428, loss is 0.0018027410842478275\n",
      "epoch: 6 step: 1429, loss is 7.158373773563653e-05\n",
      "epoch: 6 step: 1430, loss is 0.009897538460791111\n",
      "epoch: 6 step: 1431, loss is 0.07060597836971283\n",
      "epoch: 6 step: 1432, loss is 0.005267994478344917\n",
      "epoch: 6 step: 1433, loss is 0.0010587635915726423\n",
      "epoch: 6 step: 1434, loss is 0.0029455588664859533\n",
      "epoch: 6 step: 1435, loss is 0.007313984911888838\n",
      "epoch: 6 step: 1436, loss is 0.0008579891291446984\n",
      "epoch: 6 step: 1437, loss is 0.02303014136850834\n",
      "epoch: 6 step: 1438, loss is 0.0019830851815640926\n",
      "epoch: 6 step: 1439, loss is 0.0010927560506388545\n",
      "epoch: 6 step: 1440, loss is 0.1457681953907013\n",
      "epoch: 6 step: 1441, loss is 0.002350086346268654\n",
      "epoch: 6 step: 1442, loss is 0.0855586975812912\n",
      "epoch: 6 step: 1443, loss is 0.0005015379865653813\n",
      "epoch: 6 step: 1444, loss is 0.08127843588590622\n",
      "epoch: 6 step: 1445, loss is 0.04501307010650635\n",
      "epoch: 6 step: 1446, loss is 0.0003244468243792653\n",
      "epoch: 6 step: 1447, loss is 0.0019780558068305254\n",
      "epoch: 6 step: 1448, loss is 0.09570044279098511\n",
      "epoch: 6 step: 1449, loss is 0.0008959250408224761\n",
      "epoch: 6 step: 1450, loss is 0.0013471967540681362\n",
      "epoch: 6 step: 1451, loss is 0.0006616864702664316\n",
      "epoch: 6 step: 1452, loss is 0.026505468413233757\n",
      "epoch: 6 step: 1453, loss is 0.001704371883533895\n",
      "epoch: 6 step: 1454, loss is 0.05804476886987686\n",
      "epoch: 6 step: 1455, loss is 0.003409074153751135\n",
      "epoch: 6 step: 1456, loss is 0.08946174383163452\n",
      "epoch: 6 step: 1457, loss is 0.14254578948020935\n",
      "epoch: 6 step: 1458, loss is 0.1405782401561737\n",
      "epoch: 6 step: 1459, loss is 0.03714453801512718\n",
      "epoch: 6 step: 1460, loss is 0.03983895108103752\n",
      "epoch: 6 step: 1461, loss is 0.04562005400657654\n",
      "epoch: 6 step: 1462, loss is 0.0005757089820690453\n",
      "epoch: 6 step: 1463, loss is 0.0021428426261991262\n",
      "epoch: 6 step: 1464, loss is 0.0058082714676856995\n",
      "epoch: 6 step: 1465, loss is 0.002191734267398715\n",
      "epoch: 6 step: 1466, loss is 0.000795640517026186\n",
      "epoch: 6 step: 1467, loss is 0.007217743434011936\n",
      "epoch: 6 step: 1468, loss is 0.003844538936391473\n",
      "epoch: 6 step: 1469, loss is 0.11770360916852951\n",
      "epoch: 6 step: 1470, loss is 0.05150563642382622\n",
      "epoch: 6 step: 1471, loss is 0.031417105346918106\n",
      "epoch: 6 step: 1472, loss is 0.05207883566617966\n",
      "epoch: 6 step: 1473, loss is 0.002916486468166113\n",
      "epoch: 6 step: 1474, loss is 0.008466444909572601\n",
      "epoch: 6 step: 1475, loss is 0.007724943105131388\n",
      "epoch: 6 step: 1476, loss is 0.01297212764620781\n",
      "epoch: 6 step: 1477, loss is 0.03266725689172745\n",
      "epoch: 6 step: 1478, loss is 0.00999174453318119\n",
      "epoch: 6 step: 1479, loss is 0.061163049191236496\n",
      "epoch: 6 step: 1480, loss is 0.03161268308758736\n",
      "epoch: 6 step: 1481, loss is 0.0004662216524593532\n",
      "epoch: 6 step: 1482, loss is 0.0025442608166486025\n",
      "epoch: 6 step: 1483, loss is 0.0005601563025265932\n",
      "epoch: 6 step: 1484, loss is 0.0032625815365463495\n",
      "epoch: 6 step: 1485, loss is 0.0025790012441575527\n",
      "epoch: 6 step: 1486, loss is 0.0080499816685915\n",
      "epoch: 6 step: 1487, loss is 0.004731742199510336\n",
      "epoch: 6 step: 1488, loss is 0.0002766740508377552\n",
      "epoch: 6 step: 1489, loss is 0.0015409586485475302\n",
      "epoch: 6 step: 1490, loss is 0.0007436023443005979\n",
      "epoch: 6 step: 1491, loss is 0.0164160318672657\n",
      "epoch: 6 step: 1492, loss is 0.00025349081261083484\n",
      "epoch: 6 step: 1493, loss is 0.011684958823025227\n",
      "epoch: 6 step: 1494, loss is 0.01810302771627903\n",
      "epoch: 6 step: 1495, loss is 0.008255849592387676\n",
      "epoch: 6 step: 1496, loss is 0.0002614289696794003\n",
      "epoch: 6 step: 1497, loss is 0.007348372135311365\n",
      "epoch: 6 step: 1498, loss is 0.005728677846491337\n",
      "epoch: 6 step: 1499, loss is 0.010146156884729862\n",
      "epoch: 6 step: 1500, loss is 0.034245021641254425\n",
      "epoch: 6 step: 1501, loss is 0.010429294779896736\n",
      "epoch: 6 step: 1502, loss is 0.000998404691927135\n",
      "epoch: 6 step: 1503, loss is 0.0013409865787252784\n",
      "epoch: 6 step: 1504, loss is 0.0016119451029226184\n",
      "epoch: 6 step: 1505, loss is 0.004868499003350735\n",
      "epoch: 6 step: 1506, loss is 0.00034393277019262314\n",
      "epoch: 6 step: 1507, loss is 0.001858843956142664\n",
      "epoch: 6 step: 1508, loss is 0.04249405488371849\n",
      "epoch: 6 step: 1509, loss is 0.04501514881849289\n",
      "epoch: 6 step: 1510, loss is 0.008882658556103706\n",
      "epoch: 6 step: 1511, loss is 0.004673548508435488\n",
      "epoch: 6 step: 1512, loss is 0.22865335643291473\n",
      "epoch: 6 step: 1513, loss is 0.00709216995164752\n",
      "epoch: 6 step: 1514, loss is 0.05015301704406738\n",
      "epoch: 6 step: 1515, loss is 0.008672100491821766\n",
      "epoch: 6 step: 1516, loss is 0.00011680129682645202\n",
      "epoch: 6 step: 1517, loss is 0.018497755751013756\n",
      "epoch: 6 step: 1518, loss is 0.07033146917819977\n",
      "epoch: 6 step: 1519, loss is 0.030788324773311615\n",
      "epoch: 6 step: 1520, loss is 0.00012664050154853612\n",
      "epoch: 6 step: 1521, loss is 0.002438460709527135\n",
      "epoch: 6 step: 1522, loss is 0.05478847399353981\n",
      "epoch: 6 step: 1523, loss is 0.06340903043746948\n",
      "epoch: 6 step: 1524, loss is 0.006566999480128288\n",
      "epoch: 6 step: 1525, loss is 0.0010594013147056103\n",
      "epoch: 6 step: 1526, loss is 0.0006772146443836391\n",
      "epoch: 6 step: 1527, loss is 0.0018510088557377458\n",
      "epoch: 6 step: 1528, loss is 0.00012701847299467772\n",
      "epoch: 6 step: 1529, loss is 0.0006885448237881064\n",
      "epoch: 6 step: 1530, loss is 0.00224300567060709\n",
      "epoch: 6 step: 1531, loss is 0.0035437135957181454\n",
      "epoch: 6 step: 1532, loss is 0.0033425334841012955\n",
      "epoch: 6 step: 1533, loss is 0.00019008757953997701\n",
      "epoch: 6 step: 1534, loss is 0.009120602160692215\n",
      "epoch: 6 step: 1535, loss is 0.008630968630313873\n",
      "epoch: 6 step: 1536, loss is 0.0012822694843634963\n",
      "epoch: 6 step: 1537, loss is 0.0027035491075366735\n",
      "epoch: 6 step: 1538, loss is 0.0006635432364419103\n",
      "epoch: 6 step: 1539, loss is 0.005032612942159176\n",
      "epoch: 6 step: 1540, loss is 0.016085296869277954\n",
      "epoch: 6 step: 1541, loss is 0.20475547015666962\n",
      "epoch: 6 step: 1542, loss is 0.001343987532891333\n",
      "epoch: 6 step: 1543, loss is 0.05387642979621887\n",
      "epoch: 6 step: 1544, loss is 0.004681108985096216\n",
      "epoch: 6 step: 1545, loss is 0.03507865220308304\n",
      "epoch: 6 step: 1546, loss is 0.002425116952508688\n",
      "epoch: 6 step: 1547, loss is 0.03221053630113602\n",
      "epoch: 6 step: 1548, loss is 0.0015681933145970106\n",
      "epoch: 6 step: 1549, loss is 0.0040682414546608925\n",
      "epoch: 6 step: 1550, loss is 0.1508721560239792\n",
      "epoch: 6 step: 1551, loss is 0.01792862080037594\n",
      "epoch: 6 step: 1552, loss is 0.0003893343382515013\n",
      "epoch: 6 step: 1553, loss is 0.014971443451941013\n",
      "epoch: 6 step: 1554, loss is 0.02461414597928524\n",
      "epoch: 6 step: 1555, loss is 0.014260990545153618\n",
      "epoch: 6 step: 1556, loss is 0.001673331600613892\n",
      "epoch: 6 step: 1557, loss is 0.04715403914451599\n",
      "epoch: 6 step: 1558, loss is 0.000842150766402483\n",
      "epoch: 6 step: 1559, loss is 0.001779636600986123\n",
      "epoch: 6 step: 1560, loss is 0.007184784393757582\n",
      "epoch: 6 step: 1561, loss is 0.04056898131966591\n",
      "epoch: 6 step: 1562, loss is 0.00046616457984782755\n",
      "epoch: 6 step: 1563, loss is 0.14652010798454285\n",
      "epoch: 6 step: 1564, loss is 0.0011157115222886205\n",
      "epoch: 6 step: 1565, loss is 0.013904431834816933\n",
      "epoch: 6 step: 1566, loss is 0.0011966389138251543\n",
      "epoch: 6 step: 1567, loss is 0.02763407677412033\n",
      "epoch: 6 step: 1568, loss is 0.002017904771491885\n",
      "epoch: 6 step: 1569, loss is 0.00577250262722373\n",
      "epoch: 6 step: 1570, loss is 0.0032772175036370754\n",
      "epoch: 6 step: 1571, loss is 0.047919828444719315\n",
      "epoch: 6 step: 1572, loss is 0.04938865453004837\n",
      "epoch: 6 step: 1573, loss is 0.01389391254633665\n",
      "epoch: 6 step: 1574, loss is 0.002179237548261881\n",
      "epoch: 6 step: 1575, loss is 0.03942465782165527\n",
      "epoch: 6 step: 1576, loss is 0.1389191746711731\n",
      "epoch: 6 step: 1577, loss is 0.00017331846174784005\n",
      "epoch: 6 step: 1578, loss is 0.0014684019843116403\n",
      "epoch: 6 step: 1579, loss is 0.00014899893722031265\n",
      "epoch: 6 step: 1580, loss is 0.0018545258790254593\n",
      "epoch: 6 step: 1581, loss is 0.018366828560829163\n",
      "epoch: 6 step: 1582, loss is 0.010133502073585987\n",
      "epoch: 6 step: 1583, loss is 0.16859640181064606\n",
      "epoch: 6 step: 1584, loss is 0.072988361120224\n",
      "epoch: 6 step: 1585, loss is 0.002026862232014537\n",
      "epoch: 6 step: 1586, loss is 0.0016830903477966785\n",
      "epoch: 6 step: 1587, loss is 0.010122478008270264\n",
      "epoch: 6 step: 1588, loss is 0.0002126188192050904\n",
      "epoch: 6 step: 1589, loss is 0.004640684928745031\n",
      "epoch: 6 step: 1590, loss is 0.022827070206403732\n",
      "epoch: 6 step: 1591, loss is 0.0035686122719198465\n",
      "epoch: 6 step: 1592, loss is 0.027261896058917046\n",
      "epoch: 6 step: 1593, loss is 0.002511919941753149\n",
      "epoch: 6 step: 1594, loss is 0.002780751558020711\n",
      "epoch: 6 step: 1595, loss is 0.03278466314077377\n",
      "epoch: 6 step: 1596, loss is 0.12751059234142303\n",
      "epoch: 6 step: 1597, loss is 0.005514565855264664\n",
      "epoch: 6 step: 1598, loss is 0.004140355158597231\n",
      "epoch: 6 step: 1599, loss is 0.044664833694696426\n",
      "epoch: 6 step: 1600, loss is 0.032910190522670746\n",
      "epoch: 6 step: 1601, loss is 0.0032859398052096367\n",
      "epoch: 6 step: 1602, loss is 0.02835075929760933\n",
      "epoch: 6 step: 1603, loss is 0.16140316426753998\n",
      "epoch: 6 step: 1604, loss is 0.0408523827791214\n",
      "epoch: 6 step: 1605, loss is 0.033706143498420715\n",
      "epoch: 6 step: 1606, loss is 0.009340000338852406\n",
      "epoch: 6 step: 1607, loss is 0.014383126981556416\n",
      "epoch: 6 step: 1608, loss is 0.05925833433866501\n",
      "epoch: 6 step: 1609, loss is 0.015762491151690483\n",
      "epoch: 6 step: 1610, loss is 0.04805243760347366\n",
      "epoch: 6 step: 1611, loss is 0.0043646711856126785\n",
      "epoch: 6 step: 1612, loss is 0.15427398681640625\n",
      "epoch: 6 step: 1613, loss is 0.04166441783308983\n",
      "epoch: 6 step: 1614, loss is 0.00026693393010646105\n",
      "epoch: 6 step: 1615, loss is 0.0006235611508600414\n",
      "epoch: 6 step: 1616, loss is 0.00028234609635546803\n",
      "epoch: 6 step: 1617, loss is 0.004114061128348112\n",
      "epoch: 6 step: 1618, loss is 0.00210359669290483\n",
      "epoch: 6 step: 1619, loss is 0.00012742857506964356\n",
      "epoch: 6 step: 1620, loss is 0.0030123544856905937\n",
      "epoch: 6 step: 1621, loss is 0.003061897587031126\n",
      "epoch: 6 step: 1622, loss is 0.04378463327884674\n",
      "epoch: 6 step: 1623, loss is 0.008334494195878506\n",
      "epoch: 6 step: 1624, loss is 0.002956344513222575\n",
      "epoch: 6 step: 1625, loss is 0.014521697536110878\n",
      "epoch: 6 step: 1626, loss is 0.05522685870528221\n",
      "epoch: 6 step: 1627, loss is 0.0006222048541530967\n",
      "epoch: 6 step: 1628, loss is 0.002428752603009343\n",
      "epoch: 6 step: 1629, loss is 0.006408805027604103\n",
      "epoch: 6 step: 1630, loss is 0.045799046754837036\n",
      "epoch: 6 step: 1631, loss is 0.013673249632120132\n",
      "epoch: 6 step: 1632, loss is 0.009530982002615929\n",
      "epoch: 6 step: 1633, loss is 0.0006032170495018363\n",
      "epoch: 6 step: 1634, loss is 0.01153105590492487\n",
      "epoch: 6 step: 1635, loss is 0.10938393324613571\n",
      "epoch: 6 step: 1636, loss is 0.2720170319080353\n",
      "epoch: 6 step: 1637, loss is 0.0008134508971124887\n",
      "epoch: 6 step: 1638, loss is 0.08974545449018478\n",
      "epoch: 6 step: 1639, loss is 0.07834936678409576\n",
      "epoch: 6 step: 1640, loss is 0.0010098899947479367\n",
      "epoch: 6 step: 1641, loss is 0.015108085237443447\n",
      "epoch: 6 step: 1642, loss is 0.20487266778945923\n",
      "epoch: 6 step: 1643, loss is 0.006355685647577047\n",
      "epoch: 6 step: 1644, loss is 0.0011012853356078267\n",
      "epoch: 6 step: 1645, loss is 0.0002606372581794858\n",
      "epoch: 6 step: 1646, loss is 0.0429445281624794\n",
      "epoch: 6 step: 1647, loss is 0.013119816780090332\n",
      "epoch: 6 step: 1648, loss is 0.0017613806994631886\n",
      "epoch: 6 step: 1649, loss is 0.1433304399251938\n",
      "epoch: 6 step: 1650, loss is 0.18424037098884583\n",
      "epoch: 6 step: 1651, loss is 0.0032099431846290827\n",
      "epoch: 6 step: 1652, loss is 0.005005362909287214\n",
      "epoch: 6 step: 1653, loss is 0.0576893612742424\n",
      "epoch: 6 step: 1654, loss is 0.00871601514518261\n",
      "epoch: 6 step: 1655, loss is 0.00171720702201128\n",
      "epoch: 6 step: 1656, loss is 0.08811947703361511\n",
      "epoch: 6 step: 1657, loss is 0.05729952082037926\n",
      "epoch: 6 step: 1658, loss is 0.0009704800904728472\n",
      "epoch: 6 step: 1659, loss is 0.02889973856508732\n",
      "epoch: 6 step: 1660, loss is 0.003404482500627637\n",
      "epoch: 6 step: 1661, loss is 0.0018626053351908922\n",
      "epoch: 6 step: 1662, loss is 0.0012565311044454575\n",
      "epoch: 6 step: 1663, loss is 0.002296108053997159\n",
      "epoch: 6 step: 1664, loss is 0.09834640473127365\n",
      "epoch: 6 step: 1665, loss is 0.06183237582445145\n",
      "epoch: 6 step: 1666, loss is 0.0791703537106514\n",
      "epoch: 6 step: 1667, loss is 0.0015281719388440251\n",
      "epoch: 6 step: 1668, loss is 0.009610851295292377\n",
      "epoch: 6 step: 1669, loss is 0.004664287436753511\n",
      "epoch: 6 step: 1670, loss is 0.00038191518979147077\n",
      "epoch: 6 step: 1671, loss is 0.006326652131974697\n",
      "epoch: 6 step: 1672, loss is 0.016018733382225037\n",
      "epoch: 6 step: 1673, loss is 0.03478604927659035\n",
      "epoch: 6 step: 1674, loss is 0.01899387314915657\n",
      "epoch: 6 step: 1675, loss is 0.0003403996233828366\n",
      "epoch: 6 step: 1676, loss is 0.0007579665398225188\n",
      "epoch: 6 step: 1677, loss is 0.10994213819503784\n",
      "epoch: 6 step: 1678, loss is 0.027162877842783928\n",
      "epoch: 6 step: 1679, loss is 0.002028060145676136\n",
      "epoch: 6 step: 1680, loss is 0.07269231975078583\n",
      "epoch: 6 step: 1681, loss is 0.11578208208084106\n",
      "epoch: 6 step: 1682, loss is 0.008342340588569641\n",
      "epoch: 6 step: 1683, loss is 0.001800023135729134\n",
      "epoch: 6 step: 1684, loss is 0.00355067802593112\n",
      "epoch: 6 step: 1685, loss is 0.00017207107157446444\n",
      "epoch: 6 step: 1686, loss is 0.053976546972990036\n",
      "epoch: 6 step: 1687, loss is 0.02196156047284603\n",
      "epoch: 6 step: 1688, loss is 0.002712004352360964\n",
      "epoch: 6 step: 1689, loss is 0.009429465979337692\n",
      "epoch: 6 step: 1690, loss is 0.00016130767471622676\n",
      "epoch: 6 step: 1691, loss is 0.01728196255862713\n",
      "epoch: 6 step: 1692, loss is 0.01631573773920536\n",
      "epoch: 6 step: 1693, loss is 0.0005434222402982414\n",
      "epoch: 6 step: 1694, loss is 0.001955413958057761\n",
      "epoch: 6 step: 1695, loss is 0.11717066913843155\n",
      "epoch: 6 step: 1696, loss is 0.16299742460250854\n",
      "epoch: 6 step: 1697, loss is 0.0015800902619957924\n",
      "epoch: 6 step: 1698, loss is 0.002940543694421649\n",
      "epoch: 6 step: 1699, loss is 0.0022803074680268764\n",
      "epoch: 6 step: 1700, loss is 0.001970944693312049\n",
      "epoch: 6 step: 1701, loss is 0.07423001527786255\n",
      "epoch: 6 step: 1702, loss is 0.014821991324424744\n",
      "epoch: 6 step: 1703, loss is 0.0007585856947116554\n",
      "epoch: 6 step: 1704, loss is 0.008731137029826641\n",
      "epoch: 6 step: 1705, loss is 0.0037349958438426256\n",
      "epoch: 6 step: 1706, loss is 0.0025229754392057657\n",
      "epoch: 6 step: 1707, loss is 0.031526703387498856\n",
      "epoch: 6 step: 1708, loss is 0.001218498102389276\n",
      "epoch: 6 step: 1709, loss is 0.000162629468832165\n",
      "epoch: 6 step: 1710, loss is 0.01256583072245121\n",
      "epoch: 6 step: 1711, loss is 0.0005900805699639022\n",
      "epoch: 6 step: 1712, loss is 0.05796140432357788\n",
      "epoch: 6 step: 1713, loss is 0.026886433362960815\n",
      "epoch: 6 step: 1714, loss is 0.009657048620283604\n",
      "epoch: 6 step: 1715, loss is 0.146370530128479\n",
      "epoch: 6 step: 1716, loss is 0.04140891507267952\n",
      "epoch: 6 step: 1717, loss is 0.008429545909166336\n",
      "epoch: 6 step: 1718, loss is 0.01945173554122448\n",
      "epoch: 6 step: 1719, loss is 0.0027293500024825335\n",
      "epoch: 6 step: 1720, loss is 0.026069698855280876\n",
      "epoch: 6 step: 1721, loss is 0.0012407066533342004\n",
      "epoch: 6 step: 1722, loss is 0.035122402012348175\n",
      "epoch: 6 step: 1723, loss is 0.003983581438660622\n",
      "epoch: 6 step: 1724, loss is 4.7245732275769114e-05\n",
      "epoch: 6 step: 1725, loss is 0.03669463470578194\n",
      "epoch: 6 step: 1726, loss is 0.023234743624925613\n",
      "epoch: 6 step: 1727, loss is 0.005919056944549084\n",
      "epoch: 6 step: 1728, loss is 0.00032240155269391835\n",
      "epoch: 6 step: 1729, loss is 0.16488705575466156\n",
      "epoch: 6 step: 1730, loss is 0.0002175002737203613\n",
      "epoch: 6 step: 1731, loss is 0.028432510793209076\n",
      "epoch: 6 step: 1732, loss is 0.024098454043269157\n",
      "epoch: 6 step: 1733, loss is 0.0013065007515251637\n",
      "epoch: 6 step: 1734, loss is 0.11884932219982147\n",
      "epoch: 6 step: 1735, loss is 0.00683746999129653\n",
      "epoch: 6 step: 1736, loss is 0.018331747502088547\n",
      "epoch: 6 step: 1737, loss is 0.0008307364769279957\n",
      "epoch: 6 step: 1738, loss is 0.0001970131997950375\n",
      "epoch: 6 step: 1739, loss is 0.008071905001997948\n",
      "epoch: 6 step: 1740, loss is 0.10801303386688232\n",
      "epoch: 6 step: 1741, loss is 0.00709777744486928\n",
      "epoch: 6 step: 1742, loss is 0.004080433398485184\n",
      "epoch: 6 step: 1743, loss is 0.003078620880842209\n",
      "epoch: 6 step: 1744, loss is 0.03619956225156784\n",
      "epoch: 6 step: 1745, loss is 0.009026462212204933\n",
      "epoch: 6 step: 1746, loss is 0.005600322503596544\n",
      "epoch: 6 step: 1747, loss is 0.002123044105246663\n",
      "epoch: 6 step: 1748, loss is 0.09047440439462662\n",
      "epoch: 6 step: 1749, loss is 0.13539570569992065\n",
      "epoch: 6 step: 1750, loss is 0.012003429234027863\n",
      "epoch: 6 step: 1751, loss is 0.0003448878414928913\n",
      "epoch: 6 step: 1752, loss is 0.003151309210807085\n",
      "epoch: 6 step: 1753, loss is 0.0005871753091923892\n",
      "epoch: 6 step: 1754, loss is 0.006743042264133692\n",
      "epoch: 6 step: 1755, loss is 0.016293488442897797\n",
      "epoch: 6 step: 1756, loss is 0.04604467749595642\n",
      "epoch: 6 step: 1757, loss is 0.0019990631844848394\n",
      "epoch: 6 step: 1758, loss is 0.009371254593133926\n",
      "epoch: 6 step: 1759, loss is 0.0011748353717848659\n",
      "epoch: 6 step: 1760, loss is 0.002381319645792246\n",
      "epoch: 6 step: 1761, loss is 0.023685283958911896\n",
      "epoch: 6 step: 1762, loss is 0.10653167217969894\n",
      "epoch: 6 step: 1763, loss is 0.02323695458471775\n",
      "epoch: 6 step: 1764, loss is 0.004985603969544172\n",
      "epoch: 6 step: 1765, loss is 0.00523387361317873\n",
      "epoch: 6 step: 1766, loss is 0.0013920372584834695\n",
      "epoch: 6 step: 1767, loss is 0.17692337930202484\n",
      "epoch: 6 step: 1768, loss is 0.019811533391475677\n",
      "epoch: 6 step: 1769, loss is 0.008409391157329082\n",
      "epoch: 6 step: 1770, loss is 0.001980877248570323\n",
      "epoch: 6 step: 1771, loss is 0.0011501158587634563\n",
      "epoch: 6 step: 1772, loss is 0.3326338529586792\n",
      "epoch: 6 step: 1773, loss is 0.01275179535150528\n",
      "epoch: 6 step: 1774, loss is 0.16917823255062103\n",
      "epoch: 6 step: 1775, loss is 0.017788095399737358\n",
      "epoch: 6 step: 1776, loss is 0.029838919639587402\n",
      "epoch: 6 step: 1777, loss is 0.007181867957115173\n",
      "epoch: 6 step: 1778, loss is 0.10758192092180252\n",
      "epoch: 6 step: 1779, loss is 0.003136396175250411\n",
      "epoch: 6 step: 1780, loss is 0.0028342069126665592\n",
      "epoch: 6 step: 1781, loss is 0.0003218226775061339\n",
      "epoch: 6 step: 1782, loss is 0.04159257560968399\n",
      "epoch: 6 step: 1783, loss is 0.10992641001939774\n",
      "epoch: 6 step: 1784, loss is 0.005414388608187437\n",
      "epoch: 6 step: 1785, loss is 0.06022428348660469\n",
      "epoch: 6 step: 1786, loss is 0.00016105605755001307\n",
      "epoch: 6 step: 1787, loss is 0.036237794905900955\n",
      "epoch: 6 step: 1788, loss is 0.004020961932837963\n",
      "epoch: 6 step: 1789, loss is 0.01217702403664589\n",
      "epoch: 6 step: 1790, loss is 0.3195396959781647\n",
      "epoch: 6 step: 1791, loss is 0.031646985560655594\n",
      "epoch: 6 step: 1792, loss is 0.001512069720774889\n",
      "epoch: 6 step: 1793, loss is 0.001925568445585668\n",
      "epoch: 6 step: 1794, loss is 0.033797696232795715\n",
      "epoch: 6 step: 1795, loss is 0.00043218606151640415\n",
      "epoch: 6 step: 1796, loss is 0.0053778295405209064\n",
      "epoch: 6 step: 1797, loss is 0.10048824548721313\n",
      "epoch: 6 step: 1798, loss is 0.003531389171257615\n",
      "epoch: 6 step: 1799, loss is 0.0007529350114054978\n",
      "epoch: 6 step: 1800, loss is 0.004099808167666197\n",
      "epoch: 6 step: 1801, loss is 0.02428022399544716\n",
      "epoch: 6 step: 1802, loss is 0.024705344811081886\n",
      "epoch: 6 step: 1803, loss is 0.00849935319274664\n",
      "epoch: 6 step: 1804, loss is 0.029644252732396126\n",
      "epoch: 6 step: 1805, loss is 0.0036682900972664356\n",
      "epoch: 6 step: 1806, loss is 0.027259420603513718\n",
      "epoch: 6 step: 1807, loss is 0.009613560512661934\n",
      "epoch: 6 step: 1808, loss is 0.012726623564958572\n",
      "epoch: 6 step: 1809, loss is 0.016714368015527725\n",
      "epoch: 6 step: 1810, loss is 0.016447121277451515\n",
      "epoch: 6 step: 1811, loss is 0.000862885732203722\n",
      "epoch: 6 step: 1812, loss is 0.00011475289647933096\n",
      "epoch: 6 step: 1813, loss is 0.07568909227848053\n",
      "epoch: 6 step: 1814, loss is 0.0020694811828434467\n",
      "epoch: 6 step: 1815, loss is 0.005139274522662163\n",
      "epoch: 6 step: 1816, loss is 0.02185867354273796\n",
      "epoch: 6 step: 1817, loss is 0.0016698079416528344\n",
      "epoch: 6 step: 1818, loss is 0.0023637847043573856\n",
      "epoch: 6 step: 1819, loss is 0.2091473788022995\n",
      "epoch: 6 step: 1820, loss is 0.12905685603618622\n",
      "epoch: 6 step: 1821, loss is 0.00014389981515705585\n",
      "epoch: 6 step: 1822, loss is 0.14674867689609528\n",
      "epoch: 6 step: 1823, loss is 0.027461959049105644\n",
      "epoch: 6 step: 1824, loss is 0.004864257760345936\n",
      "epoch: 6 step: 1825, loss is 8.675376739120111e-05\n",
      "epoch: 6 step: 1826, loss is 0.28397414088249207\n",
      "epoch: 6 step: 1827, loss is 0.0011259712046012282\n",
      "epoch: 6 step: 1828, loss is 0.029152220115065575\n",
      "epoch: 6 step: 1829, loss is 0.00016579014481976628\n",
      "epoch: 6 step: 1830, loss is 0.046120643615722656\n",
      "epoch: 6 step: 1831, loss is 0.04243853688240051\n",
      "epoch: 6 step: 1832, loss is 0.0015408199978992343\n",
      "epoch: 6 step: 1833, loss is 0.0009915278060361743\n",
      "epoch: 6 step: 1834, loss is 0.014662202447652817\n",
      "epoch: 6 step: 1835, loss is 0.15323272347450256\n",
      "epoch: 6 step: 1836, loss is 0.014210873283445835\n",
      "epoch: 6 step: 1837, loss is 0.0015417183749377728\n",
      "epoch: 6 step: 1838, loss is 0.2756407558917999\n",
      "epoch: 6 step: 1839, loss is 0.00373565754853189\n",
      "epoch: 6 step: 1840, loss is 0.022086475044488907\n",
      "epoch: 6 step: 1841, loss is 0.0020900436211377382\n",
      "epoch: 6 step: 1842, loss is 0.009231633506715298\n",
      "epoch: 6 step: 1843, loss is 0.05723169073462486\n",
      "epoch: 6 step: 1844, loss is 0.10850352793931961\n",
      "epoch: 6 step: 1845, loss is 0.008723854087293148\n",
      "epoch: 6 step: 1846, loss is 0.004657868295907974\n",
      "epoch: 6 step: 1847, loss is 0.07626254856586456\n",
      "epoch: 6 step: 1848, loss is 0.02084159106016159\n",
      "epoch: 6 step: 1849, loss is 0.10456269979476929\n",
      "epoch: 6 step: 1850, loss is 0.0017942000413313508\n",
      "epoch: 6 step: 1851, loss is 0.0066526527516543865\n",
      "epoch: 6 step: 1852, loss is 0.1825280487537384\n",
      "epoch: 6 step: 1853, loss is 0.03492710739374161\n",
      "epoch: 6 step: 1854, loss is 0.03707302734255791\n",
      "epoch: 6 step: 1855, loss is 0.003940853290259838\n",
      "epoch: 6 step: 1856, loss is 0.001920869923196733\n",
      "epoch: 6 step: 1857, loss is 0.02071944996714592\n",
      "epoch: 6 step: 1858, loss is 0.008294373750686646\n",
      "epoch: 6 step: 1859, loss is 0.10977096110582352\n",
      "epoch: 6 step: 1860, loss is 0.0025461302138864994\n",
      "epoch: 6 step: 1861, loss is 0.0023924701381474733\n",
      "epoch: 6 step: 1862, loss is 0.09005248546600342\n",
      "epoch: 6 step: 1863, loss is 0.001744500477798283\n",
      "epoch: 6 step: 1864, loss is 0.006970556918531656\n",
      "epoch: 6 step: 1865, loss is 0.04516445845365524\n",
      "epoch: 6 step: 1866, loss is 0.005498227663338184\n",
      "epoch: 6 step: 1867, loss is 0.014343938790261745\n",
      "epoch: 6 step: 1868, loss is 0.003388525452464819\n",
      "epoch: 6 step: 1869, loss is 0.007060425356030464\n",
      "epoch: 6 step: 1870, loss is 0.003920257091522217\n",
      "epoch: 6 step: 1871, loss is 0.11523076146841049\n",
      "epoch: 6 step: 1872, loss is 0.0040492224507033825\n",
      "epoch: 6 step: 1873, loss is 0.0006067641661502421\n",
      "epoch: 6 step: 1874, loss is 0.00445179408416152\n",
      "epoch: 6 step: 1875, loss is 0.0010864099022001028\n",
      "epoch: 7 step: 1, loss is 0.000640354584902525\n",
      "epoch: 7 step: 2, loss is 0.002684064442291856\n",
      "epoch: 7 step: 3, loss is 0.017223654314875603\n",
      "epoch: 7 step: 4, loss is 0.001167994225397706\n",
      "epoch: 7 step: 5, loss is 0.08917375653982162\n",
      "epoch: 7 step: 6, loss is 0.056181855499744415\n",
      "epoch: 7 step: 7, loss is 0.04099109768867493\n",
      "epoch: 7 step: 8, loss is 0.0016550831496715546\n",
      "epoch: 7 step: 9, loss is 0.012417150661349297\n",
      "epoch: 7 step: 10, loss is 0.011226925998926163\n",
      "epoch: 7 step: 11, loss is 0.0006695603369735181\n",
      "epoch: 7 step: 12, loss is 0.022905074059963226\n",
      "epoch: 7 step: 13, loss is 0.006014141719788313\n",
      "epoch: 7 step: 14, loss is 0.02415417693555355\n",
      "epoch: 7 step: 15, loss is 0.0005225446075201035\n",
      "epoch: 7 step: 16, loss is 0.0032981361728161573\n",
      "epoch: 7 step: 17, loss is 0.0031708732713013887\n",
      "epoch: 7 step: 18, loss is 0.03194436803460121\n",
      "epoch: 7 step: 19, loss is 0.002503813011571765\n",
      "epoch: 7 step: 20, loss is 0.0031528696417808533\n",
      "epoch: 7 step: 21, loss is 0.0015047373017296195\n",
      "epoch: 7 step: 22, loss is 0.00140147446654737\n",
      "epoch: 7 step: 23, loss is 0.00010179324453929439\n",
      "epoch: 7 step: 24, loss is 0.14311304688453674\n",
      "epoch: 7 step: 25, loss is 0.00013655218936037272\n",
      "epoch: 7 step: 26, loss is 0.0174851231276989\n",
      "epoch: 7 step: 27, loss is 0.018388185650110245\n",
      "epoch: 7 step: 28, loss is 0.028889162465929985\n",
      "epoch: 7 step: 29, loss is 0.01424458622932434\n",
      "epoch: 7 step: 30, loss is 0.03000311739742756\n",
      "epoch: 7 step: 31, loss is 0.0538768395781517\n",
      "epoch: 7 step: 32, loss is 0.0016055735759437084\n",
      "epoch: 7 step: 33, loss is 0.011075607500970364\n",
      "epoch: 7 step: 34, loss is 0.000261522684013471\n",
      "epoch: 7 step: 35, loss is 0.0003371473867446184\n",
      "epoch: 7 step: 36, loss is 0.005799497477710247\n",
      "epoch: 7 step: 37, loss is 0.001835017348639667\n",
      "epoch: 7 step: 38, loss is 0.0018387628952041268\n",
      "epoch: 7 step: 39, loss is 0.002736623166128993\n",
      "epoch: 7 step: 40, loss is 0.005483458284288645\n",
      "epoch: 7 step: 41, loss is 0.0030761086381971836\n",
      "epoch: 7 step: 42, loss is 0.00018470356008037925\n",
      "epoch: 7 step: 43, loss is 0.015948915854096413\n",
      "epoch: 7 step: 44, loss is 0.0006007864722050726\n",
      "epoch: 7 step: 45, loss is 0.0003723884292412549\n",
      "epoch: 7 step: 46, loss is 0.002435497008264065\n",
      "epoch: 7 step: 47, loss is 0.0018025016179308295\n",
      "epoch: 7 step: 48, loss is 0.14204701781272888\n",
      "epoch: 7 step: 49, loss is 0.0010010479018092155\n",
      "epoch: 7 step: 50, loss is 0.007356789894402027\n",
      "epoch: 7 step: 51, loss is 0.014846613630652428\n",
      "epoch: 7 step: 52, loss is 0.03416718170046806\n",
      "epoch: 7 step: 53, loss is 0.003705943701788783\n",
      "epoch: 7 step: 54, loss is 0.05459146946668625\n",
      "epoch: 7 step: 55, loss is 0.013113168999552727\n",
      "epoch: 7 step: 56, loss is 0.08777723461389542\n",
      "epoch: 7 step: 57, loss is 0.0005530706839635968\n",
      "epoch: 7 step: 58, loss is 0.0004037632024846971\n",
      "epoch: 7 step: 59, loss is 0.05591496825218201\n",
      "epoch: 7 step: 60, loss is 0.0002596556441858411\n",
      "epoch: 7 step: 61, loss is 0.0009397407411597669\n",
      "epoch: 7 step: 62, loss is 0.034028150141239166\n",
      "epoch: 7 step: 63, loss is 0.0013688553590327501\n",
      "epoch: 7 step: 64, loss is 0.0005367013509385288\n",
      "epoch: 7 step: 65, loss is 0.07256035506725311\n",
      "epoch: 7 step: 66, loss is 0.0026968461461365223\n",
      "epoch: 7 step: 67, loss is 0.000687259016558528\n",
      "epoch: 7 step: 68, loss is 0.0024581768084317446\n",
      "epoch: 7 step: 69, loss is 0.002486944431439042\n",
      "epoch: 7 step: 70, loss is 0.009004127234220505\n",
      "epoch: 7 step: 71, loss is 0.0015778945526108146\n",
      "epoch: 7 step: 72, loss is 0.0020890566520392895\n",
      "epoch: 7 step: 73, loss is 0.0005769355338998139\n",
      "epoch: 7 step: 74, loss is 0.004670678172260523\n",
      "epoch: 7 step: 75, loss is 0.05385725200176239\n",
      "epoch: 7 step: 76, loss is 0.10190702974796295\n",
      "epoch: 7 step: 77, loss is 0.0020699328742921352\n",
      "epoch: 7 step: 78, loss is 0.009622191078960896\n",
      "epoch: 7 step: 79, loss is 0.0009725296404212713\n",
      "epoch: 7 step: 80, loss is 0.002594711957499385\n",
      "epoch: 7 step: 81, loss is 0.0012044014874845743\n",
      "epoch: 7 step: 82, loss is 0.19613756239414215\n",
      "epoch: 7 step: 83, loss is 0.003702211892232299\n",
      "epoch: 7 step: 84, loss is 0.00019688137399498373\n",
      "epoch: 7 step: 85, loss is 0.0017358927289023995\n",
      "epoch: 7 step: 86, loss is 0.014331718906760216\n",
      "epoch: 7 step: 87, loss is 0.028206193819642067\n",
      "epoch: 7 step: 88, loss is 0.010703212581574917\n",
      "epoch: 7 step: 89, loss is 0.008033739402890205\n",
      "epoch: 7 step: 90, loss is 0.04717899486422539\n",
      "epoch: 7 step: 91, loss is 0.007169756107032299\n",
      "epoch: 7 step: 92, loss is 0.007957800291478634\n",
      "epoch: 7 step: 93, loss is 0.0077497949823737144\n",
      "epoch: 7 step: 94, loss is 0.04608745872974396\n",
      "epoch: 7 step: 95, loss is 0.018906770274043083\n",
      "epoch: 7 step: 96, loss is 0.015476444736123085\n",
      "epoch: 7 step: 97, loss is 0.000914678443223238\n",
      "epoch: 7 step: 98, loss is 0.011899556033313274\n",
      "epoch: 7 step: 99, loss is 8.492824417771772e-05\n",
      "epoch: 7 step: 100, loss is 0.0004902940127067268\n",
      "epoch: 7 step: 101, loss is 0.09119945764541626\n",
      "epoch: 7 step: 102, loss is 0.004380431026220322\n",
      "epoch: 7 step: 103, loss is 0.000999267678707838\n",
      "epoch: 7 step: 104, loss is 0.001961969304829836\n",
      "epoch: 7 step: 105, loss is 0.00010576868953648955\n",
      "epoch: 7 step: 106, loss is 0.05107695981860161\n",
      "epoch: 7 step: 107, loss is 0.00022591260494664311\n",
      "epoch: 7 step: 108, loss is 0.0060761901549994946\n",
      "epoch: 7 step: 109, loss is 0.0024661950301378965\n",
      "epoch: 7 step: 110, loss is 0.0004057602200191468\n",
      "epoch: 7 step: 111, loss is 0.04195759817957878\n",
      "epoch: 7 step: 112, loss is 0.0024867483880370855\n",
      "epoch: 7 step: 113, loss is 0.0004506287514232099\n",
      "epoch: 7 step: 114, loss is 0.006613819859921932\n",
      "epoch: 7 step: 115, loss is 0.0005365504766814411\n",
      "epoch: 7 step: 116, loss is 0.02736489288508892\n",
      "epoch: 7 step: 117, loss is 0.023462623357772827\n",
      "epoch: 7 step: 118, loss is 0.004785790573805571\n",
      "epoch: 7 step: 119, loss is 0.010640357621014118\n",
      "epoch: 7 step: 120, loss is 0.002808599267154932\n",
      "epoch: 7 step: 121, loss is 0.0003164482186548412\n",
      "epoch: 7 step: 122, loss is 0.00047010547132231295\n",
      "epoch: 7 step: 123, loss is 0.0022597971837967634\n",
      "epoch: 7 step: 124, loss is 0.0014662871835753322\n",
      "epoch: 7 step: 125, loss is 0.0012439690763130784\n",
      "epoch: 7 step: 126, loss is 0.006291620433330536\n",
      "epoch: 7 step: 127, loss is 3.9858437958173454e-05\n",
      "epoch: 7 step: 128, loss is 0.08318055421113968\n",
      "epoch: 7 step: 129, loss is 0.004623929969966412\n",
      "epoch: 7 step: 130, loss is 0.0027001884300261736\n",
      "epoch: 7 step: 131, loss is 0.003419452579692006\n",
      "epoch: 7 step: 132, loss is 0.0001061492002918385\n",
      "epoch: 7 step: 133, loss is 9.543271880829707e-05\n",
      "epoch: 7 step: 134, loss is 0.03165854513645172\n",
      "epoch: 7 step: 135, loss is 0.06166261062026024\n",
      "epoch: 7 step: 136, loss is 0.09829090535640717\n",
      "epoch: 7 step: 137, loss is 0.001108134863898158\n",
      "epoch: 7 step: 138, loss is 0.0012092242250218987\n",
      "epoch: 7 step: 139, loss is 0.015763582661747932\n",
      "epoch: 7 step: 140, loss is 0.002376382239162922\n",
      "epoch: 7 step: 141, loss is 0.00017844652757048607\n",
      "epoch: 7 step: 142, loss is 0.002343188738450408\n",
      "epoch: 7 step: 143, loss is 0.007515748497098684\n",
      "epoch: 7 step: 144, loss is 0.0724145695567131\n",
      "epoch: 7 step: 145, loss is 0.02242373302578926\n",
      "epoch: 7 step: 146, loss is 0.00024275959003716707\n",
      "epoch: 7 step: 147, loss is 0.0002526819007471204\n",
      "epoch: 7 step: 148, loss is 0.000982755096629262\n",
      "epoch: 7 step: 149, loss is 0.14217503368854523\n",
      "epoch: 7 step: 150, loss is 0.022169670090079308\n",
      "epoch: 7 step: 151, loss is 0.002947630360722542\n",
      "epoch: 7 step: 152, loss is 0.00026984975556842983\n",
      "epoch: 7 step: 153, loss is 0.025257069617509842\n",
      "epoch: 7 step: 154, loss is 0.0006574126309715211\n",
      "epoch: 7 step: 155, loss is 0.032348260283470154\n",
      "epoch: 7 step: 156, loss is 0.01271581556648016\n",
      "epoch: 7 step: 157, loss is 0.002871878445148468\n",
      "epoch: 7 step: 158, loss is 0.04154752194881439\n",
      "epoch: 7 step: 159, loss is 0.003253911156207323\n",
      "epoch: 7 step: 160, loss is 0.0023464644327759743\n",
      "epoch: 7 step: 161, loss is 0.006002022884786129\n",
      "epoch: 7 step: 162, loss is 0.0014832218876108527\n",
      "epoch: 7 step: 163, loss is 0.009283171966671944\n",
      "epoch: 7 step: 164, loss is 0.001637293491512537\n",
      "epoch: 7 step: 165, loss is 0.0015911845257505774\n",
      "epoch: 7 step: 166, loss is 0.04619466885924339\n",
      "epoch: 7 step: 167, loss is 0.0004602936387527734\n",
      "epoch: 7 step: 168, loss is 0.0014704890782013535\n",
      "epoch: 7 step: 169, loss is 0.1861598640680313\n",
      "epoch: 7 step: 170, loss is 0.0006427830085158348\n",
      "epoch: 7 step: 171, loss is 0.006312913727015257\n",
      "epoch: 7 step: 172, loss is 0.00014271686086431146\n",
      "epoch: 7 step: 173, loss is 0.035769715905189514\n",
      "epoch: 7 step: 174, loss is 0.000965941755566746\n",
      "epoch: 7 step: 175, loss is 0.0028208522126078606\n",
      "epoch: 7 step: 176, loss is 6.672270683338866e-05\n",
      "epoch: 7 step: 177, loss is 0.0996888130903244\n",
      "epoch: 7 step: 178, loss is 0.016877740621566772\n",
      "epoch: 7 step: 179, loss is 0.0005176965496502817\n",
      "epoch: 7 step: 180, loss is 0.007460332941263914\n",
      "epoch: 7 step: 181, loss is 0.0006831475766375661\n",
      "epoch: 7 step: 182, loss is 0.0004213059728499502\n",
      "epoch: 7 step: 183, loss is 0.05671214312314987\n",
      "epoch: 7 step: 184, loss is 0.011087795719504356\n",
      "epoch: 7 step: 185, loss is 0.0012924487236887217\n",
      "epoch: 7 step: 186, loss is 0.022479567676782608\n",
      "epoch: 7 step: 187, loss is 0.0012609966797754169\n",
      "epoch: 7 step: 188, loss is 0.0014103439170867205\n",
      "epoch: 7 step: 189, loss is 0.019668836146593094\n",
      "epoch: 7 step: 190, loss is 0.005724403541535139\n",
      "epoch: 7 step: 191, loss is 0.15544922649860382\n",
      "epoch: 7 step: 192, loss is 0.008941319771111012\n",
      "epoch: 7 step: 193, loss is 0.004309223964810371\n",
      "epoch: 7 step: 194, loss is 0.054997336119413376\n",
      "epoch: 7 step: 195, loss is 0.006651685573160648\n",
      "epoch: 7 step: 196, loss is 0.0007302606827579439\n",
      "epoch: 7 step: 197, loss is 0.00546737015247345\n",
      "epoch: 7 step: 198, loss is 0.00540968356654048\n",
      "epoch: 7 step: 199, loss is 0.04153002053499222\n",
      "epoch: 7 step: 200, loss is 0.002858691615983844\n",
      "epoch: 7 step: 201, loss is 0.0017847124254330993\n",
      "epoch: 7 step: 202, loss is 0.0007988956058397889\n",
      "epoch: 7 step: 203, loss is 0.029994962736964226\n",
      "epoch: 7 step: 204, loss is 0.00035512857721187174\n",
      "epoch: 7 step: 205, loss is 0.0012553022243082523\n",
      "epoch: 7 step: 206, loss is 0.0020378646440804005\n",
      "epoch: 7 step: 207, loss is 0.004713163711130619\n",
      "epoch: 7 step: 208, loss is 0.029733654111623764\n",
      "epoch: 7 step: 209, loss is 0.021031955257058144\n",
      "epoch: 7 step: 210, loss is 0.0032099669333547354\n",
      "epoch: 7 step: 211, loss is 0.0012362153502181172\n",
      "epoch: 7 step: 212, loss is 0.0007020479533821344\n",
      "epoch: 7 step: 213, loss is 0.03479847311973572\n",
      "epoch: 7 step: 214, loss is 0.0026417437475174665\n",
      "epoch: 7 step: 215, loss is 0.03551386296749115\n",
      "epoch: 7 step: 216, loss is 0.0012604034272953868\n",
      "epoch: 7 step: 217, loss is 0.013052225112915039\n",
      "epoch: 7 step: 218, loss is 0.0013077635085210204\n",
      "epoch: 7 step: 219, loss is 3.247773565817624e-05\n",
      "epoch: 7 step: 220, loss is 0.005315142683684826\n",
      "epoch: 7 step: 221, loss is 0.0001869902916951105\n",
      "epoch: 7 step: 222, loss is 0.003631805069744587\n",
      "epoch: 7 step: 223, loss is 2.0777775716851465e-05\n",
      "epoch: 7 step: 224, loss is 0.0002743779041338712\n",
      "epoch: 7 step: 225, loss is 0.00028759127599187195\n",
      "epoch: 7 step: 226, loss is 0.041263896971940994\n",
      "epoch: 7 step: 227, loss is 9.955929272109643e-05\n",
      "epoch: 7 step: 228, loss is 0.001218519639223814\n",
      "epoch: 7 step: 229, loss is 0.029218746349215508\n",
      "epoch: 7 step: 230, loss is 0.12205696851015091\n",
      "epoch: 7 step: 231, loss is 0.08195493370294571\n",
      "epoch: 7 step: 232, loss is 0.051665596663951874\n",
      "epoch: 7 step: 233, loss is 0.004700245335698128\n",
      "epoch: 7 step: 234, loss is 0.11604805290699005\n",
      "epoch: 7 step: 235, loss is 0.061831410974264145\n",
      "epoch: 7 step: 236, loss is 0.010080729611217976\n",
      "epoch: 7 step: 237, loss is 0.007100126706063747\n",
      "epoch: 7 step: 238, loss is 0.0061833662912249565\n",
      "epoch: 7 step: 239, loss is 0.0019908645190298557\n",
      "epoch: 7 step: 240, loss is 0.0008475594222545624\n",
      "epoch: 7 step: 241, loss is 0.000365399377187714\n",
      "epoch: 7 step: 242, loss is 0.0003521713078953326\n",
      "epoch: 7 step: 243, loss is 0.0029545819852501154\n",
      "epoch: 7 step: 244, loss is 0.0022221794351935387\n",
      "epoch: 7 step: 245, loss is 0.01207712572067976\n",
      "epoch: 7 step: 246, loss is 0.018643323332071304\n",
      "epoch: 7 step: 247, loss is 0.00485988799482584\n",
      "epoch: 7 step: 248, loss is 0.006203506141901016\n",
      "epoch: 7 step: 249, loss is 0.002559694927185774\n",
      "epoch: 7 step: 250, loss is 0.00496294628828764\n",
      "epoch: 7 step: 251, loss is 0.009737095795571804\n",
      "epoch: 7 step: 252, loss is 0.08112265169620514\n",
      "epoch: 7 step: 253, loss is 0.06624670326709747\n",
      "epoch: 7 step: 254, loss is 0.23825110495090485\n",
      "epoch: 7 step: 255, loss is 0.17577318847179413\n",
      "epoch: 7 step: 256, loss is 0.001988964155316353\n",
      "epoch: 7 step: 257, loss is 0.0010354577098041773\n",
      "epoch: 7 step: 258, loss is 0.005176333244889975\n",
      "epoch: 7 step: 259, loss is 0.009698732756078243\n",
      "epoch: 7 step: 260, loss is 0.0006370497867465019\n",
      "epoch: 7 step: 261, loss is 0.005035218317061663\n",
      "epoch: 7 step: 262, loss is 8.63458335516043e-05\n",
      "epoch: 7 step: 263, loss is 0.022598713636398315\n",
      "epoch: 7 step: 264, loss is 0.12561218440532684\n",
      "epoch: 7 step: 265, loss is 0.002336482983082533\n",
      "epoch: 7 step: 266, loss is 0.06740786135196686\n",
      "epoch: 7 step: 267, loss is 0.08952177315950394\n",
      "epoch: 7 step: 268, loss is 0.21270643174648285\n",
      "epoch: 7 step: 269, loss is 0.17886993288993835\n",
      "epoch: 7 step: 270, loss is 0.002426460850983858\n",
      "epoch: 7 step: 271, loss is 0.004198132082819939\n",
      "epoch: 7 step: 272, loss is 0.10305717587471008\n",
      "epoch: 7 step: 273, loss is 0.005553812719881535\n",
      "epoch: 7 step: 274, loss is 0.07926587760448456\n",
      "epoch: 7 step: 275, loss is 0.0015968243824318051\n",
      "epoch: 7 step: 276, loss is 0.0012618668843060732\n",
      "epoch: 7 step: 277, loss is 0.00035430159186944366\n",
      "epoch: 7 step: 278, loss is 0.002131315879523754\n",
      "epoch: 7 step: 279, loss is 0.002025148831307888\n",
      "epoch: 7 step: 280, loss is 0.0016852477565407753\n",
      "epoch: 7 step: 281, loss is 0.03825816512107849\n",
      "epoch: 7 step: 282, loss is 0.0014382367953658104\n",
      "epoch: 7 step: 283, loss is 0.00035827094689011574\n",
      "epoch: 7 step: 284, loss is 0.019787076860666275\n",
      "epoch: 7 step: 285, loss is 0.004948382265865803\n",
      "epoch: 7 step: 286, loss is 0.001242940197698772\n",
      "epoch: 7 step: 287, loss is 0.006279684603214264\n",
      "epoch: 7 step: 288, loss is 0.04691516235470772\n",
      "epoch: 7 step: 289, loss is 0.03420935198664665\n",
      "epoch: 7 step: 290, loss is 0.06951291114091873\n",
      "epoch: 7 step: 291, loss is 0.025253480300307274\n",
      "epoch: 7 step: 292, loss is 0.10480548441410065\n",
      "epoch: 7 step: 293, loss is 0.0003859465359710157\n",
      "epoch: 7 step: 294, loss is 0.0019180853851139545\n",
      "epoch: 7 step: 295, loss is 0.11514273285865784\n",
      "epoch: 7 step: 296, loss is 0.002759054535999894\n",
      "epoch: 7 step: 297, loss is 0.0031045572832226753\n",
      "epoch: 7 step: 298, loss is 0.0037830898072570562\n",
      "epoch: 7 step: 299, loss is 0.0007418610039167106\n",
      "epoch: 7 step: 300, loss is 0.005047242622822523\n",
      "epoch: 7 step: 301, loss is 0.0013486645184457302\n",
      "epoch: 7 step: 302, loss is 0.0010473078582435846\n",
      "epoch: 7 step: 303, loss is 0.016410620883107185\n",
      "epoch: 7 step: 304, loss is 0.012276706285774708\n",
      "epoch: 7 step: 305, loss is 0.00020890473388135433\n",
      "epoch: 7 step: 306, loss is 0.0357462614774704\n",
      "epoch: 7 step: 307, loss is 0.007707621436566114\n",
      "epoch: 7 step: 308, loss is 0.0005603645113296807\n",
      "epoch: 7 step: 309, loss is 0.02062232606112957\n",
      "epoch: 7 step: 310, loss is 0.0006556606385856867\n",
      "epoch: 7 step: 311, loss is 0.00195548078045249\n",
      "epoch: 7 step: 312, loss is 0.0007591465255245566\n",
      "epoch: 7 step: 313, loss is 0.010502652265131474\n",
      "epoch: 7 step: 314, loss is 0.00042822788236662745\n",
      "epoch: 7 step: 315, loss is 0.00033852283377200365\n",
      "epoch: 7 step: 316, loss is 0.0025723963044583797\n",
      "epoch: 7 step: 317, loss is 0.043388064950704575\n",
      "epoch: 7 step: 318, loss is 0.00032766268122941256\n",
      "epoch: 7 step: 319, loss is 0.010942578315734863\n",
      "epoch: 7 step: 320, loss is 0.010584763251245022\n",
      "epoch: 7 step: 321, loss is 0.007118483539670706\n",
      "epoch: 7 step: 322, loss is 0.025748783722519875\n",
      "epoch: 7 step: 323, loss is 0.016028298065066338\n",
      "epoch: 7 step: 324, loss is 0.11710722744464874\n",
      "epoch: 7 step: 325, loss is 0.0004797591536771506\n",
      "epoch: 7 step: 326, loss is 0.0013903920771554112\n",
      "epoch: 7 step: 327, loss is 0.0011971902567893267\n",
      "epoch: 7 step: 328, loss is 0.02420518361032009\n",
      "epoch: 7 step: 329, loss is 0.013931168243288994\n",
      "epoch: 7 step: 330, loss is 0.0004925628309138119\n",
      "epoch: 7 step: 331, loss is 0.000552127487026155\n",
      "epoch: 7 step: 332, loss is 0.0015247630653902888\n",
      "epoch: 7 step: 333, loss is 0.00330124469473958\n",
      "epoch: 7 step: 334, loss is 0.0011861821403726935\n",
      "epoch: 7 step: 335, loss is 0.039012663066387177\n",
      "epoch: 7 step: 336, loss is 0.0007736181141808629\n",
      "epoch: 7 step: 337, loss is 0.022766532376408577\n",
      "epoch: 7 step: 338, loss is 0.0003048614307772368\n",
      "epoch: 7 step: 339, loss is 0.0014897853834554553\n",
      "epoch: 7 step: 340, loss is 0.005442909896373749\n",
      "epoch: 7 step: 341, loss is 0.0014499896205961704\n",
      "epoch: 7 step: 342, loss is 0.017992369830608368\n",
      "epoch: 7 step: 343, loss is 0.0011184934992343187\n",
      "epoch: 7 step: 344, loss is 0.0018455075332894921\n",
      "epoch: 7 step: 345, loss is 0.0010039207991212606\n",
      "epoch: 7 step: 346, loss is 0.0012368618045002222\n",
      "epoch: 7 step: 347, loss is 0.0001391175901517272\n",
      "epoch: 7 step: 348, loss is 0.007831339724361897\n",
      "epoch: 7 step: 349, loss is 0.0002604668552521616\n",
      "epoch: 7 step: 350, loss is 0.00021304385154508054\n",
      "epoch: 7 step: 351, loss is 0.018428772687911987\n",
      "epoch: 7 step: 352, loss is 0.011642404831945896\n",
      "epoch: 7 step: 353, loss is 0.008164159022271633\n",
      "epoch: 7 step: 354, loss is 0.0037686857394874096\n",
      "epoch: 7 step: 355, loss is 0.0009623224614188075\n",
      "epoch: 7 step: 356, loss is 0.001896167523227632\n",
      "epoch: 7 step: 357, loss is 0.11801207065582275\n",
      "epoch: 7 step: 358, loss is 0.13672401010990143\n",
      "epoch: 7 step: 359, loss is 0.005410962738096714\n",
      "epoch: 7 step: 360, loss is 0.0009920805459842086\n",
      "epoch: 7 step: 361, loss is 0.002270784694701433\n",
      "epoch: 7 step: 362, loss is 0.008720895275473595\n",
      "epoch: 7 step: 363, loss is 0.030374540016055107\n",
      "epoch: 7 step: 364, loss is 0.000355525960912928\n",
      "epoch: 7 step: 365, loss is 0.000421286589698866\n",
      "epoch: 7 step: 366, loss is 0.0020552235655486584\n",
      "epoch: 7 step: 367, loss is 0.05850796774029732\n",
      "epoch: 7 step: 368, loss is 0.027255302295088768\n",
      "epoch: 7 step: 369, loss is 0.01844865269958973\n",
      "epoch: 7 step: 370, loss is 0.00010462867794558406\n",
      "epoch: 7 step: 371, loss is 0.0002420864038867876\n",
      "epoch: 7 step: 372, loss is 0.039272889494895935\n",
      "epoch: 7 step: 373, loss is 0.01857396960258484\n",
      "epoch: 7 step: 374, loss is 0.0044364710338413715\n",
      "epoch: 7 step: 375, loss is 0.003118747379630804\n",
      "epoch: 7 step: 376, loss is 0.001309934421442449\n",
      "epoch: 7 step: 377, loss is 0.0007126941927708685\n",
      "epoch: 7 step: 378, loss is 0.003229528898373246\n",
      "epoch: 7 step: 379, loss is 0.001437669969163835\n",
      "epoch: 7 step: 380, loss is 0.00021627727255690843\n",
      "epoch: 7 step: 381, loss is 0.015493337996304035\n",
      "epoch: 7 step: 382, loss is 0.0030098508577793837\n",
      "epoch: 7 step: 383, loss is 0.024566907435655594\n",
      "epoch: 7 step: 384, loss is 0.0006876322440803051\n",
      "epoch: 7 step: 385, loss is 0.008332745172083378\n",
      "epoch: 7 step: 386, loss is 0.0013343470636755228\n",
      "epoch: 7 step: 387, loss is 0.004013910889625549\n",
      "epoch: 7 step: 388, loss is 0.0010515841422602534\n",
      "epoch: 7 step: 389, loss is 0.00010131440649274737\n",
      "epoch: 7 step: 390, loss is 0.00040116088348440826\n",
      "epoch: 7 step: 391, loss is 0.011966702528297901\n",
      "epoch: 7 step: 392, loss is 0.012555013410747051\n",
      "epoch: 7 step: 393, loss is 0.05250311642885208\n",
      "epoch: 7 step: 394, loss is 0.04598425328731537\n",
      "epoch: 7 step: 395, loss is 0.02892516553401947\n",
      "epoch: 7 step: 396, loss is 0.009396417066454887\n",
      "epoch: 7 step: 397, loss is 0.0007228522445075214\n",
      "epoch: 7 step: 398, loss is 0.041326772421598434\n",
      "epoch: 7 step: 399, loss is 0.001567781437188387\n",
      "epoch: 7 step: 400, loss is 0.00041359401075169444\n",
      "epoch: 7 step: 401, loss is 0.0006837582914158702\n",
      "epoch: 7 step: 402, loss is 0.06821323186159134\n",
      "epoch: 7 step: 403, loss is 0.004077916033565998\n",
      "epoch: 7 step: 404, loss is 0.004573935177177191\n",
      "epoch: 7 step: 405, loss is 0.003628968261182308\n",
      "epoch: 7 step: 406, loss is 0.008361692540347576\n",
      "epoch: 7 step: 407, loss is 0.011399629525840282\n",
      "epoch: 7 step: 408, loss is 0.011922982521355152\n",
      "epoch: 7 step: 409, loss is 0.017906324937939644\n",
      "epoch: 7 step: 410, loss is 0.022476080805063248\n",
      "epoch: 7 step: 411, loss is 0.026095665991306305\n",
      "epoch: 7 step: 412, loss is 0.009541409090161324\n",
      "epoch: 7 step: 413, loss is 0.0007485582027584314\n",
      "epoch: 7 step: 414, loss is 0.008185590617358685\n",
      "epoch: 7 step: 415, loss is 0.005702704656869173\n",
      "epoch: 7 step: 416, loss is 0.009279735386371613\n",
      "epoch: 7 step: 417, loss is 0.00046182810910977423\n",
      "epoch: 7 step: 418, loss is 0.08803115785121918\n",
      "epoch: 7 step: 419, loss is 0.008773096837103367\n",
      "epoch: 7 step: 420, loss is 0.026162220165133476\n",
      "epoch: 7 step: 421, loss is 0.021308889612555504\n",
      "epoch: 7 step: 422, loss is 0.002933200215920806\n",
      "epoch: 7 step: 423, loss is 0.00011093341163359582\n",
      "epoch: 7 step: 424, loss is 0.0016609790036454797\n",
      "epoch: 7 step: 425, loss is 0.004618240054696798\n",
      "epoch: 7 step: 426, loss is 0.0005266645457595587\n",
      "epoch: 7 step: 427, loss is 0.0005100457929074764\n",
      "epoch: 7 step: 428, loss is 0.027049709111452103\n",
      "epoch: 7 step: 429, loss is 0.0016333047533407807\n",
      "epoch: 7 step: 430, loss is 0.0005352927255444229\n",
      "epoch: 7 step: 431, loss is 0.0020866987761110067\n",
      "epoch: 7 step: 432, loss is 0.009687364101409912\n",
      "epoch: 7 step: 433, loss is 0.0072267628274858\n",
      "epoch: 7 step: 434, loss is 0.021946515887975693\n",
      "epoch: 7 step: 435, loss is 0.03327498584985733\n",
      "epoch: 7 step: 436, loss is 0.002407180145382881\n",
      "epoch: 7 step: 437, loss is 0.0019535149913281202\n",
      "epoch: 7 step: 438, loss is 0.0067228833213448524\n",
      "epoch: 7 step: 439, loss is 0.041473474353551865\n",
      "epoch: 7 step: 440, loss is 0.004357822239398956\n",
      "epoch: 7 step: 441, loss is 0.0019481746712699533\n",
      "epoch: 7 step: 442, loss is 0.043740224093198776\n",
      "epoch: 7 step: 443, loss is 0.00026862899539992213\n",
      "epoch: 7 step: 444, loss is 0.00024406888405792415\n",
      "epoch: 7 step: 445, loss is 0.006492685526609421\n",
      "epoch: 7 step: 446, loss is 0.0001223974395543337\n",
      "epoch: 7 step: 447, loss is 0.005215552635490894\n",
      "epoch: 7 step: 448, loss is 0.22237814962863922\n",
      "epoch: 7 step: 449, loss is 0.009176578372716904\n",
      "epoch: 7 step: 450, loss is 2.066629531327635e-05\n",
      "epoch: 7 step: 451, loss is 4.9860376748256385e-05\n",
      "epoch: 7 step: 452, loss is 0.2371661216020584\n",
      "epoch: 7 step: 453, loss is 0.0004750666848849505\n",
      "epoch: 7 step: 454, loss is 0.013374736532568932\n",
      "epoch: 7 step: 455, loss is 0.014498945325613022\n",
      "epoch: 7 step: 456, loss is 0.0002032549527939409\n",
      "epoch: 7 step: 457, loss is 0.01729707047343254\n",
      "epoch: 7 step: 458, loss is 0.036000270396471024\n",
      "epoch: 7 step: 459, loss is 4.902918226434849e-05\n",
      "epoch: 7 step: 460, loss is 0.0005221250467002392\n",
      "epoch: 7 step: 461, loss is 0.00031985979876480997\n",
      "epoch: 7 step: 462, loss is 0.0016281480202451348\n",
      "epoch: 7 step: 463, loss is 0.18498194217681885\n",
      "epoch: 7 step: 464, loss is 0.0001988263538805768\n",
      "epoch: 7 step: 465, loss is 0.014173795469105244\n",
      "epoch: 7 step: 466, loss is 0.0006265352130867541\n",
      "epoch: 7 step: 467, loss is 0.000511040969286114\n",
      "epoch: 7 step: 468, loss is 0.01245726179331541\n",
      "epoch: 7 step: 469, loss is 0.0002754738670773804\n",
      "epoch: 7 step: 470, loss is 0.07008412480354309\n",
      "epoch: 7 step: 471, loss is 0.006517644971609116\n",
      "epoch: 7 step: 472, loss is 0.00863740500062704\n",
      "epoch: 7 step: 473, loss is 0.047541067004203796\n",
      "epoch: 7 step: 474, loss is 0.013999461196362972\n",
      "epoch: 7 step: 475, loss is 0.001776291523128748\n",
      "epoch: 7 step: 476, loss is 0.006071394309401512\n",
      "epoch: 7 step: 477, loss is 0.006217420566827059\n",
      "epoch: 7 step: 478, loss is 0.001066096592694521\n",
      "epoch: 7 step: 479, loss is 0.0008446699939668179\n",
      "epoch: 7 step: 480, loss is 0.0025300183333456516\n",
      "epoch: 7 step: 481, loss is 0.0010747290216386318\n",
      "epoch: 7 step: 482, loss is 0.1768195629119873\n",
      "epoch: 7 step: 483, loss is 0.028349796310067177\n",
      "epoch: 7 step: 484, loss is 0.0003069973608944565\n",
      "epoch: 7 step: 485, loss is 0.0007823287160135806\n",
      "epoch: 7 step: 486, loss is 0.0018883084412664175\n",
      "epoch: 7 step: 487, loss is 0.004270846024155617\n",
      "epoch: 7 step: 488, loss is 0.0007761635351926088\n",
      "epoch: 7 step: 489, loss is 0.0005366930272430182\n",
      "epoch: 7 step: 490, loss is 0.019994134083390236\n",
      "epoch: 7 step: 491, loss is 0.03438541293144226\n",
      "epoch: 7 step: 492, loss is 0.004899520426988602\n",
      "epoch: 7 step: 493, loss is 0.008651452139019966\n",
      "epoch: 7 step: 494, loss is 0.0064744348637759686\n",
      "epoch: 7 step: 495, loss is 0.0011101921554654837\n",
      "epoch: 7 step: 496, loss is 0.0006149123655632138\n",
      "epoch: 7 step: 497, loss is 0.0004746576305478811\n",
      "epoch: 7 step: 498, loss is 0.0277619119733572\n",
      "epoch: 7 step: 499, loss is 0.04436419904232025\n",
      "epoch: 7 step: 500, loss is 0.000693214766215533\n",
      "epoch: 7 step: 501, loss is 0.023522110655903816\n",
      "epoch: 7 step: 502, loss is 0.043919891119003296\n",
      "epoch: 7 step: 503, loss is 3.039301373064518e-05\n",
      "epoch: 7 step: 504, loss is 0.09832410514354706\n",
      "epoch: 7 step: 505, loss is 0.029395589604973793\n",
      "epoch: 7 step: 506, loss is 0.016487758606672287\n",
      "epoch: 7 step: 507, loss is 0.0011694306740537286\n",
      "epoch: 7 step: 508, loss is 0.08769658207893372\n",
      "epoch: 7 step: 509, loss is 0.09861871600151062\n",
      "epoch: 7 step: 510, loss is 0.0026289746165275574\n",
      "epoch: 7 step: 511, loss is 0.0009736255742609501\n",
      "epoch: 7 step: 512, loss is 0.02961566299200058\n",
      "epoch: 7 step: 513, loss is 0.002502610208466649\n",
      "epoch: 7 step: 514, loss is 0.03565610945224762\n",
      "epoch: 7 step: 515, loss is 0.24010969698429108\n",
      "epoch: 7 step: 516, loss is 0.001683758688159287\n",
      "epoch: 7 step: 517, loss is 0.00051609001820907\n",
      "epoch: 7 step: 518, loss is 0.002066914224997163\n",
      "epoch: 7 step: 519, loss is 0.0010033033322542906\n",
      "epoch: 7 step: 520, loss is 0.0023428129497915506\n",
      "epoch: 7 step: 521, loss is 0.0901961475610733\n",
      "epoch: 7 step: 522, loss is 0.0006628510891459882\n",
      "epoch: 7 step: 523, loss is 0.0023076534271240234\n",
      "epoch: 7 step: 524, loss is 0.02658732235431671\n",
      "epoch: 7 step: 525, loss is 0.0024588422384113073\n",
      "epoch: 7 step: 526, loss is 0.0002126242616213858\n",
      "epoch: 7 step: 527, loss is 0.002900991588830948\n",
      "epoch: 7 step: 528, loss is 0.01027115061879158\n",
      "epoch: 7 step: 529, loss is 0.019698631018400192\n",
      "epoch: 7 step: 530, loss is 0.037399016320705414\n",
      "epoch: 7 step: 531, loss is 0.0003311926266178489\n",
      "epoch: 7 step: 532, loss is 0.0004397106822580099\n",
      "epoch: 7 step: 533, loss is 0.42930641770362854\n",
      "epoch: 7 step: 534, loss is 0.0026740850880742073\n",
      "epoch: 7 step: 535, loss is 0.05465594306588173\n",
      "epoch: 7 step: 536, loss is 0.0005168843199498951\n",
      "epoch: 7 step: 537, loss is 0.0026155051309615374\n",
      "epoch: 7 step: 538, loss is 0.0025095571763813496\n",
      "epoch: 7 step: 539, loss is 0.01519075222313404\n",
      "epoch: 7 step: 540, loss is 0.0020423198584467173\n",
      "epoch: 7 step: 541, loss is 0.01822248287498951\n",
      "epoch: 7 step: 542, loss is 0.20421715080738068\n",
      "epoch: 7 step: 543, loss is 0.018077202141284943\n",
      "epoch: 7 step: 544, loss is 0.022528300061821938\n",
      "epoch: 7 step: 545, loss is 0.0003760612744372338\n",
      "epoch: 7 step: 546, loss is 0.009852416813373566\n",
      "epoch: 7 step: 547, loss is 0.0008610285585746169\n",
      "epoch: 7 step: 548, loss is 0.002129195723682642\n",
      "epoch: 7 step: 549, loss is 0.026004355400800705\n",
      "epoch: 7 step: 550, loss is 0.14527107775211334\n",
      "epoch: 7 step: 551, loss is 0.0010898987529799342\n",
      "epoch: 7 step: 552, loss is 0.00012808118481189013\n",
      "epoch: 7 step: 553, loss is 0.019351311028003693\n",
      "epoch: 7 step: 554, loss is 0.0004479152266867459\n",
      "epoch: 7 step: 555, loss is 0.01310502178966999\n",
      "epoch: 7 step: 556, loss is 0.0006560535985045135\n",
      "epoch: 7 step: 557, loss is 0.01586221344769001\n",
      "epoch: 7 step: 558, loss is 0.0012643071822822094\n",
      "epoch: 7 step: 559, loss is 0.0003923273179680109\n",
      "epoch: 7 step: 560, loss is 0.028444858267903328\n",
      "epoch: 7 step: 561, loss is 0.004109014756977558\n",
      "epoch: 7 step: 562, loss is 0.07115747034549713\n",
      "epoch: 7 step: 563, loss is 0.0022353308741003275\n",
      "epoch: 7 step: 564, loss is 0.0030544004403054714\n",
      "epoch: 7 step: 565, loss is 0.001628057798370719\n",
      "epoch: 7 step: 566, loss is 0.000359005673089996\n",
      "epoch: 7 step: 567, loss is 0.0025981413200497627\n",
      "epoch: 7 step: 568, loss is 0.32002323865890503\n",
      "epoch: 7 step: 569, loss is 0.0019291688222438097\n",
      "epoch: 7 step: 570, loss is 0.005478682462126017\n",
      "epoch: 7 step: 571, loss is 0.0866757333278656\n",
      "epoch: 7 step: 572, loss is 0.024996541440486908\n",
      "epoch: 7 step: 573, loss is 0.0024551204405725002\n",
      "epoch: 7 step: 574, loss is 0.0009094362030737102\n",
      "epoch: 7 step: 575, loss is 0.0005212176474742591\n",
      "epoch: 7 step: 576, loss is 0.006280804518610239\n",
      "epoch: 7 step: 577, loss is 0.02030491828918457\n",
      "epoch: 7 step: 578, loss is 0.004710766952484846\n",
      "epoch: 7 step: 579, loss is 0.001692769001238048\n",
      "epoch: 7 step: 580, loss is 0.01530133094638586\n",
      "epoch: 7 step: 581, loss is 0.0005754932644777\n",
      "epoch: 7 step: 582, loss is 0.015354134142398834\n",
      "epoch: 7 step: 583, loss is 0.0019398154690861702\n",
      "epoch: 7 step: 584, loss is 0.0006864448660053313\n",
      "epoch: 7 step: 585, loss is 0.03032289259135723\n",
      "epoch: 7 step: 586, loss is 0.006133595015853643\n",
      "epoch: 7 step: 587, loss is 0.0023979058023542166\n",
      "epoch: 7 step: 588, loss is 0.00023955538927111775\n",
      "epoch: 7 step: 589, loss is 0.1201368123292923\n",
      "epoch: 7 step: 590, loss is 0.12766607105731964\n",
      "epoch: 7 step: 591, loss is 0.017928246408700943\n",
      "epoch: 7 step: 592, loss is 0.002040861640125513\n",
      "epoch: 7 step: 593, loss is 0.002362243365496397\n",
      "epoch: 7 step: 594, loss is 0.0004987748106941581\n",
      "epoch: 7 step: 595, loss is 0.0003312781627755612\n",
      "epoch: 7 step: 596, loss is 0.003018071176484227\n",
      "epoch: 7 step: 597, loss is 0.018247218802571297\n",
      "epoch: 7 step: 598, loss is 0.001231918460689485\n",
      "epoch: 7 step: 599, loss is 0.012266977690160275\n",
      "epoch: 7 step: 600, loss is 0.01447953749448061\n",
      "epoch: 7 step: 601, loss is 0.030268175527453423\n",
      "epoch: 7 step: 602, loss is 0.001053985906764865\n",
      "epoch: 7 step: 603, loss is 0.002209021942690015\n",
      "epoch: 7 step: 604, loss is 0.0009278372745029628\n",
      "epoch: 7 step: 605, loss is 0.014679181389510632\n",
      "epoch: 7 step: 606, loss is 0.0009359981631860137\n",
      "epoch: 7 step: 607, loss is 0.030353136360645294\n",
      "epoch: 7 step: 608, loss is 0.001242595026269555\n",
      "epoch: 7 step: 609, loss is 0.0031443778425455093\n",
      "epoch: 7 step: 610, loss is 0.0006156291346997023\n",
      "epoch: 7 step: 611, loss is 0.01223043818026781\n",
      "epoch: 7 step: 612, loss is 0.00282002124004066\n",
      "epoch: 7 step: 613, loss is 0.002729258732870221\n",
      "epoch: 7 step: 614, loss is 0.06044165790081024\n",
      "epoch: 7 step: 615, loss is 0.0004443516372703016\n",
      "epoch: 7 step: 616, loss is 0.0006462638266384602\n",
      "epoch: 7 step: 617, loss is 0.030356649309396744\n",
      "epoch: 7 step: 618, loss is 0.0016757509438320994\n",
      "epoch: 7 step: 619, loss is 0.0014586849138140678\n",
      "epoch: 7 step: 620, loss is 0.0024403384886682034\n",
      "epoch: 7 step: 621, loss is 0.0029571966733783484\n",
      "epoch: 7 step: 622, loss is 0.02367873676121235\n",
      "epoch: 7 step: 623, loss is 0.0011592434020712972\n",
      "epoch: 7 step: 624, loss is 0.013152405619621277\n",
      "epoch: 7 step: 625, loss is 0.009720288217067719\n",
      "epoch: 7 step: 626, loss is 0.0013908280525356531\n",
      "epoch: 7 step: 627, loss is 9.824305743677542e-05\n",
      "epoch: 7 step: 628, loss is 0.05281972512602806\n",
      "epoch: 7 step: 629, loss is 0.002743486547842622\n",
      "epoch: 7 step: 630, loss is 0.004395224619656801\n",
      "epoch: 7 step: 631, loss is 0.02928229607641697\n",
      "epoch: 7 step: 632, loss is 0.00066040875390172\n",
      "epoch: 7 step: 633, loss is 0.008949857205152512\n",
      "epoch: 7 step: 634, loss is 0.0003774504875764251\n",
      "epoch: 7 step: 635, loss is 0.022195938974618912\n",
      "epoch: 7 step: 636, loss is 0.040529731661081314\n",
      "epoch: 7 step: 637, loss is 0.005434782709926367\n",
      "epoch: 7 step: 638, loss is 0.00034563185181468725\n",
      "epoch: 7 step: 639, loss is 0.005923095159232616\n",
      "epoch: 7 step: 640, loss is 0.001767739886417985\n",
      "epoch: 7 step: 641, loss is 0.014346138574182987\n",
      "epoch: 7 step: 642, loss is 0.07130856066942215\n",
      "epoch: 7 step: 643, loss is 0.0009746127761900425\n",
      "epoch: 7 step: 644, loss is 0.001215119962580502\n",
      "epoch: 7 step: 645, loss is 8.022758265724406e-05\n",
      "epoch: 7 step: 646, loss is 0.002414502203464508\n",
      "epoch: 7 step: 647, loss is 0.01328758429735899\n",
      "epoch: 7 step: 648, loss is 0.023180151358246803\n",
      "epoch: 7 step: 649, loss is 0.0030219361651688814\n",
      "epoch: 7 step: 650, loss is 0.0010340793523937464\n",
      "epoch: 7 step: 651, loss is 0.04286642372608185\n",
      "epoch: 7 step: 652, loss is 0.00014585003373213112\n",
      "epoch: 7 step: 653, loss is 0.0009055231348611414\n",
      "epoch: 7 step: 654, loss is 0.001169271650724113\n",
      "epoch: 7 step: 655, loss is 0.07218276709318161\n",
      "epoch: 7 step: 656, loss is 0.02416655793786049\n",
      "epoch: 7 step: 657, loss is 0.001448639901354909\n",
      "epoch: 7 step: 658, loss is 0.00387918041087687\n",
      "epoch: 7 step: 659, loss is 0.0006940262392163277\n",
      "epoch: 7 step: 660, loss is 0.018017712980508804\n",
      "epoch: 7 step: 661, loss is 0.00581946037709713\n",
      "epoch: 7 step: 662, loss is 0.00041738973231986165\n",
      "epoch: 7 step: 663, loss is 0.013959207572042942\n",
      "epoch: 7 step: 664, loss is 0.0007332194945774972\n",
      "epoch: 7 step: 665, loss is 0.00594734400510788\n",
      "epoch: 7 step: 666, loss is 0.11291670799255371\n",
      "epoch: 7 step: 667, loss is 9.718949877424166e-05\n",
      "epoch: 7 step: 668, loss is 0.001466259709559381\n",
      "epoch: 7 step: 669, loss is 0.16434305906295776\n",
      "epoch: 7 step: 670, loss is 5.8650533901527524e-05\n",
      "epoch: 7 step: 671, loss is 0.00925944559276104\n",
      "epoch: 7 step: 672, loss is 0.00023791086277924478\n",
      "epoch: 7 step: 673, loss is 0.00303640472702682\n",
      "epoch: 7 step: 674, loss is 0.12939174473285675\n",
      "epoch: 7 step: 675, loss is 0.010301086120307446\n",
      "epoch: 7 step: 676, loss is 0.010889614000916481\n",
      "epoch: 7 step: 677, loss is 0.002082925522699952\n",
      "epoch: 7 step: 678, loss is 0.005865432787686586\n",
      "epoch: 7 step: 679, loss is 0.006753574591130018\n",
      "epoch: 7 step: 680, loss is 0.04097289219498634\n",
      "epoch: 7 step: 681, loss is 0.0013152770698070526\n",
      "epoch: 7 step: 682, loss is 0.16324098408222198\n",
      "epoch: 7 step: 683, loss is 0.0011405523400753736\n",
      "epoch: 7 step: 684, loss is 0.0005565828178077936\n",
      "epoch: 7 step: 685, loss is 0.009889019653201103\n",
      "epoch: 7 step: 686, loss is 0.037045639008283615\n",
      "epoch: 7 step: 687, loss is 0.005101990420371294\n",
      "epoch: 7 step: 688, loss is 0.00040088914101943374\n",
      "epoch: 7 step: 689, loss is 0.04462503641843796\n",
      "epoch: 7 step: 690, loss is 0.00217396579682827\n",
      "epoch: 7 step: 691, loss is 9.055966074811295e-05\n",
      "epoch: 7 step: 692, loss is 0.022545980289578438\n",
      "epoch: 7 step: 693, loss is 0.022402824833989143\n",
      "epoch: 7 step: 694, loss is 0.0057721612975001335\n",
      "epoch: 7 step: 695, loss is 0.041403938084840775\n",
      "epoch: 7 step: 696, loss is 0.0072369324043393135\n",
      "epoch: 7 step: 697, loss is 0.011305004358291626\n",
      "epoch: 7 step: 698, loss is 0.0066981241106987\n",
      "epoch: 7 step: 699, loss is 0.0009460995206609368\n",
      "epoch: 7 step: 700, loss is 0.07539387047290802\n",
      "epoch: 7 step: 701, loss is 0.0018895177636295557\n",
      "epoch: 7 step: 702, loss is 0.005101325921714306\n",
      "epoch: 7 step: 703, loss is 0.003782500745728612\n",
      "epoch: 7 step: 704, loss is 0.0047498648054897785\n",
      "epoch: 7 step: 705, loss is 0.0038812090642750263\n",
      "epoch: 7 step: 706, loss is 0.0030568535439670086\n",
      "epoch: 7 step: 707, loss is 0.00021631909476127476\n",
      "epoch: 7 step: 708, loss is 4.211788109387271e-05\n",
      "epoch: 7 step: 709, loss is 0.019995296373963356\n",
      "epoch: 7 step: 710, loss is 0.09582223743200302\n",
      "epoch: 7 step: 711, loss is 0.004351004958152771\n",
      "epoch: 7 step: 712, loss is 0.0005703098140656948\n",
      "epoch: 7 step: 713, loss is 0.0002775350003503263\n",
      "epoch: 7 step: 714, loss is 0.021851921454072\n",
      "epoch: 7 step: 715, loss is 0.004784840624779463\n",
      "epoch: 7 step: 716, loss is 7.238703256007284e-05\n",
      "epoch: 7 step: 717, loss is 0.0013830058742314577\n",
      "epoch: 7 step: 718, loss is 0.06734015047550201\n",
      "epoch: 7 step: 719, loss is 0.14499861001968384\n",
      "epoch: 7 step: 720, loss is 0.046360332518815994\n",
      "epoch: 7 step: 721, loss is 8.605874609202147e-05\n",
      "epoch: 7 step: 722, loss is 0.004365238826721907\n",
      "epoch: 7 step: 723, loss is 0.046477142721414566\n",
      "epoch: 7 step: 724, loss is 0.051243141293525696\n",
      "epoch: 7 step: 725, loss is 0.0018879928393289447\n",
      "epoch: 7 step: 726, loss is 0.001659852801822126\n",
      "epoch: 7 step: 727, loss is 0.00024054526875261217\n",
      "epoch: 7 step: 728, loss is 0.003493121126666665\n",
      "epoch: 7 step: 729, loss is 0.00038639450212940574\n",
      "epoch: 7 step: 730, loss is 0.000858647283166647\n",
      "epoch: 7 step: 731, loss is 0.0014252436812967062\n",
      "epoch: 7 step: 732, loss is 0.0008369709830731153\n",
      "epoch: 7 step: 733, loss is 0.0020994471851736307\n",
      "epoch: 7 step: 734, loss is 0.002006351947784424\n",
      "epoch: 7 step: 735, loss is 0.00772968539968133\n",
      "epoch: 7 step: 736, loss is 0.0006304587004706264\n",
      "epoch: 7 step: 737, loss is 0.0003420238208491355\n",
      "epoch: 7 step: 738, loss is 0.003654027357697487\n",
      "epoch: 7 step: 739, loss is 0.025593476369976997\n",
      "epoch: 7 step: 740, loss is 0.01879635825753212\n",
      "epoch: 7 step: 741, loss is 0.0013736146502196789\n",
      "epoch: 7 step: 742, loss is 0.02401188760995865\n",
      "epoch: 7 step: 743, loss is 0.0018681030487641692\n",
      "epoch: 7 step: 744, loss is 0.015433271415531635\n",
      "epoch: 7 step: 745, loss is 0.006994178518652916\n",
      "epoch: 7 step: 746, loss is 0.001405656454153359\n",
      "epoch: 7 step: 747, loss is 0.13981665670871735\n",
      "epoch: 7 step: 748, loss is 0.016903093084692955\n",
      "epoch: 7 step: 749, loss is 0.0035960336681455374\n",
      "epoch: 7 step: 750, loss is 0.008083498105406761\n",
      "epoch: 7 step: 751, loss is 0.01689230650663376\n",
      "epoch: 7 step: 752, loss is 0.00034124666126444936\n",
      "epoch: 7 step: 753, loss is 0.01051409263163805\n",
      "epoch: 7 step: 754, loss is 0.01691778376698494\n",
      "epoch: 7 step: 755, loss is 0.0012236940674483776\n",
      "epoch: 7 step: 756, loss is 0.30124613642692566\n",
      "epoch: 7 step: 757, loss is 0.008433625102043152\n",
      "epoch: 7 step: 758, loss is 0.0014893338084220886\n",
      "epoch: 7 step: 759, loss is 0.023843305185437202\n",
      "epoch: 7 step: 760, loss is 0.009041400626301765\n",
      "epoch: 7 step: 761, loss is 0.004466765094548464\n",
      "epoch: 7 step: 762, loss is 0.01722411997616291\n",
      "epoch: 7 step: 763, loss is 0.129630908370018\n",
      "epoch: 7 step: 764, loss is 0.007425036747008562\n",
      "epoch: 7 step: 765, loss is 0.0028992746956646442\n",
      "epoch: 7 step: 766, loss is 0.003186287824064493\n",
      "epoch: 7 step: 767, loss is 0.003317794995382428\n",
      "epoch: 7 step: 768, loss is 0.000417857663705945\n",
      "epoch: 7 step: 769, loss is 0.0018799440003931522\n",
      "epoch: 7 step: 770, loss is 0.2116534262895584\n",
      "epoch: 7 step: 771, loss is 0.002718694508075714\n",
      "epoch: 7 step: 772, loss is 0.003902825992554426\n",
      "epoch: 7 step: 773, loss is 0.00019089887791778892\n",
      "epoch: 7 step: 774, loss is 0.0029964125715196133\n",
      "epoch: 7 step: 775, loss is 0.00012718971993308514\n",
      "epoch: 7 step: 776, loss is 0.00023523026902694255\n",
      "epoch: 7 step: 777, loss is 0.00027037080144509673\n",
      "epoch: 7 step: 778, loss is 0.0049647674895823\n",
      "epoch: 7 step: 779, loss is 0.03893232345581055\n",
      "epoch: 7 step: 780, loss is 0.0017266594804823399\n",
      "epoch: 7 step: 781, loss is 0.013389242812991142\n",
      "epoch: 7 step: 782, loss is 0.0002535903768148273\n",
      "epoch: 7 step: 783, loss is 0.05476866289973259\n",
      "epoch: 7 step: 784, loss is 0.007409661076962948\n",
      "epoch: 7 step: 785, loss is 0.0007533477619290352\n",
      "epoch: 7 step: 786, loss is 7.575675408588722e-05\n",
      "epoch: 7 step: 787, loss is 0.016758449375629425\n",
      "epoch: 7 step: 788, loss is 0.01922464929521084\n",
      "epoch: 7 step: 789, loss is 0.0017183830495923758\n",
      "epoch: 7 step: 790, loss is 0.0002805229160003364\n",
      "epoch: 7 step: 791, loss is 0.0006067189387977123\n",
      "epoch: 7 step: 792, loss is 0.032269205898046494\n",
      "epoch: 7 step: 793, loss is 0.0004044853267259896\n",
      "epoch: 7 step: 794, loss is 9.978962043533102e-05\n",
      "epoch: 7 step: 795, loss is 0.016332607716321945\n",
      "epoch: 7 step: 796, loss is 0.07052690535783768\n",
      "epoch: 7 step: 797, loss is 0.026790501549839973\n",
      "epoch: 7 step: 798, loss is 0.0013622659025713801\n",
      "epoch: 7 step: 799, loss is 0.007040268275886774\n",
      "epoch: 7 step: 800, loss is 7.8156306699384e-05\n",
      "epoch: 7 step: 801, loss is 0.3544710576534271\n",
      "epoch: 7 step: 802, loss is 0.04074640944600105\n",
      "epoch: 7 step: 803, loss is 0.0008139318088069558\n",
      "epoch: 7 step: 804, loss is 0.0051286350935697556\n",
      "epoch: 7 step: 805, loss is 0.0007045597885735333\n",
      "epoch: 7 step: 806, loss is 0.0047934274189174175\n",
      "epoch: 7 step: 807, loss is 0.043377313762903214\n",
      "epoch: 7 step: 808, loss is 0.0015412670327350497\n",
      "epoch: 7 step: 809, loss is 0.0022026055958122015\n",
      "epoch: 7 step: 810, loss is 5.62135101063177e-05\n",
      "epoch: 7 step: 811, loss is 0.05597681552171707\n",
      "epoch: 7 step: 812, loss is 0.1408873051404953\n",
      "epoch: 7 step: 813, loss is 0.012761306948959827\n",
      "epoch: 7 step: 814, loss is 0.0026787123642861843\n",
      "epoch: 7 step: 815, loss is 0.007447313982993364\n",
      "epoch: 7 step: 816, loss is 0.0007674497901462018\n",
      "epoch: 7 step: 817, loss is 0.0063817137852311134\n",
      "epoch: 7 step: 818, loss is 0.001090793521143496\n",
      "epoch: 7 step: 819, loss is 0.3334748148918152\n",
      "epoch: 7 step: 820, loss is 0.0021630325354635715\n",
      "epoch: 7 step: 821, loss is 1.7927475710166618e-05\n",
      "epoch: 7 step: 822, loss is 0.00029145757434889674\n",
      "epoch: 7 step: 823, loss is 0.0005297681782394648\n",
      "epoch: 7 step: 824, loss is 0.0007789013907313347\n",
      "epoch: 7 step: 825, loss is 0.1031925156712532\n",
      "epoch: 7 step: 826, loss is 0.06728669255971909\n",
      "epoch: 7 step: 827, loss is 0.00020943215349689126\n",
      "epoch: 7 step: 828, loss is 0.00026607231120578945\n",
      "epoch: 7 step: 829, loss is 0.0006766124279238284\n",
      "epoch: 7 step: 830, loss is 0.0009695489425212145\n",
      "epoch: 7 step: 831, loss is 0.004528661724179983\n",
      "epoch: 7 step: 832, loss is 0.0002461099356878549\n",
      "epoch: 7 step: 833, loss is 0.00037646005512215197\n",
      "epoch: 7 step: 834, loss is 0.014808124862611294\n",
      "epoch: 7 step: 835, loss is 6.398391269613057e-05\n",
      "epoch: 7 step: 836, loss is 0.0014986947644501925\n",
      "epoch: 7 step: 837, loss is 0.10873997956514359\n",
      "epoch: 7 step: 838, loss is 0.0004180532996542752\n",
      "epoch: 7 step: 839, loss is 0.12862607836723328\n",
      "epoch: 7 step: 840, loss is 0.0019221990369260311\n",
      "epoch: 7 step: 841, loss is 0.0002911717165261507\n",
      "epoch: 7 step: 842, loss is 0.00929370615631342\n",
      "epoch: 7 step: 843, loss is 0.0029654742684215307\n",
      "epoch: 7 step: 844, loss is 0.00488319294527173\n",
      "epoch: 7 step: 845, loss is 0.0009560466278344393\n",
      "epoch: 7 step: 846, loss is 0.0001371483813272789\n",
      "epoch: 7 step: 847, loss is 0.05897956341505051\n",
      "epoch: 7 step: 848, loss is 0.005458146799355745\n",
      "epoch: 7 step: 849, loss is 0.01977175660431385\n",
      "epoch: 7 step: 850, loss is 0.20530179142951965\n",
      "epoch: 7 step: 851, loss is 0.011262084357440472\n",
      "epoch: 7 step: 852, loss is 0.000751350715290755\n",
      "epoch: 7 step: 853, loss is 0.022832442075014114\n",
      "epoch: 7 step: 854, loss is 0.004214245826005936\n",
      "epoch: 7 step: 855, loss is 0.004079265519976616\n",
      "epoch: 7 step: 856, loss is 0.0005604420439340174\n",
      "epoch: 7 step: 857, loss is 0.0002549857599660754\n",
      "epoch: 7 step: 858, loss is 0.028331585228443146\n",
      "epoch: 7 step: 859, loss is 0.0007099319482222199\n",
      "epoch: 7 step: 860, loss is 0.05454442277550697\n",
      "epoch: 7 step: 861, loss is 0.006039036437869072\n",
      "epoch: 7 step: 862, loss is 0.001853958354331553\n",
      "epoch: 7 step: 863, loss is 0.057490263134241104\n",
      "epoch: 7 step: 864, loss is 0.0005223258049227297\n",
      "epoch: 7 step: 865, loss is 0.01365557312965393\n",
      "epoch: 7 step: 866, loss is 0.016842875629663467\n",
      "epoch: 7 step: 867, loss is 0.0009686868288554251\n",
      "epoch: 7 step: 868, loss is 0.0003190903225913644\n",
      "epoch: 7 step: 869, loss is 0.004107083193957806\n",
      "epoch: 7 step: 870, loss is 0.014447866939008236\n",
      "epoch: 7 step: 871, loss is 0.0004277183616068214\n",
      "epoch: 7 step: 872, loss is 0.0004654967342503369\n",
      "epoch: 7 step: 873, loss is 0.010279426351189613\n",
      "epoch: 7 step: 874, loss is 0.0010524706449359655\n",
      "epoch: 7 step: 875, loss is 0.11317770183086395\n",
      "epoch: 7 step: 876, loss is 0.0007044521044008434\n",
      "epoch: 7 step: 877, loss is 0.006384188774973154\n",
      "epoch: 7 step: 878, loss is 0.0013464674120768905\n",
      "epoch: 7 step: 879, loss is 0.0003475718549452722\n",
      "epoch: 7 step: 880, loss is 0.0010859238682314754\n",
      "epoch: 7 step: 881, loss is 8.097059617284685e-05\n",
      "epoch: 7 step: 882, loss is 0.005429454613476992\n",
      "epoch: 7 step: 883, loss is 0.011596753261983395\n",
      "epoch: 7 step: 884, loss is 0.0033468850888311863\n",
      "epoch: 7 step: 885, loss is 0.058629706501960754\n",
      "epoch: 7 step: 886, loss is 0.0003677516069728881\n",
      "epoch: 7 step: 887, loss is 0.013624264858663082\n",
      "epoch: 7 step: 888, loss is 0.00032987017766572535\n",
      "epoch: 7 step: 889, loss is 0.0007182229892350733\n",
      "epoch: 7 step: 890, loss is 0.0041052172891795635\n",
      "epoch: 7 step: 891, loss is 0.0004883187939412892\n",
      "epoch: 7 step: 892, loss is 0.0008412593742832541\n",
      "epoch: 7 step: 893, loss is 0.0009040162549354136\n",
      "epoch: 7 step: 894, loss is 0.009024378843605518\n",
      "epoch: 7 step: 895, loss is 0.00035105086863040924\n",
      "epoch: 7 step: 896, loss is 0.004161004442721605\n",
      "epoch: 7 step: 897, loss is 0.013536397367715836\n",
      "epoch: 7 step: 898, loss is 0.02465810254216194\n",
      "epoch: 7 step: 899, loss is 0.0045850276947021484\n",
      "epoch: 7 step: 900, loss is 0.004225677810609341\n",
      "epoch: 7 step: 901, loss is 0.00817094650119543\n",
      "epoch: 7 step: 902, loss is 0.006530256476253271\n",
      "epoch: 7 step: 903, loss is 0.0021694828756153584\n",
      "epoch: 7 step: 904, loss is 0.0003447399940341711\n",
      "epoch: 7 step: 905, loss is 0.002771773375570774\n",
      "epoch: 7 step: 906, loss is 0.04112127423286438\n",
      "epoch: 7 step: 907, loss is 0.09038924425840378\n",
      "epoch: 7 step: 908, loss is 0.001384050352498889\n",
      "epoch: 7 step: 909, loss is 0.0003685504198074341\n",
      "epoch: 7 step: 910, loss is 0.0017947230953723192\n",
      "epoch: 7 step: 911, loss is 0.004090076312422752\n",
      "epoch: 7 step: 912, loss is 0.0029258988797664642\n",
      "epoch: 7 step: 913, loss is 0.0022305117454379797\n",
      "epoch: 7 step: 914, loss is 0.0018345416756346822\n",
      "epoch: 7 step: 915, loss is 0.00040770135819911957\n",
      "epoch: 7 step: 916, loss is 0.13605938851833344\n",
      "epoch: 7 step: 917, loss is 0.001581607386469841\n",
      "epoch: 7 step: 918, loss is 0.0022356531117111444\n",
      "epoch: 7 step: 919, loss is 0.0055406601168215275\n",
      "epoch: 7 step: 920, loss is 0.00430326396599412\n",
      "epoch: 7 step: 921, loss is 0.0068564205430448055\n",
      "epoch: 7 step: 922, loss is 0.0010102008236572146\n",
      "epoch: 7 step: 923, loss is 0.004087138455361128\n",
      "epoch: 7 step: 924, loss is 0.0006227901321835816\n",
      "epoch: 7 step: 925, loss is 0.025943493470549583\n",
      "epoch: 7 step: 926, loss is 0.10473459959030151\n",
      "epoch: 7 step: 927, loss is 0.005634322762489319\n",
      "epoch: 7 step: 928, loss is 0.0003271549940109253\n",
      "epoch: 7 step: 929, loss is 0.0029136629309505224\n",
      "epoch: 7 step: 930, loss is 0.0022997709456831217\n",
      "epoch: 7 step: 931, loss is 0.047645602375268936\n",
      "epoch: 7 step: 932, loss is 0.0004929604474455118\n",
      "epoch: 7 step: 933, loss is 0.045479126274585724\n",
      "epoch: 7 step: 934, loss is 0.003464254317805171\n",
      "epoch: 7 step: 935, loss is 0.042064156383275986\n",
      "epoch: 7 step: 936, loss is 0.004904875531792641\n",
      "epoch: 7 step: 937, loss is 0.00081500323722139\n",
      "epoch: 7 step: 938, loss is 0.003123147878795862\n",
      "epoch: 7 step: 939, loss is 0.0014862882671877742\n",
      "epoch: 7 step: 940, loss is 0.04124065116047859\n",
      "epoch: 7 step: 941, loss is 0.012129001319408417\n",
      "epoch: 7 step: 942, loss is 0.0036673638969659805\n",
      "epoch: 7 step: 943, loss is 0.000665889005176723\n",
      "epoch: 7 step: 944, loss is 0.018639007583260536\n",
      "epoch: 7 step: 945, loss is 0.0816141664981842\n",
      "epoch: 7 step: 946, loss is 0.0057682772167027\n",
      "epoch: 7 step: 947, loss is 0.0024660914205014706\n",
      "epoch: 7 step: 948, loss is 0.020658476278185844\n",
      "epoch: 7 step: 949, loss is 0.004519299138337374\n",
      "epoch: 7 step: 950, loss is 0.0034466569777578115\n",
      "epoch: 7 step: 951, loss is 0.001422280678525567\n",
      "epoch: 7 step: 952, loss is 0.0008596999105066061\n",
      "epoch: 7 step: 953, loss is 0.04956093057990074\n",
      "epoch: 7 step: 954, loss is 0.0212729349732399\n",
      "epoch: 7 step: 955, loss is 0.0027367044240236282\n",
      "epoch: 7 step: 956, loss is 0.0011088663013651967\n",
      "epoch: 7 step: 957, loss is 0.0013484535738825798\n",
      "epoch: 7 step: 958, loss is 0.0017745966324582696\n",
      "epoch: 7 step: 959, loss is 0.0015319210942834616\n",
      "epoch: 7 step: 960, loss is 0.00035889269202016294\n",
      "epoch: 7 step: 961, loss is 0.0033753577154129744\n",
      "epoch: 7 step: 962, loss is 0.019395876675844193\n",
      "epoch: 7 step: 963, loss is 0.00024172861594706774\n",
      "epoch: 7 step: 964, loss is 0.0004544731345959008\n",
      "epoch: 7 step: 965, loss is 0.008957979269325733\n",
      "epoch: 7 step: 966, loss is 0.0893808901309967\n",
      "epoch: 7 step: 967, loss is 0.0020517476368695498\n",
      "epoch: 7 step: 968, loss is 0.003065601922571659\n",
      "epoch: 7 step: 969, loss is 0.001804441213607788\n",
      "epoch: 7 step: 970, loss is 9.979266178561375e-05\n",
      "epoch: 7 step: 971, loss is 0.0008590854122303426\n",
      "epoch: 7 step: 972, loss is 0.034314174205064774\n",
      "epoch: 7 step: 973, loss is 0.0012380830012261868\n",
      "epoch: 7 step: 974, loss is 0.0014199463184922934\n",
      "epoch: 7 step: 975, loss is 3.337670204928145e-05\n",
      "epoch: 7 step: 976, loss is 0.0016446415102109313\n",
      "epoch: 7 step: 977, loss is 0.0015535274287685752\n",
      "epoch: 7 step: 978, loss is 0.00014461626415140927\n",
      "epoch: 7 step: 979, loss is 0.15374045073986053\n",
      "epoch: 7 step: 980, loss is 0.0001596307847648859\n",
      "epoch: 7 step: 981, loss is 6.712043978041038e-05\n",
      "epoch: 7 step: 982, loss is 0.02547069638967514\n",
      "epoch: 7 step: 983, loss is 0.0009418501285836101\n",
      "epoch: 7 step: 984, loss is 0.004914085380733013\n",
      "epoch: 7 step: 985, loss is 0.0039930567145347595\n",
      "epoch: 7 step: 986, loss is 0.003300935495644808\n",
      "epoch: 7 step: 987, loss is 0.0069178263656795025\n",
      "epoch: 7 step: 988, loss is 0.0001518673379905522\n",
      "epoch: 7 step: 989, loss is 0.0038986962754279375\n",
      "epoch: 7 step: 990, loss is 0.0016444908687844872\n",
      "epoch: 7 step: 991, loss is 8.173860260285437e-05\n",
      "epoch: 7 step: 992, loss is 0.005811634939163923\n",
      "epoch: 7 step: 993, loss is 0.004393255803734064\n",
      "epoch: 7 step: 994, loss is 0.0005707117961719632\n",
      "epoch: 7 step: 995, loss is 0.0008856312488205731\n",
      "epoch: 7 step: 996, loss is 0.002441572258248925\n",
      "epoch: 7 step: 997, loss is 0.0004067624104209244\n",
      "epoch: 7 step: 998, loss is 0.002128268824890256\n",
      "epoch: 7 step: 999, loss is 0.06157316267490387\n",
      "epoch: 7 step: 1000, loss is 0.01986847259104252\n",
      "epoch: 7 step: 1001, loss is 0.0010827179066836834\n",
      "epoch: 7 step: 1002, loss is 0.019619613885879517\n",
      "epoch: 7 step: 1003, loss is 0.003509846981614828\n",
      "epoch: 7 step: 1004, loss is 0.1268681138753891\n",
      "epoch: 7 step: 1005, loss is 0.004064278677105904\n",
      "epoch: 7 step: 1006, loss is 0.005039184354245663\n",
      "epoch: 7 step: 1007, loss is 0.005975817330181599\n",
      "epoch: 7 step: 1008, loss is 0.000428850733442232\n",
      "epoch: 7 step: 1009, loss is 0.0032660961151123047\n",
      "epoch: 7 step: 1010, loss is 0.0018399780383333564\n",
      "epoch: 7 step: 1011, loss is 0.004604364279657602\n",
      "epoch: 7 step: 1012, loss is 0.11964979767799377\n",
      "epoch: 7 step: 1013, loss is 0.000300580111797899\n",
      "epoch: 7 step: 1014, loss is 0.01111079566180706\n",
      "epoch: 7 step: 1015, loss is 0.03332091495394707\n",
      "epoch: 7 step: 1016, loss is 0.0020134819205850363\n",
      "epoch: 7 step: 1017, loss is 0.0026769235264509916\n",
      "epoch: 7 step: 1018, loss is 0.019053183495998383\n",
      "epoch: 7 step: 1019, loss is 0.026558570563793182\n",
      "epoch: 7 step: 1020, loss is 0.00891790073364973\n",
      "epoch: 7 step: 1021, loss is 0.05114642158150673\n",
      "epoch: 7 step: 1022, loss is 0.013104557991027832\n",
      "epoch: 7 step: 1023, loss is 0.0069400290958583355\n",
      "epoch: 7 step: 1024, loss is 0.006853741127997637\n",
      "epoch: 7 step: 1025, loss is 0.004184796940535307\n",
      "epoch: 7 step: 1026, loss is 0.012868255376815796\n",
      "epoch: 7 step: 1027, loss is 0.0016750517534092069\n",
      "epoch: 7 step: 1028, loss is 0.0005958043038845062\n",
      "epoch: 7 step: 1029, loss is 0.013853960670530796\n",
      "epoch: 7 step: 1030, loss is 7.29577150195837e-05\n",
      "epoch: 7 step: 1031, loss is 0.0046909647062420845\n",
      "epoch: 7 step: 1032, loss is 0.01354039553552866\n",
      "epoch: 7 step: 1033, loss is 0.23766127228736877\n",
      "epoch: 7 step: 1034, loss is 0.00513163348659873\n",
      "epoch: 7 step: 1035, loss is 0.003155375365167856\n",
      "epoch: 7 step: 1036, loss is 0.003936263732612133\n",
      "epoch: 7 step: 1037, loss is 0.2522493600845337\n",
      "epoch: 7 step: 1038, loss is 8.298662578454241e-05\n",
      "epoch: 7 step: 1039, loss is 0.004736114758998156\n",
      "epoch: 7 step: 1040, loss is 0.020504774525761604\n",
      "epoch: 7 step: 1041, loss is 0.1354571282863617\n",
      "epoch: 7 step: 1042, loss is 0.0016844203928485513\n",
      "epoch: 7 step: 1043, loss is 0.04278693348169327\n",
      "epoch: 7 step: 1044, loss is 0.001050362130627036\n",
      "epoch: 7 step: 1045, loss is 0.011202010326087475\n",
      "epoch: 7 step: 1046, loss is 0.0001696259278105572\n",
      "epoch: 7 step: 1047, loss is 0.029063140973448753\n",
      "epoch: 7 step: 1048, loss is 0.0016855464782565832\n",
      "epoch: 7 step: 1049, loss is 0.0009157083695754409\n",
      "epoch: 7 step: 1050, loss is 0.010434921830892563\n",
      "epoch: 7 step: 1051, loss is 0.052421122789382935\n",
      "epoch: 7 step: 1052, loss is 0.004289102274924517\n",
      "epoch: 7 step: 1053, loss is 0.0031906841322779655\n",
      "epoch: 7 step: 1054, loss is 0.11300131678581238\n",
      "epoch: 7 step: 1055, loss is 0.0025132382288575172\n",
      "epoch: 7 step: 1056, loss is 0.005833572708070278\n",
      "epoch: 7 step: 1057, loss is 0.004732360597699881\n",
      "epoch: 7 step: 1058, loss is 0.007601761259138584\n",
      "epoch: 7 step: 1059, loss is 0.026176055893301964\n",
      "epoch: 7 step: 1060, loss is 0.021262038499116898\n",
      "epoch: 7 step: 1061, loss is 0.0013128382852301002\n",
      "epoch: 7 step: 1062, loss is 0.004156917333602905\n",
      "epoch: 7 step: 1063, loss is 0.016825994476675987\n",
      "epoch: 7 step: 1064, loss is 0.00032897209166549146\n",
      "epoch: 7 step: 1065, loss is 0.0004249436315149069\n",
      "epoch: 7 step: 1066, loss is 0.0014475183561444283\n",
      "epoch: 7 step: 1067, loss is 0.12131763994693756\n",
      "epoch: 7 step: 1068, loss is 0.03802794590592384\n",
      "epoch: 7 step: 1069, loss is 3.3653166610747576e-05\n",
      "epoch: 7 step: 1070, loss is 0.00797611940652132\n",
      "epoch: 7 step: 1071, loss is 0.002304772147908807\n",
      "epoch: 7 step: 1072, loss is 9.362020500702783e-05\n",
      "epoch: 7 step: 1073, loss is 0.0003763242275454104\n",
      "epoch: 7 step: 1074, loss is 0.00035688476054929197\n",
      "epoch: 7 step: 1075, loss is 0.02131621539592743\n",
      "epoch: 7 step: 1076, loss is 0.036071889102458954\n",
      "epoch: 7 step: 1077, loss is 0.01634392887353897\n",
      "epoch: 7 step: 1078, loss is 0.04458579048514366\n",
      "epoch: 7 step: 1079, loss is 0.06254538148641586\n",
      "epoch: 7 step: 1080, loss is 0.05674044415354729\n",
      "epoch: 7 step: 1081, loss is 0.17407038807868958\n",
      "epoch: 7 step: 1082, loss is 0.0017200189176946878\n",
      "epoch: 7 step: 1083, loss is 0.014989534392952919\n",
      "epoch: 7 step: 1084, loss is 0.010123511776328087\n",
      "epoch: 7 step: 1085, loss is 0.0015479236608371139\n",
      "epoch: 7 step: 1086, loss is 0.03027871809899807\n",
      "epoch: 7 step: 1087, loss is 0.00565189216285944\n",
      "epoch: 7 step: 1088, loss is 0.0016221074620261788\n",
      "epoch: 7 step: 1089, loss is 0.0018987706862390041\n",
      "epoch: 7 step: 1090, loss is 0.006821885704994202\n",
      "epoch: 7 step: 1091, loss is 0.07090780138969421\n",
      "epoch: 7 step: 1092, loss is 0.0025039438623934984\n",
      "epoch: 7 step: 1093, loss is 4.04121310566552e-05\n",
      "epoch: 7 step: 1094, loss is 0.0009669734863564372\n",
      "epoch: 7 step: 1095, loss is 0.000316773250233382\n",
      "epoch: 7 step: 1096, loss is 0.10175244510173798\n",
      "epoch: 7 step: 1097, loss is 3.3136006095446646e-05\n",
      "epoch: 7 step: 1098, loss is 0.005247310269623995\n",
      "epoch: 7 step: 1099, loss is 0.07241211086511612\n",
      "epoch: 7 step: 1100, loss is 8.560390415368602e-05\n",
      "epoch: 7 step: 1101, loss is 0.03607639670372009\n",
      "epoch: 7 step: 1102, loss is 0.00039198246668092906\n",
      "epoch: 7 step: 1103, loss is 0.0002468917809892446\n",
      "epoch: 7 step: 1104, loss is 0.004084745887666941\n",
      "epoch: 7 step: 1105, loss is 0.00028544754604808986\n",
      "epoch: 7 step: 1106, loss is 0.0006718046497553587\n",
      "epoch: 7 step: 1107, loss is 0.0007487268303520977\n",
      "epoch: 7 step: 1108, loss is 0.0001799254969228059\n",
      "epoch: 7 step: 1109, loss is 0.2338409572839737\n",
      "epoch: 7 step: 1110, loss is 0.04202006757259369\n",
      "epoch: 7 step: 1111, loss is 0.0004074597964063287\n",
      "epoch: 7 step: 1112, loss is 0.1945626586675644\n",
      "epoch: 7 step: 1113, loss is 0.002787243574857712\n",
      "epoch: 7 step: 1114, loss is 0.004942403174936771\n",
      "epoch: 7 step: 1115, loss is 0.06065962463617325\n",
      "epoch: 7 step: 1116, loss is 0.0010886890813708305\n",
      "epoch: 7 step: 1117, loss is 0.056473758071660995\n",
      "epoch: 7 step: 1118, loss is 0.0008067488670349121\n",
      "epoch: 7 step: 1119, loss is 0.09120076894760132\n",
      "epoch: 7 step: 1120, loss is 0.11672372370958328\n",
      "epoch: 7 step: 1121, loss is 0.008276758715510368\n",
      "epoch: 7 step: 1122, loss is 0.1549825668334961\n",
      "epoch: 7 step: 1123, loss is 0.00038647919427603483\n",
      "epoch: 7 step: 1124, loss is 0.1681210845708847\n",
      "epoch: 7 step: 1125, loss is 0.02758999727666378\n",
      "epoch: 7 step: 1126, loss is 0.0025529777631163597\n",
      "epoch: 7 step: 1127, loss is 0.021216126158833504\n",
      "epoch: 7 step: 1128, loss is 0.0008249180973507464\n",
      "epoch: 7 step: 1129, loss is 0.02039850689470768\n",
      "epoch: 7 step: 1130, loss is 0.002839848631992936\n",
      "epoch: 7 step: 1131, loss is 0.0252867192029953\n",
      "epoch: 7 step: 1132, loss is 0.0014277976006269455\n",
      "epoch: 7 step: 1133, loss is 0.009359562769532204\n",
      "epoch: 7 step: 1134, loss is 0.007461599539965391\n",
      "epoch: 7 step: 1135, loss is 0.01190662756562233\n",
      "epoch: 7 step: 1136, loss is 0.001738574355840683\n",
      "epoch: 7 step: 1137, loss is 0.004536295309662819\n",
      "epoch: 7 step: 1138, loss is 0.003182688495144248\n",
      "epoch: 7 step: 1139, loss is 0.000977925956249237\n",
      "epoch: 7 step: 1140, loss is 0.04696355015039444\n",
      "epoch: 7 step: 1141, loss is 0.008867758326232433\n",
      "epoch: 7 step: 1142, loss is 0.002990683540701866\n",
      "epoch: 7 step: 1143, loss is 0.020382195711135864\n",
      "epoch: 7 step: 1144, loss is 0.005173501558601856\n",
      "epoch: 7 step: 1145, loss is 0.0037969956174492836\n",
      "epoch: 7 step: 1146, loss is 0.06565119326114655\n",
      "epoch: 7 step: 1147, loss is 0.0005904901772737503\n",
      "epoch: 7 step: 1148, loss is 0.006287917494773865\n",
      "epoch: 7 step: 1149, loss is 0.0001308602950302884\n",
      "epoch: 7 step: 1150, loss is 0.0090286023914814\n",
      "epoch: 7 step: 1151, loss is 0.000606481044087559\n",
      "epoch: 7 step: 1152, loss is 0.039502859115600586\n",
      "epoch: 7 step: 1153, loss is 0.0002561254659667611\n",
      "epoch: 7 step: 1154, loss is 0.008268055506050587\n",
      "epoch: 7 step: 1155, loss is 0.008028717711567879\n",
      "epoch: 7 step: 1156, loss is 0.00268877437338233\n",
      "epoch: 7 step: 1157, loss is 0.0020785932429134846\n",
      "epoch: 7 step: 1158, loss is 0.016920825466513634\n",
      "epoch: 7 step: 1159, loss is 0.003528784727677703\n",
      "epoch: 7 step: 1160, loss is 0.0022913780994713306\n",
      "epoch: 7 step: 1161, loss is 0.001336855930276215\n",
      "epoch: 7 step: 1162, loss is 0.010455785319209099\n",
      "epoch: 7 step: 1163, loss is 0.0007244948646984994\n",
      "epoch: 7 step: 1164, loss is 0.0005050842883065343\n",
      "epoch: 7 step: 1165, loss is 8.620800508651882e-05\n",
      "epoch: 7 step: 1166, loss is 0.06610742211341858\n",
      "epoch: 7 step: 1167, loss is 0.0018739770166575909\n",
      "epoch: 7 step: 1168, loss is 0.08947162330150604\n",
      "epoch: 7 step: 1169, loss is 0.003742964006960392\n",
      "epoch: 7 step: 1170, loss is 0.005987843032926321\n",
      "epoch: 7 step: 1171, loss is 0.00029129182803444564\n",
      "epoch: 7 step: 1172, loss is 0.019567374140024185\n",
      "epoch: 7 step: 1173, loss is 0.0008653653785586357\n",
      "epoch: 7 step: 1174, loss is 0.008373262360692024\n",
      "epoch: 7 step: 1175, loss is 0.005070713348686695\n",
      "epoch: 7 step: 1176, loss is 0.0011798691703006625\n",
      "epoch: 7 step: 1177, loss is 0.0008821570663712919\n",
      "epoch: 7 step: 1178, loss is 0.006668710149824619\n",
      "epoch: 7 step: 1179, loss is 0.000710523221641779\n",
      "epoch: 7 step: 1180, loss is 0.0005219661979936063\n",
      "epoch: 7 step: 1181, loss is 0.0010840636678040028\n",
      "epoch: 7 step: 1182, loss is 0.001037677749991417\n",
      "epoch: 7 step: 1183, loss is 0.0004806208598893136\n",
      "epoch: 7 step: 1184, loss is 0.010244437493383884\n",
      "epoch: 7 step: 1185, loss is 0.00227067107334733\n",
      "epoch: 7 step: 1186, loss is 8.047842857195064e-05\n",
      "epoch: 7 step: 1187, loss is 0.036799442023038864\n",
      "epoch: 7 step: 1188, loss is 0.0001457090547773987\n",
      "epoch: 7 step: 1189, loss is 0.0012816759990528226\n",
      "epoch: 7 step: 1190, loss is 0.0036408216692507267\n",
      "epoch: 7 step: 1191, loss is 0.006676400545984507\n",
      "epoch: 7 step: 1192, loss is 0.006054254248738289\n",
      "epoch: 7 step: 1193, loss is 0.00424835504963994\n",
      "epoch: 7 step: 1194, loss is 0.00730986287817359\n",
      "epoch: 7 step: 1195, loss is 0.1534351259469986\n",
      "epoch: 7 step: 1196, loss is 0.013687558472156525\n",
      "epoch: 7 step: 1197, loss is 0.010297861881554127\n",
      "epoch: 7 step: 1198, loss is 0.004033081699162722\n",
      "epoch: 7 step: 1199, loss is 0.00013579981168732047\n",
      "epoch: 7 step: 1200, loss is 0.00036859087413176894\n",
      "epoch: 7 step: 1201, loss is 0.0027306696865707636\n",
      "epoch: 7 step: 1202, loss is 0.0011826441623270512\n",
      "epoch: 7 step: 1203, loss is 0.005758670624345541\n",
      "epoch: 7 step: 1204, loss is 0.0003415682876948267\n",
      "epoch: 7 step: 1205, loss is 0.3413298726081848\n",
      "epoch: 7 step: 1206, loss is 0.0003941680188290775\n",
      "epoch: 7 step: 1207, loss is 0.004605663940310478\n",
      "epoch: 7 step: 1208, loss is 0.00026120536495000124\n",
      "epoch: 7 step: 1209, loss is 0.0007849395042285323\n",
      "epoch: 7 step: 1210, loss is 0.00017479642701800913\n",
      "epoch: 7 step: 1211, loss is 9.143980423687026e-05\n",
      "epoch: 7 step: 1212, loss is 0.0002478489768691361\n",
      "epoch: 7 step: 1213, loss is 0.0005421690875664353\n",
      "epoch: 7 step: 1214, loss is 0.001804286614060402\n",
      "epoch: 7 step: 1215, loss is 0.0014302348718047142\n",
      "epoch: 7 step: 1216, loss is 0.0013268444454297423\n",
      "epoch: 7 step: 1217, loss is 0.00577112240716815\n",
      "epoch: 7 step: 1218, loss is 0.0008850480662658811\n",
      "epoch: 7 step: 1219, loss is 0.01573990471661091\n",
      "epoch: 7 step: 1220, loss is 0.0008459625532850623\n",
      "epoch: 7 step: 1221, loss is 0.0019412892870604992\n",
      "epoch: 7 step: 1222, loss is 0.008047143928706646\n",
      "epoch: 7 step: 1223, loss is 0.019760869443416595\n",
      "epoch: 7 step: 1224, loss is 0.0002532155776862055\n",
      "epoch: 7 step: 1225, loss is 0.12800952792167664\n",
      "epoch: 7 step: 1226, loss is 0.00037478702142834663\n",
      "epoch: 7 step: 1227, loss is 0.0005371211445890367\n",
      "epoch: 7 step: 1228, loss is 0.012487895786762238\n",
      "epoch: 7 step: 1229, loss is 0.24164128303527832\n",
      "epoch: 7 step: 1230, loss is 0.0024579321034252644\n",
      "epoch: 7 step: 1231, loss is 0.0020533362403512\n",
      "epoch: 7 step: 1232, loss is 0.23009051382541656\n",
      "epoch: 7 step: 1233, loss is 0.008323177695274353\n",
      "epoch: 7 step: 1234, loss is 0.006037943065166473\n",
      "epoch: 7 step: 1235, loss is 0.028544031083583832\n",
      "epoch: 7 step: 1236, loss is 0.0016916346503421664\n",
      "epoch: 7 step: 1237, loss is 0.01407577469944954\n",
      "epoch: 7 step: 1238, loss is 0.01915326714515686\n",
      "epoch: 7 step: 1239, loss is 0.011100145988166332\n",
      "epoch: 7 step: 1240, loss is 0.0075456551276147366\n",
      "epoch: 7 step: 1241, loss is 0.012969490140676498\n",
      "epoch: 7 step: 1242, loss is 0.007181172724813223\n",
      "epoch: 7 step: 1243, loss is 0.0031539069022983313\n",
      "epoch: 7 step: 1244, loss is 0.04753252491354942\n",
      "epoch: 7 step: 1245, loss is 0.03294629976153374\n",
      "epoch: 7 step: 1246, loss is 0.0034367130137979984\n",
      "epoch: 7 step: 1247, loss is 0.0008600344881415367\n",
      "epoch: 7 step: 1248, loss is 0.15474703907966614\n",
      "epoch: 7 step: 1249, loss is 0.058978110551834106\n",
      "epoch: 7 step: 1250, loss is 0.004423452541232109\n",
      "epoch: 7 step: 1251, loss is 0.0027191948611289263\n",
      "epoch: 7 step: 1252, loss is 0.11291497200727463\n",
      "epoch: 7 step: 1253, loss is 0.0025345291942358017\n",
      "epoch: 7 step: 1254, loss is 0.001093192957341671\n",
      "epoch: 7 step: 1255, loss is 0.0062508354894816875\n",
      "epoch: 7 step: 1256, loss is 0.06460152566432953\n",
      "epoch: 7 step: 1257, loss is 0.0017033313633874059\n",
      "epoch: 7 step: 1258, loss is 0.0021594578865915537\n",
      "epoch: 7 step: 1259, loss is 0.0032077995128929615\n",
      "epoch: 7 step: 1260, loss is 8.402975799981505e-05\n",
      "epoch: 7 step: 1261, loss is 0.21416354179382324\n",
      "epoch: 7 step: 1262, loss is 0.0007714087842032313\n",
      "epoch: 7 step: 1263, loss is 0.030825242400169373\n",
      "epoch: 7 step: 1264, loss is 0.017105314880609512\n",
      "epoch: 7 step: 1265, loss is 0.0009107001824304461\n",
      "epoch: 7 step: 1266, loss is 0.010302678681910038\n",
      "epoch: 7 step: 1267, loss is 0.004387929104268551\n",
      "epoch: 7 step: 1268, loss is 0.011348702013492584\n",
      "epoch: 7 step: 1269, loss is 0.02476772852241993\n",
      "epoch: 7 step: 1270, loss is 0.05835108086466789\n",
      "epoch: 7 step: 1271, loss is 0.03024078533053398\n",
      "epoch: 7 step: 1272, loss is 0.0032945917919278145\n",
      "epoch: 7 step: 1273, loss is 0.006287675816565752\n",
      "epoch: 7 step: 1274, loss is 0.030501680448651314\n",
      "epoch: 7 step: 1275, loss is 0.030732855200767517\n",
      "epoch: 7 step: 1276, loss is 0.0005206916248425841\n",
      "epoch: 7 step: 1277, loss is 0.003554997965693474\n",
      "epoch: 7 step: 1278, loss is 0.001801280421204865\n",
      "epoch: 7 step: 1279, loss is 0.0005429819575510919\n",
      "epoch: 7 step: 1280, loss is 0.12266771495342255\n",
      "epoch: 7 step: 1281, loss is 0.0002442645200062543\n",
      "epoch: 7 step: 1282, loss is 0.007878351025283337\n",
      "epoch: 7 step: 1283, loss is 0.0010355834383517504\n",
      "epoch: 7 step: 1284, loss is 0.0742059201002121\n",
      "epoch: 7 step: 1285, loss is 0.0007907198159955442\n",
      "epoch: 7 step: 1286, loss is 0.032498057931661606\n",
      "epoch: 7 step: 1287, loss is 0.0034711556509137154\n",
      "epoch: 7 step: 1288, loss is 0.017983784899115562\n",
      "epoch: 7 step: 1289, loss is 0.005183468107134104\n",
      "epoch: 7 step: 1290, loss is 0.0012441030703485012\n",
      "epoch: 7 step: 1291, loss is 0.009725186973810196\n",
      "epoch: 7 step: 1292, loss is 0.001926305703818798\n",
      "epoch: 7 step: 1293, loss is 0.00043739829561673105\n",
      "epoch: 7 step: 1294, loss is 0.000547729548998177\n",
      "epoch: 7 step: 1295, loss is 0.00038247008342295885\n",
      "epoch: 7 step: 1296, loss is 0.04834801331162453\n",
      "epoch: 7 step: 1297, loss is 0.004893621429800987\n",
      "epoch: 7 step: 1298, loss is 0.011024536564946175\n",
      "epoch: 7 step: 1299, loss is 0.010156111791729927\n",
      "epoch: 7 step: 1300, loss is 0.182402566075325\n",
      "epoch: 7 step: 1301, loss is 0.0008522700518369675\n",
      "epoch: 7 step: 1302, loss is 0.0103958360850811\n",
      "epoch: 7 step: 1303, loss is 0.046884700655937195\n",
      "epoch: 7 step: 1304, loss is 0.0018851005006581545\n",
      "epoch: 7 step: 1305, loss is 0.009426659904420376\n",
      "epoch: 7 step: 1306, loss is 0.015612449496984482\n",
      "epoch: 7 step: 1307, loss is 0.0022953569423407316\n",
      "epoch: 7 step: 1308, loss is 0.013929047621786594\n",
      "epoch: 7 step: 1309, loss is 0.010350525379180908\n",
      "epoch: 7 step: 1310, loss is 0.01869913749396801\n",
      "epoch: 7 step: 1311, loss is 0.001390016870573163\n",
      "epoch: 7 step: 1312, loss is 0.00032726136851124465\n",
      "epoch: 7 step: 1313, loss is 0.0011053102789446712\n",
      "epoch: 7 step: 1314, loss is 0.07891519367694855\n",
      "epoch: 7 step: 1315, loss is 0.031665753573179245\n",
      "epoch: 7 step: 1316, loss is 0.0018818681128323078\n",
      "epoch: 7 step: 1317, loss is 0.008330196142196655\n",
      "epoch: 7 step: 1318, loss is 0.0029063757974654436\n",
      "epoch: 7 step: 1319, loss is 0.0002798326313495636\n",
      "epoch: 7 step: 1320, loss is 0.0008459244854748249\n",
      "epoch: 7 step: 1321, loss is 0.0005391310551203787\n",
      "epoch: 7 step: 1322, loss is 0.001212075469084084\n",
      "epoch: 7 step: 1323, loss is 0.0015143099008128047\n",
      "epoch: 7 step: 1324, loss is 0.007218935061246157\n",
      "epoch: 7 step: 1325, loss is 0.023526698350906372\n",
      "epoch: 7 step: 1326, loss is 0.009577669203281403\n",
      "epoch: 7 step: 1327, loss is 0.026769420132040977\n",
      "epoch: 7 step: 1328, loss is 0.04325392097234726\n",
      "epoch: 7 step: 1329, loss is 0.0018884820165112615\n",
      "epoch: 7 step: 1330, loss is 0.00023232222883962095\n",
      "epoch: 7 step: 1331, loss is 0.001637728768400848\n",
      "epoch: 7 step: 1332, loss is 0.18778762221336365\n",
      "epoch: 7 step: 1333, loss is 0.1909947395324707\n",
      "epoch: 7 step: 1334, loss is 0.0007986695854924619\n",
      "epoch: 7 step: 1335, loss is 0.0019088771659880877\n",
      "epoch: 7 step: 1336, loss is 0.005310529377311468\n",
      "epoch: 7 step: 1337, loss is 0.014356354251503944\n",
      "epoch: 7 step: 1338, loss is 0.006866340525448322\n",
      "epoch: 7 step: 1339, loss is 0.0035629512276500463\n",
      "epoch: 7 step: 1340, loss is 0.0002632129762787372\n",
      "epoch: 7 step: 1341, loss is 0.0007931727450340986\n",
      "epoch: 7 step: 1342, loss is 0.001829488785006106\n",
      "epoch: 7 step: 1343, loss is 0.0012767656007781625\n",
      "epoch: 7 step: 1344, loss is 0.0977858230471611\n",
      "epoch: 7 step: 1345, loss is 0.010553613305091858\n",
      "epoch: 7 step: 1346, loss is 0.0011529310140758753\n",
      "epoch: 7 step: 1347, loss is 0.09179563820362091\n",
      "epoch: 7 step: 1348, loss is 0.01822868175804615\n",
      "epoch: 7 step: 1349, loss is 0.06836248934268951\n",
      "epoch: 7 step: 1350, loss is 0.007514858152717352\n",
      "epoch: 7 step: 1351, loss is 0.12123240530490875\n",
      "epoch: 7 step: 1352, loss is 0.07790045440196991\n",
      "epoch: 7 step: 1353, loss is 0.00015805535076651722\n",
      "epoch: 7 step: 1354, loss is 0.0007386564975604415\n",
      "epoch: 7 step: 1355, loss is 0.004581849090754986\n",
      "epoch: 7 step: 1356, loss is 0.0007457882165908813\n",
      "epoch: 7 step: 1357, loss is 0.005251409485936165\n",
      "epoch: 7 step: 1358, loss is 0.010868187062442303\n",
      "epoch: 7 step: 1359, loss is 0.005709362216293812\n",
      "epoch: 7 step: 1360, loss is 0.06260678917169571\n",
      "epoch: 7 step: 1361, loss is 0.007954898290336132\n",
      "epoch: 7 step: 1362, loss is 0.0012074154801666737\n",
      "epoch: 7 step: 1363, loss is 0.0001347038778476417\n",
      "epoch: 7 step: 1364, loss is 0.06783793866634369\n",
      "epoch: 7 step: 1365, loss is 0.0035512330941855907\n",
      "epoch: 7 step: 1366, loss is 0.01066447515040636\n",
      "epoch: 7 step: 1367, loss is 0.059401366859674454\n",
      "epoch: 7 step: 1368, loss is 0.00032780406763777137\n",
      "epoch: 7 step: 1369, loss is 0.2481875717639923\n",
      "epoch: 7 step: 1370, loss is 0.002180706709623337\n",
      "epoch: 7 step: 1371, loss is 0.0006892245728522539\n",
      "epoch: 7 step: 1372, loss is 0.03000270389020443\n",
      "epoch: 7 step: 1373, loss is 0.14805878698825836\n",
      "epoch: 7 step: 1374, loss is 0.037190381437540054\n",
      "epoch: 7 step: 1375, loss is 0.00736949173733592\n",
      "epoch: 7 step: 1376, loss is 3.71818860003259e-05\n",
      "epoch: 7 step: 1377, loss is 0.0027451147325336933\n",
      "epoch: 7 step: 1378, loss is 0.0013667467283084989\n",
      "epoch: 7 step: 1379, loss is 0.002531061414629221\n",
      "epoch: 7 step: 1380, loss is 0.0013856877340003848\n",
      "epoch: 7 step: 1381, loss is 0.010956555604934692\n",
      "epoch: 7 step: 1382, loss is 0.003830663627013564\n",
      "epoch: 7 step: 1383, loss is 0.002015379024669528\n",
      "epoch: 7 step: 1384, loss is 0.00039531351649202406\n",
      "epoch: 7 step: 1385, loss is 0.005280888639390469\n",
      "epoch: 7 step: 1386, loss is 0.0017505133291706443\n",
      "epoch: 7 step: 1387, loss is 0.0007192653138190508\n",
      "epoch: 7 step: 1388, loss is 0.008103683590888977\n",
      "epoch: 7 step: 1389, loss is 0.011959493160247803\n",
      "epoch: 7 step: 1390, loss is 0.005826673936098814\n",
      "epoch: 7 step: 1391, loss is 0.0004987259162589908\n",
      "epoch: 7 step: 1392, loss is 0.00016923638759180903\n",
      "epoch: 7 step: 1393, loss is 0.05352838337421417\n",
      "epoch: 7 step: 1394, loss is 0.00512090977281332\n",
      "epoch: 7 step: 1395, loss is 0.005921202711760998\n",
      "epoch: 7 step: 1396, loss is 0.0009124334901571274\n",
      "epoch: 7 step: 1397, loss is 0.005651168059557676\n",
      "epoch: 7 step: 1398, loss is 0.0011280733160674572\n",
      "epoch: 7 step: 1399, loss is 0.206306591629982\n",
      "epoch: 7 step: 1400, loss is 0.0039779855869710445\n",
      "epoch: 7 step: 1401, loss is 0.03118034638464451\n",
      "epoch: 7 step: 1402, loss is 0.0017092509660869837\n",
      "epoch: 7 step: 1403, loss is 0.0026798625476658344\n",
      "epoch: 7 step: 1404, loss is 0.007272043265402317\n",
      "epoch: 7 step: 1405, loss is 0.00022861719480715692\n",
      "epoch: 7 step: 1406, loss is 0.0025527302641421556\n",
      "epoch: 7 step: 1407, loss is 0.0024946858175098896\n",
      "epoch: 7 step: 1408, loss is 0.0006493375985883176\n",
      "epoch: 7 step: 1409, loss is 0.019596856087446213\n",
      "epoch: 7 step: 1410, loss is 0.027543026953935623\n",
      "epoch: 7 step: 1411, loss is 0.0006251584272831678\n",
      "epoch: 7 step: 1412, loss is 0.03310937061905861\n",
      "epoch: 7 step: 1413, loss is 0.007980458438396454\n",
      "epoch: 7 step: 1414, loss is 0.12769432365894318\n",
      "epoch: 7 step: 1415, loss is 0.0034556190948933363\n",
      "epoch: 7 step: 1416, loss is 0.0042239343747496605\n",
      "epoch: 7 step: 1417, loss is 0.0068438793532550335\n",
      "epoch: 7 step: 1418, loss is 0.03853974491357803\n",
      "epoch: 7 step: 1419, loss is 0.000765136384870857\n",
      "epoch: 7 step: 1420, loss is 0.00010959538485622033\n",
      "epoch: 7 step: 1421, loss is 0.012442104518413544\n",
      "epoch: 7 step: 1422, loss is 0.006133437156677246\n",
      "epoch: 7 step: 1423, loss is 0.012643324211239815\n",
      "epoch: 7 step: 1424, loss is 0.0002589236537460238\n",
      "epoch: 7 step: 1425, loss is 2.8074658985133283e-05\n",
      "epoch: 7 step: 1426, loss is 0.00047683011507615447\n",
      "epoch: 7 step: 1427, loss is 0.0037403209134936333\n",
      "epoch: 7 step: 1428, loss is 0.0006561502232216299\n",
      "epoch: 7 step: 1429, loss is 0.07968703657388687\n",
      "epoch: 7 step: 1430, loss is 0.009095082990825176\n",
      "epoch: 7 step: 1431, loss is 0.0002534519589971751\n",
      "epoch: 7 step: 1432, loss is 0.13418807089328766\n",
      "epoch: 7 step: 1433, loss is 0.1502757966518402\n",
      "epoch: 7 step: 1434, loss is 0.0002640027378220111\n",
      "epoch: 7 step: 1435, loss is 0.03118344210088253\n",
      "epoch: 7 step: 1436, loss is 0.032497551292181015\n",
      "epoch: 7 step: 1437, loss is 0.10699620842933655\n",
      "epoch: 7 step: 1438, loss is 0.0005048835882917047\n",
      "epoch: 7 step: 1439, loss is 0.06616523861885071\n",
      "epoch: 7 step: 1440, loss is 0.00021856187959201634\n",
      "epoch: 7 step: 1441, loss is 0.030368642881512642\n",
      "epoch: 7 step: 1442, loss is 0.0003382544091437012\n",
      "epoch: 7 step: 1443, loss is 0.00027621290064416826\n",
      "epoch: 7 step: 1444, loss is 0.002159879310056567\n",
      "epoch: 7 step: 1445, loss is 0.00027024713926948607\n",
      "epoch: 7 step: 1446, loss is 0.04022781550884247\n",
      "epoch: 7 step: 1447, loss is 0.004299573600292206\n",
      "epoch: 7 step: 1448, loss is 0.0009539002203382552\n",
      "epoch: 7 step: 1449, loss is 0.002068893052637577\n",
      "epoch: 7 step: 1450, loss is 0.0008239013841375709\n",
      "epoch: 7 step: 1451, loss is 0.0011139982379972935\n",
      "epoch: 7 step: 1452, loss is 3.997625390184112e-05\n",
      "epoch: 7 step: 1453, loss is 5.318874173099175e-05\n",
      "epoch: 7 step: 1454, loss is 0.00038191964267753065\n",
      "epoch: 7 step: 1455, loss is 0.0005842619575560093\n",
      "epoch: 7 step: 1456, loss is 3.6843954148935154e-05\n",
      "epoch: 7 step: 1457, loss is 0.003290531225502491\n",
      "epoch: 7 step: 1458, loss is 0.0013882573693990707\n",
      "epoch: 7 step: 1459, loss is 0.0010533580789342523\n",
      "epoch: 7 step: 1460, loss is 0.03358504921197891\n",
      "epoch: 7 step: 1461, loss is 0.001633338863030076\n",
      "epoch: 7 step: 1462, loss is 0.004455651622265577\n",
      "epoch: 7 step: 1463, loss is 0.05651643127202988\n",
      "epoch: 7 step: 1464, loss is 0.005683131981641054\n",
      "epoch: 7 step: 1465, loss is 0.008378574624657631\n",
      "epoch: 7 step: 1466, loss is 0.0005843503749929368\n",
      "epoch: 7 step: 1467, loss is 0.00030682102078571916\n",
      "epoch: 7 step: 1468, loss is 0.003035287605598569\n",
      "epoch: 7 step: 1469, loss is 0.000151556683704257\n",
      "epoch: 7 step: 1470, loss is 0.02421778067946434\n",
      "epoch: 7 step: 1471, loss is 0.007742946036159992\n",
      "epoch: 7 step: 1472, loss is 0.0010088306153193116\n",
      "epoch: 7 step: 1473, loss is 0.005074303597211838\n",
      "epoch: 7 step: 1474, loss is 0.00258587091229856\n",
      "epoch: 7 step: 1475, loss is 0.006101946346461773\n",
      "epoch: 7 step: 1476, loss is 6.294803461059928e-05\n",
      "epoch: 7 step: 1477, loss is 0.010458150878548622\n",
      "epoch: 7 step: 1478, loss is 0.00044129439629614353\n",
      "epoch: 7 step: 1479, loss is 0.0006557943415828049\n",
      "epoch: 7 step: 1480, loss is 0.16685491800308228\n",
      "epoch: 7 step: 1481, loss is 7.071701111271977e-05\n",
      "epoch: 7 step: 1482, loss is 0.019882172346115112\n",
      "epoch: 7 step: 1483, loss is 0.002516666892915964\n",
      "epoch: 7 step: 1484, loss is 0.0002771048457361758\n",
      "epoch: 7 step: 1485, loss is 0.005873905029147863\n",
      "epoch: 7 step: 1486, loss is 0.0008383283857256174\n",
      "epoch: 7 step: 1487, loss is 0.1278103142976761\n",
      "epoch: 7 step: 1488, loss is 0.13531172275543213\n",
      "epoch: 7 step: 1489, loss is 0.00022343499585986137\n",
      "epoch: 7 step: 1490, loss is 0.11552093178033829\n",
      "epoch: 7 step: 1491, loss is 0.001021894859150052\n",
      "epoch: 7 step: 1492, loss is 0.001551916473545134\n",
      "epoch: 7 step: 1493, loss is 0.0019127032719552517\n",
      "epoch: 7 step: 1494, loss is 0.041770048439502716\n",
      "epoch: 7 step: 1495, loss is 0.05980600044131279\n",
      "epoch: 7 step: 1496, loss is 0.010359810665249825\n",
      "epoch: 7 step: 1497, loss is 0.0034493752755224705\n",
      "epoch: 7 step: 1498, loss is 0.02235409803688526\n",
      "epoch: 7 step: 1499, loss is 0.005174988880753517\n",
      "epoch: 7 step: 1500, loss is 0.0169876329600811\n",
      "epoch: 7 step: 1501, loss is 0.006184831261634827\n",
      "epoch: 7 step: 1502, loss is 0.03618742525577545\n",
      "epoch: 7 step: 1503, loss is 0.0054558394476771355\n",
      "epoch: 7 step: 1504, loss is 0.0027045521419495344\n",
      "epoch: 7 step: 1505, loss is 0.0008928041788749397\n",
      "epoch: 7 step: 1506, loss is 0.013186284340918064\n",
      "epoch: 7 step: 1507, loss is 0.2648575007915497\n",
      "epoch: 7 step: 1508, loss is 0.0004884513327851892\n",
      "epoch: 7 step: 1509, loss is 0.00016768449859227985\n",
      "epoch: 7 step: 1510, loss is 0.002847317373380065\n",
      "epoch: 7 step: 1511, loss is 0.009078522212803364\n",
      "epoch: 7 step: 1512, loss is 0.0010542256059125066\n",
      "epoch: 7 step: 1513, loss is 0.011193640530109406\n",
      "epoch: 7 step: 1514, loss is 0.007563728839159012\n",
      "epoch: 7 step: 1515, loss is 0.019586633890867233\n",
      "epoch: 7 step: 1516, loss is 0.0005269924877211452\n",
      "epoch: 7 step: 1517, loss is 0.0066375164315104485\n",
      "epoch: 7 step: 1518, loss is 0.0019432510016486049\n",
      "epoch: 7 step: 1519, loss is 0.030025910586118698\n",
      "epoch: 7 step: 1520, loss is 0.04089468717575073\n",
      "epoch: 7 step: 1521, loss is 0.07126843184232712\n",
      "epoch: 7 step: 1522, loss is 0.0027358648367226124\n",
      "epoch: 7 step: 1523, loss is 0.0008604357717558742\n",
      "epoch: 7 step: 1524, loss is 0.006733905058354139\n",
      "epoch: 7 step: 1525, loss is 0.09102248400449753\n",
      "epoch: 7 step: 1526, loss is 0.0005392799503169954\n",
      "epoch: 7 step: 1527, loss is 0.0006889592623338103\n",
      "epoch: 7 step: 1528, loss is 0.0010994856711477041\n",
      "epoch: 7 step: 1529, loss is 0.0013483347138389945\n",
      "epoch: 7 step: 1530, loss is 0.2526300847530365\n",
      "epoch: 7 step: 1531, loss is 0.006829674355685711\n",
      "epoch: 7 step: 1532, loss is 0.1500387042760849\n",
      "epoch: 7 step: 1533, loss is 0.029600953683257103\n",
      "epoch: 7 step: 1534, loss is 0.0009636158938519657\n",
      "epoch: 7 step: 1535, loss is 0.0004325432819314301\n",
      "epoch: 7 step: 1536, loss is 0.008342757821083069\n",
      "epoch: 7 step: 1537, loss is 0.0015721508534625173\n",
      "epoch: 7 step: 1538, loss is 0.02461313083767891\n",
      "epoch: 7 step: 1539, loss is 0.005977171007543802\n",
      "epoch: 7 step: 1540, loss is 0.0011443211697041988\n",
      "epoch: 7 step: 1541, loss is 0.0009445669711567461\n",
      "epoch: 7 step: 1542, loss is 0.004528536461293697\n",
      "epoch: 7 step: 1543, loss is 0.03546065837144852\n",
      "epoch: 7 step: 1544, loss is 0.008371169678866863\n",
      "epoch: 7 step: 1545, loss is 0.02432326413691044\n",
      "epoch: 7 step: 1546, loss is 0.0031366562470793724\n",
      "epoch: 7 step: 1547, loss is 0.001610907376743853\n",
      "epoch: 7 step: 1548, loss is 0.17305563390254974\n",
      "epoch: 7 step: 1549, loss is 0.009531665593385696\n",
      "epoch: 7 step: 1550, loss is 0.008902379311621189\n",
      "epoch: 7 step: 1551, loss is 0.00041955886990763247\n",
      "epoch: 7 step: 1552, loss is 0.0004911920404992998\n",
      "epoch: 7 step: 1553, loss is 0.019182376563549042\n",
      "epoch: 7 step: 1554, loss is 0.0006921032909303904\n",
      "epoch: 7 step: 1555, loss is 0.0017301159678027034\n",
      "epoch: 7 step: 1556, loss is 0.09104333817958832\n",
      "epoch: 7 step: 1557, loss is 0.0033241016790270805\n",
      "epoch: 7 step: 1558, loss is 0.0008699966128915548\n",
      "epoch: 7 step: 1559, loss is 0.012026429176330566\n",
      "epoch: 7 step: 1560, loss is 0.006839635316282511\n",
      "epoch: 7 step: 1561, loss is 0.04670802503824234\n",
      "epoch: 7 step: 1562, loss is 0.006110730580985546\n",
      "epoch: 7 step: 1563, loss is 0.0014871230814605951\n",
      "epoch: 7 step: 1564, loss is 0.006155265960842371\n",
      "epoch: 7 step: 1565, loss is 0.00050870340783149\n",
      "epoch: 7 step: 1566, loss is 0.003010585904121399\n",
      "epoch: 7 step: 1567, loss is 0.022691184654831886\n",
      "epoch: 7 step: 1568, loss is 0.00045941336429677904\n",
      "epoch: 7 step: 1569, loss is 0.035693999379873276\n",
      "epoch: 7 step: 1570, loss is 0.0997033640742302\n",
      "epoch: 7 step: 1571, loss is 0.0010001700138673186\n",
      "epoch: 7 step: 1572, loss is 0.0015567351365461946\n",
      "epoch: 7 step: 1573, loss is 0.0010097972117364407\n",
      "epoch: 7 step: 1574, loss is 0.022747909650206566\n",
      "epoch: 7 step: 1575, loss is 0.007863973267376423\n",
      "epoch: 7 step: 1576, loss is 0.00278027867898345\n",
      "epoch: 7 step: 1577, loss is 0.0017433224711567163\n",
      "epoch: 7 step: 1578, loss is 0.0007219001417979598\n",
      "epoch: 7 step: 1579, loss is 0.041498132050037384\n",
      "epoch: 7 step: 1580, loss is 0.010824991390109062\n",
      "epoch: 7 step: 1581, loss is 0.009426423348486423\n",
      "epoch: 7 step: 1582, loss is 0.004202822223305702\n",
      "epoch: 7 step: 1583, loss is 0.0004377792065497488\n",
      "epoch: 7 step: 1584, loss is 0.013844089582562447\n",
      "epoch: 7 step: 1585, loss is 0.0011477599618956447\n",
      "epoch: 7 step: 1586, loss is 0.03517727181315422\n",
      "epoch: 7 step: 1587, loss is 0.12379825860261917\n",
      "epoch: 7 step: 1588, loss is 0.016350189223885536\n",
      "epoch: 7 step: 1589, loss is 0.005893903784453869\n",
      "epoch: 7 step: 1590, loss is 0.0035708043724298477\n",
      "epoch: 7 step: 1591, loss is 0.0025757323019206524\n",
      "epoch: 7 step: 1592, loss is 0.03248296678066254\n",
      "epoch: 7 step: 1593, loss is 0.001318759866990149\n",
      "epoch: 7 step: 1594, loss is 0.0004615638463292271\n",
      "epoch: 7 step: 1595, loss is 0.19027844071388245\n",
      "epoch: 7 step: 1596, loss is 0.0321555957198143\n",
      "epoch: 7 step: 1597, loss is 0.0004035569145344198\n",
      "epoch: 7 step: 1598, loss is 0.0007819771999493241\n",
      "epoch: 7 step: 1599, loss is 0.1415950059890747\n",
      "epoch: 7 step: 1600, loss is 0.0022832800168544054\n",
      "epoch: 7 step: 1601, loss is 0.055524792522192\n",
      "epoch: 7 step: 1602, loss is 0.0003194690216332674\n",
      "epoch: 7 step: 1603, loss is 0.0059083071537315845\n",
      "epoch: 7 step: 1604, loss is 0.07942487299442291\n",
      "epoch: 7 step: 1605, loss is 0.005543881561607122\n",
      "epoch: 7 step: 1606, loss is 0.0008150020730681717\n",
      "epoch: 7 step: 1607, loss is 0.004817057400941849\n",
      "epoch: 7 step: 1608, loss is 0.007099839858710766\n",
      "epoch: 7 step: 1609, loss is 0.0017850053263828158\n",
      "epoch: 7 step: 1610, loss is 0.015020905062556267\n",
      "epoch: 7 step: 1611, loss is 0.013747581280767918\n",
      "epoch: 7 step: 1612, loss is 0.026295434683561325\n",
      "epoch: 7 step: 1613, loss is 0.07578326761722565\n",
      "epoch: 7 step: 1614, loss is 0.00535911088809371\n",
      "epoch: 7 step: 1615, loss is 0.004022085573524237\n",
      "epoch: 7 step: 1616, loss is 0.15534748136997223\n",
      "epoch: 7 step: 1617, loss is 0.05822214484214783\n",
      "epoch: 7 step: 1618, loss is 0.005036429036408663\n",
      "epoch: 7 step: 1619, loss is 0.0002608167997095734\n",
      "epoch: 7 step: 1620, loss is 0.020995765924453735\n",
      "epoch: 7 step: 1621, loss is 0.02233625389635563\n",
      "epoch: 7 step: 1622, loss is 0.0014010504819452763\n",
      "epoch: 7 step: 1623, loss is 0.00035443788510747254\n",
      "epoch: 7 step: 1624, loss is 0.04874253273010254\n",
      "epoch: 7 step: 1625, loss is 0.19059164822101593\n",
      "epoch: 7 step: 1626, loss is 0.03860655426979065\n",
      "epoch: 7 step: 1627, loss is 0.004767025820910931\n",
      "epoch: 7 step: 1628, loss is 0.1916256695985794\n",
      "epoch: 7 step: 1629, loss is 0.004008444957435131\n",
      "epoch: 7 step: 1630, loss is 0.0006139744655229151\n",
      "epoch: 7 step: 1631, loss is 0.001533881644718349\n",
      "epoch: 7 step: 1632, loss is 0.005573592148721218\n",
      "epoch: 7 step: 1633, loss is 0.0017179095884785056\n",
      "epoch: 7 step: 1634, loss is 0.0029562825802713633\n",
      "epoch: 7 step: 1635, loss is 0.15041613578796387\n",
      "epoch: 7 step: 1636, loss is 0.08718068152666092\n",
      "epoch: 7 step: 1637, loss is 0.1727132499217987\n",
      "epoch: 7 step: 1638, loss is 0.0009778530802577734\n",
      "epoch: 7 step: 1639, loss is 0.022298840805888176\n",
      "epoch: 7 step: 1640, loss is 0.0030134557746350765\n",
      "epoch: 7 step: 1641, loss is 0.1361086517572403\n",
      "epoch: 7 step: 1642, loss is 0.0003169904521200806\n",
      "epoch: 7 step: 1643, loss is 0.03919955715537071\n",
      "epoch: 7 step: 1644, loss is 0.006246476899832487\n",
      "epoch: 7 step: 1645, loss is 0.06925719231367111\n",
      "epoch: 7 step: 1646, loss is 0.003581149270758033\n",
      "epoch: 7 step: 1647, loss is 0.04636625200510025\n",
      "epoch: 7 step: 1648, loss is 0.18303649127483368\n",
      "epoch: 7 step: 1649, loss is 0.0012408863985911012\n",
      "epoch: 7 step: 1650, loss is 0.025266176089644432\n",
      "epoch: 7 step: 1651, loss is 0.028052743524312973\n",
      "epoch: 7 step: 1652, loss is 0.0016989021096378565\n",
      "epoch: 7 step: 1653, loss is 0.0005639938754029572\n",
      "epoch: 7 step: 1654, loss is 0.010268074460327625\n",
      "epoch: 7 step: 1655, loss is 0.0002468976890668273\n",
      "epoch: 7 step: 1656, loss is 0.0063465130515396595\n",
      "epoch: 7 step: 1657, loss is 0.006702151149511337\n",
      "epoch: 7 step: 1658, loss is 0.0037137640174478292\n",
      "epoch: 7 step: 1659, loss is 0.005180354695767164\n",
      "epoch: 7 step: 1660, loss is 0.02941082790493965\n",
      "epoch: 7 step: 1661, loss is 0.0004264319140929729\n",
      "epoch: 7 step: 1662, loss is 0.008374991826713085\n",
      "epoch: 7 step: 1663, loss is 0.017856381833553314\n",
      "epoch: 7 step: 1664, loss is 0.006873040460050106\n",
      "epoch: 7 step: 1665, loss is 0.0007777202408760786\n",
      "epoch: 7 step: 1666, loss is 0.006305139046162367\n",
      "epoch: 7 step: 1667, loss is 0.04166204854846001\n",
      "epoch: 7 step: 1668, loss is 0.0008917945669963956\n",
      "epoch: 7 step: 1669, loss is 0.0047192745842039585\n",
      "epoch: 7 step: 1670, loss is 0.0006732883048243821\n",
      "epoch: 7 step: 1671, loss is 0.0013661859557032585\n",
      "epoch: 7 step: 1672, loss is 0.0011304686777293682\n",
      "epoch: 7 step: 1673, loss is 0.07170115411281586\n",
      "epoch: 7 step: 1674, loss is 0.12332838028669357\n",
      "epoch: 7 step: 1675, loss is 0.002298556035384536\n",
      "epoch: 7 step: 1676, loss is 0.009886491112411022\n",
      "epoch: 7 step: 1677, loss is 0.1760871708393097\n",
      "epoch: 7 step: 1678, loss is 0.001007217331789434\n",
      "epoch: 7 step: 1679, loss is 0.0465126670897007\n",
      "epoch: 7 step: 1680, loss is 0.11549469083547592\n",
      "epoch: 7 step: 1681, loss is 0.04882000759243965\n",
      "epoch: 7 step: 1682, loss is 0.006732717156410217\n",
      "epoch: 7 step: 1683, loss is 0.031112894415855408\n",
      "epoch: 7 step: 1684, loss is 0.05279309302568436\n",
      "epoch: 7 step: 1685, loss is 0.006191805470734835\n",
      "epoch: 7 step: 1686, loss is 0.021267389878630638\n",
      "epoch: 7 step: 1687, loss is 0.002630362519994378\n",
      "epoch: 7 step: 1688, loss is 0.0325884148478508\n",
      "epoch: 7 step: 1689, loss is 0.009287497028708458\n",
      "epoch: 7 step: 1690, loss is 0.001402437686920166\n",
      "epoch: 7 step: 1691, loss is 0.005535069387406111\n",
      "epoch: 7 step: 1692, loss is 0.0012743507977575064\n",
      "epoch: 7 step: 1693, loss is 0.00033053127117455006\n",
      "epoch: 7 step: 1694, loss is 0.0022968619596213102\n",
      "epoch: 7 step: 1695, loss is 0.04475191608071327\n",
      "epoch: 7 step: 1696, loss is 0.25515812635421753\n",
      "epoch: 7 step: 1697, loss is 0.008374416269361973\n",
      "epoch: 7 step: 1698, loss is 0.006987604778259993\n",
      "epoch: 7 step: 1699, loss is 0.007742411457002163\n",
      "epoch: 7 step: 1700, loss is 0.004354663658887148\n",
      "epoch: 7 step: 1701, loss is 0.16216126084327698\n",
      "epoch: 7 step: 1702, loss is 0.02661171555519104\n",
      "epoch: 7 step: 1703, loss is 0.06807832419872284\n",
      "epoch: 7 step: 1704, loss is 0.012353909201920033\n",
      "epoch: 7 step: 1705, loss is 0.04744451120495796\n",
      "epoch: 7 step: 1706, loss is 0.09239014983177185\n",
      "epoch: 7 step: 1707, loss is 0.00609482591971755\n",
      "epoch: 7 step: 1708, loss is 0.000988727086223662\n",
      "epoch: 7 step: 1709, loss is 0.0004922041553072631\n",
      "epoch: 7 step: 1710, loss is 0.0002740064519457519\n",
      "epoch: 7 step: 1711, loss is 0.0019791261292994022\n",
      "epoch: 7 step: 1712, loss is 0.0003630435385275632\n",
      "epoch: 7 step: 1713, loss is 0.0009664689423516393\n",
      "epoch: 7 step: 1714, loss is 0.012329088523983955\n",
      "epoch: 7 step: 1715, loss is 0.0030293352901935577\n",
      "epoch: 7 step: 1716, loss is 0.0014555457746610045\n",
      "epoch: 7 step: 1717, loss is 0.0020455229096114635\n",
      "epoch: 7 step: 1718, loss is 0.1358243077993393\n",
      "epoch: 7 step: 1719, loss is 0.008838874287903309\n",
      "epoch: 7 step: 1720, loss is 0.1458204835653305\n",
      "epoch: 7 step: 1721, loss is 0.004194560926407576\n",
      "epoch: 7 step: 1722, loss is 0.013049578294157982\n",
      "epoch: 7 step: 1723, loss is 0.1337415874004364\n",
      "epoch: 7 step: 1724, loss is 0.016781551763415337\n",
      "epoch: 7 step: 1725, loss is 0.010883687995374203\n",
      "epoch: 7 step: 1726, loss is 0.011264167726039886\n",
      "epoch: 7 step: 1727, loss is 0.022117244079709053\n",
      "epoch: 7 step: 1728, loss is 0.00027911271899938583\n",
      "epoch: 7 step: 1729, loss is 0.00017591634241398424\n",
      "epoch: 7 step: 1730, loss is 0.035230863839387894\n",
      "epoch: 7 step: 1731, loss is 0.000966224237345159\n",
      "epoch: 7 step: 1732, loss is 0.0032418027985841036\n",
      "epoch: 7 step: 1733, loss is 0.00786163005977869\n",
      "epoch: 7 step: 1734, loss is 0.0028931638225913048\n",
      "epoch: 7 step: 1735, loss is 0.17356878519058228\n",
      "epoch: 7 step: 1736, loss is 0.0023451712913811207\n",
      "epoch: 7 step: 1737, loss is 0.0008004061528481543\n",
      "epoch: 7 step: 1738, loss is 0.007692466955631971\n",
      "epoch: 7 step: 1739, loss is 0.002499030437320471\n",
      "epoch: 7 step: 1740, loss is 0.013433603569865227\n",
      "epoch: 7 step: 1741, loss is 0.0015139388851821423\n",
      "epoch: 7 step: 1742, loss is 0.1894117146730423\n",
      "epoch: 7 step: 1743, loss is 0.0002485272998455912\n",
      "epoch: 7 step: 1744, loss is 0.07479091733694077\n",
      "epoch: 7 step: 1745, loss is 0.001409341231919825\n",
      "epoch: 7 step: 1746, loss is 0.006146312225610018\n",
      "epoch: 7 step: 1747, loss is 0.012872220017015934\n",
      "epoch: 7 step: 1748, loss is 0.0003315574140287936\n",
      "epoch: 7 step: 1749, loss is 0.004464213736355305\n",
      "epoch: 7 step: 1750, loss is 0.0025614097248762846\n",
      "epoch: 7 step: 1751, loss is 0.0941685289144516\n",
      "epoch: 7 step: 1752, loss is 0.0038034371100366116\n",
      "epoch: 7 step: 1753, loss is 0.002733644563704729\n",
      "epoch: 7 step: 1754, loss is 0.047990262508392334\n",
      "epoch: 7 step: 1755, loss is 0.008117491379380226\n",
      "epoch: 7 step: 1756, loss is 0.017120232805609703\n",
      "epoch: 7 step: 1757, loss is 0.06278589367866516\n",
      "epoch: 7 step: 1758, loss is 0.005537142977118492\n",
      "epoch: 7 step: 1759, loss is 0.00013112465967424214\n",
      "epoch: 7 step: 1760, loss is 0.0019413955742493272\n",
      "epoch: 7 step: 1761, loss is 0.14438000321388245\n",
      "epoch: 7 step: 1762, loss is 0.07740022242069244\n",
      "epoch: 7 step: 1763, loss is 0.0009360276162624359\n",
      "epoch: 7 step: 1764, loss is 0.0005396396736614406\n",
      "epoch: 7 step: 1765, loss is 0.005297589581459761\n",
      "epoch: 7 step: 1766, loss is 0.05067835748195648\n",
      "epoch: 7 step: 1767, loss is 0.1289074718952179\n",
      "epoch: 7 step: 1768, loss is 0.002001132583245635\n",
      "epoch: 7 step: 1769, loss is 0.043007057160139084\n",
      "epoch: 7 step: 1770, loss is 0.016345640644431114\n",
      "epoch: 7 step: 1771, loss is 0.059858474880456924\n",
      "epoch: 7 step: 1772, loss is 0.002401502337306738\n",
      "epoch: 7 step: 1773, loss is 0.007370464038103819\n",
      "epoch: 7 step: 1774, loss is 0.0022519868798553944\n",
      "epoch: 7 step: 1775, loss is 0.009470838122069836\n",
      "epoch: 7 step: 1776, loss is 0.006497853901237249\n",
      "epoch: 7 step: 1777, loss is 0.0004432344576343894\n",
      "epoch: 7 step: 1778, loss is 0.0019105003448203206\n",
      "epoch: 7 step: 1779, loss is 0.003910904284566641\n",
      "epoch: 7 step: 1780, loss is 0.004571978468447924\n",
      "epoch: 7 step: 1781, loss is 0.0015053509268909693\n",
      "epoch: 7 step: 1782, loss is 0.03567051142454147\n",
      "epoch: 7 step: 1783, loss is 4.5698718167841434e-05\n",
      "epoch: 7 step: 1784, loss is 0.002988223684951663\n",
      "epoch: 7 step: 1785, loss is 0.011418781243264675\n",
      "epoch: 7 step: 1786, loss is 0.0038822609931230545\n",
      "epoch: 7 step: 1787, loss is 0.03614531457424164\n",
      "epoch: 7 step: 1788, loss is 0.0012972063850611448\n",
      "epoch: 7 step: 1789, loss is 0.030643614009022713\n",
      "epoch: 7 step: 1790, loss is 0.003288176842033863\n",
      "epoch: 7 step: 1791, loss is 0.0809798538684845\n",
      "epoch: 7 step: 1792, loss is 0.00955123733729124\n",
      "epoch: 7 step: 1793, loss is 0.0012783447746187449\n",
      "epoch: 7 step: 1794, loss is 0.011591263115406036\n",
      "epoch: 7 step: 1795, loss is 0.0016791190719231963\n",
      "epoch: 7 step: 1796, loss is 0.007297880947589874\n",
      "epoch: 7 step: 1797, loss is 0.027978375554084778\n",
      "epoch: 7 step: 1798, loss is 0.017656246200203896\n",
      "epoch: 7 step: 1799, loss is 0.06676950305700302\n",
      "epoch: 7 step: 1800, loss is 0.0007649186882190406\n",
      "epoch: 7 step: 1801, loss is 0.053760938346385956\n",
      "epoch: 7 step: 1802, loss is 0.0003577567113097757\n",
      "epoch: 7 step: 1803, loss is 0.0022758832201361656\n",
      "epoch: 7 step: 1804, loss is 0.0002948957262560725\n",
      "epoch: 7 step: 1805, loss is 0.03796859830617905\n",
      "epoch: 7 step: 1806, loss is 0.11091913282871246\n",
      "epoch: 7 step: 1807, loss is 0.0003222375235054642\n",
      "epoch: 7 step: 1808, loss is 0.013350014574825764\n",
      "epoch: 7 step: 1809, loss is 0.15804241597652435\n",
      "epoch: 7 step: 1810, loss is 0.04928543418645859\n",
      "epoch: 7 step: 1811, loss is 0.0016377033898606896\n",
      "epoch: 7 step: 1812, loss is 0.0029243065509945154\n",
      "epoch: 7 step: 1813, loss is 0.09092485159635544\n",
      "epoch: 7 step: 1814, loss is 0.013689211569726467\n",
      "epoch: 7 step: 1815, loss is 0.009262412786483765\n",
      "epoch: 7 step: 1816, loss is 0.0326225571334362\n",
      "epoch: 7 step: 1817, loss is 0.05544111132621765\n",
      "epoch: 7 step: 1818, loss is 0.0023171757347881794\n",
      "epoch: 7 step: 1819, loss is 0.08897680789232254\n",
      "epoch: 7 step: 1820, loss is 0.009167470037937164\n",
      "epoch: 7 step: 1821, loss is 0.007500085514038801\n",
      "epoch: 7 step: 1822, loss is 0.06816834211349487\n",
      "epoch: 7 step: 1823, loss is 0.1307399868965149\n",
      "epoch: 7 step: 1824, loss is 0.04016898572444916\n",
      "epoch: 7 step: 1825, loss is 0.032808586955070496\n",
      "epoch: 7 step: 1826, loss is 0.07130740582942963\n",
      "epoch: 7 step: 1827, loss is 0.007151083089411259\n",
      "epoch: 7 step: 1828, loss is 0.0559200644493103\n",
      "epoch: 7 step: 1829, loss is 0.0006983258062973619\n",
      "epoch: 7 step: 1830, loss is 0.013397138565778732\n",
      "epoch: 7 step: 1831, loss is 0.00800061970949173\n",
      "epoch: 7 step: 1832, loss is 0.0007670021150261164\n",
      "epoch: 7 step: 1833, loss is 0.0031563621014356613\n",
      "epoch: 7 step: 1834, loss is 0.00333583471365273\n",
      "epoch: 7 step: 1835, loss is 0.0007684336742386222\n",
      "epoch: 7 step: 1836, loss is 0.058101631700992584\n",
      "epoch: 7 step: 1837, loss is 0.003392366459593177\n",
      "epoch: 7 step: 1838, loss is 0.002609062008559704\n",
      "epoch: 7 step: 1839, loss is 0.06464657187461853\n",
      "epoch: 7 step: 1840, loss is 0.0018230904825031757\n",
      "epoch: 7 step: 1841, loss is 0.006994157563894987\n",
      "epoch: 7 step: 1842, loss is 0.016445104032754898\n",
      "epoch: 7 step: 1843, loss is 0.06897491216659546\n",
      "epoch: 7 step: 1844, loss is 0.004564939998090267\n",
      "epoch: 7 step: 1845, loss is 0.009708871133625507\n",
      "epoch: 7 step: 1846, loss is 3.732497862074524e-05\n",
      "epoch: 7 step: 1847, loss is 0.0021436582319438457\n",
      "epoch: 7 step: 1848, loss is 0.06202129274606705\n",
      "epoch: 7 step: 1849, loss is 6.180067430250347e-05\n",
      "epoch: 7 step: 1850, loss is 0.002950004767626524\n",
      "epoch: 7 step: 1851, loss is 0.002389084780588746\n",
      "epoch: 7 step: 1852, loss is 0.005372060928493738\n",
      "epoch: 7 step: 1853, loss is 0.007678684778511524\n",
      "epoch: 7 step: 1854, loss is 7.140552770579234e-05\n",
      "epoch: 7 step: 1855, loss is 0.004439638461917639\n",
      "epoch: 7 step: 1856, loss is 0.36467862129211426\n",
      "epoch: 7 step: 1857, loss is 0.006749547086656094\n",
      "epoch: 7 step: 1858, loss is 0.016424424946308136\n",
      "epoch: 7 step: 1859, loss is 0.0013676714152097702\n",
      "epoch: 7 step: 1860, loss is 0.0003793048090301454\n",
      "epoch: 7 step: 1861, loss is 0.0006737033254466951\n",
      "epoch: 7 step: 1862, loss is 0.0003586237726267427\n",
      "epoch: 7 step: 1863, loss is 0.00017545977607369423\n",
      "epoch: 7 step: 1864, loss is 0.005179224070161581\n",
      "epoch: 7 step: 1865, loss is 0.03180713951587677\n",
      "epoch: 7 step: 1866, loss is 0.007011183071881533\n",
      "epoch: 7 step: 1867, loss is 0.0016089745331555605\n",
      "epoch: 7 step: 1868, loss is 0.0029969257302582264\n",
      "epoch: 7 step: 1869, loss is 0.006738557480275631\n",
      "epoch: 7 step: 1870, loss is 0.016550002619624138\n",
      "epoch: 7 step: 1871, loss is 0.009225651621818542\n",
      "epoch: 7 step: 1872, loss is 0.001474332413636148\n",
      "epoch: 7 step: 1873, loss is 0.06722813099622726\n",
      "epoch: 7 step: 1874, loss is 0.0004836567386519164\n",
      "epoch: 7 step: 1875, loss is 0.009104078635573387\n",
      "epoch: 8 step: 1, loss is 0.004212929401546717\n",
      "epoch: 8 step: 2, loss is 0.00014666140486951917\n",
      "epoch: 8 step: 3, loss is 0.06767760217189789\n",
      "epoch: 8 step: 4, loss is 0.0005166724440641701\n",
      "epoch: 8 step: 5, loss is 0.0011282966006547213\n",
      "epoch: 8 step: 6, loss is 0.0006112753180786967\n",
      "epoch: 8 step: 7, loss is 0.008732063695788383\n",
      "epoch: 8 step: 8, loss is 0.014036096632480621\n",
      "epoch: 8 step: 9, loss is 0.09859592467546463\n",
      "epoch: 8 step: 10, loss is 0.0701402798295021\n",
      "epoch: 8 step: 11, loss is 0.0367007739841938\n",
      "epoch: 8 step: 12, loss is 0.00015814159996807575\n",
      "epoch: 8 step: 13, loss is 0.15123073756694794\n",
      "epoch: 8 step: 14, loss is 0.0006014459067955613\n",
      "epoch: 8 step: 15, loss is 0.004709468223154545\n",
      "epoch: 8 step: 16, loss is 0.005005787126719952\n",
      "epoch: 8 step: 17, loss is 0.0003997516178060323\n",
      "epoch: 8 step: 18, loss is 0.00027742719976231456\n",
      "epoch: 8 step: 19, loss is 0.0012792454799637198\n",
      "epoch: 8 step: 20, loss is 0.0403304249048233\n",
      "epoch: 8 step: 21, loss is 6.927624781383201e-05\n",
      "epoch: 8 step: 22, loss is 0.0011475818464532495\n",
      "epoch: 8 step: 23, loss is 0.0005315146991051733\n",
      "epoch: 8 step: 24, loss is 0.00045583094470202923\n",
      "epoch: 8 step: 25, loss is 0.07712867856025696\n",
      "epoch: 8 step: 26, loss is 0.009044000878930092\n",
      "epoch: 8 step: 27, loss is 0.1299084573984146\n",
      "epoch: 8 step: 28, loss is 0.0036478815600275993\n",
      "epoch: 8 step: 29, loss is 0.0002334880264243111\n",
      "epoch: 8 step: 30, loss is 0.001569089712575078\n",
      "epoch: 8 step: 31, loss is 0.0018187221139669418\n",
      "epoch: 8 step: 32, loss is 0.001036844914779067\n",
      "epoch: 8 step: 33, loss is 0.0003718026855494827\n",
      "epoch: 8 step: 34, loss is 0.009505066089332104\n",
      "epoch: 8 step: 35, loss is 0.008851164020597935\n",
      "epoch: 8 step: 36, loss is 0.0015277799684554338\n",
      "epoch: 8 step: 37, loss is 0.0030362135730683804\n",
      "epoch: 8 step: 38, loss is 0.016582148149609566\n",
      "epoch: 8 step: 39, loss is 0.0037135330494493246\n",
      "epoch: 8 step: 40, loss is 0.0005122500588186085\n",
      "epoch: 8 step: 41, loss is 0.004393463954329491\n",
      "epoch: 8 step: 42, loss is 0.0990564152598381\n",
      "epoch: 8 step: 43, loss is 0.0009537734440527856\n",
      "epoch: 8 step: 44, loss is 0.0005761187057942152\n",
      "epoch: 8 step: 45, loss is 0.0034602885134518147\n",
      "epoch: 8 step: 46, loss is 0.05898883193731308\n",
      "epoch: 8 step: 47, loss is 0.1325516253709793\n",
      "epoch: 8 step: 48, loss is 0.005942299962043762\n",
      "epoch: 8 step: 49, loss is 0.006638542283326387\n",
      "epoch: 8 step: 50, loss is 0.0022733702789992094\n",
      "epoch: 8 step: 51, loss is 0.016257863491773605\n",
      "epoch: 8 step: 52, loss is 0.0010764698963612318\n",
      "epoch: 8 step: 53, loss is 0.0019221974071115255\n",
      "epoch: 8 step: 54, loss is 0.025357529520988464\n",
      "epoch: 8 step: 55, loss is 0.0029028235003352165\n",
      "epoch: 8 step: 56, loss is 0.0016197554068639874\n",
      "epoch: 8 step: 57, loss is 0.008220542222261429\n",
      "epoch: 8 step: 58, loss is 2.0389919882290997e-05\n",
      "epoch: 8 step: 59, loss is 0.0007763911271467805\n",
      "epoch: 8 step: 60, loss is 0.07906458526849747\n",
      "epoch: 8 step: 61, loss is 0.053466152399778366\n",
      "epoch: 8 step: 62, loss is 0.005099455360323191\n",
      "epoch: 8 step: 63, loss is 0.006622728891670704\n",
      "epoch: 8 step: 64, loss is 0.00021300752996467054\n",
      "epoch: 8 step: 65, loss is 0.0009636474424041808\n",
      "epoch: 8 step: 66, loss is 0.011868583969771862\n",
      "epoch: 8 step: 67, loss is 0.000805661256890744\n",
      "epoch: 8 step: 68, loss is 0.0004944597603753209\n",
      "epoch: 8 step: 69, loss is 0.007036019116640091\n",
      "epoch: 8 step: 70, loss is 0.07332462072372437\n",
      "epoch: 8 step: 71, loss is 0.02962823584675789\n",
      "epoch: 8 step: 72, loss is 0.014185170643031597\n",
      "epoch: 8 step: 73, loss is 0.04496496543288231\n",
      "epoch: 8 step: 74, loss is 0.022188255563378334\n",
      "epoch: 8 step: 75, loss is 0.035555150359869\n",
      "epoch: 8 step: 76, loss is 0.0002922862768173218\n",
      "epoch: 8 step: 77, loss is 0.03365277126431465\n",
      "epoch: 8 step: 78, loss is 0.0020960913971066475\n",
      "epoch: 8 step: 79, loss is 0.0013545738765969872\n",
      "epoch: 8 step: 80, loss is 0.022565772756934166\n",
      "epoch: 8 step: 81, loss is 0.017547057941555977\n",
      "epoch: 8 step: 82, loss is 0.05657866224646568\n",
      "epoch: 8 step: 83, loss is 0.0009639460477046669\n",
      "epoch: 8 step: 84, loss is 0.004893270321190357\n",
      "epoch: 8 step: 85, loss is 0.0032842017244547606\n",
      "epoch: 8 step: 86, loss is 0.01646443083882332\n",
      "epoch: 8 step: 87, loss is 0.00456989323720336\n",
      "epoch: 8 step: 88, loss is 0.00028114981250837445\n",
      "epoch: 8 step: 89, loss is 0.002177552552893758\n",
      "epoch: 8 step: 90, loss is 0.1333761364221573\n",
      "epoch: 8 step: 91, loss is 0.006189998239278793\n",
      "epoch: 8 step: 92, loss is 0.0018674085149541497\n",
      "epoch: 8 step: 93, loss is 0.012279949150979519\n",
      "epoch: 8 step: 94, loss is 0.0005619851872324944\n",
      "epoch: 8 step: 95, loss is 0.0014823919627815485\n",
      "epoch: 8 step: 96, loss is 0.18136464059352875\n",
      "epoch: 8 step: 97, loss is 0.0006632718141190708\n",
      "epoch: 8 step: 98, loss is 0.00386882945895195\n",
      "epoch: 8 step: 99, loss is 0.005870432127267122\n",
      "epoch: 8 step: 100, loss is 0.03670807555317879\n",
      "epoch: 8 step: 101, loss is 0.0010020126355811954\n",
      "epoch: 8 step: 102, loss is 0.23860423266887665\n",
      "epoch: 8 step: 103, loss is 0.0008089395123533905\n",
      "epoch: 8 step: 104, loss is 0.0010691068600863218\n",
      "epoch: 8 step: 105, loss is 0.0023552505299448967\n",
      "epoch: 8 step: 106, loss is 0.0891532152891159\n",
      "epoch: 8 step: 107, loss is 0.03331327065825462\n",
      "epoch: 8 step: 108, loss is 0.007000379264354706\n",
      "epoch: 8 step: 109, loss is 0.005244825500994921\n",
      "epoch: 8 step: 110, loss is 0.005569905508309603\n",
      "epoch: 8 step: 111, loss is 0.0050101615488529205\n",
      "epoch: 8 step: 112, loss is 0.011868641711771488\n",
      "epoch: 8 step: 113, loss is 0.005045850295573473\n",
      "epoch: 8 step: 114, loss is 0.006960205268114805\n",
      "epoch: 8 step: 115, loss is 0.0007711700745858252\n",
      "epoch: 8 step: 116, loss is 0.0014405407710000873\n",
      "epoch: 8 step: 117, loss is 0.10700110346078873\n",
      "epoch: 8 step: 118, loss is 0.000530162185896188\n",
      "epoch: 8 step: 119, loss is 0.0020134272053837776\n",
      "epoch: 8 step: 120, loss is 0.0002658446610439569\n",
      "epoch: 8 step: 121, loss is 0.00030104100005701184\n",
      "epoch: 8 step: 122, loss is 0.0001315939734922722\n",
      "epoch: 8 step: 123, loss is 0.0001060197755577974\n",
      "epoch: 8 step: 124, loss is 0.003518839366734028\n",
      "epoch: 8 step: 125, loss is 0.0029744887724518776\n",
      "epoch: 8 step: 126, loss is 0.07732947170734406\n",
      "epoch: 8 step: 127, loss is 0.00042338899220339954\n",
      "epoch: 8 step: 128, loss is 0.0013573882170021534\n",
      "epoch: 8 step: 129, loss is 0.0004377212026156485\n",
      "epoch: 8 step: 130, loss is 0.0005481682601384819\n",
      "epoch: 8 step: 131, loss is 0.008071276359260082\n",
      "epoch: 8 step: 132, loss is 0.013545237481594086\n",
      "epoch: 8 step: 133, loss is 0.00016773547395132482\n",
      "epoch: 8 step: 134, loss is 0.0022770087234675884\n",
      "epoch: 8 step: 135, loss is 0.002074877033010125\n",
      "epoch: 8 step: 136, loss is 0.002054702490568161\n",
      "epoch: 8 step: 137, loss is 0.0064722029492259026\n",
      "epoch: 8 step: 138, loss is 0.002648652531206608\n",
      "epoch: 8 step: 139, loss is 0.0034771261271089315\n",
      "epoch: 8 step: 140, loss is 0.025570586323738098\n",
      "epoch: 8 step: 141, loss is 0.0010317463893443346\n",
      "epoch: 8 step: 142, loss is 0.04594694823026657\n",
      "epoch: 8 step: 143, loss is 0.001768220216035843\n",
      "epoch: 8 step: 144, loss is 0.0002375691692577675\n",
      "epoch: 8 step: 145, loss is 0.0049243951216340065\n",
      "epoch: 8 step: 146, loss is 0.002338532591238618\n",
      "epoch: 8 step: 147, loss is 0.035966381430625916\n",
      "epoch: 8 step: 148, loss is 0.0007772911922074854\n",
      "epoch: 8 step: 149, loss is 0.005056592635810375\n",
      "epoch: 8 step: 150, loss is 0.007621529046446085\n",
      "epoch: 8 step: 151, loss is 0.0008128843037411571\n",
      "epoch: 8 step: 152, loss is 0.0009394715307280421\n",
      "epoch: 8 step: 153, loss is 0.0008739392505958676\n",
      "epoch: 8 step: 154, loss is 0.004185796249657869\n",
      "epoch: 8 step: 155, loss is 0.0065895430743694305\n",
      "epoch: 8 step: 156, loss is 0.07469270378351212\n",
      "epoch: 8 step: 157, loss is 0.08955472707748413\n",
      "epoch: 8 step: 158, loss is 0.0007808408117853105\n",
      "epoch: 8 step: 159, loss is 0.00047971279127523303\n",
      "epoch: 8 step: 160, loss is 0.0003525780630297959\n",
      "epoch: 8 step: 161, loss is 0.043360188603401184\n",
      "epoch: 8 step: 162, loss is 0.00013209378812462091\n",
      "epoch: 8 step: 163, loss is 0.0006648233393207192\n",
      "epoch: 8 step: 164, loss is 7.866168743930757e-05\n",
      "epoch: 8 step: 165, loss is 0.0001770248491084203\n",
      "epoch: 8 step: 166, loss is 0.0017550470074638724\n",
      "epoch: 8 step: 167, loss is 0.00401386059820652\n",
      "epoch: 8 step: 168, loss is 0.00029616605024784803\n",
      "epoch: 8 step: 169, loss is 0.00841110572218895\n",
      "epoch: 8 step: 170, loss is 0.00010009829566115513\n",
      "epoch: 8 step: 171, loss is 0.0018107445212081075\n",
      "epoch: 8 step: 172, loss is 0.0032941317185759544\n",
      "epoch: 8 step: 173, loss is 0.00020183506421744823\n",
      "epoch: 8 step: 174, loss is 0.003067160490900278\n",
      "epoch: 8 step: 175, loss is 0.0008369195275008678\n",
      "epoch: 8 step: 176, loss is 0.0008271430269815028\n",
      "epoch: 8 step: 177, loss is 0.0005967298639006913\n",
      "epoch: 8 step: 178, loss is 0.01874685101211071\n",
      "epoch: 8 step: 179, loss is 0.007077733054757118\n",
      "epoch: 8 step: 180, loss is 0.07414095848798752\n",
      "epoch: 8 step: 181, loss is 0.03904010355472565\n",
      "epoch: 8 step: 182, loss is 0.0012929675867781043\n",
      "epoch: 8 step: 183, loss is 0.004294121637940407\n",
      "epoch: 8 step: 184, loss is 0.056247226893901825\n",
      "epoch: 8 step: 185, loss is 0.001125752809457481\n",
      "epoch: 8 step: 186, loss is 0.005181584041565657\n",
      "epoch: 8 step: 187, loss is 0.0002982841688208282\n",
      "epoch: 8 step: 188, loss is 0.0005541450809687376\n",
      "epoch: 8 step: 189, loss is 0.001521562342531979\n",
      "epoch: 8 step: 190, loss is 0.004028589464724064\n",
      "epoch: 8 step: 191, loss is 0.010778713040053844\n",
      "epoch: 8 step: 192, loss is 0.009425828233361244\n",
      "epoch: 8 step: 193, loss is 0.02195015177130699\n",
      "epoch: 8 step: 194, loss is 1.6163397958735004e-05\n",
      "epoch: 8 step: 195, loss is 0.06579505652189255\n",
      "epoch: 8 step: 196, loss is 0.03245128318667412\n",
      "epoch: 8 step: 197, loss is 0.016502821817994118\n",
      "epoch: 8 step: 198, loss is 0.009506701491773129\n",
      "epoch: 8 step: 199, loss is 0.0008154556853696704\n",
      "epoch: 8 step: 200, loss is 0.014200747013092041\n",
      "epoch: 8 step: 201, loss is 0.06609360873699188\n",
      "epoch: 8 step: 202, loss is 0.0009700034279376268\n",
      "epoch: 8 step: 203, loss is 0.0009936160640791059\n",
      "epoch: 8 step: 204, loss is 0.05008497089147568\n",
      "epoch: 8 step: 205, loss is 0.0005649894010275602\n",
      "epoch: 8 step: 206, loss is 0.0012794674839824438\n",
      "epoch: 8 step: 207, loss is 0.009089821949601173\n",
      "epoch: 8 step: 208, loss is 0.0016439975006505847\n",
      "epoch: 8 step: 209, loss is 0.016465943306684494\n",
      "epoch: 8 step: 210, loss is 0.004954067524522543\n",
      "epoch: 8 step: 211, loss is 0.3338826298713684\n",
      "epoch: 8 step: 212, loss is 0.000749182712752372\n",
      "epoch: 8 step: 213, loss is 0.02700893022119999\n",
      "epoch: 8 step: 214, loss is 0.011827558279037476\n",
      "epoch: 8 step: 215, loss is 0.0017779360059648752\n",
      "epoch: 8 step: 216, loss is 0.00018730010197032243\n",
      "epoch: 8 step: 217, loss is 0.02506800927221775\n",
      "epoch: 8 step: 218, loss is 0.009805617853999138\n",
      "epoch: 8 step: 219, loss is 3.7165620597079396e-05\n",
      "epoch: 8 step: 220, loss is 0.002102629980072379\n",
      "epoch: 8 step: 221, loss is 0.06350081413984299\n",
      "epoch: 8 step: 222, loss is 0.026720713824033737\n",
      "epoch: 8 step: 223, loss is 0.016772190108895302\n",
      "epoch: 8 step: 224, loss is 0.09490416198968887\n",
      "epoch: 8 step: 225, loss is 0.004471563268452883\n",
      "epoch: 8 step: 226, loss is 0.011040463112294674\n",
      "epoch: 8 step: 227, loss is 0.0011428226716816425\n",
      "epoch: 8 step: 228, loss is 0.014251461252570152\n",
      "epoch: 8 step: 229, loss is 0.0009949272498488426\n",
      "epoch: 8 step: 230, loss is 0.0015876067336648703\n",
      "epoch: 8 step: 231, loss is 0.010574196465313435\n",
      "epoch: 8 step: 232, loss is 0.004447525832802057\n",
      "epoch: 8 step: 233, loss is 0.0017507874872535467\n",
      "epoch: 8 step: 234, loss is 0.02235986664891243\n",
      "epoch: 8 step: 235, loss is 0.09474791586399078\n",
      "epoch: 8 step: 236, loss is 0.00033644214272499084\n",
      "epoch: 8 step: 237, loss is 0.017860999330878258\n",
      "epoch: 8 step: 238, loss is 0.0034127102699130774\n",
      "epoch: 8 step: 239, loss is 0.022127598524093628\n",
      "epoch: 8 step: 240, loss is 0.005076214205473661\n",
      "epoch: 8 step: 241, loss is 0.0018110675737261772\n",
      "epoch: 8 step: 242, loss is 0.030198419466614723\n",
      "epoch: 8 step: 243, loss is 0.00187888671644032\n",
      "epoch: 8 step: 244, loss is 0.05857820808887482\n",
      "epoch: 8 step: 245, loss is 0.0018131453543901443\n",
      "epoch: 8 step: 246, loss is 0.038958389312028885\n",
      "epoch: 8 step: 247, loss is 0.012384974397718906\n",
      "epoch: 8 step: 248, loss is 0.018825780600309372\n",
      "epoch: 8 step: 249, loss is 0.01102412398904562\n",
      "epoch: 8 step: 250, loss is 0.000393976311897859\n",
      "epoch: 8 step: 251, loss is 0.0050664665177464485\n",
      "epoch: 8 step: 252, loss is 0.0016931251157075167\n",
      "epoch: 8 step: 253, loss is 0.006404774729162455\n",
      "epoch: 8 step: 254, loss is 0.11784950643777847\n",
      "epoch: 8 step: 255, loss is 0.014304683543741703\n",
      "epoch: 8 step: 256, loss is 0.0006684534600935876\n",
      "epoch: 8 step: 257, loss is 0.005706121679395437\n",
      "epoch: 8 step: 258, loss is 0.00046703737461939454\n",
      "epoch: 8 step: 259, loss is 0.0010446453234180808\n",
      "epoch: 8 step: 260, loss is 0.001315819681622088\n",
      "epoch: 8 step: 261, loss is 0.15934336185455322\n",
      "epoch: 8 step: 262, loss is 0.026234550401568413\n",
      "epoch: 8 step: 263, loss is 0.004601694643497467\n",
      "epoch: 8 step: 264, loss is 0.0008967785397544503\n",
      "epoch: 8 step: 265, loss is 0.0009556299191899598\n",
      "epoch: 8 step: 266, loss is 0.007809587754309177\n",
      "epoch: 8 step: 267, loss is 0.01011169608682394\n",
      "epoch: 8 step: 268, loss is 0.2855653762817383\n",
      "epoch: 8 step: 269, loss is 0.0002968381450045854\n",
      "epoch: 8 step: 270, loss is 0.017244549468159676\n",
      "epoch: 8 step: 271, loss is 8.458072989014909e-05\n",
      "epoch: 8 step: 272, loss is 0.028844153508543968\n",
      "epoch: 8 step: 273, loss is 0.004345867317169905\n",
      "epoch: 8 step: 274, loss is 0.046992309391498566\n",
      "epoch: 8 step: 275, loss is 0.15649566054344177\n",
      "epoch: 8 step: 276, loss is 0.0004809505771845579\n",
      "epoch: 8 step: 277, loss is 0.002589830430224538\n",
      "epoch: 8 step: 278, loss is 0.004434581380337477\n",
      "epoch: 8 step: 279, loss is 0.06932661682367325\n",
      "epoch: 8 step: 280, loss is 0.005269097164273262\n",
      "epoch: 8 step: 281, loss is 0.0014259498566389084\n",
      "epoch: 8 step: 282, loss is 0.02495078183710575\n",
      "epoch: 8 step: 283, loss is 0.015160376206040382\n",
      "epoch: 8 step: 284, loss is 0.00036469678161665797\n",
      "epoch: 8 step: 285, loss is 0.007324590347707272\n",
      "epoch: 8 step: 286, loss is 0.03815227001905441\n",
      "epoch: 8 step: 287, loss is 0.013520540669560432\n",
      "epoch: 8 step: 288, loss is 0.0003219484642613679\n",
      "epoch: 8 step: 289, loss is 0.00024893664522096515\n",
      "epoch: 8 step: 290, loss is 0.004411558620631695\n",
      "epoch: 8 step: 291, loss is 0.02749587781727314\n",
      "epoch: 8 step: 292, loss is 0.006516034249216318\n",
      "epoch: 8 step: 293, loss is 0.004373433068394661\n",
      "epoch: 8 step: 294, loss is 0.0006563104107044637\n",
      "epoch: 8 step: 295, loss is 0.0013208785094320774\n",
      "epoch: 8 step: 296, loss is 0.01380289439111948\n",
      "epoch: 8 step: 297, loss is 0.00014574846136383712\n",
      "epoch: 8 step: 298, loss is 0.00564250024035573\n",
      "epoch: 8 step: 299, loss is 0.0034062061458826065\n",
      "epoch: 8 step: 300, loss is 0.06900814175605774\n",
      "epoch: 8 step: 301, loss is 0.015870869159698486\n",
      "epoch: 8 step: 302, loss is 0.004287124145776033\n",
      "epoch: 8 step: 303, loss is 0.0013379910960793495\n",
      "epoch: 8 step: 304, loss is 0.036656271666288376\n",
      "epoch: 8 step: 305, loss is 0.00017501375987194479\n",
      "epoch: 8 step: 306, loss is 8.743222861085087e-05\n",
      "epoch: 8 step: 307, loss is 0.007300591561943293\n",
      "epoch: 8 step: 308, loss is 0.0008682844345457852\n",
      "epoch: 8 step: 309, loss is 0.0023395856842398643\n",
      "epoch: 8 step: 310, loss is 0.0009578802855685353\n",
      "epoch: 8 step: 311, loss is 0.0041785635985434055\n",
      "epoch: 8 step: 312, loss is 0.00102987140417099\n",
      "epoch: 8 step: 313, loss is 0.0011101341806352139\n",
      "epoch: 8 step: 314, loss is 0.00017899675003718585\n",
      "epoch: 8 step: 315, loss is 0.0008781051728874445\n",
      "epoch: 8 step: 316, loss is 0.14891716837882996\n",
      "epoch: 8 step: 317, loss is 0.0015288806753233075\n",
      "epoch: 8 step: 318, loss is 0.008024990558624268\n",
      "epoch: 8 step: 319, loss is 8.074150537140667e-05\n",
      "epoch: 8 step: 320, loss is 0.0034743500873446465\n",
      "epoch: 8 step: 321, loss is 0.000916239689104259\n",
      "epoch: 8 step: 322, loss is 0.005152062047272921\n",
      "epoch: 8 step: 323, loss is 0.00018701421504374593\n",
      "epoch: 8 step: 324, loss is 0.0006223348900675774\n",
      "epoch: 8 step: 325, loss is 0.0009005358442664146\n",
      "epoch: 8 step: 326, loss is 0.02003844454884529\n",
      "epoch: 8 step: 327, loss is 0.01354557741433382\n",
      "epoch: 8 step: 328, loss is 6.210623541846871e-05\n",
      "epoch: 8 step: 329, loss is 0.016877736896276474\n",
      "epoch: 8 step: 330, loss is 0.00015725674165878445\n",
      "epoch: 8 step: 331, loss is 0.0388219840824604\n",
      "epoch: 8 step: 332, loss is 3.8185520679689944e-05\n",
      "epoch: 8 step: 333, loss is 0.006982217542827129\n",
      "epoch: 8 step: 334, loss is 4.2913423385471106e-05\n",
      "epoch: 8 step: 335, loss is 3.677696440718137e-05\n",
      "epoch: 8 step: 336, loss is 0.0014391662552952766\n",
      "epoch: 8 step: 337, loss is 0.02324160933494568\n",
      "epoch: 8 step: 338, loss is 0.08957036584615707\n",
      "epoch: 8 step: 339, loss is 0.018293634057044983\n",
      "epoch: 8 step: 340, loss is 0.0034535801969468594\n",
      "epoch: 8 step: 341, loss is 0.011885223910212517\n",
      "epoch: 8 step: 342, loss is 0.004626852925866842\n",
      "epoch: 8 step: 343, loss is 0.0011404274264350533\n",
      "epoch: 8 step: 344, loss is 0.0023646492045372725\n",
      "epoch: 8 step: 345, loss is 0.0009679746581241488\n",
      "epoch: 8 step: 346, loss is 0.00014479656238108873\n",
      "epoch: 8 step: 347, loss is 0.025684379041194916\n",
      "epoch: 8 step: 348, loss is 0.00424979766830802\n",
      "epoch: 8 step: 349, loss is 4.217243622406386e-05\n",
      "epoch: 8 step: 350, loss is 0.0010343800531700253\n",
      "epoch: 8 step: 351, loss is 0.00035199709236621857\n",
      "epoch: 8 step: 352, loss is 0.00012049201177433133\n",
      "epoch: 8 step: 353, loss is 0.0005090436898171902\n",
      "epoch: 8 step: 354, loss is 0.011003381572663784\n",
      "epoch: 8 step: 355, loss is 0.0028563986998051405\n",
      "epoch: 8 step: 356, loss is 0.0007449784316122532\n",
      "epoch: 8 step: 357, loss is 0.004087722860276699\n",
      "epoch: 8 step: 358, loss is 0.03542819619178772\n",
      "epoch: 8 step: 359, loss is 0.0003946125216316432\n",
      "epoch: 8 step: 360, loss is 0.000955507333856076\n",
      "epoch: 8 step: 361, loss is 0.0006887909257784486\n",
      "epoch: 8 step: 362, loss is 0.0025203977711498737\n",
      "epoch: 8 step: 363, loss is 0.0037019483279436827\n",
      "epoch: 8 step: 364, loss is 0.0006634489982388914\n",
      "epoch: 8 step: 365, loss is 0.04297136887907982\n",
      "epoch: 8 step: 366, loss is 0.00015675519534852356\n",
      "epoch: 8 step: 367, loss is 0.0036458480171859264\n",
      "epoch: 8 step: 368, loss is 0.0015930681256577373\n",
      "epoch: 8 step: 369, loss is 0.0002161113079637289\n",
      "epoch: 8 step: 370, loss is 0.0005977340042591095\n",
      "epoch: 8 step: 371, loss is 0.015330484136939049\n",
      "epoch: 8 step: 372, loss is 0.0001890852872747928\n",
      "epoch: 8 step: 373, loss is 0.014079191721975803\n",
      "epoch: 8 step: 374, loss is 2.5233242922695354e-05\n",
      "epoch: 8 step: 375, loss is 0.0010395505232736468\n",
      "epoch: 8 step: 376, loss is 6.685745029244572e-05\n",
      "epoch: 8 step: 377, loss is 0.00027355397469364107\n",
      "epoch: 8 step: 378, loss is 0.024863677099347115\n",
      "epoch: 8 step: 379, loss is 0.10018270462751389\n",
      "epoch: 8 step: 380, loss is 0.04471990466117859\n",
      "epoch: 8 step: 381, loss is 0.0698874220252037\n",
      "epoch: 8 step: 382, loss is 0.002344190375879407\n",
      "epoch: 8 step: 383, loss is 0.0007816952420398593\n",
      "epoch: 8 step: 384, loss is 0.013825930655002594\n",
      "epoch: 8 step: 385, loss is 0.0005612795357592404\n",
      "epoch: 8 step: 386, loss is 0.017997637391090393\n",
      "epoch: 8 step: 387, loss is 0.007398558780550957\n",
      "epoch: 8 step: 388, loss is 0.0007665533921681345\n",
      "epoch: 8 step: 389, loss is 0.0023240367881953716\n",
      "epoch: 8 step: 390, loss is 0.08973559737205505\n",
      "epoch: 8 step: 391, loss is 0.0039272247813642025\n",
      "epoch: 8 step: 392, loss is 0.0019882696215063334\n",
      "epoch: 8 step: 393, loss is 0.0003367676690686494\n",
      "epoch: 8 step: 394, loss is 0.0025177227798849344\n",
      "epoch: 8 step: 395, loss is 0.00010925372043857351\n",
      "epoch: 8 step: 396, loss is 0.001664892421104014\n",
      "epoch: 8 step: 397, loss is 0.0009506914648227394\n",
      "epoch: 8 step: 398, loss is 0.0007269277120940387\n",
      "epoch: 8 step: 399, loss is 0.011391835287213326\n",
      "epoch: 8 step: 400, loss is 0.004409920424222946\n",
      "epoch: 8 step: 401, loss is 0.0011522452114149928\n",
      "epoch: 8 step: 402, loss is 0.0009671732550486922\n",
      "epoch: 8 step: 403, loss is 0.0037838518619537354\n",
      "epoch: 8 step: 404, loss is 0.00018602644558995962\n",
      "epoch: 8 step: 405, loss is 0.15406614542007446\n",
      "epoch: 8 step: 406, loss is 3.89649867429398e-05\n",
      "epoch: 8 step: 407, loss is 0.00021584454225376248\n",
      "epoch: 8 step: 408, loss is 0.009723665192723274\n",
      "epoch: 8 step: 409, loss is 0.0002778138150461018\n",
      "epoch: 8 step: 410, loss is 0.0008759184274822474\n",
      "epoch: 8 step: 411, loss is 0.0003270772867836058\n",
      "epoch: 8 step: 412, loss is 0.0017899307422339916\n",
      "epoch: 8 step: 413, loss is 0.00987098179757595\n",
      "epoch: 8 step: 414, loss is 0.006469315849244595\n",
      "epoch: 8 step: 415, loss is 0.0003386654716450721\n",
      "epoch: 8 step: 416, loss is 0.00034165362012572587\n",
      "epoch: 8 step: 417, loss is 0.005137149710208178\n",
      "epoch: 8 step: 418, loss is 0.0005099450936540961\n",
      "epoch: 8 step: 419, loss is 0.0023400806821882725\n",
      "epoch: 8 step: 420, loss is 0.008507967926561832\n",
      "epoch: 8 step: 421, loss is 0.03010164201259613\n",
      "epoch: 8 step: 422, loss is 0.004599616397172213\n",
      "epoch: 8 step: 423, loss is 0.005765698850154877\n",
      "epoch: 8 step: 424, loss is 0.016461938619613647\n",
      "epoch: 8 step: 425, loss is 0.0025106898974627256\n",
      "epoch: 8 step: 426, loss is 0.00011053700291085988\n",
      "epoch: 8 step: 427, loss is 0.0031753042712807655\n",
      "epoch: 8 step: 428, loss is 0.00045079237315803766\n",
      "epoch: 8 step: 429, loss is 0.002516115317121148\n",
      "epoch: 8 step: 430, loss is 0.019499704241752625\n",
      "epoch: 8 step: 431, loss is 0.006104794796556234\n",
      "epoch: 8 step: 432, loss is 0.0010663081193342805\n",
      "epoch: 8 step: 433, loss is 0.0004194219072815031\n",
      "epoch: 8 step: 434, loss is 0.34057897329330444\n",
      "epoch: 8 step: 435, loss is 0.03635621443390846\n",
      "epoch: 8 step: 436, loss is 0.08632509410381317\n",
      "epoch: 8 step: 437, loss is 0.006489293184131384\n",
      "epoch: 8 step: 438, loss is 5.5297066865023226e-05\n",
      "epoch: 8 step: 439, loss is 0.0030216043815016747\n",
      "epoch: 8 step: 440, loss is 9.176986350212246e-05\n",
      "epoch: 8 step: 441, loss is 0.0009635949390940368\n",
      "epoch: 8 step: 442, loss is 0.0012914194958284497\n",
      "epoch: 8 step: 443, loss is 0.005445624701678753\n",
      "epoch: 8 step: 444, loss is 0.002162408083677292\n",
      "epoch: 8 step: 445, loss is 0.030369795858860016\n",
      "epoch: 8 step: 446, loss is 0.003712475998327136\n",
      "epoch: 8 step: 447, loss is 0.0004906862741336226\n",
      "epoch: 8 step: 448, loss is 0.0032406689133495092\n",
      "epoch: 8 step: 449, loss is 0.003154598642140627\n",
      "epoch: 8 step: 450, loss is 0.010969623923301697\n",
      "epoch: 8 step: 451, loss is 0.00725547643378377\n",
      "epoch: 8 step: 452, loss is 0.018213048577308655\n",
      "epoch: 8 step: 453, loss is 0.03528294339776039\n",
      "epoch: 8 step: 454, loss is 0.07357506453990936\n",
      "epoch: 8 step: 455, loss is 0.00041700000292621553\n",
      "epoch: 8 step: 456, loss is 0.005713471677154303\n",
      "epoch: 8 step: 457, loss is 0.0011852969182655215\n",
      "epoch: 8 step: 458, loss is 0.00016186130233108997\n",
      "epoch: 8 step: 459, loss is 0.002692944835871458\n",
      "epoch: 8 step: 460, loss is 0.002328679198399186\n",
      "epoch: 8 step: 461, loss is 0.1789686381816864\n",
      "epoch: 8 step: 462, loss is 0.0008903910056687891\n",
      "epoch: 8 step: 463, loss is 3.9009286410873756e-05\n",
      "epoch: 8 step: 464, loss is 0.0002619878214318305\n",
      "epoch: 8 step: 465, loss is 0.0001480856299167499\n",
      "epoch: 8 step: 466, loss is 0.0024356767535209656\n",
      "epoch: 8 step: 467, loss is 5.725372466258705e-05\n",
      "epoch: 8 step: 468, loss is 0.026119910180568695\n",
      "epoch: 8 step: 469, loss is 0.0003515006974339485\n",
      "epoch: 8 step: 470, loss is 4.6785564336460084e-05\n",
      "epoch: 8 step: 471, loss is 0.004787990357726812\n",
      "epoch: 8 step: 472, loss is 8.690275717526674e-05\n",
      "epoch: 8 step: 473, loss is 0.0009301810059696436\n",
      "epoch: 8 step: 474, loss is 0.0026226516347378492\n",
      "epoch: 8 step: 475, loss is 0.023666847497224808\n",
      "epoch: 8 step: 476, loss is 0.0009788032621145248\n",
      "epoch: 8 step: 477, loss is 0.00043039864976890385\n",
      "epoch: 8 step: 478, loss is 0.0027987940702587366\n",
      "epoch: 8 step: 479, loss is 0.006581406109035015\n",
      "epoch: 8 step: 480, loss is 0.020879095420241356\n",
      "epoch: 8 step: 481, loss is 0.0035505169071257114\n",
      "epoch: 8 step: 482, loss is 0.0006297043873928487\n",
      "epoch: 8 step: 483, loss is 0.01693618670105934\n",
      "epoch: 8 step: 484, loss is 0.011249097064137459\n",
      "epoch: 8 step: 485, loss is 0.00044491863809525967\n",
      "epoch: 8 step: 486, loss is 0.000987174455076456\n",
      "epoch: 8 step: 487, loss is 0.00019175256602466106\n",
      "epoch: 8 step: 488, loss is 0.0009450848447158933\n",
      "epoch: 8 step: 489, loss is 0.02048037014901638\n",
      "epoch: 8 step: 490, loss is 0.00022295390954241157\n",
      "epoch: 8 step: 491, loss is 0.0013673813082277775\n",
      "epoch: 8 step: 492, loss is 0.007866316474974155\n",
      "epoch: 8 step: 493, loss is 0.23050223290920258\n",
      "epoch: 8 step: 494, loss is 0.054045453667640686\n",
      "epoch: 8 step: 495, loss is 2.912274248956237e-05\n",
      "epoch: 8 step: 496, loss is 0.0011173342354595661\n",
      "epoch: 8 step: 497, loss is 0.05266483500599861\n",
      "epoch: 8 step: 498, loss is 0.006207966711372137\n",
      "epoch: 8 step: 499, loss is 0.20225118100643158\n",
      "epoch: 8 step: 500, loss is 0.0007423237548209727\n",
      "epoch: 8 step: 501, loss is 0.0009303723927587271\n",
      "epoch: 8 step: 502, loss is 0.0013443897478282452\n",
      "epoch: 8 step: 503, loss is 0.0004524349351413548\n",
      "epoch: 8 step: 504, loss is 0.0003263652033638209\n",
      "epoch: 8 step: 505, loss is 0.009132303297519684\n",
      "epoch: 8 step: 506, loss is 0.03204633295536041\n",
      "epoch: 8 step: 507, loss is 0.0014278683811426163\n",
      "epoch: 8 step: 508, loss is 0.026716986671090126\n",
      "epoch: 8 step: 509, loss is 0.033755235373973846\n",
      "epoch: 8 step: 510, loss is 0.004344828426837921\n",
      "epoch: 8 step: 511, loss is 0.0029335266444832087\n",
      "epoch: 8 step: 512, loss is 0.0021702731028199196\n",
      "epoch: 8 step: 513, loss is 0.00409511337056756\n",
      "epoch: 8 step: 514, loss is 0.006475456990301609\n",
      "epoch: 8 step: 515, loss is 0.05808055028319359\n",
      "epoch: 8 step: 516, loss is 0.00033192880800925195\n",
      "epoch: 8 step: 517, loss is 0.0010082379449158907\n",
      "epoch: 8 step: 518, loss is 0.0004067616246175021\n",
      "epoch: 8 step: 519, loss is 0.001556950039230287\n",
      "epoch: 8 step: 520, loss is 0.11872269958257675\n",
      "epoch: 8 step: 521, loss is 0.0011791670694947243\n",
      "epoch: 8 step: 522, loss is 0.00020023818069603294\n",
      "epoch: 8 step: 523, loss is 0.15982069075107574\n",
      "epoch: 8 step: 524, loss is 0.09725983440876007\n",
      "epoch: 8 step: 525, loss is 0.00848627369850874\n",
      "epoch: 8 step: 526, loss is 9.116507862927392e-05\n",
      "epoch: 8 step: 527, loss is 0.002189184073358774\n",
      "epoch: 8 step: 528, loss is 0.004704204387962818\n",
      "epoch: 8 step: 529, loss is 0.026429586112499237\n",
      "epoch: 8 step: 530, loss is 0.0007859560428187251\n",
      "epoch: 8 step: 531, loss is 0.033741265535354614\n",
      "epoch: 8 step: 532, loss is 0.00021852100326213986\n",
      "epoch: 8 step: 533, loss is 0.008941260166466236\n",
      "epoch: 8 step: 534, loss is 0.001464797300286591\n",
      "epoch: 8 step: 535, loss is 0.04506412893533707\n",
      "epoch: 8 step: 536, loss is 0.003102884627878666\n",
      "epoch: 8 step: 537, loss is 0.00020928870071657002\n",
      "epoch: 8 step: 538, loss is 0.0031872421968728304\n",
      "epoch: 8 step: 539, loss is 0.03156791627407074\n",
      "epoch: 8 step: 540, loss is 0.003893226385116577\n",
      "epoch: 8 step: 541, loss is 0.0008993497467599809\n",
      "epoch: 8 step: 542, loss is 0.0034977677278220654\n",
      "epoch: 8 step: 543, loss is 0.014283508062362671\n",
      "epoch: 8 step: 544, loss is 0.00014744215877726674\n",
      "epoch: 8 step: 545, loss is 0.005090049002319574\n",
      "epoch: 8 step: 546, loss is 0.012849600054323673\n",
      "epoch: 8 step: 547, loss is 0.027671871706843376\n",
      "epoch: 8 step: 548, loss is 0.0007840758771635592\n",
      "epoch: 8 step: 549, loss is 0.012348254211246967\n",
      "epoch: 8 step: 550, loss is 0.0011704133357852697\n",
      "epoch: 8 step: 551, loss is 0.0010890980483964086\n",
      "epoch: 8 step: 552, loss is 0.0029787044040858746\n",
      "epoch: 8 step: 553, loss is 0.04964366927742958\n",
      "epoch: 8 step: 554, loss is 2.5299987100879662e-05\n",
      "epoch: 8 step: 555, loss is 0.0061372448690235615\n",
      "epoch: 8 step: 556, loss is 0.021820854395627975\n",
      "epoch: 8 step: 557, loss is 0.004268260672688484\n",
      "epoch: 8 step: 558, loss is 0.03158193826675415\n",
      "epoch: 8 step: 559, loss is 0.001866788137704134\n",
      "epoch: 8 step: 560, loss is 0.0019118994241580367\n",
      "epoch: 8 step: 561, loss is 0.0001126300121541135\n",
      "epoch: 8 step: 562, loss is 0.004600949585437775\n",
      "epoch: 8 step: 563, loss is 0.000523940718267113\n",
      "epoch: 8 step: 564, loss is 0.008088765665888786\n",
      "epoch: 8 step: 565, loss is 0.02045639231801033\n",
      "epoch: 8 step: 566, loss is 6.202878284966573e-05\n",
      "epoch: 8 step: 567, loss is 0.0408063605427742\n",
      "epoch: 8 step: 568, loss is 0.007738139014691114\n",
      "epoch: 8 step: 569, loss is 0.03788959980010986\n",
      "epoch: 8 step: 570, loss is 0.02994953654706478\n",
      "epoch: 8 step: 571, loss is 0.00020733715791720897\n",
      "epoch: 8 step: 572, loss is 0.09378109127283096\n",
      "epoch: 8 step: 573, loss is 0.0019857229199260473\n",
      "epoch: 8 step: 574, loss is 0.005996519234031439\n",
      "epoch: 8 step: 575, loss is 0.0023386424873024225\n",
      "epoch: 8 step: 576, loss is 0.002072507282719016\n",
      "epoch: 8 step: 577, loss is 6.967810622882098e-05\n",
      "epoch: 8 step: 578, loss is 0.0008772079017944634\n",
      "epoch: 8 step: 579, loss is 0.008949203416705132\n",
      "epoch: 8 step: 580, loss is 0.04209021106362343\n",
      "epoch: 8 step: 581, loss is 0.002739702118560672\n",
      "epoch: 8 step: 582, loss is 0.006801672745496035\n",
      "epoch: 8 step: 583, loss is 0.0009944987250491977\n",
      "epoch: 8 step: 584, loss is 0.0002927780442405492\n",
      "epoch: 8 step: 585, loss is 0.0902094841003418\n",
      "epoch: 8 step: 586, loss is 0.005980581510812044\n",
      "epoch: 8 step: 587, loss is 0.005131211597472429\n",
      "epoch: 8 step: 588, loss is 0.0007660101400688291\n",
      "epoch: 8 step: 589, loss is 0.06636476516723633\n",
      "epoch: 8 step: 590, loss is 0.07420006394386292\n",
      "epoch: 8 step: 591, loss is 7.24289784557186e-05\n",
      "epoch: 8 step: 592, loss is 0.21733272075653076\n",
      "epoch: 8 step: 593, loss is 0.0016525130486115813\n",
      "epoch: 8 step: 594, loss is 0.0002333316660951823\n",
      "epoch: 8 step: 595, loss is 0.11422238498926163\n",
      "epoch: 8 step: 596, loss is 0.002847958356142044\n",
      "epoch: 8 step: 597, loss is 0.004677705466747284\n",
      "epoch: 8 step: 598, loss is 0.006686077918857336\n",
      "epoch: 8 step: 599, loss is 0.002906572539359331\n",
      "epoch: 8 step: 600, loss is 0.03355485945940018\n",
      "epoch: 8 step: 601, loss is 0.014880422502756119\n",
      "epoch: 8 step: 602, loss is 0.0018094629049301147\n",
      "epoch: 8 step: 603, loss is 0.017085246741771698\n",
      "epoch: 8 step: 604, loss is 0.027384694665670395\n",
      "epoch: 8 step: 605, loss is 0.007284216582775116\n",
      "epoch: 8 step: 606, loss is 0.0032980393152683973\n",
      "epoch: 8 step: 607, loss is 0.0003214508469682187\n",
      "epoch: 8 step: 608, loss is 0.010913125239312649\n",
      "epoch: 8 step: 609, loss is 0.026521753519773483\n",
      "epoch: 8 step: 610, loss is 0.0078001730144023895\n",
      "epoch: 8 step: 611, loss is 0.05723889172077179\n",
      "epoch: 8 step: 612, loss is 0.007138139568269253\n",
      "epoch: 8 step: 613, loss is 0.001658375607803464\n",
      "epoch: 8 step: 614, loss is 0.0004906585672870278\n",
      "epoch: 8 step: 615, loss is 0.0015035546384751797\n",
      "epoch: 8 step: 616, loss is 0.004882597364485264\n",
      "epoch: 8 step: 617, loss is 0.0035803616046905518\n",
      "epoch: 8 step: 618, loss is 0.0017838715575635433\n",
      "epoch: 8 step: 619, loss is 5.5076845455914736e-05\n",
      "epoch: 8 step: 620, loss is 0.004176171962171793\n",
      "epoch: 8 step: 621, loss is 0.0728948563337326\n",
      "epoch: 8 step: 622, loss is 0.011704443953931332\n",
      "epoch: 8 step: 623, loss is 0.007631287910044193\n",
      "epoch: 8 step: 624, loss is 0.0025668470188975334\n",
      "epoch: 8 step: 625, loss is 0.15478545427322388\n",
      "epoch: 8 step: 626, loss is 0.048223432153463364\n",
      "epoch: 8 step: 627, loss is 0.09672915935516357\n",
      "epoch: 8 step: 628, loss is 0.00014028338773641735\n",
      "epoch: 8 step: 629, loss is 0.08240637183189392\n",
      "epoch: 8 step: 630, loss is 0.056825581938028336\n",
      "epoch: 8 step: 631, loss is 0.0002658402663655579\n",
      "epoch: 8 step: 632, loss is 0.00913426373153925\n",
      "epoch: 8 step: 633, loss is 0.0004033987643197179\n",
      "epoch: 8 step: 634, loss is 0.006269032601267099\n",
      "epoch: 8 step: 635, loss is 0.01743505336344242\n",
      "epoch: 8 step: 636, loss is 0.002637692494317889\n",
      "epoch: 8 step: 637, loss is 0.0011479462264105678\n",
      "epoch: 8 step: 638, loss is 0.008007672615349293\n",
      "epoch: 8 step: 639, loss is 0.0005995306419208646\n",
      "epoch: 8 step: 640, loss is 0.0004430223489180207\n",
      "epoch: 8 step: 641, loss is 0.008650504983961582\n",
      "epoch: 8 step: 642, loss is 0.0014212202513590455\n",
      "epoch: 8 step: 643, loss is 0.00044919646461494267\n",
      "epoch: 8 step: 644, loss is 0.006818393245339394\n",
      "epoch: 8 step: 645, loss is 0.002177504589781165\n",
      "epoch: 8 step: 646, loss is 0.0041982089169323444\n",
      "epoch: 8 step: 647, loss is 0.0006422038422897458\n",
      "epoch: 8 step: 648, loss is 0.0010774151887744665\n",
      "epoch: 8 step: 649, loss is 0.0003569344989955425\n",
      "epoch: 8 step: 650, loss is 0.005503301974385977\n",
      "epoch: 8 step: 651, loss is 0.03352433070540428\n",
      "epoch: 8 step: 652, loss is 0.00835439097136259\n",
      "epoch: 8 step: 653, loss is 0.0007864288636483252\n",
      "epoch: 8 step: 654, loss is 0.00027846102602779865\n",
      "epoch: 8 step: 655, loss is 0.17640015482902527\n",
      "epoch: 8 step: 656, loss is 0.006628572940826416\n",
      "epoch: 8 step: 657, loss is 0.0021339335944503546\n",
      "epoch: 8 step: 658, loss is 0.0024482249282300472\n",
      "epoch: 8 step: 659, loss is 0.10179751366376877\n",
      "epoch: 8 step: 660, loss is 0.0013131487648934126\n",
      "epoch: 8 step: 661, loss is 0.008289681747555733\n",
      "epoch: 8 step: 662, loss is 0.0008374309982173145\n",
      "epoch: 8 step: 663, loss is 0.0009909318760037422\n",
      "epoch: 8 step: 664, loss is 0.00035985425347462296\n",
      "epoch: 8 step: 665, loss is 0.009469132870435715\n",
      "epoch: 8 step: 666, loss is 0.0009024773025885224\n",
      "epoch: 8 step: 667, loss is 0.010878151282668114\n",
      "epoch: 8 step: 668, loss is 0.007233213633298874\n",
      "epoch: 8 step: 669, loss is 0.000639343517832458\n",
      "epoch: 8 step: 670, loss is 0.0011054390342906117\n",
      "epoch: 8 step: 671, loss is 0.009922133758664131\n",
      "epoch: 8 step: 672, loss is 0.00011455453204689547\n",
      "epoch: 8 step: 673, loss is 8.519005496054888e-05\n",
      "epoch: 8 step: 674, loss is 0.00033454596996307373\n",
      "epoch: 8 step: 675, loss is 0.00010839223250513896\n",
      "epoch: 8 step: 676, loss is 0.024884121492505074\n",
      "epoch: 8 step: 677, loss is 0.017339302226901054\n",
      "epoch: 8 step: 678, loss is 0.01915624365210533\n",
      "epoch: 8 step: 679, loss is 0.0006356998346745968\n",
      "epoch: 8 step: 680, loss is 0.00037791550857946277\n",
      "epoch: 8 step: 681, loss is 0.014868738129734993\n",
      "epoch: 8 step: 682, loss is 0.0011606290936470032\n",
      "epoch: 8 step: 683, loss is 0.0014103801222518086\n",
      "epoch: 8 step: 684, loss is 0.0008070858893916011\n",
      "epoch: 8 step: 685, loss is 0.02330181561410427\n",
      "epoch: 8 step: 686, loss is 0.0008806040859781206\n",
      "epoch: 8 step: 687, loss is 0.00014502603153232485\n",
      "epoch: 8 step: 688, loss is 0.0013840849278494716\n",
      "epoch: 8 step: 689, loss is 0.020589910447597504\n",
      "epoch: 8 step: 690, loss is 0.002249676501378417\n",
      "epoch: 8 step: 691, loss is 4.836352673009969e-05\n",
      "epoch: 8 step: 692, loss is 0.0046568624675273895\n",
      "epoch: 8 step: 693, loss is 0.008476711809635162\n",
      "epoch: 8 step: 694, loss is 0.0015177760506048799\n",
      "epoch: 8 step: 695, loss is 0.00026662691379897296\n",
      "epoch: 8 step: 696, loss is 0.00432925671339035\n",
      "epoch: 8 step: 697, loss is 0.0024651535786688328\n",
      "epoch: 8 step: 698, loss is 0.011980654671788216\n",
      "epoch: 8 step: 699, loss is 0.0019214563071727753\n",
      "epoch: 8 step: 700, loss is 0.010177411139011383\n",
      "epoch: 8 step: 701, loss is 0.0002466953592374921\n",
      "epoch: 8 step: 702, loss is 0.0008239159942604601\n",
      "epoch: 8 step: 703, loss is 0.0001659384579397738\n",
      "epoch: 8 step: 704, loss is 0.0009513227269053459\n",
      "epoch: 8 step: 705, loss is 0.00173496687784791\n",
      "epoch: 8 step: 706, loss is 0.00011041139805456623\n",
      "epoch: 8 step: 707, loss is 0.0006151297129690647\n",
      "epoch: 8 step: 708, loss is 0.005625794641673565\n",
      "epoch: 8 step: 709, loss is 7.422108319588006e-05\n",
      "epoch: 8 step: 710, loss is 0.0018012544605880976\n",
      "epoch: 8 step: 711, loss is 0.02102586254477501\n",
      "epoch: 8 step: 712, loss is 0.0004112671595066786\n",
      "epoch: 8 step: 713, loss is 0.013525365851819515\n",
      "epoch: 8 step: 714, loss is 0.001100957510061562\n",
      "epoch: 8 step: 715, loss is 0.1756238341331482\n",
      "epoch: 8 step: 716, loss is 0.0003044964687433094\n",
      "epoch: 8 step: 717, loss is 0.0019422074547037482\n",
      "epoch: 8 step: 718, loss is 0.002812977647408843\n",
      "epoch: 8 step: 719, loss is 0.0007726604235358536\n",
      "epoch: 8 step: 720, loss is 0.0019151604501530528\n",
      "epoch: 8 step: 721, loss is 0.002143311547115445\n",
      "epoch: 8 step: 722, loss is 0.00012074460391886532\n",
      "epoch: 8 step: 723, loss is 0.0018815008224919438\n",
      "epoch: 8 step: 724, loss is 0.004470949526876211\n",
      "epoch: 8 step: 725, loss is 0.0003062862379010767\n",
      "epoch: 8 step: 726, loss is 0.00010966096306219697\n",
      "epoch: 8 step: 727, loss is 0.0012782076373696327\n",
      "epoch: 8 step: 728, loss is 0.0019255062798038125\n",
      "epoch: 8 step: 729, loss is 0.13384050130844116\n",
      "epoch: 8 step: 730, loss is 0.032092176377773285\n",
      "epoch: 8 step: 731, loss is 0.1006101593375206\n",
      "epoch: 8 step: 732, loss is 0.022981220856308937\n",
      "epoch: 8 step: 733, loss is 0.07570613920688629\n",
      "epoch: 8 step: 734, loss is 0.004619312938302755\n",
      "epoch: 8 step: 735, loss is 8.703341882210225e-05\n",
      "epoch: 8 step: 736, loss is 0.0004147861327510327\n",
      "epoch: 8 step: 737, loss is 0.0001480314676882699\n",
      "epoch: 8 step: 738, loss is 0.002128094667568803\n",
      "epoch: 8 step: 739, loss is 0.0032568767201155424\n",
      "epoch: 8 step: 740, loss is 0.0018256669864058495\n",
      "epoch: 8 step: 741, loss is 0.10299656540155411\n",
      "epoch: 8 step: 742, loss is 0.00025988591369241476\n",
      "epoch: 8 step: 743, loss is 0.002437886781990528\n",
      "epoch: 8 step: 744, loss is 0.01279409509152174\n",
      "epoch: 8 step: 745, loss is 0.0005090117338113487\n",
      "epoch: 8 step: 746, loss is 0.00539984367787838\n",
      "epoch: 8 step: 747, loss is 0.00021532064420171082\n",
      "epoch: 8 step: 748, loss is 0.019029930233955383\n",
      "epoch: 8 step: 749, loss is 0.002127234125509858\n",
      "epoch: 8 step: 750, loss is 0.08708644658327103\n",
      "epoch: 8 step: 751, loss is 9.299963858211413e-05\n",
      "epoch: 8 step: 752, loss is 0.008054696023464203\n",
      "epoch: 8 step: 753, loss is 0.0010956876212731004\n",
      "epoch: 8 step: 754, loss is 0.014483245089650154\n",
      "epoch: 8 step: 755, loss is 0.0041000088676810265\n",
      "epoch: 8 step: 756, loss is 0.0008447967120446265\n",
      "epoch: 8 step: 757, loss is 0.0006574496510438621\n",
      "epoch: 8 step: 758, loss is 0.0006613372243009508\n",
      "epoch: 8 step: 759, loss is 0.0006541878683492541\n",
      "epoch: 8 step: 760, loss is 0.0001584853744134307\n",
      "epoch: 8 step: 761, loss is 0.0019362171879038215\n",
      "epoch: 8 step: 762, loss is 0.0008892391342669725\n",
      "epoch: 8 step: 763, loss is 0.00017514542560093105\n",
      "epoch: 8 step: 764, loss is 0.00010809980449266732\n",
      "epoch: 8 step: 765, loss is 0.001322782482020557\n",
      "epoch: 8 step: 766, loss is 0.0003290303284302354\n",
      "epoch: 8 step: 767, loss is 0.00018102787726093084\n",
      "epoch: 8 step: 768, loss is 0.030369393527507782\n",
      "epoch: 8 step: 769, loss is 0.0009467428317293525\n",
      "epoch: 8 step: 770, loss is 0.157836452126503\n",
      "epoch: 8 step: 771, loss is 0.0003703878610394895\n",
      "epoch: 8 step: 772, loss is 0.0010923347435891628\n",
      "epoch: 8 step: 773, loss is 0.008259834721684456\n",
      "epoch: 8 step: 774, loss is 0.038821179419755936\n",
      "epoch: 8 step: 775, loss is 0.0024204100482165813\n",
      "epoch: 8 step: 776, loss is 0.000912349671125412\n",
      "epoch: 8 step: 777, loss is 0.009267119690775871\n",
      "epoch: 8 step: 778, loss is 0.004712491296231747\n",
      "epoch: 8 step: 779, loss is 0.0028465467039495707\n",
      "epoch: 8 step: 780, loss is 0.0011797667248174548\n",
      "epoch: 8 step: 781, loss is 0.002317694714292884\n",
      "epoch: 8 step: 782, loss is 0.02930712141096592\n",
      "epoch: 8 step: 783, loss is 0.009054390713572502\n",
      "epoch: 8 step: 784, loss is 0.001555055845528841\n",
      "epoch: 8 step: 785, loss is 0.0009722169488668442\n",
      "epoch: 8 step: 786, loss is 0.0005288246902637184\n",
      "epoch: 8 step: 787, loss is 0.0011269268579781055\n",
      "epoch: 8 step: 788, loss is 0.001956906635314226\n",
      "epoch: 8 step: 789, loss is 0.0004984132829122245\n",
      "epoch: 8 step: 790, loss is 0.0035650823265314102\n",
      "epoch: 8 step: 791, loss is 0.0004440959310159087\n",
      "epoch: 8 step: 792, loss is 0.0021202329080551863\n",
      "epoch: 8 step: 793, loss is 0.02872387133538723\n",
      "epoch: 8 step: 794, loss is 0.009286523796617985\n",
      "epoch: 8 step: 795, loss is 0.0018071659142151475\n",
      "epoch: 8 step: 796, loss is 0.0010635979706421494\n",
      "epoch: 8 step: 797, loss is 6.31596558378078e-05\n",
      "epoch: 8 step: 798, loss is 0.009438024833798409\n",
      "epoch: 8 step: 799, loss is 0.022819465026259422\n",
      "epoch: 8 step: 800, loss is 0.0014509119791910052\n",
      "epoch: 8 step: 801, loss is 0.00011653763067442924\n",
      "epoch: 8 step: 802, loss is 0.0009502085158601403\n",
      "epoch: 8 step: 803, loss is 0.00023432206944562495\n",
      "epoch: 8 step: 804, loss is 0.0002048579917754978\n",
      "epoch: 8 step: 805, loss is 0.001357745029963553\n",
      "epoch: 8 step: 806, loss is 0.00019428688392508775\n",
      "epoch: 8 step: 807, loss is 0.0009787436574697495\n",
      "epoch: 8 step: 808, loss is 0.017901720479130745\n",
      "epoch: 8 step: 809, loss is 0.001444942899979651\n",
      "epoch: 8 step: 810, loss is 0.00032750124228186905\n",
      "epoch: 8 step: 811, loss is 0.003958251792937517\n",
      "epoch: 8 step: 812, loss is 0.00040877817082218826\n",
      "epoch: 8 step: 813, loss is 0.005825844593346119\n",
      "epoch: 8 step: 814, loss is 0.0045588635839521885\n",
      "epoch: 8 step: 815, loss is 0.00030805604183115065\n",
      "epoch: 8 step: 816, loss is 0.005783672444522381\n",
      "epoch: 8 step: 817, loss is 4.189974060864188e-05\n",
      "epoch: 8 step: 818, loss is 0.0019794332329183817\n",
      "epoch: 8 step: 819, loss is 0.000999301322735846\n",
      "epoch: 8 step: 820, loss is 0.006712985225021839\n",
      "epoch: 8 step: 821, loss is 0.0003640445356722921\n",
      "epoch: 8 step: 822, loss is 0.01129166129976511\n",
      "epoch: 8 step: 823, loss is 2.5411758542759344e-05\n",
      "epoch: 8 step: 824, loss is 0.0036313822492957115\n",
      "epoch: 8 step: 825, loss is 0.0003033732355106622\n",
      "epoch: 8 step: 826, loss is 0.0031993319280445576\n",
      "epoch: 8 step: 827, loss is 0.039058346301317215\n",
      "epoch: 8 step: 828, loss is 0.001641523907892406\n",
      "epoch: 8 step: 829, loss is 0.0005208661314100027\n",
      "epoch: 8 step: 830, loss is 0.0004876737657468766\n",
      "epoch: 8 step: 831, loss is 0.005629876162856817\n",
      "epoch: 8 step: 832, loss is 5.834791227243841e-05\n",
      "epoch: 8 step: 833, loss is 0.0006077722064219415\n",
      "epoch: 8 step: 834, loss is 0.012844879180192947\n",
      "epoch: 8 step: 835, loss is 0.03477181866765022\n",
      "epoch: 8 step: 836, loss is 0.005276801530271769\n",
      "epoch: 8 step: 837, loss is 0.011929841712117195\n",
      "epoch: 8 step: 838, loss is 0.0011740224435925484\n",
      "epoch: 8 step: 839, loss is 0.0029766561929136515\n",
      "epoch: 8 step: 840, loss is 0.00032076844945549965\n",
      "epoch: 8 step: 841, loss is 0.001983261900022626\n",
      "epoch: 8 step: 842, loss is 0.00038074218900874257\n",
      "epoch: 8 step: 843, loss is 0.014148584567010403\n",
      "epoch: 8 step: 844, loss is 3.72787362721283e-05\n",
      "epoch: 8 step: 845, loss is 0.001674840459600091\n",
      "epoch: 8 step: 846, loss is 0.0004934928147122264\n",
      "epoch: 8 step: 847, loss is 0.00387124833650887\n",
      "epoch: 8 step: 848, loss is 0.0015579978935420513\n",
      "epoch: 8 step: 849, loss is 0.0012333865743130445\n",
      "epoch: 8 step: 850, loss is 0.007556899916380644\n",
      "epoch: 8 step: 851, loss is 0.00617187237367034\n",
      "epoch: 8 step: 852, loss is 0.001507255481556058\n",
      "epoch: 8 step: 853, loss is 0.0006088372902013361\n",
      "epoch: 8 step: 854, loss is 6.574254075530916e-05\n",
      "epoch: 8 step: 855, loss is 0.004128832835704088\n",
      "epoch: 8 step: 856, loss is 1.8338019799557514e-05\n",
      "epoch: 8 step: 857, loss is 0.0008802262600511312\n",
      "epoch: 8 step: 858, loss is 0.015475236810743809\n",
      "epoch: 8 step: 859, loss is 0.0010823009070008993\n",
      "epoch: 8 step: 860, loss is 6.062625834601931e-05\n",
      "epoch: 8 step: 861, loss is 0.003954339772462845\n",
      "epoch: 8 step: 862, loss is 3.27440575347282e-05\n",
      "epoch: 8 step: 863, loss is 0.0002943887666333467\n",
      "epoch: 8 step: 864, loss is 0.004540368914604187\n",
      "epoch: 8 step: 865, loss is 1.398753829562338e-05\n",
      "epoch: 8 step: 866, loss is 0.011612809263169765\n",
      "epoch: 8 step: 867, loss is 0.007076077628880739\n",
      "epoch: 8 step: 868, loss is 0.004257896449416876\n",
      "epoch: 8 step: 869, loss is 0.0012018202105537057\n",
      "epoch: 8 step: 870, loss is 0.0022167132701724768\n",
      "epoch: 8 step: 871, loss is 0.00017798665794543922\n",
      "epoch: 8 step: 872, loss is 0.022773336619138718\n",
      "epoch: 8 step: 873, loss is 0.003199704922735691\n",
      "epoch: 8 step: 874, loss is 0.000134961141156964\n",
      "epoch: 8 step: 875, loss is 0.06667804718017578\n",
      "epoch: 8 step: 876, loss is 0.030825065448880196\n",
      "epoch: 8 step: 877, loss is 0.0007907673134468496\n",
      "epoch: 8 step: 878, loss is 7.306151383090764e-05\n",
      "epoch: 8 step: 879, loss is 0.0020148507319390774\n",
      "epoch: 8 step: 880, loss is 0.0013684731675311923\n",
      "epoch: 8 step: 881, loss is 0.000682955898810178\n",
      "epoch: 8 step: 882, loss is 0.11832808703184128\n",
      "epoch: 8 step: 883, loss is 0.009191518649458885\n",
      "epoch: 8 step: 884, loss is 0.12349162250757217\n",
      "epoch: 8 step: 885, loss is 0.00010701286373659968\n",
      "epoch: 8 step: 886, loss is 0.0028884964995086193\n",
      "epoch: 8 step: 887, loss is 0.0015078260330483317\n",
      "epoch: 8 step: 888, loss is 0.0008235713467001915\n",
      "epoch: 8 step: 889, loss is 0.0007143946131691337\n",
      "epoch: 8 step: 890, loss is 0.0001013618748402223\n",
      "epoch: 8 step: 891, loss is 0.000365118874469772\n",
      "epoch: 8 step: 892, loss is 0.00036089945933781564\n",
      "epoch: 8 step: 893, loss is 0.00011436700879130512\n",
      "epoch: 8 step: 894, loss is 0.0014767873799428344\n",
      "epoch: 8 step: 895, loss is 0.004859321750700474\n",
      "epoch: 8 step: 896, loss is 0.010468367487192154\n",
      "epoch: 8 step: 897, loss is 0.00016255854279734194\n",
      "epoch: 8 step: 898, loss is 0.002380932914093137\n",
      "epoch: 8 step: 899, loss is 0.18834105134010315\n",
      "epoch: 8 step: 900, loss is 0.01220216415822506\n",
      "epoch: 8 step: 901, loss is 0.0012020876165479422\n",
      "epoch: 8 step: 902, loss is 0.0010687295580282807\n",
      "epoch: 8 step: 903, loss is 0.00035183026921004057\n",
      "epoch: 8 step: 904, loss is 0.0005968784680590034\n",
      "epoch: 8 step: 905, loss is 0.07585258781909943\n",
      "epoch: 8 step: 906, loss is 0.0011551878415048122\n",
      "epoch: 8 step: 907, loss is 0.001129929325543344\n",
      "epoch: 8 step: 908, loss is 0.028668144717812538\n",
      "epoch: 8 step: 909, loss is 2.8452959668356925e-05\n",
      "epoch: 8 step: 910, loss is 3.46925480698701e-05\n",
      "epoch: 8 step: 911, loss is 0.005069872364401817\n",
      "epoch: 8 step: 912, loss is 0.00021340162493288517\n",
      "epoch: 8 step: 913, loss is 0.0012865960597991943\n",
      "epoch: 8 step: 914, loss is 0.018293850123882294\n",
      "epoch: 8 step: 915, loss is 0.0014680837048217654\n",
      "epoch: 8 step: 916, loss is 0.00023338280152529478\n",
      "epoch: 8 step: 917, loss is 0.0009129261015914381\n",
      "epoch: 8 step: 918, loss is 7.85944503149949e-05\n",
      "epoch: 8 step: 919, loss is 0.0006959240417927504\n",
      "epoch: 8 step: 920, loss is 0.0011763550573959947\n",
      "epoch: 8 step: 921, loss is 0.009849349968135357\n",
      "epoch: 8 step: 922, loss is 0.002033763565123081\n",
      "epoch: 8 step: 923, loss is 0.10709414631128311\n",
      "epoch: 8 step: 924, loss is 0.0005374711472541094\n",
      "epoch: 8 step: 925, loss is 0.008792960084974766\n",
      "epoch: 8 step: 926, loss is 0.10389264672994614\n",
      "epoch: 8 step: 927, loss is 0.014444474130868912\n",
      "epoch: 8 step: 928, loss is 0.0003266852581873536\n",
      "epoch: 8 step: 929, loss is 0.0016048637917265296\n",
      "epoch: 8 step: 930, loss is 0.29884693026542664\n",
      "epoch: 8 step: 931, loss is 0.003820364596322179\n",
      "epoch: 8 step: 932, loss is 0.0036828957963734865\n",
      "epoch: 8 step: 933, loss is 2.17538090510061e-05\n",
      "epoch: 8 step: 934, loss is 0.013498492538928986\n",
      "epoch: 8 step: 935, loss is 0.010141732171177864\n",
      "epoch: 8 step: 936, loss is 0.00098927051294595\n",
      "epoch: 8 step: 937, loss is 0.0061882613226771355\n",
      "epoch: 8 step: 938, loss is 0.0006928891525603831\n",
      "epoch: 8 step: 939, loss is 9.226797556038946e-05\n",
      "epoch: 8 step: 940, loss is 0.031678952276706696\n",
      "epoch: 8 step: 941, loss is 0.3049972355365753\n",
      "epoch: 8 step: 942, loss is 0.05674673989415169\n",
      "epoch: 8 step: 943, loss is 0.0009210148709826171\n",
      "epoch: 8 step: 944, loss is 0.00023830999271012843\n",
      "epoch: 8 step: 945, loss is 0.003919875714927912\n",
      "epoch: 8 step: 946, loss is 0.00024768002913333476\n",
      "epoch: 8 step: 947, loss is 0.00038667855551466346\n",
      "epoch: 8 step: 948, loss is 0.0012697940692305565\n",
      "epoch: 8 step: 949, loss is 0.00037157750921323895\n",
      "epoch: 8 step: 950, loss is 0.000543937785550952\n",
      "epoch: 8 step: 951, loss is 0.0005854534101672471\n",
      "epoch: 8 step: 952, loss is 0.0025869791861623526\n",
      "epoch: 8 step: 953, loss is 0.14494489133358002\n",
      "epoch: 8 step: 954, loss is 0.005077738780528307\n",
      "epoch: 8 step: 955, loss is 0.004643013700842857\n",
      "epoch: 8 step: 956, loss is 0.05026618018746376\n",
      "epoch: 8 step: 957, loss is 0.006566013675183058\n",
      "epoch: 8 step: 958, loss is 0.013663012534379959\n",
      "epoch: 8 step: 959, loss is 0.004855009727180004\n",
      "epoch: 8 step: 960, loss is 0.017299745231866837\n",
      "epoch: 8 step: 961, loss is 0.009480994194746017\n",
      "epoch: 8 step: 962, loss is 0.0043625920079648495\n",
      "epoch: 8 step: 963, loss is 0.0006458774441853166\n",
      "epoch: 8 step: 964, loss is 0.00041446159593760967\n",
      "epoch: 8 step: 965, loss is 0.0007671377388760448\n",
      "epoch: 8 step: 966, loss is 0.015584681183099747\n",
      "epoch: 8 step: 967, loss is 0.03967740759253502\n",
      "epoch: 8 step: 968, loss is 0.16853155195713043\n",
      "epoch: 8 step: 969, loss is 0.010333645157516003\n",
      "epoch: 8 step: 970, loss is 0.0015736324712634087\n",
      "epoch: 8 step: 971, loss is 4.923167216475122e-05\n",
      "epoch: 8 step: 972, loss is 0.0027385838329792023\n",
      "epoch: 8 step: 973, loss is 0.00041464969399385154\n",
      "epoch: 8 step: 974, loss is 0.00016552998567931354\n",
      "epoch: 8 step: 975, loss is 0.00038292977842502296\n",
      "epoch: 8 step: 976, loss is 0.0038799478206783533\n",
      "epoch: 8 step: 977, loss is 0.00023236792185343802\n",
      "epoch: 8 step: 978, loss is 0.017810162156820297\n",
      "epoch: 8 step: 979, loss is 0.001844311598688364\n",
      "epoch: 8 step: 980, loss is 0.000656559132039547\n",
      "epoch: 8 step: 981, loss is 0.007309213746339083\n",
      "epoch: 8 step: 982, loss is 0.03160783648490906\n",
      "epoch: 8 step: 983, loss is 0.003345734905451536\n",
      "epoch: 8 step: 984, loss is 0.0009654905879870057\n",
      "epoch: 8 step: 985, loss is 0.011224772781133652\n",
      "epoch: 8 step: 986, loss is 0.0037414662074297667\n",
      "epoch: 8 step: 987, loss is 0.000532032223418355\n",
      "epoch: 8 step: 988, loss is 0.0027696588076651096\n",
      "epoch: 8 step: 989, loss is 0.0026255387347191572\n",
      "epoch: 8 step: 990, loss is 0.0008848732686601579\n",
      "epoch: 8 step: 991, loss is 0.0075614615343511105\n",
      "epoch: 8 step: 992, loss is 0.021861154586076736\n",
      "epoch: 8 step: 993, loss is 0.1436958611011505\n",
      "epoch: 8 step: 994, loss is 0.013043072074651718\n",
      "epoch: 8 step: 995, loss is 0.0022030025720596313\n",
      "epoch: 8 step: 996, loss is 0.0005691300029866397\n",
      "epoch: 8 step: 997, loss is 2.5700292098918e-05\n",
      "epoch: 8 step: 998, loss is 0.04269189015030861\n",
      "epoch: 8 step: 999, loss is 0.0001203764186357148\n",
      "epoch: 8 step: 1000, loss is 4.794651613337919e-05\n",
      "epoch: 8 step: 1001, loss is 0.001324117067269981\n",
      "epoch: 8 step: 1002, loss is 0.004191798623651266\n",
      "epoch: 8 step: 1003, loss is 0.0010409828973934054\n",
      "epoch: 8 step: 1004, loss is 0.002090618247166276\n",
      "epoch: 8 step: 1005, loss is 0.001379598630592227\n",
      "epoch: 8 step: 1006, loss is 5.433155092759989e-05\n",
      "epoch: 8 step: 1007, loss is 0.022366875782608986\n",
      "epoch: 8 step: 1008, loss is 0.00017075195501092821\n",
      "epoch: 8 step: 1009, loss is 0.0015568025410175323\n",
      "epoch: 8 step: 1010, loss is 0.0002017998485825956\n",
      "epoch: 8 step: 1011, loss is 0.0008937142556533217\n",
      "epoch: 8 step: 1012, loss is 0.00395361939445138\n",
      "epoch: 8 step: 1013, loss is 0.000866894843056798\n",
      "epoch: 8 step: 1014, loss is 8.271069236798212e-05\n",
      "epoch: 8 step: 1015, loss is 0.018733466044068336\n",
      "epoch: 8 step: 1016, loss is 0.0005867487634532154\n",
      "epoch: 8 step: 1017, loss is 0.13054810464382172\n",
      "epoch: 8 step: 1018, loss is 0.0028562883380800486\n",
      "epoch: 8 step: 1019, loss is 2.5806026314967312e-05\n",
      "epoch: 8 step: 1020, loss is 0.00039771650335751474\n",
      "epoch: 8 step: 1021, loss is 0.047594789415597916\n",
      "epoch: 8 step: 1022, loss is 0.007257327903062105\n",
      "epoch: 8 step: 1023, loss is 0.1701308935880661\n",
      "epoch: 8 step: 1024, loss is 4.751185770146549e-05\n",
      "epoch: 8 step: 1025, loss is 0.00173381925560534\n",
      "epoch: 8 step: 1026, loss is 0.0005770984571427107\n",
      "epoch: 8 step: 1027, loss is 0.0015480625443160534\n",
      "epoch: 8 step: 1028, loss is 0.00027498818235471845\n",
      "epoch: 8 step: 1029, loss is 0.0819384753704071\n",
      "epoch: 8 step: 1030, loss is 0.0005311461281962693\n",
      "epoch: 8 step: 1031, loss is 0.00044334519770927727\n",
      "epoch: 8 step: 1032, loss is 0.031965162605047226\n",
      "epoch: 8 step: 1033, loss is 0.0019917315803468227\n",
      "epoch: 8 step: 1034, loss is 8.982637518784031e-05\n",
      "epoch: 8 step: 1035, loss is 0.01775153912603855\n",
      "epoch: 8 step: 1036, loss is 0.006960097700357437\n",
      "epoch: 8 step: 1037, loss is 0.0006189941195771098\n",
      "epoch: 8 step: 1038, loss is 0.025736160576343536\n",
      "epoch: 8 step: 1039, loss is 0.0004903253284282982\n",
      "epoch: 8 step: 1040, loss is 0.005799314938485622\n",
      "epoch: 8 step: 1041, loss is 0.0006554268184117973\n",
      "epoch: 8 step: 1042, loss is 0.08392397314310074\n",
      "epoch: 8 step: 1043, loss is 0.08600609749555588\n",
      "epoch: 8 step: 1044, loss is 0.00267842598259449\n",
      "epoch: 8 step: 1045, loss is 0.002152263419702649\n",
      "epoch: 8 step: 1046, loss is 0.1557534784078598\n",
      "epoch: 8 step: 1047, loss is 0.01049619447439909\n",
      "epoch: 8 step: 1048, loss is 0.004260426852852106\n",
      "epoch: 8 step: 1049, loss is 0.0073800464160740376\n",
      "epoch: 8 step: 1050, loss is 0.0031514144502580166\n",
      "epoch: 8 step: 1051, loss is 0.00023281967150978744\n",
      "epoch: 8 step: 1052, loss is 0.2813820540904999\n",
      "epoch: 8 step: 1053, loss is 0.032358232885599136\n",
      "epoch: 8 step: 1054, loss is 0.03333071991801262\n",
      "epoch: 8 step: 1055, loss is 0.00015333935152739286\n",
      "epoch: 8 step: 1056, loss is 4.6998702600831166e-05\n",
      "epoch: 8 step: 1057, loss is 0.001110279350541532\n",
      "epoch: 8 step: 1058, loss is 0.03801432251930237\n",
      "epoch: 8 step: 1059, loss is 0.026326727122068405\n",
      "epoch: 8 step: 1060, loss is 0.023091858252882957\n",
      "epoch: 8 step: 1061, loss is 0.030640827491879463\n",
      "epoch: 8 step: 1062, loss is 0.0643681064248085\n",
      "epoch: 8 step: 1063, loss is 0.018577683717012405\n",
      "epoch: 8 step: 1064, loss is 0.0001493865711381659\n",
      "epoch: 8 step: 1065, loss is 0.003510622074827552\n",
      "epoch: 8 step: 1066, loss is 0.0005658267764374614\n",
      "epoch: 8 step: 1067, loss is 0.008536304347217083\n",
      "epoch: 8 step: 1068, loss is 0.003940246999263763\n",
      "epoch: 8 step: 1069, loss is 0.001700428780168295\n",
      "epoch: 8 step: 1070, loss is 0.03976769000291824\n",
      "epoch: 8 step: 1071, loss is 0.13701146841049194\n",
      "epoch: 8 step: 1072, loss is 0.10334306955337524\n",
      "epoch: 8 step: 1073, loss is 0.022703638300299644\n",
      "epoch: 8 step: 1074, loss is 0.002135226037353277\n",
      "epoch: 8 step: 1075, loss is 5.347405021893792e-05\n",
      "epoch: 8 step: 1076, loss is 0.15964825451374054\n",
      "epoch: 8 step: 1077, loss is 0.008981719613075256\n",
      "epoch: 8 step: 1078, loss is 0.0018976678838953376\n",
      "epoch: 8 step: 1079, loss is 0.015702685341238976\n",
      "epoch: 8 step: 1080, loss is 0.0007839192403480411\n",
      "epoch: 8 step: 1081, loss is 0.0049553015269339085\n",
      "epoch: 8 step: 1082, loss is 0.00014449637092184275\n",
      "epoch: 8 step: 1083, loss is 0.0002966191677842289\n",
      "epoch: 8 step: 1084, loss is 0.0014155401149764657\n",
      "epoch: 8 step: 1085, loss is 0.011408115737140179\n",
      "epoch: 8 step: 1086, loss is 0.0003297976800240576\n",
      "epoch: 8 step: 1087, loss is 0.030841371044516563\n",
      "epoch: 8 step: 1088, loss is 0.00023512644111178815\n",
      "epoch: 8 step: 1089, loss is 0.00014822871889919043\n",
      "epoch: 8 step: 1090, loss is 0.0010973738972097635\n",
      "epoch: 8 step: 1091, loss is 0.016721881926059723\n",
      "epoch: 8 step: 1092, loss is 0.011472118087112904\n",
      "epoch: 8 step: 1093, loss is 0.004862810485064983\n",
      "epoch: 8 step: 1094, loss is 8.245868230005726e-05\n",
      "epoch: 8 step: 1095, loss is 0.07074731588363647\n",
      "epoch: 8 step: 1096, loss is 0.0009353065979667008\n",
      "epoch: 8 step: 1097, loss is 0.0005176793201826513\n",
      "epoch: 8 step: 1098, loss is 0.040754035115242004\n",
      "epoch: 8 step: 1099, loss is 0.052921827882528305\n",
      "epoch: 8 step: 1100, loss is 0.01633244752883911\n",
      "epoch: 8 step: 1101, loss is 0.0014279762981459498\n",
      "epoch: 8 step: 1102, loss is 0.0011040859390050173\n",
      "epoch: 8 step: 1103, loss is 0.0038777836598455906\n",
      "epoch: 8 step: 1104, loss is 0.0007404256612062454\n",
      "epoch: 8 step: 1105, loss is 0.004363562911748886\n",
      "epoch: 8 step: 1106, loss is 0.005636482499539852\n",
      "epoch: 8 step: 1107, loss is 0.00023064916604198515\n",
      "epoch: 8 step: 1108, loss is 0.0014215752016752958\n",
      "epoch: 8 step: 1109, loss is 0.007219548337161541\n",
      "epoch: 8 step: 1110, loss is 0.0015270430594682693\n",
      "epoch: 8 step: 1111, loss is 0.00012005763710476458\n",
      "epoch: 8 step: 1112, loss is 0.04451297968626022\n",
      "epoch: 8 step: 1113, loss is 0.00027491027140058577\n",
      "epoch: 8 step: 1114, loss is 0.0006431661895476282\n",
      "epoch: 8 step: 1115, loss is 0.0002929219917859882\n",
      "epoch: 8 step: 1116, loss is 7.182556873885915e-05\n",
      "epoch: 8 step: 1117, loss is 0.0019342424347996712\n",
      "epoch: 8 step: 1118, loss is 0.002191574778407812\n",
      "epoch: 8 step: 1119, loss is 0.0012080156011506915\n",
      "epoch: 8 step: 1120, loss is 0.001571680186316371\n",
      "epoch: 8 step: 1121, loss is 0.0011790043208748102\n",
      "epoch: 8 step: 1122, loss is 0.0020840386860072613\n",
      "epoch: 8 step: 1123, loss is 0.0008310779230669141\n",
      "epoch: 8 step: 1124, loss is 0.08228570222854614\n",
      "epoch: 8 step: 1125, loss is 0.0014900679234415293\n",
      "epoch: 8 step: 1126, loss is 0.0031680401880294085\n",
      "epoch: 8 step: 1127, loss is 0.0003205481916666031\n",
      "epoch: 8 step: 1128, loss is 0.0039897821843624115\n",
      "epoch: 8 step: 1129, loss is 0.004954049363732338\n",
      "epoch: 8 step: 1130, loss is 0.011856969445943832\n",
      "epoch: 8 step: 1131, loss is 0.020475992932915688\n",
      "epoch: 8 step: 1132, loss is 0.0003427296469453722\n",
      "epoch: 8 step: 1133, loss is 0.0033029946498572826\n",
      "epoch: 8 step: 1134, loss is 0.2830069661140442\n",
      "epoch: 8 step: 1135, loss is 0.04335625842213631\n",
      "epoch: 8 step: 1136, loss is 0.0007599765667691827\n",
      "epoch: 8 step: 1137, loss is 0.0006362681742757559\n",
      "epoch: 8 step: 1138, loss is 0.00020876660710200667\n",
      "epoch: 8 step: 1139, loss is 0.037446118891239166\n",
      "epoch: 8 step: 1140, loss is 0.0020659167785197496\n",
      "epoch: 8 step: 1141, loss is 0.0014232128160074353\n",
      "epoch: 8 step: 1142, loss is 0.00241682305932045\n",
      "epoch: 8 step: 1143, loss is 0.0007672635256312788\n",
      "epoch: 8 step: 1144, loss is 0.0423877090215683\n",
      "epoch: 8 step: 1145, loss is 0.00035811090492643416\n",
      "epoch: 8 step: 1146, loss is 0.00031551774009130895\n",
      "epoch: 8 step: 1147, loss is 0.00018956787243951112\n",
      "epoch: 8 step: 1148, loss is 0.0003330582694616169\n",
      "epoch: 8 step: 1149, loss is 0.19815126061439514\n",
      "epoch: 8 step: 1150, loss is 0.06554733961820602\n",
      "epoch: 8 step: 1151, loss is 0.0018292114837095141\n",
      "epoch: 8 step: 1152, loss is 0.0011033618357032537\n",
      "epoch: 8 step: 1153, loss is 0.0017893596086651087\n",
      "epoch: 8 step: 1154, loss is 0.0021578387822955847\n",
      "epoch: 8 step: 1155, loss is 0.0008560098358429968\n",
      "epoch: 8 step: 1156, loss is 0.0024349105078727007\n",
      "epoch: 8 step: 1157, loss is 0.09684135019779205\n",
      "epoch: 8 step: 1158, loss is 7.131083839340135e-05\n",
      "epoch: 8 step: 1159, loss is 3.779732287512161e-05\n",
      "epoch: 8 step: 1160, loss is 0.0009888767963275313\n",
      "epoch: 8 step: 1161, loss is 0.00045002068509347737\n",
      "epoch: 8 step: 1162, loss is 0.02019878290593624\n",
      "epoch: 8 step: 1163, loss is 0.011650165542960167\n",
      "epoch: 8 step: 1164, loss is 0.0005533908843062818\n",
      "epoch: 8 step: 1165, loss is 2.0549688997562043e-05\n",
      "epoch: 8 step: 1166, loss is 0.00017374037997797132\n",
      "epoch: 8 step: 1167, loss is 0.0013651726767420769\n",
      "epoch: 8 step: 1168, loss is 0.004675168078392744\n",
      "epoch: 8 step: 1169, loss is 0.00348581001162529\n",
      "epoch: 8 step: 1170, loss is 0.10025455802679062\n",
      "epoch: 8 step: 1171, loss is 0.0003656904445961118\n",
      "epoch: 8 step: 1172, loss is 0.07134760916233063\n",
      "epoch: 8 step: 1173, loss is 0.0001331103267148137\n",
      "epoch: 8 step: 1174, loss is 0.004763278178870678\n",
      "epoch: 8 step: 1175, loss is 0.019573017954826355\n",
      "epoch: 8 step: 1176, loss is 0.011640511453151703\n",
      "epoch: 8 step: 1177, loss is 0.0013079517520964146\n",
      "epoch: 8 step: 1178, loss is 0.042776141315698624\n",
      "epoch: 8 step: 1179, loss is 0.020368102937936783\n",
      "epoch: 8 step: 1180, loss is 0.0025013370905071497\n",
      "epoch: 8 step: 1181, loss is 0.0010903701186180115\n",
      "epoch: 8 step: 1182, loss is 0.0035093706101179123\n",
      "epoch: 8 step: 1183, loss is 0.011055240407586098\n",
      "epoch: 8 step: 1184, loss is 0.015782006084918976\n",
      "epoch: 8 step: 1185, loss is 0.0008784321835264564\n",
      "epoch: 8 step: 1186, loss is 0.001331075793132186\n",
      "epoch: 8 step: 1187, loss is 0.007711879909038544\n",
      "epoch: 8 step: 1188, loss is 0.0010323976166546345\n",
      "epoch: 8 step: 1189, loss is 0.1824684590101242\n",
      "epoch: 8 step: 1190, loss is 0.002839491469785571\n",
      "epoch: 8 step: 1191, loss is 0.01644338108599186\n",
      "epoch: 8 step: 1192, loss is 0.00028760579880326986\n",
      "epoch: 8 step: 1193, loss is 0.001893230015411973\n",
      "epoch: 8 step: 1194, loss is 0.0003962878545280546\n",
      "epoch: 8 step: 1195, loss is 0.00016928148397710174\n",
      "epoch: 8 step: 1196, loss is 0.006784601137042046\n",
      "epoch: 8 step: 1197, loss is 0.18359945714473724\n",
      "epoch: 8 step: 1198, loss is 0.002349742455407977\n",
      "epoch: 8 step: 1199, loss is 0.0192429069429636\n",
      "epoch: 8 step: 1200, loss is 0.029036428779363632\n",
      "epoch: 8 step: 1201, loss is 0.022107163444161415\n",
      "epoch: 8 step: 1202, loss is 0.01738928258419037\n",
      "epoch: 8 step: 1203, loss is 0.01113702729344368\n",
      "epoch: 8 step: 1204, loss is 0.0002978773263748735\n",
      "epoch: 8 step: 1205, loss is 0.006687039975076914\n",
      "epoch: 8 step: 1206, loss is 0.0005273655988276005\n",
      "epoch: 8 step: 1207, loss is 0.06434548646211624\n",
      "epoch: 8 step: 1208, loss is 0.0009872320806607604\n",
      "epoch: 8 step: 1209, loss is 0.023126721382141113\n",
      "epoch: 8 step: 1210, loss is 0.03621944412589073\n",
      "epoch: 8 step: 1211, loss is 0.045126233249902725\n",
      "epoch: 8 step: 1212, loss is 0.0013273003278300166\n",
      "epoch: 8 step: 1213, loss is 0.0378173366189003\n",
      "epoch: 8 step: 1214, loss is 0.013947829604148865\n",
      "epoch: 8 step: 1215, loss is 6.765576108591631e-05\n",
      "epoch: 8 step: 1216, loss is 0.0697219967842102\n",
      "epoch: 8 step: 1217, loss is 0.0004993252223357558\n",
      "epoch: 8 step: 1218, loss is 0.03403156250715256\n",
      "epoch: 8 step: 1219, loss is 0.011838983744382858\n",
      "epoch: 8 step: 1220, loss is 0.00025848564109764993\n",
      "epoch: 8 step: 1221, loss is 0.0028419725131243467\n",
      "epoch: 8 step: 1222, loss is 0.001196433906443417\n",
      "epoch: 8 step: 1223, loss is 0.12294664978981018\n",
      "epoch: 8 step: 1224, loss is 0.0010542115196585655\n",
      "epoch: 8 step: 1225, loss is 0.0017313881544396281\n",
      "epoch: 8 step: 1226, loss is 0.0008434945484623313\n",
      "epoch: 8 step: 1227, loss is 0.0010933217126876116\n",
      "epoch: 8 step: 1228, loss is 0.0013949030544608831\n",
      "epoch: 8 step: 1229, loss is 0.0003886660560965538\n",
      "epoch: 8 step: 1230, loss is 0.02937043085694313\n",
      "epoch: 8 step: 1231, loss is 0.07548686861991882\n",
      "epoch: 8 step: 1232, loss is 0.006518859416246414\n",
      "epoch: 8 step: 1233, loss is 0.0014163876185193658\n",
      "epoch: 8 step: 1234, loss is 0.0021297892089933157\n",
      "epoch: 8 step: 1235, loss is 0.0004919557832181454\n",
      "epoch: 8 step: 1236, loss is 0.020688358694314957\n",
      "epoch: 8 step: 1237, loss is 0.016002807766199112\n",
      "epoch: 8 step: 1238, loss is 0.002744810888543725\n",
      "epoch: 8 step: 1239, loss is 0.0010911651188507676\n",
      "epoch: 8 step: 1240, loss is 0.0024202403146773577\n",
      "epoch: 8 step: 1241, loss is 0.07083006203174591\n",
      "epoch: 8 step: 1242, loss is 0.0031425212509930134\n",
      "epoch: 8 step: 1243, loss is 0.02298901602625847\n",
      "epoch: 8 step: 1244, loss is 0.004598697647452354\n",
      "epoch: 8 step: 1245, loss is 0.03545906022191048\n",
      "epoch: 8 step: 1246, loss is 0.0006213324377313256\n",
      "epoch: 8 step: 1247, loss is 0.10396984964609146\n",
      "epoch: 8 step: 1248, loss is 0.006494099274277687\n",
      "epoch: 8 step: 1249, loss is 0.0003346969315316528\n",
      "epoch: 8 step: 1250, loss is 0.00141244032420218\n",
      "epoch: 8 step: 1251, loss is 0.0006932963151484728\n",
      "epoch: 8 step: 1252, loss is 0.00038587531889788806\n",
      "epoch: 8 step: 1253, loss is 0.003930751234292984\n",
      "epoch: 8 step: 1254, loss is 0.0017702397890388966\n",
      "epoch: 8 step: 1255, loss is 0.00042954078526236117\n",
      "epoch: 8 step: 1256, loss is 0.000768589205108583\n",
      "epoch: 8 step: 1257, loss is 0.0474233478307724\n",
      "epoch: 8 step: 1258, loss is 9.50469693634659e-05\n",
      "epoch: 8 step: 1259, loss is 0.06987863034009933\n",
      "epoch: 8 step: 1260, loss is 0.061211343854665756\n",
      "epoch: 8 step: 1261, loss is 0.001506782486103475\n",
      "epoch: 8 step: 1262, loss is 0.03082246333360672\n",
      "epoch: 8 step: 1263, loss is 0.004029051400721073\n",
      "epoch: 8 step: 1264, loss is 0.00022836591233499348\n",
      "epoch: 8 step: 1265, loss is 0.0021347617730498314\n",
      "epoch: 8 step: 1266, loss is 0.009004597552120686\n",
      "epoch: 8 step: 1267, loss is 0.019048286601901054\n",
      "epoch: 8 step: 1268, loss is 0.0003389818884897977\n",
      "epoch: 8 step: 1269, loss is 0.00014603226736653596\n",
      "epoch: 8 step: 1270, loss is 0.04530614987015724\n",
      "epoch: 8 step: 1271, loss is 2.5353770979563706e-05\n",
      "epoch: 8 step: 1272, loss is 0.17651298642158508\n",
      "epoch: 8 step: 1273, loss is 0.005746824201196432\n",
      "epoch: 8 step: 1274, loss is 3.551573900040239e-05\n",
      "epoch: 8 step: 1275, loss is 3.8735201087547466e-05\n",
      "epoch: 8 step: 1276, loss is 0.002129301195964217\n",
      "epoch: 8 step: 1277, loss is 7.587055733893067e-05\n",
      "epoch: 8 step: 1278, loss is 0.0008239810122177005\n",
      "epoch: 8 step: 1279, loss is 0.00016635132487863302\n",
      "epoch: 8 step: 1280, loss is 0.0007101391674950719\n",
      "epoch: 8 step: 1281, loss is 0.07066294550895691\n",
      "epoch: 8 step: 1282, loss is 0.0021762687247246504\n",
      "epoch: 8 step: 1283, loss is 5.8988087403122336e-05\n",
      "epoch: 8 step: 1284, loss is 9.818006947170943e-05\n",
      "epoch: 8 step: 1285, loss is 0.006714686285704374\n",
      "epoch: 8 step: 1286, loss is 0.00168820028193295\n",
      "epoch: 8 step: 1287, loss is 0.006112202070653439\n",
      "epoch: 8 step: 1288, loss is 0.00015204322698991746\n",
      "epoch: 8 step: 1289, loss is 0.09472622722387314\n",
      "epoch: 8 step: 1290, loss is 4.044897650601342e-05\n",
      "epoch: 8 step: 1291, loss is 0.0015684510581195354\n",
      "epoch: 8 step: 1292, loss is 0.0021807942539453506\n",
      "epoch: 8 step: 1293, loss is 0.004168697167187929\n",
      "epoch: 8 step: 1294, loss is 0.00037488286034204066\n",
      "epoch: 8 step: 1295, loss is 2.333394695597235e-05\n",
      "epoch: 8 step: 1296, loss is 0.1480952352285385\n",
      "epoch: 8 step: 1297, loss is 0.06585253030061722\n",
      "epoch: 8 step: 1298, loss is 0.025192370638251305\n",
      "epoch: 8 step: 1299, loss is 0.001441126107238233\n",
      "epoch: 8 step: 1300, loss is 0.03477834537625313\n",
      "epoch: 8 step: 1301, loss is 0.0020316641312092543\n",
      "epoch: 8 step: 1302, loss is 0.004178485833108425\n",
      "epoch: 8 step: 1303, loss is 0.0019129953579977155\n",
      "epoch: 8 step: 1304, loss is 0.003266087267547846\n",
      "epoch: 8 step: 1305, loss is 0.0009625277598388493\n",
      "epoch: 8 step: 1306, loss is 0.0017618496203795075\n",
      "epoch: 8 step: 1307, loss is 0.0006461357115767896\n",
      "epoch: 8 step: 1308, loss is 0.0007233421201817691\n",
      "epoch: 8 step: 1309, loss is 0.004072270821779966\n",
      "epoch: 8 step: 1310, loss is 0.00047383055789396167\n",
      "epoch: 8 step: 1311, loss is 0.003344831056892872\n",
      "epoch: 8 step: 1312, loss is 2.180998671974521e-05\n",
      "epoch: 8 step: 1313, loss is 0.0002419468219159171\n",
      "epoch: 8 step: 1314, loss is 0.07564744353294373\n",
      "epoch: 8 step: 1315, loss is 0.001207050052471459\n",
      "epoch: 8 step: 1316, loss is 0.002303959336131811\n",
      "epoch: 8 step: 1317, loss is 0.0655585378408432\n",
      "epoch: 8 step: 1318, loss is 9.166947711491957e-05\n",
      "epoch: 8 step: 1319, loss is 0.0005809618742205203\n",
      "epoch: 8 step: 1320, loss is 0.061434876173734665\n",
      "epoch: 8 step: 1321, loss is 0.03990408033132553\n",
      "epoch: 8 step: 1322, loss is 0.0013294662348926067\n",
      "epoch: 8 step: 1323, loss is 0.028150521218776703\n",
      "epoch: 8 step: 1324, loss is 0.0005413892213255167\n",
      "epoch: 8 step: 1325, loss is 0.07947564125061035\n",
      "epoch: 8 step: 1326, loss is 0.021961759775877\n",
      "epoch: 8 step: 1327, loss is 0.0076806251890957355\n",
      "epoch: 8 step: 1328, loss is 0.00611687358468771\n",
      "epoch: 8 step: 1329, loss is 0.0545676052570343\n",
      "epoch: 8 step: 1330, loss is 0.003133421065285802\n",
      "epoch: 8 step: 1331, loss is 0.0002840817323885858\n",
      "epoch: 8 step: 1332, loss is 0.01419177558273077\n",
      "epoch: 8 step: 1333, loss is 0.008270644582808018\n",
      "epoch: 8 step: 1334, loss is 0.06605413556098938\n",
      "epoch: 8 step: 1335, loss is 0.0019516008906066418\n",
      "epoch: 8 step: 1336, loss is 0.04434492811560631\n",
      "epoch: 8 step: 1337, loss is 0.024892868474125862\n",
      "epoch: 8 step: 1338, loss is 0.0008827163255773485\n",
      "epoch: 8 step: 1339, loss is 0.03202797845005989\n",
      "epoch: 8 step: 1340, loss is 0.0033562968019396067\n",
      "epoch: 8 step: 1341, loss is 0.000207517747185193\n",
      "epoch: 8 step: 1342, loss is 0.001144288806244731\n",
      "epoch: 8 step: 1343, loss is 0.002739100717008114\n",
      "epoch: 8 step: 1344, loss is 0.00015195528976619244\n",
      "epoch: 8 step: 1345, loss is 0.0038849739357829094\n",
      "epoch: 8 step: 1346, loss is 0.00019452538981568068\n",
      "epoch: 8 step: 1347, loss is 0.008509272709488869\n",
      "epoch: 8 step: 1348, loss is 0.005542621947824955\n",
      "epoch: 8 step: 1349, loss is 0.003488505259156227\n",
      "epoch: 8 step: 1350, loss is 0.004930933006107807\n",
      "epoch: 8 step: 1351, loss is 0.16209501028060913\n",
      "epoch: 8 step: 1352, loss is 0.016175637021660805\n",
      "epoch: 8 step: 1353, loss is 0.016323626041412354\n",
      "epoch: 8 step: 1354, loss is 0.017670705914497375\n",
      "epoch: 8 step: 1355, loss is 0.002835645107552409\n",
      "epoch: 8 step: 1356, loss is 0.0011971392668783665\n",
      "epoch: 8 step: 1357, loss is 0.0020625765901058912\n",
      "epoch: 8 step: 1358, loss is 0.0013250653864815831\n",
      "epoch: 8 step: 1359, loss is 0.0003122042980976403\n",
      "epoch: 8 step: 1360, loss is 0.004215492866933346\n",
      "epoch: 8 step: 1361, loss is 0.0014396760379895568\n",
      "epoch: 8 step: 1362, loss is 0.0007104550604708493\n",
      "epoch: 8 step: 1363, loss is 0.013494151644408703\n",
      "epoch: 8 step: 1364, loss is 0.006726876832544804\n",
      "epoch: 8 step: 1365, loss is 0.0026304174680262804\n",
      "epoch: 8 step: 1366, loss is 0.08179356902837753\n",
      "epoch: 8 step: 1367, loss is 0.04732024297118187\n",
      "epoch: 8 step: 1368, loss is 0.0020882943645119667\n",
      "epoch: 8 step: 1369, loss is 0.012199669145047665\n",
      "epoch: 8 step: 1370, loss is 0.07544707506895065\n",
      "epoch: 8 step: 1371, loss is 0.016699930652976036\n",
      "epoch: 8 step: 1372, loss is 0.0007513592136092484\n",
      "epoch: 8 step: 1373, loss is 0.025583676993846893\n",
      "epoch: 8 step: 1374, loss is 0.0002940435952041298\n",
      "epoch: 8 step: 1375, loss is 0.0010123499669134617\n",
      "epoch: 8 step: 1376, loss is 0.022887403145432472\n",
      "epoch: 8 step: 1377, loss is 0.008837451227009296\n",
      "epoch: 8 step: 1378, loss is 5.540967686101794e-05\n",
      "epoch: 8 step: 1379, loss is 0.00026478737709112465\n",
      "epoch: 8 step: 1380, loss is 0.006614674348384142\n",
      "epoch: 8 step: 1381, loss is 0.01548987440764904\n",
      "epoch: 8 step: 1382, loss is 0.0005785023677162826\n",
      "epoch: 8 step: 1383, loss is 0.0013232468627393246\n",
      "epoch: 8 step: 1384, loss is 0.19083786010742188\n",
      "epoch: 8 step: 1385, loss is 0.061075933277606964\n",
      "epoch: 8 step: 1386, loss is 0.0013825829373672605\n",
      "epoch: 8 step: 1387, loss is 0.0002215109416283667\n",
      "epoch: 8 step: 1388, loss is 0.0004898571642115712\n",
      "epoch: 8 step: 1389, loss is 0.011370666325092316\n",
      "epoch: 8 step: 1390, loss is 0.0020354408770799637\n",
      "epoch: 8 step: 1391, loss is 0.0339883416891098\n",
      "epoch: 8 step: 1392, loss is 0.00018833676585927606\n",
      "epoch: 8 step: 1393, loss is 0.004809742793440819\n",
      "epoch: 8 step: 1394, loss is 0.0011638347059488297\n",
      "epoch: 8 step: 1395, loss is 0.02765481173992157\n",
      "epoch: 8 step: 1396, loss is 0.01520858146250248\n",
      "epoch: 8 step: 1397, loss is 0.016797879710793495\n",
      "epoch: 8 step: 1398, loss is 4.16569018852897e-05\n",
      "epoch: 8 step: 1399, loss is 0.00035759626189246774\n",
      "epoch: 8 step: 1400, loss is 0.004300700966268778\n",
      "epoch: 8 step: 1401, loss is 0.0020909185986965895\n",
      "epoch: 8 step: 1402, loss is 6.450188084272668e-05\n",
      "epoch: 8 step: 1403, loss is 0.0033760140649974346\n",
      "epoch: 8 step: 1404, loss is 0.030430307611823082\n",
      "epoch: 8 step: 1405, loss is 0.0005806707195006311\n",
      "epoch: 8 step: 1406, loss is 0.0011824974790215492\n",
      "epoch: 8 step: 1407, loss is 0.0002131672081304714\n",
      "epoch: 8 step: 1408, loss is 0.0004144295526202768\n",
      "epoch: 8 step: 1409, loss is 0.0003838245465885848\n",
      "epoch: 8 step: 1410, loss is 0.005639142822474241\n",
      "epoch: 8 step: 1411, loss is 0.06267577409744263\n",
      "epoch: 8 step: 1412, loss is 0.0027696015313267708\n",
      "epoch: 8 step: 1413, loss is 0.000262671266682446\n",
      "epoch: 8 step: 1414, loss is 0.0004115509509574622\n",
      "epoch: 8 step: 1415, loss is 0.008937971666455269\n",
      "epoch: 8 step: 1416, loss is 0.0009466325282119215\n",
      "epoch: 8 step: 1417, loss is 1.1593483577598818e-05\n",
      "epoch: 8 step: 1418, loss is 0.3095315396785736\n",
      "epoch: 8 step: 1419, loss is 0.001250959001481533\n",
      "epoch: 8 step: 1420, loss is 0.13873928785324097\n",
      "epoch: 8 step: 1421, loss is 0.0013792550889775157\n",
      "epoch: 8 step: 1422, loss is 0.00026626349426805973\n",
      "epoch: 8 step: 1423, loss is 0.0035554745700210333\n",
      "epoch: 8 step: 1424, loss is 0.0015299483202397823\n",
      "epoch: 8 step: 1425, loss is 8.886552677722648e-05\n",
      "epoch: 8 step: 1426, loss is 0.0022944628726691008\n",
      "epoch: 8 step: 1427, loss is 0.004549943841993809\n",
      "epoch: 8 step: 1428, loss is 0.05106951668858528\n",
      "epoch: 8 step: 1429, loss is 0.002949756570160389\n",
      "epoch: 8 step: 1430, loss is 0.0005625406629405916\n",
      "epoch: 8 step: 1431, loss is 0.04773122817277908\n",
      "epoch: 8 step: 1432, loss is 0.000393106194678694\n",
      "epoch: 8 step: 1433, loss is 8.589360368205234e-05\n",
      "epoch: 8 step: 1434, loss is 0.018361516296863556\n",
      "epoch: 8 step: 1435, loss is 0.16303817927837372\n",
      "epoch: 8 step: 1436, loss is 0.0071110534481704235\n",
      "epoch: 8 step: 1437, loss is 0.00046019881847314537\n",
      "epoch: 8 step: 1438, loss is 0.003334175096824765\n",
      "epoch: 8 step: 1439, loss is 0.00044106715358793736\n",
      "epoch: 8 step: 1440, loss is 0.001064810436218977\n",
      "epoch: 8 step: 1441, loss is 0.004592391662299633\n",
      "epoch: 8 step: 1442, loss is 0.01705326698720455\n",
      "epoch: 8 step: 1443, loss is 0.0014179869322106242\n",
      "epoch: 8 step: 1444, loss is 0.000196288398001343\n",
      "epoch: 8 step: 1445, loss is 0.0010850050020962954\n",
      "epoch: 8 step: 1446, loss is 0.1947404444217682\n",
      "epoch: 8 step: 1447, loss is 0.00021346582798287272\n",
      "epoch: 8 step: 1448, loss is 0.0007226386805996299\n",
      "epoch: 8 step: 1449, loss is 0.0020930732134729624\n",
      "epoch: 8 step: 1450, loss is 0.0004941831575706601\n",
      "epoch: 8 step: 1451, loss is 0.00013237723032943904\n",
      "epoch: 8 step: 1452, loss is 0.008870698511600494\n",
      "epoch: 8 step: 1453, loss is 0.05121954530477524\n",
      "epoch: 8 step: 1454, loss is 0.0044950502924621105\n",
      "epoch: 8 step: 1455, loss is 6.728404696332291e-05\n",
      "epoch: 8 step: 1456, loss is 0.004811590537428856\n",
      "epoch: 8 step: 1457, loss is 0.06845321506261826\n",
      "epoch: 8 step: 1458, loss is 0.005700723268091679\n",
      "epoch: 8 step: 1459, loss is 0.0008998495759442449\n",
      "epoch: 8 step: 1460, loss is 0.2038528174161911\n",
      "epoch: 8 step: 1461, loss is 0.2587917149066925\n",
      "epoch: 8 step: 1462, loss is 0.00023529346799477935\n",
      "epoch: 8 step: 1463, loss is 0.013253802433609962\n",
      "epoch: 8 step: 1464, loss is 0.0022767686750739813\n",
      "epoch: 8 step: 1465, loss is 0.11144545674324036\n",
      "epoch: 8 step: 1466, loss is 0.0006897859275341034\n",
      "epoch: 8 step: 1467, loss is 5.129038981976919e-05\n",
      "epoch: 8 step: 1468, loss is 0.0013534151948988438\n",
      "epoch: 8 step: 1469, loss is 0.005022239871323109\n",
      "epoch: 8 step: 1470, loss is 0.05366433411836624\n",
      "epoch: 8 step: 1471, loss is 0.049766600131988525\n",
      "epoch: 8 step: 1472, loss is 0.08951117098331451\n",
      "epoch: 8 step: 1473, loss is 0.0007601246470585465\n",
      "epoch: 8 step: 1474, loss is 0.03326110169291496\n",
      "epoch: 8 step: 1475, loss is 0.0036507032345980406\n",
      "epoch: 8 step: 1476, loss is 0.011974361725151539\n",
      "epoch: 8 step: 1477, loss is 0.017832480370998383\n",
      "epoch: 8 step: 1478, loss is 0.00018506977357901633\n",
      "epoch: 8 step: 1479, loss is 0.0005042906850576401\n",
      "epoch: 8 step: 1480, loss is 0.11919675022363663\n",
      "epoch: 8 step: 1481, loss is 0.001753165852278471\n",
      "epoch: 8 step: 1482, loss is 0.0005708136013709009\n",
      "epoch: 8 step: 1483, loss is 0.0006504561752080917\n",
      "epoch: 8 step: 1484, loss is 0.020532522350549698\n",
      "epoch: 8 step: 1485, loss is 0.11234398186206818\n",
      "epoch: 8 step: 1486, loss is 0.017474789172410965\n",
      "epoch: 8 step: 1487, loss is 0.007699731271713972\n",
      "epoch: 8 step: 1488, loss is 0.00814493652433157\n",
      "epoch: 8 step: 1489, loss is 0.023041771724820137\n",
      "epoch: 8 step: 1490, loss is 0.14574195444583893\n",
      "epoch: 8 step: 1491, loss is 0.025811228901147842\n",
      "epoch: 8 step: 1492, loss is 0.008884520269930363\n",
      "epoch: 8 step: 1493, loss is 0.020137706771492958\n",
      "epoch: 8 step: 1494, loss is 0.009422698989510536\n",
      "epoch: 8 step: 1495, loss is 0.0006776157533749938\n",
      "epoch: 8 step: 1496, loss is 0.0033844050485640764\n",
      "epoch: 8 step: 1497, loss is 0.004795973189175129\n",
      "epoch: 8 step: 1498, loss is 0.00047488059499301016\n",
      "epoch: 8 step: 1499, loss is 4.838603490497917e-05\n",
      "epoch: 8 step: 1500, loss is 0.0027114334516227245\n",
      "epoch: 8 step: 1501, loss is 0.21718356013298035\n",
      "epoch: 8 step: 1502, loss is 0.01091672945767641\n",
      "epoch: 8 step: 1503, loss is 0.0004507660341914743\n",
      "epoch: 8 step: 1504, loss is 0.00023156509269028902\n",
      "epoch: 8 step: 1505, loss is 0.0002106451865984127\n",
      "epoch: 8 step: 1506, loss is 0.03766345977783203\n",
      "epoch: 8 step: 1507, loss is 0.0010405420325696468\n",
      "epoch: 8 step: 1508, loss is 0.00829006265848875\n",
      "epoch: 8 step: 1509, loss is 0.05165430158376694\n",
      "epoch: 8 step: 1510, loss is 8.059990068431944e-05\n",
      "epoch: 8 step: 1511, loss is 0.00831415131688118\n",
      "epoch: 8 step: 1512, loss is 0.019917558878660202\n",
      "epoch: 8 step: 1513, loss is 0.00014194249524734914\n",
      "epoch: 8 step: 1514, loss is 0.022215260192751884\n",
      "epoch: 8 step: 1515, loss is 0.006529789883643389\n",
      "epoch: 8 step: 1516, loss is 0.0047967624850571156\n",
      "epoch: 8 step: 1517, loss is 0.04429512470960617\n",
      "epoch: 8 step: 1518, loss is 0.08457621186971664\n",
      "epoch: 8 step: 1519, loss is 0.0017430632142350078\n",
      "epoch: 8 step: 1520, loss is 0.01154510211199522\n",
      "epoch: 8 step: 1521, loss is 0.0009579301695339382\n",
      "epoch: 8 step: 1522, loss is 0.00030706613324582577\n",
      "epoch: 8 step: 1523, loss is 0.008343273773789406\n",
      "epoch: 8 step: 1524, loss is 0.006995092611759901\n",
      "epoch: 8 step: 1525, loss is 0.00024386394943576306\n",
      "epoch: 8 step: 1526, loss is 0.004394961055368185\n",
      "epoch: 8 step: 1527, loss is 0.0014440780505537987\n",
      "epoch: 8 step: 1528, loss is 0.0008595159742981195\n",
      "epoch: 8 step: 1529, loss is 0.001942537957802415\n",
      "epoch: 8 step: 1530, loss is 4.827276279684156e-05\n",
      "epoch: 8 step: 1531, loss is 0.05129224434494972\n",
      "epoch: 8 step: 1532, loss is 0.23201552033424377\n",
      "epoch: 8 step: 1533, loss is 0.00012296783097553998\n",
      "epoch: 8 step: 1534, loss is 0.05419456213712692\n",
      "epoch: 8 step: 1535, loss is 0.008193275891244411\n",
      "epoch: 8 step: 1536, loss is 0.007507903967052698\n",
      "epoch: 8 step: 1537, loss is 0.00010303088492946699\n",
      "epoch: 8 step: 1538, loss is 0.0010410526301711798\n",
      "epoch: 8 step: 1539, loss is 0.030912797898054123\n",
      "epoch: 8 step: 1540, loss is 0.00039304065285250545\n",
      "epoch: 8 step: 1541, loss is 0.0012694451725110412\n",
      "epoch: 8 step: 1542, loss is 0.003504612483084202\n",
      "epoch: 8 step: 1543, loss is 0.0010000894544646144\n",
      "epoch: 8 step: 1544, loss is 0.0002075264201266691\n",
      "epoch: 8 step: 1545, loss is 0.0006290751625783741\n",
      "epoch: 8 step: 1546, loss is 0.0013581681996583939\n",
      "epoch: 8 step: 1547, loss is 0.0007136725471355021\n",
      "epoch: 8 step: 1548, loss is 0.0013387617655098438\n",
      "epoch: 8 step: 1549, loss is 0.0020586817990988493\n",
      "epoch: 8 step: 1550, loss is 0.02156546711921692\n",
      "epoch: 8 step: 1551, loss is 0.012718351557850838\n",
      "epoch: 8 step: 1552, loss is 0.0009213566081598401\n",
      "epoch: 8 step: 1553, loss is 0.11493619531393051\n",
      "epoch: 8 step: 1554, loss is 0.00727990735322237\n",
      "epoch: 8 step: 1555, loss is 0.013182519003748894\n",
      "epoch: 8 step: 1556, loss is 0.0012045446783304214\n",
      "epoch: 8 step: 1557, loss is 0.0008186664199456573\n",
      "epoch: 8 step: 1558, loss is 0.04509194940328598\n",
      "epoch: 8 step: 1559, loss is 0.00039590944652445614\n",
      "epoch: 8 step: 1560, loss is 0.0014874598709866405\n",
      "epoch: 8 step: 1561, loss is 0.00015180787886492908\n",
      "epoch: 8 step: 1562, loss is 0.0028402383904904127\n",
      "epoch: 8 step: 1563, loss is 0.004038161598145962\n",
      "epoch: 8 step: 1564, loss is 0.0004091101582162082\n",
      "epoch: 8 step: 1565, loss is 0.003879514755681157\n",
      "epoch: 8 step: 1566, loss is 0.03167576715350151\n",
      "epoch: 8 step: 1567, loss is 0.003912304062396288\n",
      "epoch: 8 step: 1568, loss is 2.9555698347394355e-05\n",
      "epoch: 8 step: 1569, loss is 0.012128385715186596\n",
      "epoch: 8 step: 1570, loss is 0.009134980849921703\n",
      "epoch: 8 step: 1571, loss is 0.00019981942023150623\n",
      "epoch: 8 step: 1572, loss is 0.00015201779024209827\n",
      "epoch: 8 step: 1573, loss is 0.012753601185977459\n",
      "epoch: 8 step: 1574, loss is 0.0010862296912819147\n",
      "epoch: 8 step: 1575, loss is 0.00015531368262600154\n",
      "epoch: 8 step: 1576, loss is 0.0020627633202821016\n",
      "epoch: 8 step: 1577, loss is 0.0025723124854266644\n",
      "epoch: 8 step: 1578, loss is 0.036604143679142\n",
      "epoch: 8 step: 1579, loss is 0.10608081519603729\n",
      "epoch: 8 step: 1580, loss is 0.0011494455393403769\n",
      "epoch: 8 step: 1581, loss is 0.04595279321074486\n",
      "epoch: 8 step: 1582, loss is 0.00030946984770707786\n",
      "epoch: 8 step: 1583, loss is 0.0010351003147661686\n",
      "epoch: 8 step: 1584, loss is 0.002252561040222645\n",
      "epoch: 8 step: 1585, loss is 0.002583425724878907\n",
      "epoch: 8 step: 1586, loss is 0.0002750125713646412\n",
      "epoch: 8 step: 1587, loss is 0.003840462537482381\n",
      "epoch: 8 step: 1588, loss is 0.0793919786810875\n",
      "epoch: 8 step: 1589, loss is 0.009368611499667168\n",
      "epoch: 8 step: 1590, loss is 0.000727700418792665\n",
      "epoch: 8 step: 1591, loss is 0.0003331097250338644\n",
      "epoch: 8 step: 1592, loss is 0.001473711570724845\n",
      "epoch: 8 step: 1593, loss is 0.00011112415086245164\n",
      "epoch: 8 step: 1594, loss is 0.0015555447898805141\n",
      "epoch: 8 step: 1595, loss is 0.14681905508041382\n",
      "epoch: 8 step: 1596, loss is 0.0021491521038115025\n",
      "epoch: 8 step: 1597, loss is 0.17105883359909058\n",
      "epoch: 8 step: 1598, loss is 0.005330739077180624\n",
      "epoch: 8 step: 1599, loss is 0.005026433616876602\n",
      "epoch: 8 step: 1600, loss is 0.0030061332508921623\n",
      "epoch: 8 step: 1601, loss is 0.044030677527189255\n",
      "epoch: 8 step: 1602, loss is 0.000814022496342659\n",
      "epoch: 8 step: 1603, loss is 0.0016138798091560602\n",
      "epoch: 8 step: 1604, loss is 0.04042601212859154\n",
      "epoch: 8 step: 1605, loss is 0.0001601773692527786\n",
      "epoch: 8 step: 1606, loss is 0.00023947368026711047\n",
      "epoch: 8 step: 1607, loss is 0.028352823108434677\n",
      "epoch: 8 step: 1608, loss is 0.010079137980937958\n",
      "epoch: 8 step: 1609, loss is 0.00010039078915724531\n",
      "epoch: 8 step: 1610, loss is 0.00042120632133446634\n",
      "epoch: 8 step: 1611, loss is 0.12110727280378342\n",
      "epoch: 8 step: 1612, loss is 0.017116384580731392\n",
      "epoch: 8 step: 1613, loss is 0.10533895343542099\n",
      "epoch: 8 step: 1614, loss is 0.0003742720000445843\n",
      "epoch: 8 step: 1615, loss is 0.000682175625115633\n",
      "epoch: 8 step: 1616, loss is 0.003467871341854334\n",
      "epoch: 8 step: 1617, loss is 0.0005727670504711568\n",
      "epoch: 8 step: 1618, loss is 0.00019035098375752568\n",
      "epoch: 8 step: 1619, loss is 0.10935818403959274\n",
      "epoch: 8 step: 1620, loss is 0.00016800813318695873\n",
      "epoch: 8 step: 1621, loss is 0.0021458647679537535\n",
      "epoch: 8 step: 1622, loss is 0.0003643397940322757\n",
      "epoch: 8 step: 1623, loss is 0.01172186341136694\n",
      "epoch: 8 step: 1624, loss is 0.00037730869371443987\n",
      "epoch: 8 step: 1625, loss is 0.0003517263103276491\n",
      "epoch: 8 step: 1626, loss is 0.012105814181268215\n",
      "epoch: 8 step: 1627, loss is 0.003158875973895192\n",
      "epoch: 8 step: 1628, loss is 0.0048563843593001366\n",
      "epoch: 8 step: 1629, loss is 0.0011446119751781225\n",
      "epoch: 8 step: 1630, loss is 0.002783158328384161\n",
      "epoch: 8 step: 1631, loss is 0.0034381290897727013\n",
      "epoch: 8 step: 1632, loss is 0.00027289692661724985\n",
      "epoch: 8 step: 1633, loss is 0.0009157044114544988\n",
      "epoch: 8 step: 1634, loss is 0.01773969829082489\n",
      "epoch: 8 step: 1635, loss is 0.0004489802522584796\n",
      "epoch: 8 step: 1636, loss is 0.006404602900147438\n",
      "epoch: 8 step: 1637, loss is 0.00581200560554862\n",
      "epoch: 8 step: 1638, loss is 0.008355279453098774\n",
      "epoch: 8 step: 1639, loss is 0.00865849293768406\n",
      "epoch: 8 step: 1640, loss is 0.000161278760060668\n",
      "epoch: 8 step: 1641, loss is 0.029782233759760857\n",
      "epoch: 8 step: 1642, loss is 0.03114284574985504\n",
      "epoch: 8 step: 1643, loss is 0.0012284134281799197\n",
      "epoch: 8 step: 1644, loss is 0.010551794432103634\n",
      "epoch: 8 step: 1645, loss is 0.0007736007682979107\n",
      "epoch: 8 step: 1646, loss is 0.08889039605855942\n",
      "epoch: 8 step: 1647, loss is 0.0005286866798996925\n",
      "epoch: 8 step: 1648, loss is 0.012492064386606216\n",
      "epoch: 8 step: 1649, loss is 0.446072518825531\n",
      "epoch: 8 step: 1650, loss is 0.012287362478673458\n",
      "epoch: 8 step: 1651, loss is 0.009374475106596947\n",
      "epoch: 8 step: 1652, loss is 0.002984170336276293\n",
      "epoch: 8 step: 1653, loss is 0.01463851798325777\n",
      "epoch: 8 step: 1654, loss is 0.0049962145276367664\n",
      "epoch: 8 step: 1655, loss is 0.0003945825737901032\n",
      "epoch: 8 step: 1656, loss is 0.000391277571907267\n",
      "epoch: 8 step: 1657, loss is 0.0009542533662170172\n",
      "epoch: 8 step: 1658, loss is 0.0015750015154480934\n",
      "epoch: 8 step: 1659, loss is 0.00041145386057905853\n",
      "epoch: 8 step: 1660, loss is 0.0036324714310467243\n",
      "epoch: 8 step: 1661, loss is 0.018102191388607025\n",
      "epoch: 8 step: 1662, loss is 0.10391130298376083\n",
      "epoch: 8 step: 1663, loss is 0.01087966375052929\n",
      "epoch: 8 step: 1664, loss is 0.036478739231824875\n",
      "epoch: 8 step: 1665, loss is 0.00510013522580266\n",
      "epoch: 8 step: 1666, loss is 0.029338086023926735\n",
      "epoch: 8 step: 1667, loss is 0.013830709271132946\n",
      "epoch: 8 step: 1668, loss is 0.0002738897455856204\n",
      "epoch: 8 step: 1669, loss is 0.007396991364657879\n",
      "epoch: 8 step: 1670, loss is 0.00023205840261653066\n",
      "epoch: 8 step: 1671, loss is 0.00038577665691263974\n",
      "epoch: 8 step: 1672, loss is 0.0992983877658844\n",
      "epoch: 8 step: 1673, loss is 0.00037267038715071976\n",
      "epoch: 8 step: 1674, loss is 0.00017635068797972053\n",
      "epoch: 8 step: 1675, loss is 0.01025440264493227\n",
      "epoch: 8 step: 1676, loss is 0.0007222330896183848\n",
      "epoch: 8 step: 1677, loss is 0.04787999019026756\n",
      "epoch: 8 step: 1678, loss is 0.015069209970533848\n",
      "epoch: 8 step: 1679, loss is 0.005907171871513128\n",
      "epoch: 8 step: 1680, loss is 0.00018104385526385158\n",
      "epoch: 8 step: 1681, loss is 0.023011738434433937\n",
      "epoch: 8 step: 1682, loss is 0.0021007093600928783\n",
      "epoch: 8 step: 1683, loss is 0.001034952001646161\n",
      "epoch: 8 step: 1684, loss is 0.003126846393570304\n",
      "epoch: 8 step: 1685, loss is 0.018161088228225708\n",
      "epoch: 8 step: 1686, loss is 0.00024675813619978726\n",
      "epoch: 8 step: 1687, loss is 0.012877888977527618\n",
      "epoch: 8 step: 1688, loss is 0.00043049806845374405\n",
      "epoch: 8 step: 1689, loss is 0.00044318230357021093\n",
      "epoch: 8 step: 1690, loss is 0.0009386264719069004\n",
      "epoch: 8 step: 1691, loss is 0.0007493213634006679\n",
      "epoch: 8 step: 1692, loss is 0.005333079025149345\n",
      "epoch: 8 step: 1693, loss is 0.00016471560229547322\n",
      "epoch: 8 step: 1694, loss is 0.006357395555824041\n",
      "epoch: 8 step: 1695, loss is 0.002084531122818589\n",
      "epoch: 8 step: 1696, loss is 0.0013223984278738499\n",
      "epoch: 8 step: 1697, loss is 0.0023110415786504745\n",
      "epoch: 8 step: 1698, loss is 0.007435083854943514\n",
      "epoch: 8 step: 1699, loss is 0.019054049625992775\n",
      "epoch: 8 step: 1700, loss is 0.0019626200664788485\n",
      "epoch: 8 step: 1701, loss is 0.05758677423000336\n",
      "epoch: 8 step: 1702, loss is 0.000716561742592603\n",
      "epoch: 8 step: 1703, loss is 0.10175412893295288\n",
      "epoch: 8 step: 1704, loss is 0.028060274198651314\n",
      "epoch: 8 step: 1705, loss is 0.008034778758883476\n",
      "epoch: 8 step: 1706, loss is 0.005078628659248352\n",
      "epoch: 8 step: 1707, loss is 0.008457692340016365\n",
      "epoch: 8 step: 1708, loss is 0.09249921888113022\n",
      "epoch: 8 step: 1709, loss is 0.013344552367925644\n",
      "epoch: 8 step: 1710, loss is 2.3579415938002057e-05\n",
      "epoch: 8 step: 1711, loss is 0.002482171868905425\n",
      "epoch: 8 step: 1712, loss is 0.002078023971989751\n",
      "epoch: 8 step: 1713, loss is 0.0030414978973567486\n",
      "epoch: 8 step: 1714, loss is 0.004319814033806324\n",
      "epoch: 8 step: 1715, loss is 0.047444138675928116\n",
      "epoch: 8 step: 1716, loss is 0.06970579922199249\n",
      "epoch: 8 step: 1717, loss is 0.008296612650156021\n",
      "epoch: 8 step: 1718, loss is 0.07994276285171509\n",
      "epoch: 8 step: 1719, loss is 0.0017662510508671403\n",
      "epoch: 8 step: 1720, loss is 0.007189807947725058\n",
      "epoch: 8 step: 1721, loss is 0.030903533101081848\n",
      "epoch: 8 step: 1722, loss is 0.019148318096995354\n",
      "epoch: 8 step: 1723, loss is 0.0007116867345757782\n",
      "epoch: 8 step: 1724, loss is 0.01715931110084057\n",
      "epoch: 8 step: 1725, loss is 0.00035709020448848605\n",
      "epoch: 8 step: 1726, loss is 0.06362177431583405\n",
      "epoch: 8 step: 1727, loss is 0.044809598475694656\n",
      "epoch: 8 step: 1728, loss is 0.006433574948459864\n",
      "epoch: 8 step: 1729, loss is 0.0003379691916052252\n",
      "epoch: 8 step: 1730, loss is 0.0052672261372208595\n",
      "epoch: 8 step: 1731, loss is 0.0003377450630068779\n",
      "epoch: 8 step: 1732, loss is 0.014918328262865543\n",
      "epoch: 8 step: 1733, loss is 0.013060560449957848\n",
      "epoch: 8 step: 1734, loss is 0.00010427213419461623\n",
      "epoch: 8 step: 1735, loss is 0.04395752027630806\n",
      "epoch: 8 step: 1736, loss is 0.02373645454645157\n",
      "epoch: 8 step: 1737, loss is 0.013928557746112347\n",
      "epoch: 8 step: 1738, loss is 0.001237169955857098\n",
      "epoch: 8 step: 1739, loss is 0.005307226907461882\n",
      "epoch: 8 step: 1740, loss is 0.0030231638811528683\n",
      "epoch: 8 step: 1741, loss is 0.022777296602725983\n",
      "epoch: 8 step: 1742, loss is 0.0011529275216162205\n",
      "epoch: 8 step: 1743, loss is 0.016092371195554733\n",
      "epoch: 8 step: 1744, loss is 0.007793977856636047\n",
      "epoch: 8 step: 1745, loss is 0.018725605681538582\n",
      "epoch: 8 step: 1746, loss is 0.00020198906713631004\n",
      "epoch: 8 step: 1747, loss is 0.00010380912135588005\n",
      "epoch: 8 step: 1748, loss is 0.001025992794893682\n",
      "epoch: 8 step: 1749, loss is 5.976406464469619e-05\n",
      "epoch: 8 step: 1750, loss is 0.014949188567698002\n",
      "epoch: 8 step: 1751, loss is 0.0003886973427142948\n",
      "epoch: 8 step: 1752, loss is 0.005229964852333069\n",
      "epoch: 8 step: 1753, loss is 0.03764718025922775\n",
      "epoch: 8 step: 1754, loss is 0.0010885462397709489\n",
      "epoch: 8 step: 1755, loss is 0.0012867647456005216\n",
      "epoch: 8 step: 1756, loss is 0.004590130411088467\n",
      "epoch: 8 step: 1757, loss is 0.0053193289786577225\n",
      "epoch: 8 step: 1758, loss is 0.0006596406456083059\n",
      "epoch: 8 step: 1759, loss is 6.0983889852650464e-05\n",
      "epoch: 8 step: 1760, loss is 0.00038555890205316246\n",
      "epoch: 8 step: 1761, loss is 0.002917328616604209\n",
      "epoch: 8 step: 1762, loss is 0.00028803394525311887\n",
      "epoch: 8 step: 1763, loss is 0.00018319988157600164\n",
      "epoch: 8 step: 1764, loss is 0.002182880649343133\n",
      "epoch: 8 step: 1765, loss is 0.00930092390626669\n",
      "epoch: 8 step: 1766, loss is 0.015025367960333824\n",
      "epoch: 8 step: 1767, loss is 0.00011441570677561685\n",
      "epoch: 8 step: 1768, loss is 0.000277933751931414\n",
      "epoch: 8 step: 1769, loss is 0.0007477580802515149\n",
      "epoch: 8 step: 1770, loss is 0.0004478104237932712\n",
      "epoch: 8 step: 1771, loss is 0.045506447553634644\n",
      "epoch: 8 step: 1772, loss is 0.0038248426280915737\n",
      "epoch: 8 step: 1773, loss is 0.0001313139364356175\n",
      "epoch: 8 step: 1774, loss is 4.894613812211901e-05\n",
      "epoch: 8 step: 1775, loss is 0.024242039769887924\n",
      "epoch: 8 step: 1776, loss is 0.02137850597500801\n",
      "epoch: 8 step: 1777, loss is 0.0010205470025539398\n",
      "epoch: 8 step: 1778, loss is 0.07580116391181946\n",
      "epoch: 8 step: 1779, loss is 0.04875737801194191\n",
      "epoch: 8 step: 1780, loss is 0.00019952768343500793\n",
      "epoch: 8 step: 1781, loss is 0.013723703101277351\n",
      "epoch: 8 step: 1782, loss is 0.0013729602796956897\n",
      "epoch: 8 step: 1783, loss is 0.10581069439649582\n",
      "epoch: 8 step: 1784, loss is 0.0007123159011825919\n",
      "epoch: 8 step: 1785, loss is 0.07438988983631134\n",
      "epoch: 8 step: 1786, loss is 0.004935413133352995\n",
      "epoch: 8 step: 1787, loss is 0.04709756001830101\n",
      "epoch: 8 step: 1788, loss is 0.10188556462526321\n",
      "epoch: 8 step: 1789, loss is 0.0004382547049317509\n",
      "epoch: 8 step: 1790, loss is 0.0012662538792937994\n",
      "epoch: 8 step: 1791, loss is 0.016602346673607826\n",
      "epoch: 8 step: 1792, loss is 0.009580818936228752\n",
      "epoch: 8 step: 1793, loss is 0.09894432127475739\n",
      "epoch: 8 step: 1794, loss is 0.0021832482889294624\n",
      "epoch: 8 step: 1795, loss is 0.0004427122767083347\n",
      "epoch: 8 step: 1796, loss is 0.0012130432296544313\n",
      "epoch: 8 step: 1797, loss is 0.07076119631528854\n",
      "epoch: 8 step: 1798, loss is 0.03891262039542198\n",
      "epoch: 8 step: 1799, loss is 0.0009503675391897559\n",
      "epoch: 8 step: 1800, loss is 0.0006638520862907171\n",
      "epoch: 8 step: 1801, loss is 0.02113773114979267\n",
      "epoch: 8 step: 1802, loss is 0.14604374766349792\n",
      "epoch: 8 step: 1803, loss is 0.0760214701294899\n",
      "epoch: 8 step: 1804, loss is 0.00024606502847746015\n",
      "epoch: 8 step: 1805, loss is 0.007938162423670292\n",
      "epoch: 8 step: 1806, loss is 0.09381604194641113\n",
      "epoch: 8 step: 1807, loss is 4.643931606551632e-05\n",
      "epoch: 8 step: 1808, loss is 0.008938844315707684\n",
      "epoch: 8 step: 1809, loss is 0.002902089385315776\n",
      "epoch: 8 step: 1810, loss is 0.006456615868955851\n",
      "epoch: 8 step: 1811, loss is 0.006355248857289553\n",
      "epoch: 8 step: 1812, loss is 0.0003352616331540048\n",
      "epoch: 8 step: 1813, loss is 0.04442691057920456\n",
      "epoch: 8 step: 1814, loss is 0.005521690938621759\n",
      "epoch: 8 step: 1815, loss is 0.05476536229252815\n",
      "epoch: 8 step: 1816, loss is 0.00012947649520356208\n",
      "epoch: 8 step: 1817, loss is 0.0026057218201458454\n",
      "epoch: 8 step: 1818, loss is 0.03940153121948242\n",
      "epoch: 8 step: 1819, loss is 0.0182158462703228\n",
      "epoch: 8 step: 1820, loss is 0.0011390455765649676\n",
      "epoch: 8 step: 1821, loss is 0.07073841243982315\n",
      "epoch: 8 step: 1822, loss is 0.00045731046702712774\n",
      "epoch: 8 step: 1823, loss is 0.02733447030186653\n",
      "epoch: 8 step: 1824, loss is 0.006292436271905899\n",
      "epoch: 8 step: 1825, loss is 0.012354528531432152\n",
      "epoch: 8 step: 1826, loss is 0.0862126350402832\n",
      "epoch: 8 step: 1827, loss is 0.0003800385456997901\n",
      "epoch: 8 step: 1828, loss is 0.0017217781860381365\n",
      "epoch: 8 step: 1829, loss is 0.003089104313403368\n",
      "epoch: 8 step: 1830, loss is 7.508505223086104e-05\n",
      "epoch: 8 step: 1831, loss is 0.07749325782060623\n",
      "epoch: 8 step: 1832, loss is 0.014189585112035275\n",
      "epoch: 8 step: 1833, loss is 0.00228110165335238\n",
      "epoch: 8 step: 1834, loss is 0.0464530773460865\n",
      "epoch: 8 step: 1835, loss is 0.008381991647183895\n",
      "epoch: 8 step: 1836, loss is 1.4231216482585296e-05\n",
      "epoch: 8 step: 1837, loss is 0.0010666484013199806\n",
      "epoch: 8 step: 1838, loss is 0.0068370928056538105\n",
      "epoch: 8 step: 1839, loss is 0.03153946250677109\n",
      "epoch: 8 step: 1840, loss is 0.002628846326842904\n",
      "epoch: 8 step: 1841, loss is 0.0011048648739233613\n",
      "epoch: 8 step: 1842, loss is 0.01242330763489008\n",
      "epoch: 8 step: 1843, loss is 0.10142964124679565\n",
      "epoch: 8 step: 1844, loss is 0.05646710470318794\n",
      "epoch: 8 step: 1845, loss is 0.0010859568137675524\n",
      "epoch: 8 step: 1846, loss is 0.0041752103716135025\n",
      "epoch: 8 step: 1847, loss is 0.29260194301605225\n",
      "epoch: 8 step: 1848, loss is 0.11716869473457336\n",
      "epoch: 8 step: 1849, loss is 0.003104767296463251\n",
      "epoch: 8 step: 1850, loss is 1.6159494407474995e-05\n",
      "epoch: 8 step: 1851, loss is 0.0062838708981871605\n",
      "epoch: 8 step: 1852, loss is 0.0013641944387927651\n",
      "epoch: 8 step: 1853, loss is 0.014912860468029976\n",
      "epoch: 8 step: 1854, loss is 5.671717008226551e-05\n",
      "epoch: 8 step: 1855, loss is 0.006453624460846186\n",
      "epoch: 8 step: 1856, loss is 0.0007999176741577685\n",
      "epoch: 8 step: 1857, loss is 0.013578803278505802\n",
      "epoch: 8 step: 1858, loss is 0.0006673677708022296\n",
      "epoch: 8 step: 1859, loss is 0.0008895131759345531\n",
      "epoch: 8 step: 1860, loss is 0.0027922671288251877\n",
      "epoch: 8 step: 1861, loss is 0.01406907755881548\n",
      "epoch: 8 step: 1862, loss is 0.0013976666377857327\n",
      "epoch: 8 step: 1863, loss is 0.043121110647916794\n",
      "epoch: 8 step: 1864, loss is 0.03591911122202873\n",
      "epoch: 8 step: 1865, loss is 0.047744251787662506\n",
      "epoch: 8 step: 1866, loss is 0.07566793262958527\n",
      "epoch: 8 step: 1867, loss is 0.00022060662740841508\n",
      "epoch: 8 step: 1868, loss is 0.005734700709581375\n",
      "epoch: 8 step: 1869, loss is 0.0037393991369754076\n",
      "epoch: 8 step: 1870, loss is 0.021757960319519043\n",
      "epoch: 8 step: 1871, loss is 9.144925570581108e-05\n",
      "epoch: 8 step: 1872, loss is 0.0009537233272567391\n",
      "epoch: 8 step: 1873, loss is 0.0005449667805805802\n",
      "epoch: 8 step: 1874, loss is 0.0005643168115057051\n",
      "epoch: 8 step: 1875, loss is 0.039236925542354584\n",
      "epoch: 9 step: 1, loss is 0.0014067193260416389\n",
      "epoch: 9 step: 2, loss is 0.16091197729110718\n",
      "epoch: 9 step: 3, loss is 0.00040601653745397925\n",
      "epoch: 9 step: 4, loss is 4.583671397995204e-05\n",
      "epoch: 9 step: 5, loss is 0.010522299446165562\n",
      "epoch: 9 step: 6, loss is 0.0016766859916970134\n",
      "epoch: 9 step: 7, loss is 0.00045846248394809663\n",
      "epoch: 9 step: 8, loss is 0.017836691811680794\n",
      "epoch: 9 step: 9, loss is 0.001291009597480297\n",
      "epoch: 9 step: 10, loss is 0.00024974174448288977\n",
      "epoch: 9 step: 11, loss is 0.0008476539514958858\n",
      "epoch: 9 step: 12, loss is 0.0245372261852026\n",
      "epoch: 9 step: 13, loss is 0.0010679723927751184\n",
      "epoch: 9 step: 14, loss is 0.00015565658395644277\n",
      "epoch: 9 step: 15, loss is 0.00788139272481203\n",
      "epoch: 9 step: 16, loss is 0.10165932774543762\n",
      "epoch: 9 step: 17, loss is 0.05633841082453728\n",
      "epoch: 9 step: 18, loss is 0.020863380283117294\n",
      "epoch: 9 step: 19, loss is 0.035184796899557114\n",
      "epoch: 9 step: 20, loss is 5.1296272431500256e-05\n",
      "epoch: 9 step: 21, loss is 0.0007283882005140185\n",
      "epoch: 9 step: 22, loss is 0.00980545673519373\n",
      "epoch: 9 step: 23, loss is 0.00010532858868828043\n",
      "epoch: 9 step: 24, loss is 0.37002384662628174\n",
      "epoch: 9 step: 25, loss is 0.008306337520480156\n",
      "epoch: 9 step: 26, loss is 0.0003310876782052219\n",
      "epoch: 9 step: 27, loss is 0.0011870074085891247\n",
      "epoch: 9 step: 28, loss is 0.0030741477385163307\n",
      "epoch: 9 step: 29, loss is 0.16078366339206696\n",
      "epoch: 9 step: 30, loss is 0.004985431209206581\n",
      "epoch: 9 step: 31, loss is 0.00014664189075119793\n",
      "epoch: 9 step: 32, loss is 0.2032107710838318\n",
      "epoch: 9 step: 33, loss is 0.03016706183552742\n",
      "epoch: 9 step: 34, loss is 0.005493870470672846\n",
      "epoch: 9 step: 35, loss is 0.0016542229568585753\n",
      "epoch: 9 step: 36, loss is 0.0015438590198755264\n",
      "epoch: 9 step: 37, loss is 0.0003067040233872831\n",
      "epoch: 9 step: 38, loss is 0.003806586377322674\n",
      "epoch: 9 step: 39, loss is 0.0001960248191608116\n",
      "epoch: 9 step: 40, loss is 0.003043306525796652\n",
      "epoch: 9 step: 41, loss is 0.0011825243709608912\n",
      "epoch: 9 step: 42, loss is 0.12265393882989883\n",
      "epoch: 9 step: 43, loss is 0.0614977590739727\n",
      "epoch: 9 step: 44, loss is 0.0028609693981707096\n",
      "epoch: 9 step: 45, loss is 0.02471931278705597\n",
      "epoch: 9 step: 46, loss is 0.019503848627209663\n",
      "epoch: 9 step: 47, loss is 0.00028429305530153215\n",
      "epoch: 9 step: 48, loss is 0.0005142600275576115\n",
      "epoch: 9 step: 49, loss is 0.0006208650884218514\n",
      "epoch: 9 step: 50, loss is 0.0025735297240316868\n",
      "epoch: 9 step: 51, loss is 0.0001359093439532444\n",
      "epoch: 9 step: 52, loss is 0.0006125865620560944\n",
      "epoch: 9 step: 53, loss is 0.10395000874996185\n",
      "epoch: 9 step: 54, loss is 0.00582975335419178\n",
      "epoch: 9 step: 55, loss is 0.05289977043867111\n",
      "epoch: 9 step: 56, loss is 0.03095145896077156\n",
      "epoch: 9 step: 57, loss is 0.01890324242413044\n",
      "epoch: 9 step: 58, loss is 0.0019878854509443045\n",
      "epoch: 9 step: 59, loss is 0.004133876413106918\n",
      "epoch: 9 step: 60, loss is 0.007677105255424976\n",
      "epoch: 9 step: 61, loss is 0.005654681473970413\n",
      "epoch: 9 step: 62, loss is 0.00016698779654689133\n",
      "epoch: 9 step: 63, loss is 0.00033120549051091075\n",
      "epoch: 9 step: 64, loss is 0.0038384287618100643\n",
      "epoch: 9 step: 65, loss is 0.03251760080456734\n",
      "epoch: 9 step: 66, loss is 0.00016242664423771203\n",
      "epoch: 9 step: 67, loss is 0.005171434488147497\n",
      "epoch: 9 step: 68, loss is 0.002470623701810837\n",
      "epoch: 9 step: 69, loss is 0.006028059404343367\n",
      "epoch: 9 step: 70, loss is 0.0012947370996698737\n",
      "epoch: 9 step: 71, loss is 0.07092943042516708\n",
      "epoch: 9 step: 72, loss is 0.0016554483445361257\n",
      "epoch: 9 step: 73, loss is 2.7974918339168653e-05\n",
      "epoch: 9 step: 74, loss is 0.0002809857833199203\n",
      "epoch: 9 step: 75, loss is 0.005498338025063276\n",
      "epoch: 9 step: 76, loss is 0.01963096857070923\n",
      "epoch: 9 step: 77, loss is 0.003559334669262171\n",
      "epoch: 9 step: 78, loss is 0.05400478467345238\n",
      "epoch: 9 step: 79, loss is 0.033661793917417526\n",
      "epoch: 9 step: 80, loss is 0.015130894258618355\n",
      "epoch: 9 step: 81, loss is 0.001843679929152131\n",
      "epoch: 9 step: 82, loss is 0.0006076544523239136\n",
      "epoch: 9 step: 83, loss is 0.0006152750574983656\n",
      "epoch: 9 step: 84, loss is 0.00940486416220665\n",
      "epoch: 9 step: 85, loss is 0.00019556510960683227\n",
      "epoch: 9 step: 86, loss is 0.0703936368227005\n",
      "epoch: 9 step: 87, loss is 0.00022011612600181252\n",
      "epoch: 9 step: 88, loss is 0.00030888980836607516\n",
      "epoch: 9 step: 89, loss is 0.000807648291811347\n",
      "epoch: 9 step: 90, loss is 0.0023054187186062336\n",
      "epoch: 9 step: 91, loss is 0.09904609620571136\n",
      "epoch: 9 step: 92, loss is 0.011994690634310246\n",
      "epoch: 9 step: 93, loss is 0.0001561660028528422\n",
      "epoch: 9 step: 94, loss is 0.00032285493216477334\n",
      "epoch: 9 step: 95, loss is 0.0001512570888735354\n",
      "epoch: 9 step: 96, loss is 0.00023967472952790558\n",
      "epoch: 9 step: 97, loss is 0.001787582878023386\n",
      "epoch: 9 step: 98, loss is 0.0010221677366644144\n",
      "epoch: 9 step: 99, loss is 0.002838738029822707\n",
      "epoch: 9 step: 100, loss is 0.0022031781263649464\n",
      "epoch: 9 step: 101, loss is 0.00037575812893919647\n",
      "epoch: 9 step: 102, loss is 0.005679973866790533\n",
      "epoch: 9 step: 103, loss is 0.0012551118852570653\n",
      "epoch: 9 step: 104, loss is 0.0001286368496948853\n",
      "epoch: 9 step: 105, loss is 0.07807275652885437\n",
      "epoch: 9 step: 106, loss is 0.01776128076016903\n",
      "epoch: 9 step: 107, loss is 0.0003101229958701879\n",
      "epoch: 9 step: 108, loss is 0.0002892597985919565\n",
      "epoch: 9 step: 109, loss is 0.05256984382867813\n",
      "epoch: 9 step: 110, loss is 0.03462355211377144\n",
      "epoch: 9 step: 111, loss is 0.06876721233129501\n",
      "epoch: 9 step: 112, loss is 0.045171935111284256\n",
      "epoch: 9 step: 113, loss is 0.0012587312376126647\n",
      "epoch: 9 step: 114, loss is 0.013435887172818184\n",
      "epoch: 9 step: 115, loss is 0.0008053234196268022\n",
      "epoch: 9 step: 116, loss is 0.01106170378625393\n",
      "epoch: 9 step: 117, loss is 0.006538797169923782\n",
      "epoch: 9 step: 118, loss is 0.017339251935482025\n",
      "epoch: 9 step: 119, loss is 0.0001559921947773546\n",
      "epoch: 9 step: 120, loss is 0.0003497484722174704\n",
      "epoch: 9 step: 121, loss is 0.05163434520363808\n",
      "epoch: 9 step: 122, loss is 0.00022030677064321935\n",
      "epoch: 9 step: 123, loss is 0.0068584103137254715\n",
      "epoch: 9 step: 124, loss is 0.010849188081920147\n",
      "epoch: 9 step: 125, loss is 0.0006617274484597147\n",
      "epoch: 9 step: 126, loss is 0.0003334734064992517\n",
      "epoch: 9 step: 127, loss is 0.002808778081089258\n",
      "epoch: 9 step: 128, loss is 0.02669657953083515\n",
      "epoch: 9 step: 129, loss is 0.005924994125962257\n",
      "epoch: 9 step: 130, loss is 0.0011879216181114316\n",
      "epoch: 9 step: 131, loss is 0.008768264204263687\n",
      "epoch: 9 step: 132, loss is 0.0006416864925995469\n",
      "epoch: 9 step: 133, loss is 0.008842875249683857\n",
      "epoch: 9 step: 134, loss is 0.00022301283024717122\n",
      "epoch: 9 step: 135, loss is 0.0004764756595250219\n",
      "epoch: 9 step: 136, loss is 0.019044773653149605\n",
      "epoch: 9 step: 137, loss is 0.013424774631857872\n",
      "epoch: 9 step: 138, loss is 0.0028561679646372795\n",
      "epoch: 9 step: 139, loss is 0.0036772144958376884\n",
      "epoch: 9 step: 140, loss is 0.00011654115951387212\n",
      "epoch: 9 step: 141, loss is 0.00041218148544430733\n",
      "epoch: 9 step: 142, loss is 0.00011624081525951624\n",
      "epoch: 9 step: 143, loss is 0.0009743052651174366\n",
      "epoch: 9 step: 144, loss is 0.020873894914984703\n",
      "epoch: 9 step: 145, loss is 0.012821405194699764\n",
      "epoch: 9 step: 146, loss is 0.00020949052122887224\n",
      "epoch: 9 step: 147, loss is 0.015238920226693153\n",
      "epoch: 9 step: 148, loss is 0.04153908044099808\n",
      "epoch: 9 step: 149, loss is 0.005287024192512035\n",
      "epoch: 9 step: 150, loss is 5.8873218222288415e-05\n",
      "epoch: 9 step: 151, loss is 0.0001577639632159844\n",
      "epoch: 9 step: 152, loss is 0.000716898706741631\n",
      "epoch: 9 step: 153, loss is 0.038738615810871124\n",
      "epoch: 9 step: 154, loss is 0.04102931544184685\n",
      "epoch: 9 step: 155, loss is 0.00570890586823225\n",
      "epoch: 9 step: 156, loss is 0.01181584969162941\n",
      "epoch: 9 step: 157, loss is 0.014088115654885769\n",
      "epoch: 9 step: 158, loss is 0.0005300079938024282\n",
      "epoch: 9 step: 159, loss is 0.002268339041620493\n",
      "epoch: 9 step: 160, loss is 0.0005200818995945156\n",
      "epoch: 9 step: 161, loss is 0.00026652825181372464\n",
      "epoch: 9 step: 162, loss is 1.629776852496434e-05\n",
      "epoch: 9 step: 163, loss is 0.0016085002571344376\n",
      "epoch: 9 step: 164, loss is 0.01858850196003914\n",
      "epoch: 9 step: 165, loss is 0.00018552746041677892\n",
      "epoch: 9 step: 166, loss is 0.00014949997421354055\n",
      "epoch: 9 step: 167, loss is 0.003102843649685383\n",
      "epoch: 9 step: 168, loss is 0.004955925978720188\n",
      "epoch: 9 step: 169, loss is 0.00822528637945652\n",
      "epoch: 9 step: 170, loss is 5.504002911038697e-05\n",
      "epoch: 9 step: 171, loss is 3.46061606251169e-05\n",
      "epoch: 9 step: 172, loss is 0.00043950413237325847\n",
      "epoch: 9 step: 173, loss is 0.0036760992370545864\n",
      "epoch: 9 step: 174, loss is 0.0009657582850195467\n",
      "epoch: 9 step: 175, loss is 0.0003661015070974827\n",
      "epoch: 9 step: 176, loss is 0.005938787944614887\n",
      "epoch: 9 step: 177, loss is 0.005016610957682133\n",
      "epoch: 9 step: 178, loss is 0.004297299776226282\n",
      "epoch: 9 step: 179, loss is 0.003001124830916524\n",
      "epoch: 9 step: 180, loss is 0.0013221215922385454\n",
      "epoch: 9 step: 181, loss is 0.007709960453212261\n",
      "epoch: 9 step: 182, loss is 0.00025239528622478247\n",
      "epoch: 9 step: 183, loss is 0.0053056711331009865\n",
      "epoch: 9 step: 184, loss is 8.709848043508828e-05\n",
      "epoch: 9 step: 185, loss is 0.13239896297454834\n",
      "epoch: 9 step: 186, loss is 0.008165603503584862\n",
      "epoch: 9 step: 187, loss is 0.0011896367650479078\n",
      "epoch: 9 step: 188, loss is 0.032930511981248856\n",
      "epoch: 9 step: 189, loss is 0.003583119949325919\n",
      "epoch: 9 step: 190, loss is 0.0001031056308420375\n",
      "epoch: 9 step: 191, loss is 0.0004498199268709868\n",
      "epoch: 9 step: 192, loss is 0.0002161616284865886\n",
      "epoch: 9 step: 193, loss is 0.09857039153575897\n",
      "epoch: 9 step: 194, loss is 2.7345178750692867e-05\n",
      "epoch: 9 step: 195, loss is 0.00042752877925522625\n",
      "epoch: 9 step: 196, loss is 0.00036343446117825806\n",
      "epoch: 9 step: 197, loss is 0.00010013172141043469\n",
      "epoch: 9 step: 198, loss is 0.00036082224687561393\n",
      "epoch: 9 step: 199, loss is 0.02992558479309082\n",
      "epoch: 9 step: 200, loss is 0.0007809383678250015\n",
      "epoch: 9 step: 201, loss is 0.008872779086232185\n",
      "epoch: 9 step: 202, loss is 0.0031538766343146563\n",
      "epoch: 9 step: 203, loss is 0.0016532153822481632\n",
      "epoch: 9 step: 204, loss is 0.0060693020932376385\n",
      "epoch: 9 step: 205, loss is 0.0020625910256057978\n",
      "epoch: 9 step: 206, loss is 7.863491191528738e-05\n",
      "epoch: 9 step: 207, loss is 0.02477271296083927\n",
      "epoch: 9 step: 208, loss is 0.09870532155036926\n",
      "epoch: 9 step: 209, loss is 0.0009518603910692036\n",
      "epoch: 9 step: 210, loss is 0.018183885142207146\n",
      "epoch: 9 step: 211, loss is 0.0019919387996196747\n",
      "epoch: 9 step: 212, loss is 0.014813518151640892\n",
      "epoch: 9 step: 213, loss is 0.010326503776013851\n",
      "epoch: 9 step: 214, loss is 9.422760194865987e-05\n",
      "epoch: 9 step: 215, loss is 0.005374946631491184\n",
      "epoch: 9 step: 216, loss is 0.004531144164502621\n",
      "epoch: 9 step: 217, loss is 0.010619385167956352\n",
      "epoch: 9 step: 218, loss is 0.00026911101303994656\n",
      "epoch: 9 step: 219, loss is 0.00036254158476367593\n",
      "epoch: 9 step: 220, loss is 0.0917477160692215\n",
      "epoch: 9 step: 221, loss is 0.00017295761790592223\n",
      "epoch: 9 step: 222, loss is 0.0013071757275611162\n",
      "epoch: 9 step: 223, loss is 6.989084795350209e-05\n",
      "epoch: 9 step: 224, loss is 5.5721837270539254e-05\n",
      "epoch: 9 step: 225, loss is 0.001966718351468444\n",
      "epoch: 9 step: 226, loss is 0.00016564394172746688\n",
      "epoch: 9 step: 227, loss is 0.0036381667014211416\n",
      "epoch: 9 step: 228, loss is 0.0013616884825751185\n",
      "epoch: 9 step: 229, loss is 0.0021103734616190195\n",
      "epoch: 9 step: 230, loss is 0.04196849465370178\n",
      "epoch: 9 step: 231, loss is 0.010025067254900932\n",
      "epoch: 9 step: 232, loss is 0.0002715841692406684\n",
      "epoch: 9 step: 233, loss is 0.00046209091669879854\n",
      "epoch: 9 step: 234, loss is 0.0008214753470383584\n",
      "epoch: 9 step: 235, loss is 0.0010376207064837217\n",
      "epoch: 9 step: 236, loss is 0.0005116950487717986\n",
      "epoch: 9 step: 237, loss is 0.0006643402739427984\n",
      "epoch: 9 step: 238, loss is 4.178913150099106e-05\n",
      "epoch: 9 step: 239, loss is 0.0012872668448835611\n",
      "epoch: 9 step: 240, loss is 0.07373932749032974\n",
      "epoch: 9 step: 241, loss is 0.0032175814267247915\n",
      "epoch: 9 step: 242, loss is 5.420119850896299e-05\n",
      "epoch: 9 step: 243, loss is 0.0002507801400497556\n",
      "epoch: 9 step: 244, loss is 0.00045153399696573615\n",
      "epoch: 9 step: 245, loss is 0.0007222419953905046\n",
      "epoch: 9 step: 246, loss is 0.0005997829721309245\n",
      "epoch: 9 step: 247, loss is 0.00046801293501630425\n",
      "epoch: 9 step: 248, loss is 0.001234237221069634\n",
      "epoch: 9 step: 249, loss is 0.00043421576265245676\n",
      "epoch: 9 step: 250, loss is 0.005523170810192823\n",
      "epoch: 9 step: 251, loss is 1.5829862604732625e-05\n",
      "epoch: 9 step: 252, loss is 0.0011832966702058911\n",
      "epoch: 9 step: 253, loss is 0.0029583058785647154\n",
      "epoch: 9 step: 254, loss is 6.15251119597815e-05\n",
      "epoch: 9 step: 255, loss is 0.018718253821134567\n",
      "epoch: 9 step: 256, loss is 0.0002449447347316891\n",
      "epoch: 9 step: 257, loss is 0.003153806086629629\n",
      "epoch: 9 step: 258, loss is 8.825655095279217e-05\n",
      "epoch: 9 step: 259, loss is 8.896960207493976e-05\n",
      "epoch: 9 step: 260, loss is 0.00016891732229851186\n",
      "epoch: 9 step: 261, loss is 0.0007478099432773888\n",
      "epoch: 9 step: 262, loss is 4.903959052171558e-05\n",
      "epoch: 9 step: 263, loss is 0.0013876958983018994\n",
      "epoch: 9 step: 264, loss is 0.0006131763802841306\n",
      "epoch: 9 step: 265, loss is 0.001329916762188077\n",
      "epoch: 9 step: 266, loss is 0.009038339368999004\n",
      "epoch: 9 step: 267, loss is 0.0011343747610226274\n",
      "epoch: 9 step: 268, loss is 0.0001021510106511414\n",
      "epoch: 9 step: 269, loss is 0.0005249985260888934\n",
      "epoch: 9 step: 270, loss is 0.0010905307717621326\n",
      "epoch: 9 step: 271, loss is 0.0024863819126039743\n",
      "epoch: 9 step: 272, loss is 0.07012593001127243\n",
      "epoch: 9 step: 273, loss is 0.0006465864717029035\n",
      "epoch: 9 step: 274, loss is 0.00040386023465543985\n",
      "epoch: 9 step: 275, loss is 7.0490590587724e-05\n",
      "epoch: 9 step: 276, loss is 0.00010525932157179341\n",
      "epoch: 9 step: 277, loss is 0.00010574942280072719\n",
      "epoch: 9 step: 278, loss is 0.00018946448108181357\n",
      "epoch: 9 step: 279, loss is 0.0030965881887823343\n",
      "epoch: 9 step: 280, loss is 7.453453144989908e-05\n",
      "epoch: 9 step: 281, loss is 0.0004485471290536225\n",
      "epoch: 9 step: 282, loss is 0.0013520697830244899\n",
      "epoch: 9 step: 283, loss is 0.0010555554181337357\n",
      "epoch: 9 step: 284, loss is 0.0026657807175070047\n",
      "epoch: 9 step: 285, loss is 0.00134511839132756\n",
      "epoch: 9 step: 286, loss is 0.003364560194313526\n",
      "epoch: 9 step: 287, loss is 0.00018285158148501068\n",
      "epoch: 9 step: 288, loss is 0.20405539870262146\n",
      "epoch: 9 step: 289, loss is 0.00050118297804147\n",
      "epoch: 9 step: 290, loss is 2.6408766643726267e-05\n",
      "epoch: 9 step: 291, loss is 0.012726649641990662\n",
      "epoch: 9 step: 292, loss is 0.0011130878701806068\n",
      "epoch: 9 step: 293, loss is 0.0005891642067581415\n",
      "epoch: 9 step: 294, loss is 7.901152275735512e-05\n",
      "epoch: 9 step: 295, loss is 0.08417653292417526\n",
      "epoch: 9 step: 296, loss is 6.796159141231328e-05\n",
      "epoch: 9 step: 297, loss is 0.0010758350836113095\n",
      "epoch: 9 step: 298, loss is 0.002214901614934206\n",
      "epoch: 9 step: 299, loss is 7.394586282316595e-05\n",
      "epoch: 9 step: 300, loss is 0.03816590458154678\n",
      "epoch: 9 step: 301, loss is 0.00020815434982068837\n",
      "epoch: 9 step: 302, loss is 0.0024774796329438686\n",
      "epoch: 9 step: 303, loss is 0.00016202499682549387\n",
      "epoch: 9 step: 304, loss is 0.0018377047963440418\n",
      "epoch: 9 step: 305, loss is 6.696962373098359e-05\n",
      "epoch: 9 step: 306, loss is 0.004680669866502285\n",
      "epoch: 9 step: 307, loss is 0.0005594315007328987\n",
      "epoch: 9 step: 308, loss is 0.0017696002032607794\n",
      "epoch: 9 step: 309, loss is 3.710034434334375e-05\n",
      "epoch: 9 step: 310, loss is 0.0011169231729581952\n",
      "epoch: 9 step: 311, loss is 0.00023073494958225638\n",
      "epoch: 9 step: 312, loss is 7.41510302759707e-05\n",
      "epoch: 9 step: 313, loss is 0.00034233758924528956\n",
      "epoch: 9 step: 314, loss is 0.0003328513412270695\n",
      "epoch: 9 step: 315, loss is 0.00021291436860337853\n",
      "epoch: 9 step: 316, loss is 0.003161329310387373\n",
      "epoch: 9 step: 317, loss is 4.318428909755312e-05\n",
      "epoch: 9 step: 318, loss is 0.00010796891001518816\n",
      "epoch: 9 step: 319, loss is 0.08975592255592346\n",
      "epoch: 9 step: 320, loss is 0.0035710406955331564\n",
      "epoch: 9 step: 321, loss is 0.06136065721511841\n",
      "epoch: 9 step: 322, loss is 8.241922478191555e-05\n",
      "epoch: 9 step: 323, loss is 0.01656435802578926\n",
      "epoch: 9 step: 324, loss is 0.0014770678244531155\n",
      "epoch: 9 step: 325, loss is 0.005330931395292282\n",
      "epoch: 9 step: 326, loss is 0.0014827909180894494\n",
      "epoch: 9 step: 327, loss is 0.00029933007317595184\n",
      "epoch: 9 step: 328, loss is 0.00026270304806530476\n",
      "epoch: 9 step: 329, loss is 0.007633053697645664\n",
      "epoch: 9 step: 330, loss is 0.0013198175001889467\n",
      "epoch: 9 step: 331, loss is 0.14675277471542358\n",
      "epoch: 9 step: 332, loss is 0.001793967792764306\n",
      "epoch: 9 step: 333, loss is 0.012324280105531216\n",
      "epoch: 9 step: 334, loss is 0.07777123153209686\n",
      "epoch: 9 step: 335, loss is 7.105099939508364e-05\n",
      "epoch: 9 step: 336, loss is 0.12669481337070465\n",
      "epoch: 9 step: 337, loss is 0.0033131169620901346\n",
      "epoch: 9 step: 338, loss is 0.0016047590179368854\n",
      "epoch: 9 step: 339, loss is 0.0038598915562033653\n",
      "epoch: 9 step: 340, loss is 0.0007998403743840754\n",
      "epoch: 9 step: 341, loss is 0.015601646155118942\n",
      "epoch: 9 step: 342, loss is 0.0001281137519981712\n",
      "epoch: 9 step: 343, loss is 0.07167274504899979\n",
      "epoch: 9 step: 344, loss is 0.0016169565496966243\n",
      "epoch: 9 step: 345, loss is 0.1202596127986908\n",
      "epoch: 9 step: 346, loss is 0.20051583647727966\n",
      "epoch: 9 step: 347, loss is 0.00022788696514908224\n",
      "epoch: 9 step: 348, loss is 0.002980076475068927\n",
      "epoch: 9 step: 349, loss is 0.0030609723180532455\n",
      "epoch: 9 step: 350, loss is 0.0010593929328024387\n",
      "epoch: 9 step: 351, loss is 0.002317983191460371\n",
      "epoch: 9 step: 352, loss is 0.008688145317137241\n",
      "epoch: 9 step: 353, loss is 0.06398018449544907\n",
      "epoch: 9 step: 354, loss is 0.00028990927967242897\n",
      "epoch: 9 step: 355, loss is 0.009253904223442078\n",
      "epoch: 9 step: 356, loss is 0.015802429988980293\n",
      "epoch: 9 step: 357, loss is 0.00019411450193729252\n",
      "epoch: 9 step: 358, loss is 0.0008622866589576006\n",
      "epoch: 9 step: 359, loss is 0.011037292890250683\n",
      "epoch: 9 step: 360, loss is 0.0007857856689952314\n",
      "epoch: 9 step: 361, loss is 0.10392671823501587\n",
      "epoch: 9 step: 362, loss is 0.029455766081809998\n",
      "epoch: 9 step: 363, loss is 0.026605231687426567\n",
      "epoch: 9 step: 364, loss is 0.04851193353533745\n",
      "epoch: 9 step: 365, loss is 0.013086364604532719\n",
      "epoch: 9 step: 366, loss is 0.0030550812371075153\n",
      "epoch: 9 step: 367, loss is 0.06753022223711014\n",
      "epoch: 9 step: 368, loss is 0.01582929491996765\n",
      "epoch: 9 step: 369, loss is 0.003388561075553298\n",
      "epoch: 9 step: 370, loss is 0.028545105829834938\n",
      "epoch: 9 step: 371, loss is 0.0016227312153205276\n",
      "epoch: 9 step: 372, loss is 0.011498027481138706\n",
      "epoch: 9 step: 373, loss is 0.053730469197034836\n",
      "epoch: 9 step: 374, loss is 0.04605957493185997\n",
      "epoch: 9 step: 375, loss is 0.031780894845724106\n",
      "epoch: 9 step: 376, loss is 0.0012410805793479085\n",
      "epoch: 9 step: 377, loss is 0.006581511814147234\n",
      "epoch: 9 step: 378, loss is 0.22118642926216125\n",
      "epoch: 9 step: 379, loss is 0.00015877740224823356\n",
      "epoch: 9 step: 380, loss is 0.1410084217786789\n",
      "epoch: 9 step: 381, loss is 0.13423578441143036\n",
      "epoch: 9 step: 382, loss is 0.0004114480398129672\n",
      "epoch: 9 step: 383, loss is 0.0032958209048956633\n",
      "epoch: 9 step: 384, loss is 0.0005842910031788051\n",
      "epoch: 9 step: 385, loss is 0.0043725124560296535\n",
      "epoch: 9 step: 386, loss is 0.007973536849021912\n",
      "epoch: 9 step: 387, loss is 0.0010713061783462763\n",
      "epoch: 9 step: 388, loss is 0.0009997932938858867\n",
      "epoch: 9 step: 389, loss is 0.00012234365567564964\n",
      "epoch: 9 step: 390, loss is 0.0015569573733955622\n",
      "epoch: 9 step: 391, loss is 0.00878881849348545\n",
      "epoch: 9 step: 392, loss is 0.0007239978876896203\n",
      "epoch: 9 step: 393, loss is 0.0006048238137736917\n",
      "epoch: 9 step: 394, loss is 0.012193964794278145\n",
      "epoch: 9 step: 395, loss is 0.007828885689377785\n",
      "epoch: 9 step: 396, loss is 0.0024962895549833775\n",
      "epoch: 9 step: 397, loss is 0.0021049908827990294\n",
      "epoch: 9 step: 398, loss is 0.022603115066885948\n",
      "epoch: 9 step: 399, loss is 0.027117682620882988\n",
      "epoch: 9 step: 400, loss is 0.037319477647542953\n",
      "epoch: 9 step: 401, loss is 0.0020043037366122007\n",
      "epoch: 9 step: 402, loss is 0.0005868200096301734\n",
      "epoch: 9 step: 403, loss is 0.000991523265838623\n",
      "epoch: 9 step: 404, loss is 0.0003329734900034964\n",
      "epoch: 9 step: 405, loss is 0.02617592364549637\n",
      "epoch: 9 step: 406, loss is 0.004911932162940502\n",
      "epoch: 9 step: 407, loss is 0.00021443248260766268\n",
      "epoch: 9 step: 408, loss is 0.007372438441962004\n",
      "epoch: 9 step: 409, loss is 0.00036168479709886014\n",
      "epoch: 9 step: 410, loss is 0.0013071178691461682\n",
      "epoch: 9 step: 411, loss is 0.0003646222467068583\n",
      "epoch: 9 step: 412, loss is 0.00013975566253066063\n",
      "epoch: 9 step: 413, loss is 0.07601989060640335\n",
      "epoch: 9 step: 414, loss is 0.025735938921570778\n",
      "epoch: 9 step: 415, loss is 0.0016689191106706858\n",
      "epoch: 9 step: 416, loss is 0.008577763102948666\n",
      "epoch: 9 step: 417, loss is 0.0016131739830598235\n",
      "epoch: 9 step: 418, loss is 0.0628659576177597\n",
      "epoch: 9 step: 419, loss is 0.048057667911052704\n",
      "epoch: 9 step: 420, loss is 0.06874103844165802\n",
      "epoch: 9 step: 421, loss is 0.0493481308221817\n",
      "epoch: 9 step: 422, loss is 0.026539897546172142\n",
      "epoch: 9 step: 423, loss is 0.0007154222694225609\n",
      "epoch: 9 step: 424, loss is 0.10093449801206589\n",
      "epoch: 9 step: 425, loss is 0.010526568628847599\n",
      "epoch: 9 step: 426, loss is 0.01030308660119772\n",
      "epoch: 9 step: 427, loss is 0.0006192239816300571\n",
      "epoch: 9 step: 428, loss is 0.0020670571830123663\n",
      "epoch: 9 step: 429, loss is 0.2274281084537506\n",
      "epoch: 9 step: 430, loss is 0.07620605081319809\n",
      "epoch: 9 step: 431, loss is 0.007451115641742945\n",
      "epoch: 9 step: 432, loss is 0.0008221252355724573\n",
      "epoch: 9 step: 433, loss is 0.002511114114895463\n",
      "epoch: 9 step: 434, loss is 0.0014982866123318672\n",
      "epoch: 9 step: 435, loss is 0.004497479647397995\n",
      "epoch: 9 step: 436, loss is 0.0003027644706889987\n",
      "epoch: 9 step: 437, loss is 0.009063479490578175\n",
      "epoch: 9 step: 438, loss is 0.02381129190325737\n",
      "epoch: 9 step: 439, loss is 0.005542691331356764\n",
      "epoch: 9 step: 440, loss is 0.001076868618838489\n",
      "epoch: 9 step: 441, loss is 0.00013144058175384998\n",
      "epoch: 9 step: 442, loss is 0.00010742187441792339\n",
      "epoch: 9 step: 443, loss is 0.00029627425828948617\n",
      "epoch: 9 step: 444, loss is 0.00017457814828958362\n",
      "epoch: 9 step: 445, loss is 0.05612848699092865\n",
      "epoch: 9 step: 446, loss is 0.1266041398048401\n",
      "epoch: 9 step: 447, loss is 0.016554204747080803\n",
      "epoch: 9 step: 448, loss is 0.018047796562314034\n",
      "epoch: 9 step: 449, loss is 0.03168578818440437\n",
      "epoch: 9 step: 450, loss is 0.0008809242863208055\n",
      "epoch: 9 step: 451, loss is 0.0005902470438741148\n",
      "epoch: 9 step: 452, loss is 0.016707228496670723\n",
      "epoch: 9 step: 453, loss is 0.0004317416751291603\n",
      "epoch: 9 step: 454, loss is 0.17995430529117584\n",
      "epoch: 9 step: 455, loss is 0.01972871832549572\n",
      "epoch: 9 step: 456, loss is 0.03080878220498562\n",
      "epoch: 9 step: 457, loss is 0.0010274677770212293\n",
      "epoch: 9 step: 458, loss is 0.007758732885122299\n",
      "epoch: 9 step: 459, loss is 0.04236134514212608\n",
      "epoch: 9 step: 460, loss is 4.7527260903734714e-05\n",
      "epoch: 9 step: 461, loss is 0.005274736322462559\n",
      "epoch: 9 step: 462, loss is 0.018532585352659225\n",
      "epoch: 9 step: 463, loss is 0.021953487768769264\n",
      "epoch: 9 step: 464, loss is 0.0028715082444250584\n",
      "epoch: 9 step: 465, loss is 0.02978227660059929\n",
      "epoch: 9 step: 466, loss is 0.007342318072915077\n",
      "epoch: 9 step: 467, loss is 0.00018065569747705013\n",
      "epoch: 9 step: 468, loss is 0.008726809173822403\n",
      "epoch: 9 step: 469, loss is 0.015352410264313221\n",
      "epoch: 9 step: 470, loss is 0.07622150331735611\n",
      "epoch: 9 step: 471, loss is 0.03846839442849159\n",
      "epoch: 9 step: 472, loss is 0.0008551661157980561\n",
      "epoch: 9 step: 473, loss is 0.12588343024253845\n",
      "epoch: 9 step: 474, loss is 0.0002849604934453964\n",
      "epoch: 9 step: 475, loss is 0.0004030555428471416\n",
      "epoch: 9 step: 476, loss is 0.0009648285340517759\n",
      "epoch: 9 step: 477, loss is 0.0006466066697612405\n",
      "epoch: 9 step: 478, loss is 0.002265203045681119\n",
      "epoch: 9 step: 479, loss is 0.0053712776862084866\n",
      "epoch: 9 step: 480, loss is 0.0004258094122633338\n",
      "epoch: 9 step: 481, loss is 0.055111363530159\n",
      "epoch: 9 step: 482, loss is 0.16072708368301392\n",
      "epoch: 9 step: 483, loss is 0.0003262156096752733\n",
      "epoch: 9 step: 484, loss is 0.028385497629642487\n",
      "epoch: 9 step: 485, loss is 0.009217855520546436\n",
      "epoch: 9 step: 486, loss is 0.002886061090976\n",
      "epoch: 9 step: 487, loss is 0.027864689007401466\n",
      "epoch: 9 step: 488, loss is 0.0008078155806288123\n",
      "epoch: 9 step: 489, loss is 0.02425575815141201\n",
      "epoch: 9 step: 490, loss is 0.001624213415198028\n",
      "epoch: 9 step: 491, loss is 0.03778309375047684\n",
      "epoch: 9 step: 492, loss is 0.022908106446266174\n",
      "epoch: 9 step: 493, loss is 0.046942830085754395\n",
      "epoch: 9 step: 494, loss is 0.004339931532740593\n",
      "epoch: 9 step: 495, loss is 0.003056757617741823\n",
      "epoch: 9 step: 496, loss is 0.0003139315522275865\n",
      "epoch: 9 step: 497, loss is 0.022333091124892235\n",
      "epoch: 9 step: 498, loss is 0.00020465280977077782\n",
      "epoch: 9 step: 499, loss is 0.0028723913710564375\n",
      "epoch: 9 step: 500, loss is 0.00035559688694775105\n",
      "epoch: 9 step: 501, loss is 0.002585216425359249\n",
      "epoch: 9 step: 502, loss is 0.08085286617279053\n",
      "epoch: 9 step: 503, loss is 0.00013715712702833116\n",
      "epoch: 9 step: 504, loss is 0.0028244450222700834\n",
      "epoch: 9 step: 505, loss is 0.02608991228044033\n",
      "epoch: 9 step: 506, loss is 0.012742078863084316\n",
      "epoch: 9 step: 507, loss is 0.0007564224069938064\n",
      "epoch: 9 step: 508, loss is 0.000669703702442348\n",
      "epoch: 9 step: 509, loss is 0.008127612993121147\n",
      "epoch: 9 step: 510, loss is 0.0008779004565440118\n",
      "epoch: 9 step: 511, loss is 0.0010214740177616477\n",
      "epoch: 9 step: 512, loss is 0.00668676570057869\n",
      "epoch: 9 step: 513, loss is 0.0023217429406940937\n",
      "epoch: 9 step: 514, loss is 0.09949694573879242\n",
      "epoch: 9 step: 515, loss is 0.000919341400731355\n",
      "epoch: 9 step: 516, loss is 0.0008858810761012137\n",
      "epoch: 9 step: 517, loss is 0.0008654031553305686\n",
      "epoch: 9 step: 518, loss is 0.0030200679320842028\n",
      "epoch: 9 step: 519, loss is 0.00024880742421373725\n",
      "epoch: 9 step: 520, loss is 0.0005896909860894084\n",
      "epoch: 9 step: 521, loss is 6.13842494203709e-05\n",
      "epoch: 9 step: 522, loss is 0.0003345551376696676\n",
      "epoch: 9 step: 523, loss is 0.002337600803002715\n",
      "epoch: 9 step: 524, loss is 0.002948180539533496\n",
      "epoch: 9 step: 525, loss is 0.009239869192242622\n",
      "epoch: 9 step: 526, loss is 0.0005511527415364981\n",
      "epoch: 9 step: 527, loss is 0.0017621380975469947\n",
      "epoch: 9 step: 528, loss is 0.0006352398777380586\n",
      "epoch: 9 step: 529, loss is 0.0013095272006466985\n",
      "epoch: 9 step: 530, loss is 0.12402693927288055\n",
      "epoch: 9 step: 531, loss is 0.19060035049915314\n",
      "epoch: 9 step: 532, loss is 0.0003986596711911261\n",
      "epoch: 9 step: 533, loss is 0.0043024360202252865\n",
      "epoch: 9 step: 534, loss is 0.12214808911085129\n",
      "epoch: 9 step: 535, loss is 0.019942348822951317\n",
      "epoch: 9 step: 536, loss is 0.011324415914714336\n",
      "epoch: 9 step: 537, loss is 0.007358590606600046\n",
      "epoch: 9 step: 538, loss is 0.004620143678039312\n",
      "epoch: 9 step: 539, loss is 0.002186698606237769\n",
      "epoch: 9 step: 540, loss is 0.0189896821975708\n",
      "epoch: 9 step: 541, loss is 0.0054390449076890945\n",
      "epoch: 9 step: 542, loss is 0.12600748240947723\n",
      "epoch: 9 step: 543, loss is 0.01581491157412529\n",
      "epoch: 9 step: 544, loss is 0.04091056063771248\n",
      "epoch: 9 step: 545, loss is 0.002077172975987196\n",
      "epoch: 9 step: 546, loss is 0.0011237080907449126\n",
      "epoch: 9 step: 547, loss is 0.0001684309245320037\n",
      "epoch: 9 step: 548, loss is 0.00023181058350019157\n",
      "epoch: 9 step: 549, loss is 0.001456236932426691\n",
      "epoch: 9 step: 550, loss is 0.0334615595638752\n",
      "epoch: 9 step: 551, loss is 0.00980005320161581\n",
      "epoch: 9 step: 552, loss is 0.001120461616665125\n",
      "epoch: 9 step: 553, loss is 0.0019173017935827374\n",
      "epoch: 9 step: 554, loss is 0.0052595557644963264\n",
      "epoch: 9 step: 555, loss is 0.012295471504330635\n",
      "epoch: 9 step: 556, loss is 0.0071061220951378345\n",
      "epoch: 9 step: 557, loss is 0.006830149795860052\n",
      "epoch: 9 step: 558, loss is 0.0003115795261692256\n",
      "epoch: 9 step: 559, loss is 0.008530576713383198\n",
      "epoch: 9 step: 560, loss is 0.007968099787831306\n",
      "epoch: 9 step: 561, loss is 0.010259121656417847\n",
      "epoch: 9 step: 562, loss is 0.000748704478610307\n",
      "epoch: 9 step: 563, loss is 0.039606060832738876\n",
      "epoch: 9 step: 564, loss is 0.10247932374477386\n",
      "epoch: 9 step: 565, loss is 0.029707489535212517\n",
      "epoch: 9 step: 566, loss is 0.0006821315037086606\n",
      "epoch: 9 step: 567, loss is 0.0016946400282904506\n",
      "epoch: 9 step: 568, loss is 5.7755314628593624e-05\n",
      "epoch: 9 step: 569, loss is 0.003739398904144764\n",
      "epoch: 9 step: 570, loss is 0.03382940590381622\n",
      "epoch: 9 step: 571, loss is 0.001622236450202763\n",
      "epoch: 9 step: 572, loss is 0.00018098222790285945\n",
      "epoch: 9 step: 573, loss is 0.004987200256437063\n",
      "epoch: 9 step: 574, loss is 0.00468410225585103\n",
      "epoch: 9 step: 575, loss is 0.000996365211904049\n",
      "epoch: 9 step: 576, loss is 0.0016513930168002844\n",
      "epoch: 9 step: 577, loss is 0.008054979145526886\n",
      "epoch: 9 step: 578, loss is 0.00010645470320014283\n",
      "epoch: 9 step: 579, loss is 0.011425373144447803\n",
      "epoch: 9 step: 580, loss is 0.011143635958433151\n",
      "epoch: 9 step: 581, loss is 0.0019128246931359172\n",
      "epoch: 9 step: 582, loss is 0.00013452440907713026\n",
      "epoch: 9 step: 583, loss is 0.0007237822865135968\n",
      "epoch: 9 step: 584, loss is 0.1820858269929886\n",
      "epoch: 9 step: 585, loss is 0.00016695352678652853\n",
      "epoch: 9 step: 586, loss is 0.000852712313644588\n",
      "epoch: 9 step: 587, loss is 0.0392870157957077\n",
      "epoch: 9 step: 588, loss is 0.0001759539736667648\n",
      "epoch: 9 step: 589, loss is 0.03250020742416382\n",
      "epoch: 9 step: 590, loss is 0.06722453981637955\n",
      "epoch: 9 step: 591, loss is 0.0007583581027574837\n",
      "epoch: 9 step: 592, loss is 0.000350170157616958\n",
      "epoch: 9 step: 593, loss is 0.0035876247566193342\n",
      "epoch: 9 step: 594, loss is 0.009382825344800949\n",
      "epoch: 9 step: 595, loss is 0.0002741189091466367\n",
      "epoch: 9 step: 596, loss is 0.001240717712789774\n",
      "epoch: 9 step: 597, loss is 0.0005229268572293222\n",
      "epoch: 9 step: 598, loss is 0.07618813961744308\n",
      "epoch: 9 step: 599, loss is 0.015361178666353226\n",
      "epoch: 9 step: 600, loss is 0.00015273633471224457\n",
      "epoch: 9 step: 601, loss is 0.00019755169341806322\n",
      "epoch: 9 step: 602, loss is 0.11739819496870041\n",
      "epoch: 9 step: 603, loss is 0.08148036897182465\n",
      "epoch: 9 step: 604, loss is 0.007881883531808853\n",
      "epoch: 9 step: 605, loss is 0.011636829935014248\n",
      "epoch: 9 step: 606, loss is 0.0034918072633445263\n",
      "epoch: 9 step: 607, loss is 0.0034658540971577168\n",
      "epoch: 9 step: 608, loss is 0.0011325411032885313\n",
      "epoch: 9 step: 609, loss is 0.005912936292588711\n",
      "epoch: 9 step: 610, loss is 0.00024189543910324574\n",
      "epoch: 9 step: 611, loss is 0.0003534310671966523\n",
      "epoch: 9 step: 612, loss is 0.0038794097490608692\n",
      "epoch: 9 step: 613, loss is 0.008075898513197899\n",
      "epoch: 9 step: 614, loss is 0.0012847061734646559\n",
      "epoch: 9 step: 615, loss is 0.0026143977884203196\n",
      "epoch: 9 step: 616, loss is 0.001025288482196629\n",
      "epoch: 9 step: 617, loss is 0.0019067992689087987\n",
      "epoch: 9 step: 618, loss is 0.1468622237443924\n",
      "epoch: 9 step: 619, loss is 0.004237966611981392\n",
      "epoch: 9 step: 620, loss is 0.008023074828088284\n",
      "epoch: 9 step: 621, loss is 0.0016853644046932459\n",
      "epoch: 9 step: 622, loss is 0.0008436708594672382\n",
      "epoch: 9 step: 623, loss is 0.004588089417666197\n",
      "epoch: 9 step: 624, loss is 0.0006386664463207126\n",
      "epoch: 9 step: 625, loss is 0.00017991104687098414\n",
      "epoch: 9 step: 626, loss is 0.00169035280123353\n",
      "epoch: 9 step: 627, loss is 0.009517693892121315\n",
      "epoch: 9 step: 628, loss is 0.061685267835855484\n",
      "epoch: 9 step: 629, loss is 0.029480431228876114\n",
      "epoch: 9 step: 630, loss is 0.00027822222909890115\n",
      "epoch: 9 step: 631, loss is 0.037005092948675156\n",
      "epoch: 9 step: 632, loss is 0.02327745035290718\n",
      "epoch: 9 step: 633, loss is 0.00896080769598484\n",
      "epoch: 9 step: 634, loss is 0.0004490759165491909\n",
      "epoch: 9 step: 635, loss is 0.004034980200231075\n",
      "epoch: 9 step: 636, loss is 0.0001800986792659387\n",
      "epoch: 9 step: 637, loss is 0.1362655758857727\n",
      "epoch: 9 step: 638, loss is 0.0002053923817584291\n",
      "epoch: 9 step: 639, loss is 0.0012242762604728341\n",
      "epoch: 9 step: 640, loss is 0.00024537823628634214\n",
      "epoch: 9 step: 641, loss is 0.09363215416669846\n",
      "epoch: 9 step: 642, loss is 6.433531962102279e-05\n",
      "epoch: 9 step: 643, loss is 0.0006050808588042855\n",
      "epoch: 9 step: 644, loss is 0.008369015529751778\n",
      "epoch: 9 step: 645, loss is 0.00032898111385293305\n",
      "epoch: 9 step: 646, loss is 0.003827072912827134\n",
      "epoch: 9 step: 647, loss is 0.006964558735489845\n",
      "epoch: 9 step: 648, loss is 0.0027558139991015196\n",
      "epoch: 9 step: 649, loss is 0.0006298311054706573\n",
      "epoch: 9 step: 650, loss is 0.00010256414680043235\n",
      "epoch: 9 step: 651, loss is 0.011667984537780285\n",
      "epoch: 9 step: 652, loss is 0.003649634076282382\n",
      "epoch: 9 step: 653, loss is 0.005412447266280651\n",
      "epoch: 9 step: 654, loss is 0.0010450154077261686\n",
      "epoch: 9 step: 655, loss is 0.034359294921159744\n",
      "epoch: 9 step: 656, loss is 5.861483805347234e-05\n",
      "epoch: 9 step: 657, loss is 0.014872485771775246\n",
      "epoch: 9 step: 658, loss is 0.00025469850515946746\n",
      "epoch: 9 step: 659, loss is 0.015770550817251205\n",
      "epoch: 9 step: 660, loss is 0.0007449961267411709\n",
      "epoch: 9 step: 661, loss is 0.00015483517199754715\n",
      "epoch: 9 step: 662, loss is 0.00023993181821424514\n",
      "epoch: 9 step: 663, loss is 0.01062080543488264\n",
      "epoch: 9 step: 664, loss is 0.0006785716977901757\n",
      "epoch: 9 step: 665, loss is 0.032066281884908676\n",
      "epoch: 9 step: 666, loss is 0.0006560861365869641\n",
      "epoch: 9 step: 667, loss is 0.0007101448136381805\n",
      "epoch: 9 step: 668, loss is 0.0010754343820735812\n",
      "epoch: 9 step: 669, loss is 0.04910511150956154\n",
      "epoch: 9 step: 670, loss is 0.0717896968126297\n",
      "epoch: 9 step: 671, loss is 0.015964506193995476\n",
      "epoch: 9 step: 672, loss is 0.00011770762648666278\n",
      "epoch: 9 step: 673, loss is 0.0018151573603972793\n",
      "epoch: 9 step: 674, loss is 0.005267367232590914\n",
      "epoch: 9 step: 675, loss is 0.003734947880730033\n",
      "epoch: 9 step: 676, loss is 0.06821775436401367\n",
      "epoch: 9 step: 677, loss is 0.002098808065056801\n",
      "epoch: 9 step: 678, loss is 0.00023183033044915646\n",
      "epoch: 9 step: 679, loss is 9.481172310188413e-05\n",
      "epoch: 9 step: 680, loss is 0.013354345224797726\n",
      "epoch: 9 step: 681, loss is 0.0007275845855474472\n",
      "epoch: 9 step: 682, loss is 0.025865992531180382\n",
      "epoch: 9 step: 683, loss is 0.09791674464941025\n",
      "epoch: 9 step: 684, loss is 0.0014081848785281181\n",
      "epoch: 9 step: 685, loss is 0.0006558781606145203\n",
      "epoch: 9 step: 686, loss is 0.10329902172088623\n",
      "epoch: 9 step: 687, loss is 0.0010209231404587626\n",
      "epoch: 9 step: 688, loss is 0.0008855137275531888\n",
      "epoch: 9 step: 689, loss is 0.0012026617769151926\n",
      "epoch: 9 step: 690, loss is 0.0012655338505282998\n",
      "epoch: 9 step: 691, loss is 0.23199282586574554\n",
      "epoch: 9 step: 692, loss is 0.0033377206418663263\n",
      "epoch: 9 step: 693, loss is 0.004476555157452822\n",
      "epoch: 9 step: 694, loss is 1.6868903912836686e-05\n",
      "epoch: 9 step: 695, loss is 0.02158731408417225\n",
      "epoch: 9 step: 696, loss is 4.380663449410349e-05\n",
      "epoch: 9 step: 697, loss is 0.0004452630237210542\n",
      "epoch: 9 step: 698, loss is 0.00013022436178289354\n",
      "epoch: 9 step: 699, loss is 0.01094731967896223\n",
      "epoch: 9 step: 700, loss is 0.012001140974462032\n",
      "epoch: 9 step: 701, loss is 0.006488072220236063\n",
      "epoch: 9 step: 702, loss is 0.00012723247346002609\n",
      "epoch: 9 step: 703, loss is 4.8394907935289666e-05\n",
      "epoch: 9 step: 704, loss is 0.009071017615497112\n",
      "epoch: 9 step: 705, loss is 0.0001232630165759474\n",
      "epoch: 9 step: 706, loss is 0.00017804699018597603\n",
      "epoch: 9 step: 707, loss is 0.009815639816224575\n",
      "epoch: 9 step: 708, loss is 0.09592455625534058\n",
      "epoch: 9 step: 709, loss is 7.467251271009445e-05\n",
      "epoch: 9 step: 710, loss is 0.0007331381202675402\n",
      "epoch: 9 step: 711, loss is 0.06295880675315857\n",
      "epoch: 9 step: 712, loss is 0.022249702364206314\n",
      "epoch: 9 step: 713, loss is 0.009800801984965801\n",
      "epoch: 9 step: 714, loss is 0.0002652825496625155\n",
      "epoch: 9 step: 715, loss is 7.253453077282757e-05\n",
      "epoch: 9 step: 716, loss is 0.0059269932098686695\n",
      "epoch: 9 step: 717, loss is 0.011957664042711258\n",
      "epoch: 9 step: 718, loss is 0.003276155097410083\n",
      "epoch: 9 step: 719, loss is 0.0032664041500538588\n",
      "epoch: 9 step: 720, loss is 0.016637083142995834\n",
      "epoch: 9 step: 721, loss is 0.0048301443457603455\n",
      "epoch: 9 step: 722, loss is 0.0012430909555405378\n",
      "epoch: 9 step: 723, loss is 0.04816919565200806\n",
      "epoch: 9 step: 724, loss is 0.002405945910140872\n",
      "epoch: 9 step: 725, loss is 0.0002825253177434206\n",
      "epoch: 9 step: 726, loss is 0.06596579402685165\n",
      "epoch: 9 step: 727, loss is 0.0006497674039565027\n",
      "epoch: 9 step: 728, loss is 0.0036191365215927362\n",
      "epoch: 9 step: 729, loss is 0.0014135738601908088\n",
      "epoch: 9 step: 730, loss is 0.0001233711082022637\n",
      "epoch: 9 step: 731, loss is 5.142123336554505e-05\n",
      "epoch: 9 step: 732, loss is 0.0022893757559359074\n",
      "epoch: 9 step: 733, loss is 0.02510746754705906\n",
      "epoch: 9 step: 734, loss is 0.013656537048518658\n",
      "epoch: 9 step: 735, loss is 0.0009684420074336231\n",
      "epoch: 9 step: 736, loss is 0.004639605991542339\n",
      "epoch: 9 step: 737, loss is 0.03070511668920517\n",
      "epoch: 9 step: 738, loss is 0.0017325596418231726\n",
      "epoch: 9 step: 739, loss is 0.0002100742276525125\n",
      "epoch: 9 step: 740, loss is 0.0010635927319526672\n",
      "epoch: 9 step: 741, loss is 0.025578023865818977\n",
      "epoch: 9 step: 742, loss is 8.821815572446212e-05\n",
      "epoch: 9 step: 743, loss is 0.005910928826779127\n",
      "epoch: 9 step: 744, loss is 0.01957644335925579\n",
      "epoch: 9 step: 745, loss is 0.017554758116602898\n",
      "epoch: 9 step: 746, loss is 0.0009427224285900593\n",
      "epoch: 9 step: 747, loss is 0.027173183858394623\n",
      "epoch: 9 step: 748, loss is 4.137412179261446e-05\n",
      "epoch: 9 step: 749, loss is 0.0004358034930191934\n",
      "epoch: 9 step: 750, loss is 0.007351412903517485\n",
      "epoch: 9 step: 751, loss is 0.21596761047840118\n",
      "epoch: 9 step: 752, loss is 0.00037086225347593427\n",
      "epoch: 9 step: 753, loss is 2.8743070288328454e-05\n",
      "epoch: 9 step: 754, loss is 0.0036476384848356247\n",
      "epoch: 9 step: 755, loss is 9.836987737799063e-05\n",
      "epoch: 9 step: 756, loss is 0.0012017290573567152\n",
      "epoch: 9 step: 757, loss is 0.024400100111961365\n",
      "epoch: 9 step: 758, loss is 0.0002936096570920199\n",
      "epoch: 9 step: 759, loss is 0.0036678106989711523\n",
      "epoch: 9 step: 760, loss is 0.0008399937069043517\n",
      "epoch: 9 step: 761, loss is 0.0005426433635875583\n",
      "epoch: 9 step: 762, loss is 0.0004239502304699272\n",
      "epoch: 9 step: 763, loss is 0.0013116303598508239\n",
      "epoch: 9 step: 764, loss is 0.10313334316015244\n",
      "epoch: 9 step: 765, loss is 0.01435864344239235\n",
      "epoch: 9 step: 766, loss is 0.0014601998263970017\n",
      "epoch: 9 step: 767, loss is 0.04385744407773018\n",
      "epoch: 9 step: 768, loss is 7.640773219463881e-06\n",
      "epoch: 9 step: 769, loss is 0.0024420376867055893\n",
      "epoch: 9 step: 770, loss is 0.00012811239867005497\n",
      "epoch: 9 step: 771, loss is 0.09441350400447845\n",
      "epoch: 9 step: 772, loss is 0.00015042150334920734\n",
      "epoch: 9 step: 773, loss is 0.1340104192495346\n",
      "epoch: 9 step: 774, loss is 0.0004437761672306806\n",
      "epoch: 9 step: 775, loss is 4.892054494121112e-05\n",
      "epoch: 9 step: 776, loss is 0.002382323844358325\n",
      "epoch: 9 step: 777, loss is 0.0067868661135435104\n",
      "epoch: 9 step: 778, loss is 0.0006435810355469584\n",
      "epoch: 9 step: 779, loss is 0.0018761067185550928\n",
      "epoch: 9 step: 780, loss is 0.005351790226995945\n",
      "epoch: 9 step: 781, loss is 0.03765465319156647\n",
      "epoch: 9 step: 782, loss is 0.005153233651071787\n",
      "epoch: 9 step: 783, loss is 0.00038483235402964056\n",
      "epoch: 9 step: 784, loss is 0.00035278243012726307\n",
      "epoch: 9 step: 785, loss is 0.0742383822798729\n",
      "epoch: 9 step: 786, loss is 0.006503298878669739\n",
      "epoch: 9 step: 787, loss is 0.0011344430968165398\n",
      "epoch: 9 step: 788, loss is 0.00018738752987701446\n",
      "epoch: 9 step: 789, loss is 0.0009772325865924358\n",
      "epoch: 9 step: 790, loss is 0.017890343442559242\n",
      "epoch: 9 step: 791, loss is 8.731528942007571e-05\n",
      "epoch: 9 step: 792, loss is 0.0006978957098908722\n",
      "epoch: 9 step: 793, loss is 0.0027553862892091274\n",
      "epoch: 9 step: 794, loss is 0.0029800189658999443\n",
      "epoch: 9 step: 795, loss is 0.011046228930354118\n",
      "epoch: 9 step: 796, loss is 0.0008430464658886194\n",
      "epoch: 9 step: 797, loss is 0.011650944128632545\n",
      "epoch: 9 step: 798, loss is 0.0008870921446941793\n",
      "epoch: 9 step: 799, loss is 0.0034761419519782066\n",
      "epoch: 9 step: 800, loss is 0.0046117277815938\n",
      "epoch: 9 step: 801, loss is 0.0069688535295426846\n",
      "epoch: 9 step: 802, loss is 0.110099196434021\n",
      "epoch: 9 step: 803, loss is 0.0004470531130209565\n",
      "epoch: 9 step: 804, loss is 0.010817897506058216\n",
      "epoch: 9 step: 805, loss is 0.00024578801821917295\n",
      "epoch: 9 step: 806, loss is 0.0009837191319093108\n",
      "epoch: 9 step: 807, loss is 0.0010223757708445191\n",
      "epoch: 9 step: 808, loss is 0.0016321829752996564\n",
      "epoch: 9 step: 809, loss is 0.0055580176413059235\n",
      "epoch: 9 step: 810, loss is 0.0009718015789985657\n",
      "epoch: 9 step: 811, loss is 0.0005155171966180205\n",
      "epoch: 9 step: 812, loss is 0.00574948824942112\n",
      "epoch: 9 step: 813, loss is 0.0016035469016060233\n",
      "epoch: 9 step: 814, loss is 0.0009278329089283943\n",
      "epoch: 9 step: 815, loss is 2.327032052562572e-05\n",
      "epoch: 9 step: 816, loss is 0.010605592280626297\n",
      "epoch: 9 step: 817, loss is 0.0034301569685339928\n",
      "epoch: 9 step: 818, loss is 0.0029286849312484264\n",
      "epoch: 9 step: 819, loss is 9.55860668909736e-05\n",
      "epoch: 9 step: 820, loss is 0.006854098755866289\n",
      "epoch: 9 step: 821, loss is 0.006947970949113369\n",
      "epoch: 9 step: 822, loss is 0.001990318764001131\n",
      "epoch: 9 step: 823, loss is 0.12149856239557266\n",
      "epoch: 9 step: 824, loss is 0.01758717931807041\n",
      "epoch: 9 step: 825, loss is 7.266917236847803e-05\n",
      "epoch: 9 step: 826, loss is 0.0320744626224041\n",
      "epoch: 9 step: 827, loss is 0.003068930935114622\n",
      "epoch: 9 step: 828, loss is 0.009427049197256565\n",
      "epoch: 9 step: 829, loss is 0.007911368273198605\n",
      "epoch: 9 step: 830, loss is 0.004287100862711668\n",
      "epoch: 9 step: 831, loss is 0.00019335369870532304\n",
      "epoch: 9 step: 832, loss is 0.000246086303377524\n",
      "epoch: 9 step: 833, loss is 0.021784014999866486\n",
      "epoch: 9 step: 834, loss is 0.10409373044967651\n",
      "epoch: 9 step: 835, loss is 0.0003630928404163569\n",
      "epoch: 9 step: 836, loss is 7.438150350935757e-05\n",
      "epoch: 9 step: 837, loss is 0.015330404043197632\n",
      "epoch: 9 step: 838, loss is 0.00328882597386837\n",
      "epoch: 9 step: 839, loss is 0.003950390033423901\n",
      "epoch: 9 step: 840, loss is 0.018770422786474228\n",
      "epoch: 9 step: 841, loss is 0.06978293508291245\n",
      "epoch: 9 step: 842, loss is 0.001441636704839766\n",
      "epoch: 9 step: 843, loss is 0.0022084866650402546\n",
      "epoch: 9 step: 844, loss is 0.12451646476984024\n",
      "epoch: 9 step: 845, loss is 0.00010387366637587547\n",
      "epoch: 9 step: 846, loss is 7.765980262774974e-05\n",
      "epoch: 9 step: 847, loss is 0.00448589026927948\n",
      "epoch: 9 step: 848, loss is 0.01553189754486084\n",
      "epoch: 9 step: 849, loss is 0.003985229413956404\n",
      "epoch: 9 step: 850, loss is 0.0014874256448820233\n",
      "epoch: 9 step: 851, loss is 0.07092035561800003\n",
      "epoch: 9 step: 852, loss is 0.0007945274119265378\n",
      "epoch: 9 step: 853, loss is 0.0003045780467800796\n",
      "epoch: 9 step: 854, loss is 0.03720793500542641\n",
      "epoch: 9 step: 855, loss is 0.008206784725189209\n",
      "epoch: 9 step: 856, loss is 0.003821348072960973\n",
      "epoch: 9 step: 857, loss is 0.004218292888253927\n",
      "epoch: 9 step: 858, loss is 0.03779034689068794\n",
      "epoch: 9 step: 859, loss is 0.0010737148113548756\n",
      "epoch: 9 step: 860, loss is 0.024655647575855255\n",
      "epoch: 9 step: 861, loss is 0.04327200725674629\n",
      "epoch: 9 step: 862, loss is 0.00042702830978669226\n",
      "epoch: 9 step: 863, loss is 0.04547163099050522\n",
      "epoch: 9 step: 864, loss is 0.00040453727706335485\n",
      "epoch: 9 step: 865, loss is 3.854399255942553e-05\n",
      "epoch: 9 step: 866, loss is 0.0017527559539303184\n",
      "epoch: 9 step: 867, loss is 0.0002638046280480921\n",
      "epoch: 9 step: 868, loss is 0.00022319906565826386\n",
      "epoch: 9 step: 869, loss is 0.00038306813803501427\n",
      "epoch: 9 step: 870, loss is 0.015161524526774883\n",
      "epoch: 9 step: 871, loss is 0.0016381633467972279\n",
      "epoch: 9 step: 872, loss is 0.058576878160238266\n",
      "epoch: 9 step: 873, loss is 0.03863286226987839\n",
      "epoch: 9 step: 874, loss is 0.001651579630561173\n",
      "epoch: 9 step: 875, loss is 0.011847380548715591\n",
      "epoch: 9 step: 876, loss is 0.0031182081438601017\n",
      "epoch: 9 step: 877, loss is 0.0007173466729000211\n",
      "epoch: 9 step: 878, loss is 0.09852763265371323\n",
      "epoch: 9 step: 879, loss is 0.00040570544661022723\n",
      "epoch: 9 step: 880, loss is 0.0008201479213312268\n",
      "epoch: 9 step: 881, loss is 0.0011224402114748955\n",
      "epoch: 9 step: 882, loss is 1.1122438081656583e-05\n",
      "epoch: 9 step: 883, loss is 8.491170592606068e-05\n",
      "epoch: 9 step: 884, loss is 0.006649936083704233\n",
      "epoch: 9 step: 885, loss is 0.0005614534602500498\n",
      "epoch: 9 step: 886, loss is 0.0012225741520524025\n",
      "epoch: 9 step: 887, loss is 5.0402843044139445e-05\n",
      "epoch: 9 step: 888, loss is 0.0063309622928500175\n",
      "epoch: 9 step: 889, loss is 0.0007354948902502656\n",
      "epoch: 9 step: 890, loss is 0.006122291088104248\n",
      "epoch: 9 step: 891, loss is 0.039697788655757904\n",
      "epoch: 9 step: 892, loss is 0.0020593248773366213\n",
      "epoch: 9 step: 893, loss is 0.14070503413677216\n",
      "epoch: 9 step: 894, loss is 0.00011293149145785719\n",
      "epoch: 9 step: 895, loss is 0.00011778088810387999\n",
      "epoch: 9 step: 896, loss is 0.07057290524244308\n",
      "epoch: 9 step: 897, loss is 0.003322592470794916\n",
      "epoch: 9 step: 898, loss is 0.0016918587498366833\n",
      "epoch: 9 step: 899, loss is 0.04528491571545601\n",
      "epoch: 9 step: 900, loss is 0.00033129239454865456\n",
      "epoch: 9 step: 901, loss is 0.0009345486760139465\n",
      "epoch: 9 step: 902, loss is 0.0245298333466053\n",
      "epoch: 9 step: 903, loss is 0.006630633492022753\n",
      "epoch: 9 step: 904, loss is 0.0023651591036468744\n",
      "epoch: 9 step: 905, loss is 0.0032118840608745813\n",
      "epoch: 9 step: 906, loss is 0.0008865395211614668\n",
      "epoch: 9 step: 907, loss is 0.00045298333861865103\n",
      "epoch: 9 step: 908, loss is 0.0010906512616202235\n",
      "epoch: 9 step: 909, loss is 0.0013309456408023834\n",
      "epoch: 9 step: 910, loss is 0.001222277176566422\n",
      "epoch: 9 step: 911, loss is 0.0027447440661489964\n",
      "epoch: 9 step: 912, loss is 0.0008339086780324578\n",
      "epoch: 9 step: 913, loss is 0.000549972930457443\n",
      "epoch: 9 step: 914, loss is 0.040206003934144974\n",
      "epoch: 9 step: 915, loss is 0.0071701654233038425\n",
      "epoch: 9 step: 916, loss is 0.001422908273525536\n",
      "epoch: 9 step: 917, loss is 9.116603177972138e-05\n",
      "epoch: 9 step: 918, loss is 0.0019483778160065413\n",
      "epoch: 9 step: 919, loss is 0.00013293440861161798\n",
      "epoch: 9 step: 920, loss is 0.16152121126651764\n",
      "epoch: 9 step: 921, loss is 4.360398816061206e-05\n",
      "epoch: 9 step: 922, loss is 0.00019496781169436872\n",
      "epoch: 9 step: 923, loss is 0.0023922964464873075\n",
      "epoch: 9 step: 924, loss is 0.20832188427448273\n",
      "epoch: 9 step: 925, loss is 0.0014372417936101556\n",
      "epoch: 9 step: 926, loss is 0.0011102865682914853\n",
      "epoch: 9 step: 927, loss is 0.00014925991126801819\n",
      "epoch: 9 step: 928, loss is 0.0005256125587038696\n",
      "epoch: 9 step: 929, loss is 0.014928588643670082\n",
      "epoch: 9 step: 930, loss is 0.0015470227226614952\n",
      "epoch: 9 step: 931, loss is 0.00032958798692561686\n",
      "epoch: 9 step: 932, loss is 0.0054329619742929935\n",
      "epoch: 9 step: 933, loss is 4.6698289224877954e-05\n",
      "epoch: 9 step: 934, loss is 0.050743553787469864\n",
      "epoch: 9 step: 935, loss is 0.0002513438812457025\n",
      "epoch: 9 step: 936, loss is 0.0003842217556666583\n",
      "epoch: 9 step: 937, loss is 0.0006556747248396277\n",
      "epoch: 9 step: 938, loss is 0.048210423439741135\n",
      "epoch: 9 step: 939, loss is 0.0006945431232452393\n",
      "epoch: 9 step: 940, loss is 7.326156628550962e-05\n",
      "epoch: 9 step: 941, loss is 0.00035174336517229676\n",
      "epoch: 9 step: 942, loss is 0.008158104494214058\n",
      "epoch: 9 step: 943, loss is 0.005898608826100826\n",
      "epoch: 9 step: 944, loss is 0.002920673694461584\n",
      "epoch: 9 step: 945, loss is 0.0007113257888704538\n",
      "epoch: 9 step: 946, loss is 0.00018627388635650277\n",
      "epoch: 9 step: 947, loss is 0.0003951360995415598\n",
      "epoch: 9 step: 948, loss is 0.0001963913528015837\n",
      "epoch: 9 step: 949, loss is 3.4362328733550385e-05\n",
      "epoch: 9 step: 950, loss is 0.05281737819314003\n",
      "epoch: 9 step: 951, loss is 0.002208912745118141\n",
      "epoch: 9 step: 952, loss is 0.003082851879298687\n",
      "epoch: 9 step: 953, loss is 0.003677529748529196\n",
      "epoch: 9 step: 954, loss is 0.01584988459944725\n",
      "epoch: 9 step: 955, loss is 1.6662585039739497e-05\n",
      "epoch: 9 step: 956, loss is 0.005705157294869423\n",
      "epoch: 9 step: 957, loss is 0.004151435568928719\n",
      "epoch: 9 step: 958, loss is 0.010832350701093674\n",
      "epoch: 9 step: 959, loss is 0.00031335200765170157\n",
      "epoch: 9 step: 960, loss is 0.0009321647812612355\n",
      "epoch: 9 step: 961, loss is 0.0006038903957232833\n",
      "epoch: 9 step: 962, loss is 0.013013509102165699\n",
      "epoch: 9 step: 963, loss is 0.0012175310403108597\n",
      "epoch: 9 step: 964, loss is 0.0900309681892395\n",
      "epoch: 9 step: 965, loss is 0.00017496512737125158\n",
      "epoch: 9 step: 966, loss is 0.002497059525921941\n",
      "epoch: 9 step: 967, loss is 0.0005507887108251452\n",
      "epoch: 9 step: 968, loss is 0.00021930453658569604\n",
      "epoch: 9 step: 969, loss is 0.0017175961984321475\n",
      "epoch: 9 step: 970, loss is 0.0017147567123174667\n",
      "epoch: 9 step: 971, loss is 0.0005230215028859675\n",
      "epoch: 9 step: 972, loss is 0.0007286886102519929\n",
      "epoch: 9 step: 973, loss is 8.352952136192471e-05\n",
      "epoch: 9 step: 974, loss is 0.00029624722083099186\n",
      "epoch: 9 step: 975, loss is 0.0006906241760589182\n",
      "epoch: 9 step: 976, loss is 0.002208814024925232\n",
      "epoch: 9 step: 977, loss is 0.0031840712763369083\n",
      "epoch: 9 step: 978, loss is 0.005133412778377533\n",
      "epoch: 9 step: 979, loss is 0.0009264417458325624\n",
      "epoch: 9 step: 980, loss is 2.4938526621554047e-05\n",
      "epoch: 9 step: 981, loss is 0.01303662359714508\n",
      "epoch: 9 step: 982, loss is 0.12265057116746902\n",
      "epoch: 9 step: 983, loss is 0.06403851509094238\n",
      "epoch: 9 step: 984, loss is 0.0010066009126603603\n",
      "epoch: 9 step: 985, loss is 0.00013544141256716102\n",
      "epoch: 9 step: 986, loss is 0.1200622022151947\n",
      "epoch: 9 step: 987, loss is 0.02080325223505497\n",
      "epoch: 9 step: 988, loss is 0.00044754755799658597\n",
      "epoch: 9 step: 989, loss is 0.0003534814459271729\n",
      "epoch: 9 step: 990, loss is 0.0013556859921664\n",
      "epoch: 9 step: 991, loss is 0.0008703458588570356\n",
      "epoch: 9 step: 992, loss is 0.00021225036471150815\n",
      "epoch: 9 step: 993, loss is 0.0025336965918540955\n",
      "epoch: 9 step: 994, loss is 0.026047976687550545\n",
      "epoch: 9 step: 995, loss is 0.0003239573270548135\n",
      "epoch: 9 step: 996, loss is 0.00021283549722284079\n",
      "epoch: 9 step: 997, loss is 0.015492160804569721\n",
      "epoch: 9 step: 998, loss is 0.002159767085686326\n",
      "epoch: 9 step: 999, loss is 3.526450018398464e-05\n",
      "epoch: 9 step: 1000, loss is 0.005344212055206299\n",
      "epoch: 9 step: 1001, loss is 1.6764022348070284e-06\n",
      "epoch: 9 step: 1002, loss is 0.021008817479014397\n",
      "epoch: 9 step: 1003, loss is 0.15697802603244781\n",
      "epoch: 9 step: 1004, loss is 0.0004302168090362102\n",
      "epoch: 9 step: 1005, loss is 0.0008571326616220176\n",
      "epoch: 9 step: 1006, loss is 0.0002010279131354764\n",
      "epoch: 9 step: 1007, loss is 4.441990313353017e-05\n",
      "epoch: 9 step: 1008, loss is 0.31640079617500305\n",
      "epoch: 9 step: 1009, loss is 0.001488597714342177\n",
      "epoch: 9 step: 1010, loss is 9.375977970194072e-05\n",
      "epoch: 9 step: 1011, loss is 0.00028558133635669947\n",
      "epoch: 9 step: 1012, loss is 0.003999718464910984\n",
      "epoch: 9 step: 1013, loss is 2.8427493816707283e-05\n",
      "epoch: 9 step: 1014, loss is 0.19681553542613983\n",
      "epoch: 9 step: 1015, loss is 0.002118639647960663\n",
      "epoch: 9 step: 1016, loss is 0.005285987164825201\n",
      "epoch: 9 step: 1017, loss is 0.0010244979057461023\n",
      "epoch: 9 step: 1018, loss is 0.06348587572574615\n",
      "epoch: 9 step: 1019, loss is 0.020193804055452347\n",
      "epoch: 9 step: 1020, loss is 0.0025973671581596136\n",
      "epoch: 9 step: 1021, loss is 0.008026053197681904\n",
      "epoch: 9 step: 1022, loss is 0.014826711267232895\n",
      "epoch: 9 step: 1023, loss is 0.028044939041137695\n",
      "epoch: 9 step: 1024, loss is 0.032671891152858734\n",
      "epoch: 9 step: 1025, loss is 0.0013825604692101479\n",
      "epoch: 9 step: 1026, loss is 0.00253659812733531\n",
      "epoch: 9 step: 1027, loss is 0.0184043999761343\n",
      "epoch: 9 step: 1028, loss is 0.009511972777545452\n",
      "epoch: 9 step: 1029, loss is 0.0031525511294603348\n",
      "epoch: 9 step: 1030, loss is 9.257289639208466e-05\n",
      "epoch: 9 step: 1031, loss is 0.03177589550614357\n",
      "epoch: 9 step: 1032, loss is 0.024037061259150505\n",
      "epoch: 9 step: 1033, loss is 7.917989569250494e-05\n",
      "epoch: 9 step: 1034, loss is 0.001113847247324884\n",
      "epoch: 9 step: 1035, loss is 0.009864911437034607\n",
      "epoch: 9 step: 1036, loss is 0.0013090139254927635\n",
      "epoch: 9 step: 1037, loss is 6.517770088976249e-05\n",
      "epoch: 9 step: 1038, loss is 0.001988200470805168\n",
      "epoch: 9 step: 1039, loss is 0.011180022731423378\n",
      "epoch: 9 step: 1040, loss is 2.7853140636580065e-05\n",
      "epoch: 9 step: 1041, loss is 0.017115484923124313\n",
      "epoch: 9 step: 1042, loss is 0.0006441667210310698\n",
      "epoch: 9 step: 1043, loss is 0.0005059828399680555\n",
      "epoch: 9 step: 1044, loss is 0.07228923588991165\n",
      "epoch: 9 step: 1045, loss is 0.10429912060499191\n",
      "epoch: 9 step: 1046, loss is 3.3640433684922755e-05\n",
      "epoch: 9 step: 1047, loss is 0.0016614150954410434\n",
      "epoch: 9 step: 1048, loss is 0.0011770859127864242\n",
      "epoch: 9 step: 1049, loss is 0.00016918889014050364\n",
      "epoch: 9 step: 1050, loss is 0.05604015663266182\n",
      "epoch: 9 step: 1051, loss is 0.0005536655662581325\n",
      "epoch: 9 step: 1052, loss is 0.0001240155252162367\n",
      "epoch: 9 step: 1053, loss is 0.0010965759865939617\n",
      "epoch: 9 step: 1054, loss is 0.01846121996641159\n",
      "epoch: 9 step: 1055, loss is 6.639095227001235e-05\n",
      "epoch: 9 step: 1056, loss is 0.012575970962643623\n",
      "epoch: 9 step: 1057, loss is 0.0005793888121843338\n",
      "epoch: 9 step: 1058, loss is 5.561420402955264e-05\n",
      "epoch: 9 step: 1059, loss is 0.001971636898815632\n",
      "epoch: 9 step: 1060, loss is 0.0003312363405711949\n",
      "epoch: 9 step: 1061, loss is 0.01572052761912346\n",
      "epoch: 9 step: 1062, loss is 0.0010194084607064724\n",
      "epoch: 9 step: 1063, loss is 0.003503534710034728\n",
      "epoch: 9 step: 1064, loss is 0.0020479424856603146\n",
      "epoch: 9 step: 1065, loss is 0.0002258176391478628\n",
      "epoch: 9 step: 1066, loss is 0.0006402363651432097\n",
      "epoch: 9 step: 1067, loss is 0.0006988918757997453\n",
      "epoch: 9 step: 1068, loss is 0.0002318238402949646\n",
      "epoch: 9 step: 1069, loss is 0.000951997353695333\n",
      "epoch: 9 step: 1070, loss is 0.0037736203521490097\n",
      "epoch: 9 step: 1071, loss is 0.031130550429224968\n",
      "epoch: 9 step: 1072, loss is 0.03191525489091873\n",
      "epoch: 9 step: 1073, loss is 0.0002698704192880541\n",
      "epoch: 9 step: 1074, loss is 0.0880020409822464\n",
      "epoch: 9 step: 1075, loss is 0.0010324721224606037\n",
      "epoch: 9 step: 1076, loss is 0.009637595154345036\n",
      "epoch: 9 step: 1077, loss is 0.03475458174943924\n",
      "epoch: 9 step: 1078, loss is 0.0010033966973423958\n",
      "epoch: 9 step: 1079, loss is 0.0026926829013973475\n",
      "epoch: 9 step: 1080, loss is 0.003957776818424463\n",
      "epoch: 9 step: 1081, loss is 0.00018737367645371705\n",
      "epoch: 9 step: 1082, loss is 0.0031615537591278553\n",
      "epoch: 9 step: 1083, loss is 0.0005151241784915328\n",
      "epoch: 9 step: 1084, loss is 0.00023034884361550212\n",
      "epoch: 9 step: 1085, loss is 0.09454117715358734\n",
      "epoch: 9 step: 1086, loss is 0.0011812745360657573\n",
      "epoch: 9 step: 1087, loss is 0.0006467873463407159\n",
      "epoch: 9 step: 1088, loss is 0.019750934094190598\n",
      "epoch: 9 step: 1089, loss is 0.0001036466273944825\n",
      "epoch: 9 step: 1090, loss is 0.0007131535094231367\n",
      "epoch: 9 step: 1091, loss is 1.8121245375368744e-05\n",
      "epoch: 9 step: 1092, loss is 0.0003466808993835002\n",
      "epoch: 9 step: 1093, loss is 0.007767952512949705\n",
      "epoch: 9 step: 1094, loss is 0.0014893788611516356\n",
      "epoch: 9 step: 1095, loss is 0.021114347502589226\n",
      "epoch: 9 step: 1096, loss is 0.000551355944480747\n",
      "epoch: 9 step: 1097, loss is 0.0014763132203370333\n",
      "epoch: 9 step: 1098, loss is 0.0011976801324635744\n",
      "epoch: 9 step: 1099, loss is 4.597310908138752e-05\n",
      "epoch: 9 step: 1100, loss is 0.00521488580852747\n",
      "epoch: 9 step: 1101, loss is 0.0040762899443507195\n",
      "epoch: 9 step: 1102, loss is 0.0006645004614256322\n",
      "epoch: 9 step: 1103, loss is 0.012036293745040894\n",
      "epoch: 9 step: 1104, loss is 0.05227765813469887\n",
      "epoch: 9 step: 1105, loss is 3.9949005440576e-05\n",
      "epoch: 9 step: 1106, loss is 6.4226860558846965e-06\n",
      "epoch: 9 step: 1107, loss is 0.007208768278360367\n",
      "epoch: 9 step: 1108, loss is 0.0031674837227910757\n",
      "epoch: 9 step: 1109, loss is 0.005204839166253805\n",
      "epoch: 9 step: 1110, loss is 0.020484544336795807\n",
      "epoch: 9 step: 1111, loss is 7.385030767181888e-05\n",
      "epoch: 9 step: 1112, loss is 0.0002660205645952374\n",
      "epoch: 9 step: 1113, loss is 4.5846121793147177e-05\n",
      "epoch: 9 step: 1114, loss is 0.0006550033576786518\n",
      "epoch: 9 step: 1115, loss is 1.0852077139134053e-05\n",
      "epoch: 9 step: 1116, loss is 0.0008796977926976979\n",
      "epoch: 9 step: 1117, loss is 0.000138082483317703\n",
      "epoch: 9 step: 1118, loss is 0.009332498535513878\n",
      "epoch: 9 step: 1119, loss is 8.401140803471208e-05\n",
      "epoch: 9 step: 1120, loss is 3.334703069413081e-05\n",
      "epoch: 9 step: 1121, loss is 0.17306697368621826\n",
      "epoch: 9 step: 1122, loss is 4.164921119809151e-05\n",
      "epoch: 9 step: 1123, loss is 4.9316135118715465e-05\n",
      "epoch: 9 step: 1124, loss is 0.0013643283164128661\n",
      "epoch: 9 step: 1125, loss is 0.0002851688186638057\n",
      "epoch: 9 step: 1126, loss is 0.00018471931980457157\n",
      "epoch: 9 step: 1127, loss is 0.018820716068148613\n",
      "epoch: 9 step: 1128, loss is 0.0006490825326181948\n",
      "epoch: 9 step: 1129, loss is 0.08371107280254364\n",
      "epoch: 9 step: 1130, loss is 0.014211495406925678\n",
      "epoch: 9 step: 1131, loss is 0.018694397062063217\n",
      "epoch: 9 step: 1132, loss is 0.03573981672525406\n",
      "epoch: 9 step: 1133, loss is 9.252924064639956e-05\n",
      "epoch: 9 step: 1134, loss is 0.022943321615457535\n",
      "epoch: 9 step: 1135, loss is 0.00026399234775453806\n",
      "epoch: 9 step: 1136, loss is 1.3011368537263479e-05\n",
      "epoch: 9 step: 1137, loss is 0.0191060621291399\n",
      "epoch: 9 step: 1138, loss is 0.003095327876508236\n",
      "epoch: 9 step: 1139, loss is 0.000727490521967411\n",
      "epoch: 9 step: 1140, loss is 0.0014423264656215906\n",
      "epoch: 9 step: 1141, loss is 0.10010427236557007\n",
      "epoch: 9 step: 1142, loss is 0.004729131236672401\n",
      "epoch: 9 step: 1143, loss is 0.001400934997946024\n",
      "epoch: 9 step: 1144, loss is 0.0006677618948742747\n",
      "epoch: 9 step: 1145, loss is 1.4345556337502785e-05\n",
      "epoch: 9 step: 1146, loss is 0.0033262271899729967\n",
      "epoch: 9 step: 1147, loss is 0.005575535818934441\n",
      "epoch: 9 step: 1148, loss is 0.0003763666027225554\n",
      "epoch: 9 step: 1149, loss is 0.0006553065031766891\n",
      "epoch: 9 step: 1150, loss is 0.020365940406918526\n",
      "epoch: 9 step: 1151, loss is 0.0004667044850066304\n",
      "epoch: 9 step: 1152, loss is 0.007360982242971659\n",
      "epoch: 9 step: 1153, loss is 0.0015637242468073964\n",
      "epoch: 9 step: 1154, loss is 2.2675914806313813e-05\n",
      "epoch: 9 step: 1155, loss is 4.210627230349928e-05\n",
      "epoch: 9 step: 1156, loss is 0.02386358566582203\n",
      "epoch: 9 step: 1157, loss is 0.00063799211056903\n",
      "epoch: 9 step: 1158, loss is 0.004030776210129261\n",
      "epoch: 9 step: 1159, loss is 0.013370925560593605\n",
      "epoch: 9 step: 1160, loss is 0.08506250381469727\n",
      "epoch: 9 step: 1161, loss is 0.00015263486420735717\n",
      "epoch: 9 step: 1162, loss is 0.011883370578289032\n",
      "epoch: 9 step: 1163, loss is 2.4093937099678442e-05\n",
      "epoch: 9 step: 1164, loss is 0.00010466585808899254\n",
      "epoch: 9 step: 1165, loss is 0.00021626603847835213\n",
      "epoch: 9 step: 1166, loss is 0.07011368870735168\n",
      "epoch: 9 step: 1167, loss is 0.002775917062535882\n",
      "epoch: 9 step: 1168, loss is 0.00024780802777968347\n",
      "epoch: 9 step: 1169, loss is 0.0001781771279638633\n",
      "epoch: 9 step: 1170, loss is 0.007491310592740774\n",
      "epoch: 9 step: 1171, loss is 0.0021733324974775314\n",
      "epoch: 9 step: 1172, loss is 0.00034060407779179513\n",
      "epoch: 9 step: 1173, loss is 0.0018339576199650764\n",
      "epoch: 9 step: 1174, loss is 0.0005543779698200524\n",
      "epoch: 9 step: 1175, loss is 0.004748482722789049\n",
      "epoch: 9 step: 1176, loss is 0.0019629839807748795\n",
      "epoch: 9 step: 1177, loss is 4.1227260226150975e-05\n",
      "epoch: 9 step: 1178, loss is 0.0011269471142441034\n",
      "epoch: 9 step: 1179, loss is 0.030689498409628868\n",
      "epoch: 9 step: 1180, loss is 0.0005098028923384845\n",
      "epoch: 9 step: 1181, loss is 0.00018490198999643326\n",
      "epoch: 9 step: 1182, loss is 0.022902457043528557\n",
      "epoch: 9 step: 1183, loss is 0.0005701511981897056\n",
      "epoch: 9 step: 1184, loss is 0.5251583456993103\n",
      "epoch: 9 step: 1185, loss is 0.04125404357910156\n",
      "epoch: 9 step: 1186, loss is 0.0024666308891028166\n",
      "epoch: 9 step: 1187, loss is 0.00025141023797914386\n",
      "epoch: 9 step: 1188, loss is 0.04830915108323097\n",
      "epoch: 9 step: 1189, loss is 0.0005091691855341196\n",
      "epoch: 9 step: 1190, loss is 2.8766056857421063e-05\n",
      "epoch: 9 step: 1191, loss is 0.03971192613244057\n",
      "epoch: 9 step: 1192, loss is 7.442267087753862e-05\n",
      "epoch: 9 step: 1193, loss is 0.0017157443799078465\n",
      "epoch: 9 step: 1194, loss is 0.0004276651015970856\n",
      "epoch: 9 step: 1195, loss is 0.017222795635461807\n",
      "epoch: 9 step: 1196, loss is 0.0019330047070980072\n",
      "epoch: 9 step: 1197, loss is 0.0014100789558142424\n",
      "epoch: 9 step: 1198, loss is 0.006392297800630331\n",
      "epoch: 9 step: 1199, loss is 0.01334170252084732\n",
      "epoch: 9 step: 1200, loss is 0.00016934916493482888\n",
      "epoch: 9 step: 1201, loss is 0.005722657311707735\n",
      "epoch: 9 step: 1202, loss is 0.17377954721450806\n",
      "epoch: 9 step: 1203, loss is 0.0015737172216176987\n",
      "epoch: 9 step: 1204, loss is 0.0001416326267644763\n",
      "epoch: 9 step: 1205, loss is 0.02257899008691311\n",
      "epoch: 9 step: 1206, loss is 0.004918905906379223\n",
      "epoch: 9 step: 1207, loss is 0.009663252159953117\n",
      "epoch: 9 step: 1208, loss is 0.0002377138298470527\n",
      "epoch: 9 step: 1209, loss is 0.012238573282957077\n",
      "epoch: 9 step: 1210, loss is 0.04203838109970093\n",
      "epoch: 9 step: 1211, loss is 0.12452472746372223\n",
      "epoch: 9 step: 1212, loss is 6.558484892593697e-05\n",
      "epoch: 9 step: 1213, loss is 0.0019352667732164264\n",
      "epoch: 9 step: 1214, loss is 5.6326720368815586e-05\n",
      "epoch: 9 step: 1215, loss is 0.004117885604500771\n",
      "epoch: 9 step: 1216, loss is 0.0005987497861497104\n",
      "epoch: 9 step: 1217, loss is 0.008012223057448864\n",
      "epoch: 9 step: 1218, loss is 0.0031289581675082445\n",
      "epoch: 9 step: 1219, loss is 0.0073556057177484035\n",
      "epoch: 9 step: 1220, loss is 0.00013125532132107764\n",
      "epoch: 9 step: 1221, loss is 5.827107816003263e-05\n",
      "epoch: 9 step: 1222, loss is 0.0010923809604719281\n",
      "epoch: 9 step: 1223, loss is 0.0011841683881357312\n",
      "epoch: 9 step: 1224, loss is 0.00035689014475792646\n",
      "epoch: 9 step: 1225, loss is 0.0021989471279084682\n",
      "epoch: 9 step: 1226, loss is 0.307079017162323\n",
      "epoch: 9 step: 1227, loss is 0.17557895183563232\n",
      "epoch: 9 step: 1228, loss is 0.00028160365764051676\n",
      "epoch: 9 step: 1229, loss is 0.015209982171654701\n",
      "epoch: 9 step: 1230, loss is 0.013472905382514\n",
      "epoch: 9 step: 1231, loss is 0.0009855948155745864\n",
      "epoch: 9 step: 1232, loss is 0.0004649232141673565\n",
      "epoch: 9 step: 1233, loss is 0.1142081543803215\n",
      "epoch: 9 step: 1234, loss is 0.0032374884467571974\n",
      "epoch: 9 step: 1235, loss is 0.00024000917619559914\n",
      "epoch: 9 step: 1236, loss is 0.0002508297620806843\n",
      "epoch: 9 step: 1237, loss is 4.030646960018203e-05\n",
      "epoch: 9 step: 1238, loss is 0.0013134779874235392\n",
      "epoch: 9 step: 1239, loss is 0.00010887009557336569\n",
      "epoch: 9 step: 1240, loss is 0.011748109944164753\n",
      "epoch: 9 step: 1241, loss is 0.010863459669053555\n",
      "epoch: 9 step: 1242, loss is 0.06828966736793518\n",
      "epoch: 9 step: 1243, loss is 0.0032053422182798386\n",
      "epoch: 9 step: 1244, loss is 3.958936213166453e-05\n",
      "epoch: 9 step: 1245, loss is 0.0005266137304715812\n",
      "epoch: 9 step: 1246, loss is 0.1654626727104187\n",
      "epoch: 9 step: 1247, loss is 0.0011272395495325327\n",
      "epoch: 9 step: 1248, loss is 0.046130068600177765\n",
      "epoch: 9 step: 1249, loss is 0.012662987224757671\n",
      "epoch: 9 step: 1250, loss is 0.017139777541160583\n",
      "epoch: 9 step: 1251, loss is 0.0016170383896678686\n",
      "epoch: 9 step: 1252, loss is 0.03569641336798668\n",
      "epoch: 9 step: 1253, loss is 0.00014678454317618161\n",
      "epoch: 9 step: 1254, loss is 0.003189477836713195\n",
      "epoch: 9 step: 1255, loss is 0.00010732517694123089\n",
      "epoch: 9 step: 1256, loss is 0.000736110785510391\n",
      "epoch: 9 step: 1257, loss is 0.0037617210764437914\n",
      "epoch: 9 step: 1258, loss is 0.004172849003225565\n",
      "epoch: 9 step: 1259, loss is 0.0017709712265059352\n",
      "epoch: 9 step: 1260, loss is 0.0034894610289484262\n",
      "epoch: 9 step: 1261, loss is 0.006897251587361097\n",
      "epoch: 9 step: 1262, loss is 0.0003547378582879901\n",
      "epoch: 9 step: 1263, loss is 0.007287285756319761\n",
      "epoch: 9 step: 1264, loss is 0.00035090232267975807\n",
      "epoch: 9 step: 1265, loss is 0.041318781673908234\n",
      "epoch: 9 step: 1266, loss is 0.00027672835858538747\n",
      "epoch: 9 step: 1267, loss is 0.014713717624545097\n",
      "epoch: 9 step: 1268, loss is 0.0004517741035670042\n",
      "epoch: 9 step: 1269, loss is 0.003254198469221592\n",
      "epoch: 9 step: 1270, loss is 0.01246910635381937\n",
      "epoch: 9 step: 1271, loss is 0.0024873437359929085\n",
      "epoch: 9 step: 1272, loss is 0.0013411249965429306\n",
      "epoch: 9 step: 1273, loss is 0.009841658174991608\n",
      "epoch: 9 step: 1274, loss is 0.004204347729682922\n",
      "epoch: 9 step: 1275, loss is 0.008377274498343468\n",
      "epoch: 9 step: 1276, loss is 0.002586337272077799\n",
      "epoch: 9 step: 1277, loss is 0.039985813200473785\n",
      "epoch: 9 step: 1278, loss is 0.0037862746976315975\n",
      "epoch: 9 step: 1279, loss is 0.00020431476878002286\n",
      "epoch: 9 step: 1280, loss is 0.1795109659433365\n",
      "epoch: 9 step: 1281, loss is 0.009443388320505619\n",
      "epoch: 9 step: 1282, loss is 0.0045435791835188866\n",
      "epoch: 9 step: 1283, loss is 0.003855711780488491\n",
      "epoch: 9 step: 1284, loss is 0.0007184345740824938\n",
      "epoch: 9 step: 1285, loss is 0.010727445594966412\n",
      "epoch: 9 step: 1286, loss is 0.0007348870858550072\n",
      "epoch: 9 step: 1287, loss is 0.0009157348540611565\n",
      "epoch: 9 step: 1288, loss is 0.060908153653144836\n",
      "epoch: 9 step: 1289, loss is 0.05329660698771477\n",
      "epoch: 9 step: 1290, loss is 0.001983347348868847\n",
      "epoch: 9 step: 1291, loss is 0.0028338232077658176\n",
      "epoch: 9 step: 1292, loss is 0.026571612805128098\n",
      "epoch: 9 step: 1293, loss is 0.05284389108419418\n",
      "epoch: 9 step: 1294, loss is 0.0001374076964566484\n",
      "epoch: 9 step: 1295, loss is 0.007559900637716055\n",
      "epoch: 9 step: 1296, loss is 0.17202486097812653\n",
      "epoch: 9 step: 1297, loss is 0.023878438398241997\n",
      "epoch: 9 step: 1298, loss is 0.0022960698697715998\n",
      "epoch: 9 step: 1299, loss is 9.931199747370556e-05\n",
      "epoch: 9 step: 1300, loss is 0.001734886784106493\n",
      "epoch: 9 step: 1301, loss is 0.00045670795952901244\n",
      "epoch: 9 step: 1302, loss is 0.0011209426447749138\n",
      "epoch: 9 step: 1303, loss is 0.008071115240454674\n",
      "epoch: 9 step: 1304, loss is 0.0008117302786558867\n",
      "epoch: 9 step: 1305, loss is 0.004215874243527651\n",
      "epoch: 9 step: 1306, loss is 0.0062610381282866\n",
      "epoch: 9 step: 1307, loss is 0.00034077634336426854\n",
      "epoch: 9 step: 1308, loss is 0.016554884612560272\n",
      "epoch: 9 step: 1309, loss is 0.0007407877128571272\n",
      "epoch: 9 step: 1310, loss is 0.003450800897553563\n",
      "epoch: 9 step: 1311, loss is 0.000570152245927602\n",
      "epoch: 9 step: 1312, loss is 0.05778862535953522\n",
      "epoch: 9 step: 1313, loss is 0.04580429196357727\n",
      "epoch: 9 step: 1314, loss is 0.0005110512138344347\n",
      "epoch: 9 step: 1315, loss is 0.0009588874527253211\n",
      "epoch: 9 step: 1316, loss is 0.00016794234397821128\n",
      "epoch: 9 step: 1317, loss is 0.020133253186941147\n",
      "epoch: 9 step: 1318, loss is 0.00019847118528559804\n",
      "epoch: 9 step: 1319, loss is 0.17211434245109558\n",
      "epoch: 9 step: 1320, loss is 0.029708698391914368\n",
      "epoch: 9 step: 1321, loss is 0.11660478264093399\n",
      "epoch: 9 step: 1322, loss is 6.691471207886934e-05\n",
      "epoch: 9 step: 1323, loss is 0.006887699943035841\n",
      "epoch: 9 step: 1324, loss is 0.038048725575208664\n",
      "epoch: 9 step: 1325, loss is 0.00016118126222863793\n",
      "epoch: 9 step: 1326, loss is 0.0011429332662373781\n",
      "epoch: 9 step: 1327, loss is 0.00035982520785182714\n",
      "epoch: 9 step: 1328, loss is 0.007007276639342308\n",
      "epoch: 9 step: 1329, loss is 0.050118569284677505\n",
      "epoch: 9 step: 1330, loss is 0.016424356028437614\n",
      "epoch: 9 step: 1331, loss is 0.0008576972759328783\n",
      "epoch: 9 step: 1332, loss is 0.00022174892365001142\n",
      "epoch: 9 step: 1333, loss is 0.019084738567471504\n",
      "epoch: 9 step: 1334, loss is 0.0026897278148680925\n",
      "epoch: 9 step: 1335, loss is 7.877202006056905e-05\n",
      "epoch: 9 step: 1336, loss is 0.004856941755861044\n",
      "epoch: 9 step: 1337, loss is 0.0075423880480229855\n",
      "epoch: 9 step: 1338, loss is 0.0027206032536923885\n",
      "epoch: 9 step: 1339, loss is 6.933490658411756e-05\n",
      "epoch: 9 step: 1340, loss is 0.005105650518089533\n",
      "epoch: 9 step: 1341, loss is 0.0010681336279958487\n",
      "epoch: 9 step: 1342, loss is 0.003477053251117468\n",
      "epoch: 9 step: 1343, loss is 0.07376175373792648\n",
      "epoch: 9 step: 1344, loss is 0.0037698738742619753\n",
      "epoch: 9 step: 1345, loss is 0.1409134566783905\n",
      "epoch: 9 step: 1346, loss is 0.0038924473337829113\n",
      "epoch: 9 step: 1347, loss is 0.0006120348698459566\n",
      "epoch: 9 step: 1348, loss is 0.008607794530689716\n",
      "epoch: 9 step: 1349, loss is 0.0019690936896950006\n",
      "epoch: 9 step: 1350, loss is 0.000134225920191966\n",
      "epoch: 9 step: 1351, loss is 9.793622302822769e-05\n",
      "epoch: 9 step: 1352, loss is 0.012784811668097973\n",
      "epoch: 9 step: 1353, loss is 8.708220411790535e-05\n",
      "epoch: 9 step: 1354, loss is 0.00020757914171554148\n",
      "epoch: 9 step: 1355, loss is 0.004761457908898592\n",
      "epoch: 9 step: 1356, loss is 0.03354394808411598\n",
      "epoch: 9 step: 1357, loss is 0.004549510311335325\n",
      "epoch: 9 step: 1358, loss is 0.002113250084221363\n",
      "epoch: 9 step: 1359, loss is 0.00027396719087846577\n",
      "epoch: 9 step: 1360, loss is 0.015268273651599884\n",
      "epoch: 9 step: 1361, loss is 0.07936213165521622\n",
      "epoch: 9 step: 1362, loss is 0.005769542884081602\n",
      "epoch: 9 step: 1363, loss is 0.0800815150141716\n",
      "epoch: 9 step: 1364, loss is 9.635727474233136e-05\n",
      "epoch: 9 step: 1365, loss is 0.0003299538802821189\n",
      "epoch: 9 step: 1366, loss is 0.0031609246507287025\n",
      "epoch: 9 step: 1367, loss is 2.580683030828368e-05\n",
      "epoch: 9 step: 1368, loss is 3.7797341065015644e-05\n",
      "epoch: 9 step: 1369, loss is 0.0003871096996590495\n",
      "epoch: 9 step: 1370, loss is 0.019568145275115967\n",
      "epoch: 9 step: 1371, loss is 0.014305489137768745\n",
      "epoch: 9 step: 1372, loss is 4.990882007405162e-05\n",
      "epoch: 9 step: 1373, loss is 0.0015087885549291968\n",
      "epoch: 9 step: 1374, loss is 0.0010352215031161904\n",
      "epoch: 9 step: 1375, loss is 0.23462380468845367\n",
      "epoch: 9 step: 1376, loss is 0.25351694226264954\n",
      "epoch: 9 step: 1377, loss is 0.0036364695988595486\n",
      "epoch: 9 step: 1378, loss is 0.0004813712148461491\n",
      "epoch: 9 step: 1379, loss is 0.11558592319488525\n",
      "epoch: 9 step: 1380, loss is 0.001480848528444767\n",
      "epoch: 9 step: 1381, loss is 0.02033046819269657\n",
      "epoch: 9 step: 1382, loss is 0.0001451624557375908\n",
      "epoch: 9 step: 1383, loss is 0.06169399246573448\n",
      "epoch: 9 step: 1384, loss is 0.0011550692142918706\n",
      "epoch: 9 step: 1385, loss is 0.00019235798390582204\n",
      "epoch: 9 step: 1386, loss is 0.0022957914043217897\n",
      "epoch: 9 step: 1387, loss is 0.004508212674409151\n",
      "epoch: 9 step: 1388, loss is 0.02379339374601841\n",
      "epoch: 9 step: 1389, loss is 0.1092546284198761\n",
      "epoch: 9 step: 1390, loss is 0.0006935944547876716\n",
      "epoch: 9 step: 1391, loss is 0.09470539540052414\n",
      "epoch: 9 step: 1392, loss is 0.0009623717050999403\n",
      "epoch: 9 step: 1393, loss is 0.00041961774695664644\n",
      "epoch: 9 step: 1394, loss is 0.019868778064846992\n",
      "epoch: 9 step: 1395, loss is 0.007104102056473494\n",
      "epoch: 9 step: 1396, loss is 0.007742404472082853\n",
      "epoch: 9 step: 1397, loss is 0.00868326798081398\n",
      "epoch: 9 step: 1398, loss is 0.020426016300916672\n",
      "epoch: 9 step: 1399, loss is 0.09440536051988602\n",
      "epoch: 9 step: 1400, loss is 0.003812688635662198\n",
      "epoch: 9 step: 1401, loss is 0.0014440144877880812\n",
      "epoch: 9 step: 1402, loss is 0.004926466848701239\n",
      "epoch: 9 step: 1403, loss is 0.01820201240479946\n",
      "epoch: 9 step: 1404, loss is 0.023086538538336754\n",
      "epoch: 9 step: 1405, loss is 0.0031731929630041122\n",
      "epoch: 9 step: 1406, loss is 0.004070085473358631\n",
      "epoch: 9 step: 1407, loss is 0.002748659113422036\n",
      "epoch: 9 step: 1408, loss is 0.004995875060558319\n",
      "epoch: 9 step: 1409, loss is 0.07535523176193237\n",
      "epoch: 9 step: 1410, loss is 0.0009208621922880411\n",
      "epoch: 9 step: 1411, loss is 0.0003075341519434005\n",
      "epoch: 9 step: 1412, loss is 0.0030363353434950113\n",
      "epoch: 9 step: 1413, loss is 0.01696658506989479\n",
      "epoch: 9 step: 1414, loss is 0.020256012678146362\n",
      "epoch: 9 step: 1415, loss is 0.0002366643602726981\n",
      "epoch: 9 step: 1416, loss is 5.637291178572923e-05\n",
      "epoch: 9 step: 1417, loss is 0.0421348437666893\n",
      "epoch: 9 step: 1418, loss is 0.0005507558234967291\n",
      "epoch: 9 step: 1419, loss is 0.0004762177704833448\n",
      "epoch: 9 step: 1420, loss is 0.014176933094859123\n",
      "epoch: 9 step: 1421, loss is 0.007905798964202404\n",
      "epoch: 9 step: 1422, loss is 0.00035796689917333424\n",
      "epoch: 9 step: 1423, loss is 0.013674875721335411\n",
      "epoch: 9 step: 1424, loss is 0.0015465121250599623\n",
      "epoch: 9 step: 1425, loss is 0.05029726028442383\n",
      "epoch: 9 step: 1426, loss is 0.00021020548592787236\n",
      "epoch: 9 step: 1427, loss is 0.002912466647103429\n",
      "epoch: 9 step: 1428, loss is 3.608117549447343e-05\n",
      "epoch: 9 step: 1429, loss is 0.3328656256198883\n",
      "epoch: 9 step: 1430, loss is 0.007730603218078613\n",
      "epoch: 9 step: 1431, loss is 0.00028294138610363007\n",
      "epoch: 9 step: 1432, loss is 0.007104817312210798\n",
      "epoch: 9 step: 1433, loss is 0.00029362901113927364\n",
      "epoch: 9 step: 1434, loss is 0.0010124457767233253\n",
      "epoch: 9 step: 1435, loss is 0.10940014570951462\n",
      "epoch: 9 step: 1436, loss is 0.00010546350677032024\n",
      "epoch: 9 step: 1437, loss is 0.00115079281385988\n",
      "epoch: 9 step: 1438, loss is 0.04815398156642914\n",
      "epoch: 9 step: 1439, loss is 0.0017253124387934804\n",
      "epoch: 9 step: 1440, loss is 0.001039246330037713\n",
      "epoch: 9 step: 1441, loss is 0.11234645545482635\n",
      "epoch: 9 step: 1442, loss is 0.006570764817297459\n",
      "epoch: 9 step: 1443, loss is 0.0014347252435982227\n",
      "epoch: 9 step: 1444, loss is 0.03522880747914314\n",
      "epoch: 9 step: 1445, loss is 0.03608105331659317\n",
      "epoch: 9 step: 1446, loss is 0.007200645282864571\n",
      "epoch: 9 step: 1447, loss is 7.020369957899675e-05\n",
      "epoch: 9 step: 1448, loss is 0.00398640101775527\n",
      "epoch: 9 step: 1449, loss is 0.02461734600365162\n",
      "epoch: 9 step: 1450, loss is 0.008569934405386448\n",
      "epoch: 9 step: 1451, loss is 0.0005664736963808537\n",
      "epoch: 9 step: 1452, loss is 0.05119888111948967\n",
      "epoch: 9 step: 1453, loss is 0.39870789647102356\n",
      "epoch: 9 step: 1454, loss is 0.0081219132989645\n",
      "epoch: 9 step: 1455, loss is 3.1470772228203714e-05\n",
      "epoch: 9 step: 1456, loss is 6.954163836780936e-05\n",
      "epoch: 9 step: 1457, loss is 0.0924544483423233\n",
      "epoch: 9 step: 1458, loss is 0.0021439306437969208\n",
      "epoch: 9 step: 1459, loss is 0.00864268746227026\n",
      "epoch: 9 step: 1460, loss is 0.0005142447771504521\n",
      "epoch: 9 step: 1461, loss is 0.0070273252204060555\n",
      "epoch: 9 step: 1462, loss is 0.0010472012218087912\n",
      "epoch: 9 step: 1463, loss is 0.0030022969003766775\n",
      "epoch: 9 step: 1464, loss is 0.002436951268464327\n",
      "epoch: 9 step: 1465, loss is 0.0017229007789865136\n",
      "epoch: 9 step: 1466, loss is 0.027904300019145012\n",
      "epoch: 9 step: 1467, loss is 0.0009742506081238389\n",
      "epoch: 9 step: 1468, loss is 0.006126422435045242\n",
      "epoch: 9 step: 1469, loss is 0.0026522534899413586\n",
      "epoch: 9 step: 1470, loss is 0.0007005349616520107\n",
      "epoch: 9 step: 1471, loss is 0.004138459917157888\n",
      "epoch: 9 step: 1472, loss is 0.01821441948413849\n",
      "epoch: 9 step: 1473, loss is 0.038448434323072433\n",
      "epoch: 9 step: 1474, loss is 0.0009095066925510764\n",
      "epoch: 9 step: 1475, loss is 0.0009534144774079323\n",
      "epoch: 9 step: 1476, loss is 0.015404527075588703\n",
      "epoch: 9 step: 1477, loss is 0.00785442627966404\n",
      "epoch: 9 step: 1478, loss is 0.18596889078617096\n",
      "epoch: 9 step: 1479, loss is 0.004703497514128685\n",
      "epoch: 9 step: 1480, loss is 0.001976173836737871\n",
      "epoch: 9 step: 1481, loss is 0.0016240171389654279\n",
      "epoch: 9 step: 1482, loss is 0.009068600833415985\n",
      "epoch: 9 step: 1483, loss is 0.0007941974326968193\n",
      "epoch: 9 step: 1484, loss is 0.0018667546100914478\n",
      "epoch: 9 step: 1485, loss is 0.004837652202695608\n",
      "epoch: 9 step: 1486, loss is 0.01421067863702774\n",
      "epoch: 9 step: 1487, loss is 0.0002812183811329305\n",
      "epoch: 9 step: 1488, loss is 0.009835549630224705\n",
      "epoch: 9 step: 1489, loss is 0.01734163984656334\n",
      "epoch: 9 step: 1490, loss is 0.006970243528485298\n",
      "epoch: 9 step: 1491, loss is 0.026593942195177078\n",
      "epoch: 9 step: 1492, loss is 0.08622152358293533\n",
      "epoch: 9 step: 1493, loss is 0.00045786562259308994\n",
      "epoch: 9 step: 1494, loss is 0.01876656524837017\n",
      "epoch: 9 step: 1495, loss is 0.0001987431023735553\n",
      "epoch: 9 step: 1496, loss is 0.17357268929481506\n",
      "epoch: 9 step: 1497, loss is 0.04231198504567146\n",
      "epoch: 9 step: 1498, loss is 0.0019402200123295188\n",
      "epoch: 9 step: 1499, loss is 0.007552567403763533\n",
      "epoch: 9 step: 1500, loss is 0.001303920871578157\n",
      "epoch: 9 step: 1501, loss is 0.007902394980192184\n",
      "epoch: 9 step: 1502, loss is 0.021960454061627388\n",
      "epoch: 9 step: 1503, loss is 0.12641842663288116\n",
      "epoch: 9 step: 1504, loss is 0.0064094699919223785\n",
      "epoch: 9 step: 1505, loss is 0.00039761463995091617\n",
      "epoch: 9 step: 1506, loss is 0.0629468560218811\n",
      "epoch: 9 step: 1507, loss is 0.0012983131455257535\n",
      "epoch: 9 step: 1508, loss is 0.005913021508604288\n",
      "epoch: 9 step: 1509, loss is 0.002646219916641712\n",
      "epoch: 9 step: 1510, loss is 0.00792777445167303\n",
      "epoch: 9 step: 1511, loss is 0.005447186063975096\n",
      "epoch: 9 step: 1512, loss is 0.0002034163335338235\n",
      "epoch: 9 step: 1513, loss is 0.00803372636437416\n",
      "epoch: 9 step: 1514, loss is 0.002942289924249053\n",
      "epoch: 9 step: 1515, loss is 0.0004623901622835547\n",
      "epoch: 9 step: 1516, loss is 0.0010639969259500504\n",
      "epoch: 9 step: 1517, loss is 0.0022368638310581446\n",
      "epoch: 9 step: 1518, loss is 0.0028919645119458437\n",
      "epoch: 9 step: 1519, loss is 0.04640456289052963\n",
      "epoch: 9 step: 1520, loss is 0.00017188937636092305\n",
      "epoch: 9 step: 1521, loss is 0.04502213001251221\n",
      "epoch: 9 step: 1522, loss is 0.019949253648519516\n",
      "epoch: 9 step: 1523, loss is 0.008897588588297367\n",
      "epoch: 9 step: 1524, loss is 0.022497469559311867\n",
      "epoch: 9 step: 1525, loss is 0.017253534868359566\n",
      "epoch: 9 step: 1526, loss is 0.002050790237262845\n",
      "epoch: 9 step: 1527, loss is 0.0009569252142682672\n",
      "epoch: 9 step: 1528, loss is 0.0006233220919966698\n",
      "epoch: 9 step: 1529, loss is 0.0008140889112837613\n",
      "epoch: 9 step: 1530, loss is 0.005976950749754906\n",
      "epoch: 9 step: 1531, loss is 0.01594839058816433\n",
      "epoch: 9 step: 1532, loss is 0.04982422664761543\n",
      "epoch: 9 step: 1533, loss is 0.04620622098445892\n",
      "epoch: 9 step: 1534, loss is 0.0005974440719000995\n",
      "epoch: 9 step: 1535, loss is 0.09847575426101685\n",
      "epoch: 9 step: 1536, loss is 0.00043127971002832055\n",
      "epoch: 9 step: 1537, loss is 0.0011509530013427138\n",
      "epoch: 9 step: 1538, loss is 0.026357300579547882\n",
      "epoch: 9 step: 1539, loss is 0.0031346448231488466\n",
      "epoch: 9 step: 1540, loss is 0.0080848578363657\n",
      "epoch: 9 step: 1541, loss is 0.0021693669259548187\n",
      "epoch: 9 step: 1542, loss is 0.18278075754642487\n",
      "epoch: 9 step: 1543, loss is 0.0011063721030950546\n",
      "epoch: 9 step: 1544, loss is 0.0008607188938185573\n",
      "epoch: 9 step: 1545, loss is 0.01812112331390381\n",
      "epoch: 9 step: 1546, loss is 0.00021982019825372845\n",
      "epoch: 9 step: 1547, loss is 0.0005355364992283285\n",
      "epoch: 9 step: 1548, loss is 0.01249597780406475\n",
      "epoch: 9 step: 1549, loss is 0.0021373657509684563\n",
      "epoch: 9 step: 1550, loss is 7.68104728194885e-05\n",
      "epoch: 9 step: 1551, loss is 0.00744641711935401\n",
      "epoch: 9 step: 1552, loss is 0.0009375399677082896\n",
      "epoch: 9 step: 1553, loss is 0.0024437312968075275\n",
      "epoch: 9 step: 1554, loss is 0.02950083278119564\n",
      "epoch: 9 step: 1555, loss is 0.0013473581057041883\n",
      "epoch: 9 step: 1556, loss is 0.00017062282131519169\n",
      "epoch: 9 step: 1557, loss is 4.862850619247183e-05\n",
      "epoch: 9 step: 1558, loss is 0.16614559292793274\n",
      "epoch: 9 step: 1559, loss is 0.00020872596360277385\n",
      "epoch: 9 step: 1560, loss is 0.001127976574935019\n",
      "epoch: 9 step: 1561, loss is 0.0005665122298523784\n",
      "epoch: 9 step: 1562, loss is 0.001024156459607184\n",
      "epoch: 9 step: 1563, loss is 0.0042740944772958755\n",
      "epoch: 9 step: 1564, loss is 0.0007985179545357823\n",
      "epoch: 9 step: 1565, loss is 0.00014693086268380284\n",
      "epoch: 9 step: 1566, loss is 0.0700649693608284\n",
      "epoch: 9 step: 1567, loss is 0.00015146020450629294\n",
      "epoch: 9 step: 1568, loss is 0.025880590081214905\n",
      "epoch: 9 step: 1569, loss is 0.003872439032420516\n",
      "epoch: 9 step: 1570, loss is 0.0006709643639624119\n",
      "epoch: 9 step: 1571, loss is 0.0035032047890126705\n",
      "epoch: 9 step: 1572, loss is 5.9660458646249026e-05\n",
      "epoch: 9 step: 1573, loss is 0.01162839587777853\n",
      "epoch: 9 step: 1574, loss is 0.00023330507974606007\n",
      "epoch: 9 step: 1575, loss is 0.01735129952430725\n",
      "epoch: 9 step: 1576, loss is 0.01221838966012001\n",
      "epoch: 9 step: 1577, loss is 0.0010542763629928231\n",
      "epoch: 9 step: 1578, loss is 6.055803896742873e-05\n",
      "epoch: 9 step: 1579, loss is 0.017934367060661316\n",
      "epoch: 9 step: 1580, loss is 0.002174819353967905\n",
      "epoch: 9 step: 1581, loss is 0.008999420329928398\n",
      "epoch: 9 step: 1582, loss is 0.0005193095421418548\n",
      "epoch: 9 step: 1583, loss is 9.85735168796964e-05\n",
      "epoch: 9 step: 1584, loss is 0.002046452136710286\n",
      "epoch: 9 step: 1585, loss is 0.0006079482845962048\n",
      "epoch: 9 step: 1586, loss is 0.0028364432509988546\n",
      "epoch: 9 step: 1587, loss is 0.000922319246456027\n",
      "epoch: 9 step: 1588, loss is 0.006829360034316778\n",
      "epoch: 9 step: 1589, loss is 0.00012302592222113162\n",
      "epoch: 9 step: 1590, loss is 0.10662300884723663\n",
      "epoch: 9 step: 1591, loss is 0.006253321189433336\n",
      "epoch: 9 step: 1592, loss is 0.000826970674097538\n",
      "epoch: 9 step: 1593, loss is 0.00037951243575662374\n",
      "epoch: 9 step: 1594, loss is 0.0015251451404765248\n",
      "epoch: 9 step: 1595, loss is 0.09260914474725723\n",
      "epoch: 9 step: 1596, loss is 0.002265331568196416\n",
      "epoch: 9 step: 1597, loss is 0.0003103941853623837\n",
      "epoch: 9 step: 1598, loss is 0.1929246187210083\n",
      "epoch: 9 step: 1599, loss is 0.0076110889203846455\n",
      "epoch: 9 step: 1600, loss is 0.0006874658865854144\n",
      "epoch: 9 step: 1601, loss is 0.003963290248066187\n",
      "epoch: 9 step: 1602, loss is 0.0087163420394063\n",
      "epoch: 9 step: 1603, loss is 0.024224409833550453\n",
      "epoch: 9 step: 1604, loss is 0.001408419106155634\n",
      "epoch: 9 step: 1605, loss is 0.0007229155744425952\n",
      "epoch: 9 step: 1606, loss is 0.005611257161945105\n",
      "epoch: 9 step: 1607, loss is 0.018036924302577972\n",
      "epoch: 9 step: 1608, loss is 0.00023129259352572262\n",
      "epoch: 9 step: 1609, loss is 0.00292864628136158\n",
      "epoch: 9 step: 1610, loss is 0.025076037272810936\n",
      "epoch: 9 step: 1611, loss is 0.007357093971222639\n",
      "epoch: 9 step: 1612, loss is 0.002839939668774605\n",
      "epoch: 9 step: 1613, loss is 4.599255771609023e-05\n",
      "epoch: 9 step: 1614, loss is 0.0006004674360156059\n",
      "epoch: 9 step: 1615, loss is 0.0003069763188250363\n",
      "epoch: 9 step: 1616, loss is 0.22240188717842102\n",
      "epoch: 9 step: 1617, loss is 0.0012307740980759263\n",
      "epoch: 9 step: 1618, loss is 0.0005709494580514729\n",
      "epoch: 9 step: 1619, loss is 0.027639959007501602\n",
      "epoch: 9 step: 1620, loss is 0.000320861377986148\n",
      "epoch: 9 step: 1621, loss is 0.0006581353372894228\n",
      "epoch: 9 step: 1622, loss is 0.00048194194096140563\n",
      "epoch: 9 step: 1623, loss is 0.001101348432712257\n",
      "epoch: 9 step: 1624, loss is 0.0034050485119223595\n",
      "epoch: 9 step: 1625, loss is 0.0012660595821216702\n",
      "epoch: 9 step: 1626, loss is 0.0004913864540867507\n",
      "epoch: 9 step: 1627, loss is 0.07954375445842743\n",
      "epoch: 9 step: 1628, loss is 0.01324430014938116\n",
      "epoch: 9 step: 1629, loss is 0.0004517035558819771\n",
      "epoch: 9 step: 1630, loss is 0.016298184171319008\n",
      "epoch: 9 step: 1631, loss is 0.0031516330782324076\n",
      "epoch: 9 step: 1632, loss is 0.008826544508337975\n",
      "epoch: 9 step: 1633, loss is 0.026766454800963402\n",
      "epoch: 9 step: 1634, loss is 0.003859203541651368\n",
      "epoch: 9 step: 1635, loss is 0.0009889798238873482\n",
      "epoch: 9 step: 1636, loss is 7.701908180024475e-05\n",
      "epoch: 9 step: 1637, loss is 0.0002449584426358342\n",
      "epoch: 9 step: 1638, loss is 0.0008162908488884568\n",
      "epoch: 9 step: 1639, loss is 0.0013450449332594872\n",
      "epoch: 9 step: 1640, loss is 0.004611288197338581\n",
      "epoch: 9 step: 1641, loss is 0.010071596130728722\n",
      "epoch: 9 step: 1642, loss is 0.00041234309901483357\n",
      "epoch: 9 step: 1643, loss is 0.07244978100061417\n",
      "epoch: 9 step: 1644, loss is 0.0011575513053685427\n",
      "epoch: 9 step: 1645, loss is 0.0016631989274173975\n",
      "epoch: 9 step: 1646, loss is 0.0045317234471440315\n",
      "epoch: 9 step: 1647, loss is 0.0006399163394235075\n",
      "epoch: 9 step: 1648, loss is 0.0012998984893783927\n",
      "epoch: 9 step: 1649, loss is 0.001538161770440638\n",
      "epoch: 9 step: 1650, loss is 0.005835548043251038\n",
      "epoch: 9 step: 1651, loss is 0.0007474562153220177\n",
      "epoch: 9 step: 1652, loss is 0.0025441450998187065\n",
      "epoch: 9 step: 1653, loss is 0.07387667894363403\n",
      "epoch: 9 step: 1654, loss is 0.00030951134976930916\n",
      "epoch: 9 step: 1655, loss is 0.010178127326071262\n",
      "epoch: 9 step: 1656, loss is 0.00045843294356018305\n",
      "epoch: 9 step: 1657, loss is 0.0007582136895507574\n",
      "epoch: 9 step: 1658, loss is 0.07678107917308807\n",
      "epoch: 9 step: 1659, loss is 0.004019397776573896\n",
      "epoch: 9 step: 1660, loss is 0.002485511125996709\n",
      "epoch: 9 step: 1661, loss is 0.015957139432430267\n",
      "epoch: 9 step: 1662, loss is 2.8009495508740656e-05\n",
      "epoch: 9 step: 1663, loss is 0.015038816258311272\n",
      "epoch: 9 step: 1664, loss is 0.02262529730796814\n",
      "epoch: 9 step: 1665, loss is 0.00025625736452639103\n",
      "epoch: 9 step: 1666, loss is 0.052917737513780594\n",
      "epoch: 9 step: 1667, loss is 0.00291023263707757\n",
      "epoch: 9 step: 1668, loss is 0.005384624004364014\n",
      "epoch: 9 step: 1669, loss is 0.0034938035532832146\n",
      "epoch: 9 step: 1670, loss is 0.0005699299508705735\n",
      "epoch: 9 step: 1671, loss is 2.8903024940518662e-05\n",
      "epoch: 9 step: 1672, loss is 0.00018285871192347258\n",
      "epoch: 9 step: 1673, loss is 0.007782343775033951\n",
      "epoch: 9 step: 1674, loss is 0.05910502001643181\n",
      "epoch: 9 step: 1675, loss is 0.00020197968115098774\n",
      "epoch: 9 step: 1676, loss is 1.1079506293754093e-05\n",
      "epoch: 9 step: 1677, loss is 0.029078487306833267\n",
      "epoch: 9 step: 1678, loss is 0.0596013143658638\n",
      "epoch: 9 step: 1679, loss is 0.000403014273615554\n",
      "epoch: 9 step: 1680, loss is 0.00316561758518219\n",
      "epoch: 9 step: 1681, loss is 0.008070854470133781\n",
      "epoch: 9 step: 1682, loss is 5.685769428964704e-05\n",
      "epoch: 9 step: 1683, loss is 0.20517876744270325\n",
      "epoch: 9 step: 1684, loss is 0.12722013890743256\n",
      "epoch: 9 step: 1685, loss is 0.012777200900018215\n",
      "epoch: 9 step: 1686, loss is 0.00020369830599520355\n",
      "epoch: 9 step: 1687, loss is 0.004739194642752409\n",
      "epoch: 9 step: 1688, loss is 0.0015900215366855264\n",
      "epoch: 9 step: 1689, loss is 0.00015682133380323648\n",
      "epoch: 9 step: 1690, loss is 0.005205427762120962\n",
      "epoch: 9 step: 1691, loss is 0.0020643863826990128\n",
      "epoch: 9 step: 1692, loss is 0.005736557301133871\n",
      "epoch: 9 step: 1693, loss is 0.0012991384137421846\n",
      "epoch: 9 step: 1694, loss is 0.16365662217140198\n",
      "epoch: 9 step: 1695, loss is 0.004420698620378971\n",
      "epoch: 9 step: 1696, loss is 0.0019620757084339857\n",
      "epoch: 9 step: 1697, loss is 0.003158524166792631\n",
      "epoch: 9 step: 1698, loss is 0.07170241326093674\n",
      "epoch: 9 step: 1699, loss is 0.0034740569535642862\n",
      "epoch: 9 step: 1700, loss is 0.0010829849634319544\n",
      "epoch: 9 step: 1701, loss is 0.00029608781915158033\n",
      "epoch: 9 step: 1702, loss is 0.00025038342573679984\n",
      "epoch: 9 step: 1703, loss is 0.019942300394177437\n",
      "epoch: 9 step: 1704, loss is 0.0016634052153676748\n",
      "epoch: 9 step: 1705, loss is 0.10298998653888702\n",
      "epoch: 9 step: 1706, loss is 0.0004976664204150438\n",
      "epoch: 9 step: 1707, loss is 0.0044112177565693855\n",
      "epoch: 9 step: 1708, loss is 0.025182479992508888\n",
      "epoch: 9 step: 1709, loss is 0.0010165947023779154\n",
      "epoch: 9 step: 1710, loss is 0.018048355355858803\n",
      "epoch: 9 step: 1711, loss is 0.00010347810166422278\n",
      "epoch: 9 step: 1712, loss is 0.018029402941465378\n",
      "epoch: 9 step: 1713, loss is 0.000805277843028307\n",
      "epoch: 9 step: 1714, loss is 0.0010557218920439482\n",
      "epoch: 9 step: 1715, loss is 0.0015436809044331312\n",
      "epoch: 9 step: 1716, loss is 0.00024219334591180086\n",
      "epoch: 9 step: 1717, loss is 5.9700312704080716e-05\n",
      "epoch: 9 step: 1718, loss is 0.04663306847214699\n",
      "epoch: 9 step: 1719, loss is 0.09177007526159286\n",
      "epoch: 9 step: 1720, loss is 0.00036592932883650064\n",
      "epoch: 9 step: 1721, loss is 0.0056907786056399345\n",
      "epoch: 9 step: 1722, loss is 0.0002644856576807797\n",
      "epoch: 9 step: 1723, loss is 0.00212071742862463\n",
      "epoch: 9 step: 1724, loss is 0.0007965599070303142\n",
      "epoch: 9 step: 1725, loss is 0.0014066246803849936\n",
      "epoch: 9 step: 1726, loss is 0.00017657529679127038\n",
      "epoch: 9 step: 1727, loss is 0.0032844310626387596\n",
      "epoch: 9 step: 1728, loss is 0.000148062186781317\n",
      "epoch: 9 step: 1729, loss is 0.004082055296748877\n",
      "epoch: 9 step: 1730, loss is 0.0013413806445896626\n",
      "epoch: 9 step: 1731, loss is 0.0007130550220608711\n",
      "epoch: 9 step: 1732, loss is 6.882139859953895e-05\n",
      "epoch: 9 step: 1733, loss is 0.03127065673470497\n",
      "epoch: 9 step: 1734, loss is 0.006646042689681053\n",
      "epoch: 9 step: 1735, loss is 0.000875948288012296\n",
      "epoch: 9 step: 1736, loss is 0.012856979854404926\n",
      "epoch: 9 step: 1737, loss is 0.00883390661329031\n",
      "epoch: 9 step: 1738, loss is 0.0008241264149546623\n",
      "epoch: 9 step: 1739, loss is 0.0017892637988552451\n",
      "epoch: 9 step: 1740, loss is 0.008716550655663013\n",
      "epoch: 9 step: 1741, loss is 0.012765132822096348\n",
      "epoch: 9 step: 1742, loss is 0.0020295351278036833\n",
      "epoch: 9 step: 1743, loss is 0.00017441292584408075\n",
      "epoch: 9 step: 1744, loss is 0.02587556466460228\n",
      "epoch: 9 step: 1745, loss is 0.0005492577911354601\n",
      "epoch: 9 step: 1746, loss is 7.28424420231022e-05\n",
      "epoch: 9 step: 1747, loss is 0.005310531239956617\n",
      "epoch: 9 step: 1748, loss is 0.020344335585832596\n",
      "epoch: 9 step: 1749, loss is 0.03710553050041199\n",
      "epoch: 9 step: 1750, loss is 0.0031324592418968678\n",
      "epoch: 9 step: 1751, loss is 0.0005904603749513626\n",
      "epoch: 9 step: 1752, loss is 0.00027491062064655125\n",
      "epoch: 9 step: 1753, loss is 0.004230591934174299\n",
      "epoch: 9 step: 1754, loss is 0.00467084813863039\n",
      "epoch: 9 step: 1755, loss is 0.05885015428066254\n",
      "epoch: 9 step: 1756, loss is 5.084249278297648e-05\n",
      "epoch: 9 step: 1757, loss is 0.0003275281342212111\n",
      "epoch: 9 step: 1758, loss is 0.0009404064039699733\n",
      "epoch: 9 step: 1759, loss is 0.0072652180679142475\n",
      "epoch: 9 step: 1760, loss is 0.08602474629878998\n",
      "epoch: 9 step: 1761, loss is 0.000549868680536747\n",
      "epoch: 9 step: 1762, loss is 0.05598853901028633\n",
      "epoch: 9 step: 1763, loss is 0.044182587414979935\n",
      "epoch: 9 step: 1764, loss is 0.00018998837913386524\n",
      "epoch: 9 step: 1765, loss is 0.0005025246064178646\n",
      "epoch: 9 step: 1766, loss is 0.0005128722987137735\n",
      "epoch: 9 step: 1767, loss is 0.003897221526131034\n",
      "epoch: 9 step: 1768, loss is 0.001612321357242763\n",
      "epoch: 9 step: 1769, loss is 0.08576102554798126\n",
      "epoch: 9 step: 1770, loss is 0.018366539850831032\n",
      "epoch: 9 step: 1771, loss is 0.0011077428935095668\n",
      "epoch: 9 step: 1772, loss is 0.0014464587438851595\n",
      "epoch: 9 step: 1773, loss is 0.0034758979454636574\n",
      "epoch: 9 step: 1774, loss is 0.00018948361685033888\n",
      "epoch: 9 step: 1775, loss is 0.0002033175405813381\n",
      "epoch: 9 step: 1776, loss is 0.006786588579416275\n",
      "epoch: 9 step: 1777, loss is 0.0033043636940419674\n",
      "epoch: 9 step: 1778, loss is 0.08387142419815063\n",
      "epoch: 9 step: 1779, loss is 0.049136899411678314\n",
      "epoch: 9 step: 1780, loss is 0.06572149693965912\n",
      "epoch: 9 step: 1781, loss is 0.0022278488613665104\n",
      "epoch: 9 step: 1782, loss is 0.08526135981082916\n",
      "epoch: 9 step: 1783, loss is 0.08938000351190567\n",
      "epoch: 9 step: 1784, loss is 0.15762077271938324\n",
      "epoch: 9 step: 1785, loss is 0.001660251640714705\n",
      "epoch: 9 step: 1786, loss is 0.0011581098660826683\n",
      "epoch: 9 step: 1787, loss is 0.001364684896543622\n",
      "epoch: 9 step: 1788, loss is 0.0008760618511587381\n",
      "epoch: 9 step: 1789, loss is 0.00042067497270181775\n",
      "epoch: 9 step: 1790, loss is 0.000642051047179848\n",
      "epoch: 9 step: 1791, loss is 0.05034080892801285\n",
      "epoch: 9 step: 1792, loss is 0.0063972375355660915\n",
      "epoch: 9 step: 1793, loss is 5.183760731597431e-05\n",
      "epoch: 9 step: 1794, loss is 0.007026859559118748\n",
      "epoch: 9 step: 1795, loss is 0.0004803381161764264\n",
      "epoch: 9 step: 1796, loss is 0.0004395555006340146\n",
      "epoch: 9 step: 1797, loss is 4.704199454863556e-05\n",
      "epoch: 9 step: 1798, loss is 0.0034288293682038784\n",
      "epoch: 9 step: 1799, loss is 0.00020031201711390167\n",
      "epoch: 9 step: 1800, loss is 0.0022989935241639614\n",
      "epoch: 9 step: 1801, loss is 0.01002207025885582\n",
      "epoch: 9 step: 1802, loss is 0.00027620914625003934\n",
      "epoch: 9 step: 1803, loss is 0.06455171853303909\n",
      "epoch: 9 step: 1804, loss is 0.08750301599502563\n",
      "epoch: 9 step: 1805, loss is 0.013464353047311306\n",
      "epoch: 9 step: 1806, loss is 0.0014987466856837273\n",
      "epoch: 9 step: 1807, loss is 0.006581103894859552\n",
      "epoch: 9 step: 1808, loss is 0.010552194900810719\n",
      "epoch: 9 step: 1809, loss is 0.018452560529112816\n",
      "epoch: 9 step: 1810, loss is 0.02141864039003849\n",
      "epoch: 9 step: 1811, loss is 0.0015838741092011333\n",
      "epoch: 9 step: 1812, loss is 0.0009965591598302126\n",
      "epoch: 9 step: 1813, loss is 0.016636980697512627\n",
      "epoch: 9 step: 1814, loss is 0.0007504397653974593\n",
      "epoch: 9 step: 1815, loss is 0.15509293973445892\n",
      "epoch: 9 step: 1816, loss is 0.0004509103891905397\n",
      "epoch: 9 step: 1817, loss is 0.05169856175780296\n",
      "epoch: 9 step: 1818, loss is 2.3522765332018025e-05\n",
      "epoch: 9 step: 1819, loss is 0.0007638519746251404\n",
      "epoch: 9 step: 1820, loss is 0.004460281692445278\n",
      "epoch: 9 step: 1821, loss is 0.0032019938807934523\n",
      "epoch: 9 step: 1822, loss is 0.0003459000727161765\n",
      "epoch: 9 step: 1823, loss is 0.0007275053067132831\n",
      "epoch: 9 step: 1824, loss is 0.00028905519866384566\n",
      "epoch: 9 step: 1825, loss is 0.004299012012779713\n",
      "epoch: 9 step: 1826, loss is 0.0003789283218793571\n",
      "epoch: 9 step: 1827, loss is 7.549264410044998e-05\n",
      "epoch: 9 step: 1828, loss is 0.002686255145817995\n",
      "epoch: 9 step: 1829, loss is 0.001008603605441749\n",
      "epoch: 9 step: 1830, loss is 0.0007194381905719638\n",
      "epoch: 9 step: 1831, loss is 0.004764623008668423\n",
      "epoch: 9 step: 1832, loss is 0.006729642860591412\n",
      "epoch: 9 step: 1833, loss is 0.030162382870912552\n",
      "epoch: 9 step: 1834, loss is 0.00026833434822037816\n",
      "epoch: 9 step: 1835, loss is 0.04631170630455017\n",
      "epoch: 9 step: 1836, loss is 0.0008682147017680109\n",
      "epoch: 9 step: 1837, loss is 0.052968576550483704\n",
      "epoch: 9 step: 1838, loss is 0.0002929393376689404\n",
      "epoch: 9 step: 1839, loss is 0.13307972252368927\n",
      "epoch: 9 step: 1840, loss is 0.0005646549980156124\n",
      "epoch: 9 step: 1841, loss is 0.015989569947123528\n",
      "epoch: 9 step: 1842, loss is 9.307421714765951e-05\n",
      "epoch: 9 step: 1843, loss is 0.002963556442409754\n",
      "epoch: 9 step: 1844, loss is 0.0005018436932004988\n",
      "epoch: 9 step: 1845, loss is 0.0005349012208171189\n",
      "epoch: 9 step: 1846, loss is 0.000175135544850491\n",
      "epoch: 9 step: 1847, loss is 0.0008062135893851519\n",
      "epoch: 9 step: 1848, loss is 0.00016524875536561012\n",
      "epoch: 9 step: 1849, loss is 9.525621135253459e-05\n",
      "epoch: 9 step: 1850, loss is 0.01781080849468708\n",
      "epoch: 9 step: 1851, loss is 0.006257318891584873\n",
      "epoch: 9 step: 1852, loss is 9.639208292355761e-05\n",
      "epoch: 9 step: 1853, loss is 0.002353905700147152\n",
      "epoch: 9 step: 1854, loss is 0.00034138074261136353\n",
      "epoch: 9 step: 1855, loss is 1.009218340186635e-05\n",
      "epoch: 9 step: 1856, loss is 0.05969038978219032\n",
      "epoch: 9 step: 1857, loss is 0.020816558972001076\n",
      "epoch: 9 step: 1858, loss is 0.023821601644158363\n",
      "epoch: 9 step: 1859, loss is 0.0009199734777212143\n",
      "epoch: 9 step: 1860, loss is 0.007488148286938667\n",
      "epoch: 9 step: 1861, loss is 0.09554285556077957\n",
      "epoch: 9 step: 1862, loss is 0.0003884344478137791\n",
      "epoch: 9 step: 1863, loss is 0.028891321271657944\n",
      "epoch: 9 step: 1864, loss is 0.041114188730716705\n",
      "epoch: 9 step: 1865, loss is 0.0011170705547556281\n",
      "epoch: 9 step: 1866, loss is 0.0005574114620685577\n",
      "epoch: 9 step: 1867, loss is 7.031852146610618e-05\n",
      "epoch: 9 step: 1868, loss is 0.004489092156291008\n",
      "epoch: 9 step: 1869, loss is 0.0006051006494089961\n",
      "epoch: 9 step: 1870, loss is 0.014880162663757801\n",
      "epoch: 9 step: 1871, loss is 0.019766174256801605\n",
      "epoch: 9 step: 1872, loss is 0.0010034267324954271\n",
      "epoch: 9 step: 1873, loss is 0.007941324263811111\n",
      "epoch: 9 step: 1874, loss is 0.0005725084338337183\n",
      "epoch: 9 step: 1875, loss is 0.00042681925697252154\n",
      "epoch: 10 step: 1, loss is 0.0002593373355921358\n",
      "epoch: 10 step: 2, loss is 0.00013057110481895506\n",
      "epoch: 10 step: 3, loss is 0.0039655910804867744\n",
      "epoch: 10 step: 4, loss is 0.005476405378431082\n",
      "epoch: 10 step: 5, loss is 0.00015170844562817365\n",
      "epoch: 10 step: 6, loss is 0.00014510010078083724\n",
      "epoch: 10 step: 7, loss is 0.01920423097908497\n",
      "epoch: 10 step: 8, loss is 0.16484034061431885\n",
      "epoch: 10 step: 9, loss is 0.0005519173573702574\n",
      "epoch: 10 step: 10, loss is 0.003017820417881012\n",
      "epoch: 10 step: 11, loss is 0.00032338936580345035\n",
      "epoch: 10 step: 12, loss is 0.00018181386985816061\n",
      "epoch: 10 step: 13, loss is 0.004263258073478937\n",
      "epoch: 10 step: 14, loss is 0.0008292450220324099\n",
      "epoch: 10 step: 15, loss is 2.6805235393112525e-05\n",
      "epoch: 10 step: 16, loss is 0.009593164548277855\n",
      "epoch: 10 step: 17, loss is 0.10102979093790054\n",
      "epoch: 10 step: 18, loss is 8.032675395952538e-05\n",
      "epoch: 10 step: 19, loss is 0.0007103402749635279\n",
      "epoch: 10 step: 20, loss is 5.036938455305062e-05\n",
      "epoch: 10 step: 21, loss is 0.023992981761693954\n",
      "epoch: 10 step: 22, loss is 0.0012056123232468963\n",
      "epoch: 10 step: 23, loss is 0.0882277861237526\n",
      "epoch: 10 step: 24, loss is 0.006576860789209604\n",
      "epoch: 10 step: 25, loss is 0.00022343108139466494\n",
      "epoch: 10 step: 26, loss is 0.0044943117536604404\n",
      "epoch: 10 step: 27, loss is 0.01937011256814003\n",
      "epoch: 10 step: 28, loss is 0.0005905971629545093\n",
      "epoch: 10 step: 29, loss is 0.017545389011502266\n",
      "epoch: 10 step: 30, loss is 0.0013571990421041846\n",
      "epoch: 10 step: 31, loss is 0.002037998288869858\n",
      "epoch: 10 step: 32, loss is 0.03203431889414787\n",
      "epoch: 10 step: 33, loss is 0.00782017596065998\n",
      "epoch: 10 step: 34, loss is 0.00037934433203190565\n",
      "epoch: 10 step: 35, loss is 0.010191623121500015\n",
      "epoch: 10 step: 36, loss is 0.005808483809232712\n",
      "epoch: 10 step: 37, loss is 0.04985934868454933\n",
      "epoch: 10 step: 38, loss is 0.00013239594409242272\n",
      "epoch: 10 step: 39, loss is 0.0019324769964441657\n",
      "epoch: 10 step: 40, loss is 0.00010848321107914671\n",
      "epoch: 10 step: 41, loss is 0.0068527343682944775\n",
      "epoch: 10 step: 42, loss is 0.0030921902507543564\n",
      "epoch: 10 step: 43, loss is 0.009527050890028477\n",
      "epoch: 10 step: 44, loss is 0.0017304508946835995\n",
      "epoch: 10 step: 45, loss is 0.0024580725003033876\n",
      "epoch: 10 step: 46, loss is 0.0016542889643460512\n",
      "epoch: 10 step: 47, loss is 0.0009830263443291187\n",
      "epoch: 10 step: 48, loss is 0.0002695956500247121\n",
      "epoch: 10 step: 49, loss is 0.0020715605933219194\n",
      "epoch: 10 step: 50, loss is 9.773318015504628e-05\n",
      "epoch: 10 step: 51, loss is 0.00011599274876061827\n",
      "epoch: 10 step: 52, loss is 0.0010186455911025405\n",
      "epoch: 10 step: 53, loss is 0.00017528671014588326\n",
      "epoch: 10 step: 54, loss is 4.786123463418335e-05\n",
      "epoch: 10 step: 55, loss is 0.002056533470749855\n",
      "epoch: 10 step: 56, loss is 0.007744635920971632\n",
      "epoch: 10 step: 57, loss is 0.0061177341267466545\n",
      "epoch: 10 step: 58, loss is 0.0017204965697601438\n",
      "epoch: 10 step: 59, loss is 0.15480203926563263\n",
      "epoch: 10 step: 60, loss is 5.361918010748923e-05\n",
      "epoch: 10 step: 61, loss is 0.0008733252761885524\n",
      "epoch: 10 step: 62, loss is 0.0025537998881191015\n",
      "epoch: 10 step: 63, loss is 0.03255145996809006\n",
      "epoch: 10 step: 64, loss is 1.2498913747549523e-05\n",
      "epoch: 10 step: 65, loss is 0.0020168109331279993\n",
      "epoch: 10 step: 66, loss is 0.0009834045777097344\n",
      "epoch: 10 step: 67, loss is 1.0948992894554976e-05\n",
      "epoch: 10 step: 68, loss is 0.00032791640842333436\n",
      "epoch: 10 step: 69, loss is 0.00018057780107483268\n",
      "epoch: 10 step: 70, loss is 0.0010015503503382206\n",
      "epoch: 10 step: 71, loss is 0.00011939493560930714\n",
      "epoch: 10 step: 72, loss is 0.00975602213293314\n",
      "epoch: 10 step: 73, loss is 0.0010500815697014332\n",
      "epoch: 10 step: 74, loss is 0.0051716398447752\n",
      "epoch: 10 step: 75, loss is 0.0028525800444185734\n",
      "epoch: 10 step: 76, loss is 0.001436911872588098\n",
      "epoch: 10 step: 77, loss is 0.00017512183694634587\n",
      "epoch: 10 step: 78, loss is 0.03566671535372734\n",
      "epoch: 10 step: 79, loss is 6.822025898145512e-05\n",
      "epoch: 10 step: 80, loss is 0.0027008620090782642\n",
      "epoch: 10 step: 81, loss is 0.11550422012805939\n",
      "epoch: 10 step: 82, loss is 0.012594537809491158\n",
      "epoch: 10 step: 83, loss is 0.0011291593546047807\n",
      "epoch: 10 step: 84, loss is 0.00022319027630146593\n",
      "epoch: 10 step: 85, loss is 0.016379639506340027\n",
      "epoch: 10 step: 86, loss is 0.00928095355629921\n",
      "epoch: 10 step: 87, loss is 0.0002793140592984855\n",
      "epoch: 10 step: 88, loss is 0.0009241006919182837\n",
      "epoch: 10 step: 89, loss is 0.0008302513742819428\n",
      "epoch: 10 step: 90, loss is 0.007986417040228844\n",
      "epoch: 10 step: 91, loss is 0.21249273419380188\n",
      "epoch: 10 step: 92, loss is 0.0006183860241435468\n",
      "epoch: 10 step: 93, loss is 9.652126755099744e-05\n",
      "epoch: 10 step: 94, loss is 0.0014372736914083362\n",
      "epoch: 10 step: 95, loss is 0.0017394053284078836\n",
      "epoch: 10 step: 96, loss is 0.0005317098111845553\n",
      "epoch: 10 step: 97, loss is 0.004239752888679504\n",
      "epoch: 10 step: 98, loss is 0.0030671991407871246\n",
      "epoch: 10 step: 99, loss is 0.011170104146003723\n",
      "epoch: 10 step: 100, loss is 1.1548459042387549e-06\n",
      "epoch: 10 step: 101, loss is 0.00037528458051383495\n",
      "epoch: 10 step: 102, loss is 0.0044381283223629\n",
      "epoch: 10 step: 103, loss is 0.0015589895192533731\n",
      "epoch: 10 step: 104, loss is 0.0003463590401224792\n",
      "epoch: 10 step: 105, loss is 0.000426997896283865\n",
      "epoch: 10 step: 106, loss is 0.001747669419273734\n",
      "epoch: 10 step: 107, loss is 0.0025626695714890957\n",
      "epoch: 10 step: 108, loss is 0.0007631711196154356\n",
      "epoch: 10 step: 109, loss is 0.0007694435771554708\n",
      "epoch: 10 step: 110, loss is 0.0017056743381544948\n",
      "epoch: 10 step: 111, loss is 0.0012968736700713634\n",
      "epoch: 10 step: 112, loss is 0.0058013382367789745\n",
      "epoch: 10 step: 113, loss is 0.01607082039117813\n",
      "epoch: 10 step: 114, loss is 0.00019538940978236496\n",
      "epoch: 10 step: 115, loss is 0.003983247093856335\n",
      "epoch: 10 step: 116, loss is 0.0002762574004009366\n",
      "epoch: 10 step: 117, loss is 0.001989578828215599\n",
      "epoch: 10 step: 118, loss is 5.439918822958134e-05\n",
      "epoch: 10 step: 119, loss is 0.0017421470256522298\n",
      "epoch: 10 step: 120, loss is 0.013916155323386192\n",
      "epoch: 10 step: 121, loss is 0.015315797179937363\n",
      "epoch: 10 step: 122, loss is 0.00034839962609112263\n",
      "epoch: 10 step: 123, loss is 0.0052397530525922775\n",
      "epoch: 10 step: 124, loss is 0.00013815939018968493\n",
      "epoch: 10 step: 125, loss is 5.713929203920998e-05\n",
      "epoch: 10 step: 126, loss is 0.0008344616508111358\n",
      "epoch: 10 step: 127, loss is 0.14982140064239502\n",
      "epoch: 10 step: 128, loss is 1.9907995010726154e-05\n",
      "epoch: 10 step: 129, loss is 0.00039094561361707747\n",
      "epoch: 10 step: 130, loss is 0.024561231955885887\n",
      "epoch: 10 step: 131, loss is 9.699162183096632e-05\n",
      "epoch: 10 step: 132, loss is 0.18170064687728882\n",
      "epoch: 10 step: 133, loss is 0.008507867343723774\n",
      "epoch: 10 step: 134, loss is 0.004235295113176107\n",
      "epoch: 10 step: 135, loss is 0.050382282584905624\n",
      "epoch: 10 step: 136, loss is 0.0014369247946888208\n",
      "epoch: 10 step: 137, loss is 0.0006306981667876244\n",
      "epoch: 10 step: 138, loss is 0.00027698377380147576\n",
      "epoch: 10 step: 139, loss is 0.004994125105440617\n",
      "epoch: 10 step: 140, loss is 2.2202871150511783e-06\n",
      "epoch: 10 step: 141, loss is 0.0017297924496233463\n",
      "epoch: 10 step: 142, loss is 0.00046190983266569674\n",
      "epoch: 10 step: 143, loss is 1.3213145393820014e-05\n",
      "epoch: 10 step: 144, loss is 0.01536257378757\n",
      "epoch: 10 step: 145, loss is 0.0008327904506586492\n",
      "epoch: 10 step: 146, loss is 4.091093796887435e-05\n",
      "epoch: 10 step: 147, loss is 0.005659245420247316\n",
      "epoch: 10 step: 148, loss is 0.00023693534603808075\n",
      "epoch: 10 step: 149, loss is 0.0002789159188978374\n",
      "epoch: 10 step: 150, loss is 0.05287136882543564\n",
      "epoch: 10 step: 151, loss is 0.00019548367708921432\n",
      "epoch: 10 step: 152, loss is 0.001907743513584137\n",
      "epoch: 10 step: 153, loss is 0.0018481158185750246\n",
      "epoch: 10 step: 154, loss is 0.00012153479474363849\n",
      "epoch: 10 step: 155, loss is 0.005479163955897093\n",
      "epoch: 10 step: 156, loss is 0.0058276415802538395\n",
      "epoch: 10 step: 157, loss is 0.0001091661470127292\n",
      "epoch: 10 step: 158, loss is 0.0026838721241801977\n",
      "epoch: 10 step: 159, loss is 6.63157261442393e-05\n",
      "epoch: 10 step: 160, loss is 0.0040673986077308655\n",
      "epoch: 10 step: 161, loss is 0.004621246363967657\n",
      "epoch: 10 step: 162, loss is 0.0016638130182400346\n",
      "epoch: 10 step: 163, loss is 0.0004347689391579479\n",
      "epoch: 10 step: 164, loss is 6.555666914209723e-05\n",
      "epoch: 10 step: 165, loss is 0.0011264232452958822\n",
      "epoch: 10 step: 166, loss is 0.012462781742215157\n",
      "epoch: 10 step: 167, loss is 0.0071619851514697075\n",
      "epoch: 10 step: 168, loss is 0.0025800408329814672\n",
      "epoch: 10 step: 169, loss is 2.488680365786422e-05\n",
      "epoch: 10 step: 170, loss is 0.0023290528915822506\n",
      "epoch: 10 step: 171, loss is 0.004615821409970522\n",
      "epoch: 10 step: 172, loss is 0.00015546138456556946\n",
      "epoch: 10 step: 173, loss is 0.012637987732887268\n",
      "epoch: 10 step: 174, loss is 0.0004799744056072086\n",
      "epoch: 10 step: 175, loss is 0.014215455390512943\n",
      "epoch: 10 step: 176, loss is 0.034738123416900635\n",
      "epoch: 10 step: 177, loss is 0.00025706933229230344\n",
      "epoch: 10 step: 178, loss is 0.0005815302720293403\n",
      "epoch: 10 step: 179, loss is 0.004895060323178768\n",
      "epoch: 10 step: 180, loss is 0.002656160155311227\n",
      "epoch: 10 step: 181, loss is 0.00011933503992622718\n",
      "epoch: 10 step: 182, loss is 0.007135866209864616\n",
      "epoch: 10 step: 183, loss is 0.0013692006468772888\n",
      "epoch: 10 step: 184, loss is 0.0009721554815769196\n",
      "epoch: 10 step: 185, loss is 0.0009623585501685739\n",
      "epoch: 10 step: 186, loss is 0.001615842105820775\n",
      "epoch: 10 step: 187, loss is 0.0009959063027054071\n",
      "epoch: 10 step: 188, loss is 0.0001877259201137349\n",
      "epoch: 10 step: 189, loss is 0.0002849650336429477\n",
      "epoch: 10 step: 190, loss is 0.016662949696183205\n",
      "epoch: 10 step: 191, loss is 0.0009313065675087273\n",
      "epoch: 10 step: 192, loss is 0.00615700613707304\n",
      "epoch: 10 step: 193, loss is 0.09804335236549377\n",
      "epoch: 10 step: 194, loss is 0.00974840298295021\n",
      "epoch: 10 step: 195, loss is 0.0034811438526958227\n",
      "epoch: 10 step: 196, loss is 0.0030447563622146845\n",
      "epoch: 10 step: 197, loss is 3.565082442946732e-05\n",
      "epoch: 10 step: 198, loss is 0.000212132406886667\n",
      "epoch: 10 step: 199, loss is 0.01798434369266033\n",
      "epoch: 10 step: 200, loss is 0.0006380584090948105\n",
      "epoch: 10 step: 201, loss is 0.00010982665844494477\n",
      "epoch: 10 step: 202, loss is 0.00024239960475824773\n",
      "epoch: 10 step: 203, loss is 0.0021334197372198105\n",
      "epoch: 10 step: 204, loss is 4.319344225223176e-05\n",
      "epoch: 10 step: 205, loss is 0.0009383868309669197\n",
      "epoch: 10 step: 206, loss is 0.00011883318074978888\n",
      "epoch: 10 step: 207, loss is 0.00026049636653624475\n",
      "epoch: 10 step: 208, loss is 0.05253661796450615\n",
      "epoch: 10 step: 209, loss is 0.005630899220705032\n",
      "epoch: 10 step: 210, loss is 5.3469251724891365e-05\n",
      "epoch: 10 step: 211, loss is 0.0033983606845140457\n",
      "epoch: 10 step: 212, loss is 0.02061387337744236\n",
      "epoch: 10 step: 213, loss is 0.00065617635846138\n",
      "epoch: 10 step: 214, loss is 0.00048154438263736665\n",
      "epoch: 10 step: 215, loss is 0.00790818314999342\n",
      "epoch: 10 step: 216, loss is 0.25849583745002747\n",
      "epoch: 10 step: 217, loss is 0.03868790715932846\n",
      "epoch: 10 step: 218, loss is 1.902594158309512e-05\n",
      "epoch: 10 step: 219, loss is 0.0045440006069839\n",
      "epoch: 10 step: 220, loss is 0.0012970763491466641\n",
      "epoch: 10 step: 221, loss is 0.01632871851325035\n",
      "epoch: 10 step: 222, loss is 0.00809620413929224\n",
      "epoch: 10 step: 223, loss is 4.980942321708426e-05\n",
      "epoch: 10 step: 224, loss is 4.213969805277884e-05\n",
      "epoch: 10 step: 225, loss is 1.8843644284061156e-05\n",
      "epoch: 10 step: 226, loss is 0.012334409169852734\n",
      "epoch: 10 step: 227, loss is 0.0006207022815942764\n",
      "epoch: 10 step: 228, loss is 0.0007295832620002329\n",
      "epoch: 10 step: 229, loss is 0.0003144435759168118\n",
      "epoch: 10 step: 230, loss is 0.00021803437266498804\n",
      "epoch: 10 step: 231, loss is 0.002782220486551523\n",
      "epoch: 10 step: 232, loss is 9.419343405170366e-05\n",
      "epoch: 10 step: 233, loss is 0.0006889414507895708\n",
      "epoch: 10 step: 234, loss is 0.0023757589515298605\n",
      "epoch: 10 step: 235, loss is 4.6903121983632445e-05\n",
      "epoch: 10 step: 236, loss is 0.0002753735170699656\n",
      "epoch: 10 step: 237, loss is 0.0012713153846561909\n",
      "epoch: 10 step: 238, loss is 9.80389304459095e-05\n",
      "epoch: 10 step: 239, loss is 0.0037161321379244328\n",
      "epoch: 10 step: 240, loss is 1.3312357623362914e-05\n",
      "epoch: 10 step: 241, loss is 0.002174978842958808\n",
      "epoch: 10 step: 242, loss is 0.0030547857750207186\n",
      "epoch: 10 step: 243, loss is 0.01501746941357851\n",
      "epoch: 10 step: 244, loss is 0.012050634250044823\n",
      "epoch: 10 step: 245, loss is 9.517016587778926e-05\n",
      "epoch: 10 step: 246, loss is 0.01120962854474783\n",
      "epoch: 10 step: 247, loss is 0.0005029407911933959\n",
      "epoch: 10 step: 248, loss is 0.0020697060972452164\n",
      "epoch: 10 step: 249, loss is 0.0006321964901871979\n",
      "epoch: 10 step: 250, loss is 0.0004721613950096071\n",
      "epoch: 10 step: 251, loss is 0.0001521793456049636\n",
      "epoch: 10 step: 252, loss is 0.17561917006969452\n",
      "epoch: 10 step: 253, loss is 0.00023033138131722808\n",
      "epoch: 10 step: 254, loss is 9.783649147721007e-05\n",
      "epoch: 10 step: 255, loss is 0.00025667808949947357\n",
      "epoch: 10 step: 256, loss is 0.005738037638366222\n",
      "epoch: 10 step: 257, loss is 5.6757857237244025e-05\n",
      "epoch: 10 step: 258, loss is 0.0015402539866045117\n",
      "epoch: 10 step: 259, loss is 0.00031721440609544516\n",
      "epoch: 10 step: 260, loss is 8.534439984941855e-05\n",
      "epoch: 10 step: 261, loss is 0.0012818160466849804\n",
      "epoch: 10 step: 262, loss is 0.000840632535982877\n",
      "epoch: 10 step: 263, loss is 4.0010832890402526e-05\n",
      "epoch: 10 step: 264, loss is 3.196813486283645e-05\n",
      "epoch: 10 step: 265, loss is 0.000997633207589388\n",
      "epoch: 10 step: 266, loss is 0.00028082160861231387\n",
      "epoch: 10 step: 267, loss is 0.004017307888716459\n",
      "epoch: 10 step: 268, loss is 0.011487073265016079\n",
      "epoch: 10 step: 269, loss is 0.00013001440674997866\n",
      "epoch: 10 step: 270, loss is 0.0032995149958878756\n",
      "epoch: 10 step: 271, loss is 0.0003045129997190088\n",
      "epoch: 10 step: 272, loss is 0.0003335973306093365\n",
      "epoch: 10 step: 273, loss is 0.04478472098708153\n",
      "epoch: 10 step: 274, loss is 0.012394418939948082\n",
      "epoch: 10 step: 275, loss is 0.00013557319471146911\n",
      "epoch: 10 step: 276, loss is 0.0263727568089962\n",
      "epoch: 10 step: 277, loss is 0.0008321922505274415\n",
      "epoch: 10 step: 278, loss is 0.0033076407853513956\n",
      "epoch: 10 step: 279, loss is 0.0006597205647267401\n",
      "epoch: 10 step: 280, loss is 0.001007246202789247\n",
      "epoch: 10 step: 281, loss is 3.629390266723931e-05\n",
      "epoch: 10 step: 282, loss is 0.08546222746372223\n",
      "epoch: 10 step: 283, loss is 0.003427688032388687\n",
      "epoch: 10 step: 284, loss is 0.0003738008963409811\n",
      "epoch: 10 step: 285, loss is 0.07820028066635132\n",
      "epoch: 10 step: 286, loss is 5.9081692597828805e-05\n",
      "epoch: 10 step: 287, loss is 0.00026072110631503165\n",
      "epoch: 10 step: 288, loss is 0.012568341568112373\n",
      "epoch: 10 step: 289, loss is 0.003453860292211175\n",
      "epoch: 10 step: 290, loss is 0.0019368439679965377\n",
      "epoch: 10 step: 291, loss is 0.0005572743248194456\n",
      "epoch: 10 step: 292, loss is 0.10420501977205276\n",
      "epoch: 10 step: 293, loss is 0.0001441115018678829\n",
      "epoch: 10 step: 294, loss is 0.0659770518541336\n",
      "epoch: 10 step: 295, loss is 0.11623871326446533\n",
      "epoch: 10 step: 296, loss is 0.00041257808334194124\n",
      "epoch: 10 step: 297, loss is 0.03955730423331261\n",
      "epoch: 10 step: 298, loss is 0.0005466591683216393\n",
      "epoch: 10 step: 299, loss is 0.2952445149421692\n",
      "epoch: 10 step: 300, loss is 0.05676032975316048\n",
      "epoch: 10 step: 301, loss is 0.0013431940460577607\n",
      "epoch: 10 step: 302, loss is 0.0038174986839294434\n",
      "epoch: 10 step: 303, loss is 0.017114106565713882\n",
      "epoch: 10 step: 304, loss is 0.03153207153081894\n",
      "epoch: 10 step: 305, loss is 0.012203107587993145\n",
      "epoch: 10 step: 306, loss is 0.017644710838794708\n",
      "epoch: 10 step: 307, loss is 0.0004894798621535301\n",
      "epoch: 10 step: 308, loss is 0.014681236818432808\n",
      "epoch: 10 step: 309, loss is 0.0011024933774024248\n",
      "epoch: 10 step: 310, loss is 0.1689039021730423\n",
      "epoch: 10 step: 311, loss is 0.0015097911236807704\n",
      "epoch: 10 step: 312, loss is 0.004200500436127186\n",
      "epoch: 10 step: 313, loss is 0.005262511782348156\n",
      "epoch: 10 step: 314, loss is 0.007895605638623238\n",
      "epoch: 10 step: 315, loss is 0.009988339617848396\n",
      "epoch: 10 step: 316, loss is 0.018323073163628578\n",
      "epoch: 10 step: 317, loss is 0.0011070906184613705\n",
      "epoch: 10 step: 318, loss is 0.004768420476466417\n",
      "epoch: 10 step: 319, loss is 0.0009821977000683546\n",
      "epoch: 10 step: 320, loss is 0.0016125217080116272\n",
      "epoch: 10 step: 321, loss is 0.04927279055118561\n",
      "epoch: 10 step: 322, loss is 0.00023611636424902827\n",
      "epoch: 10 step: 323, loss is 0.00010455357551109046\n",
      "epoch: 10 step: 324, loss is 0.013421843759715557\n",
      "epoch: 10 step: 325, loss is 1.882702599687036e-05\n",
      "epoch: 10 step: 326, loss is 0.0007269724737852812\n",
      "epoch: 10 step: 327, loss is 0.03940391167998314\n",
      "epoch: 10 step: 328, loss is 0.009512016549706459\n",
      "epoch: 10 step: 329, loss is 0.023792047053575516\n",
      "epoch: 10 step: 330, loss is 4.525987242232077e-05\n",
      "epoch: 10 step: 331, loss is 0.0002054128417512402\n",
      "epoch: 10 step: 332, loss is 0.0004505228134803474\n",
      "epoch: 10 step: 333, loss is 0.003950975369662046\n",
      "epoch: 10 step: 334, loss is 0.0019832116086035967\n",
      "epoch: 10 step: 335, loss is 0.0004026886890642345\n",
      "epoch: 10 step: 336, loss is 0.0014590591890737414\n",
      "epoch: 10 step: 337, loss is 0.053484849631786346\n",
      "epoch: 10 step: 338, loss is 0.0007088640704751015\n",
      "epoch: 10 step: 339, loss is 0.0027803247794508934\n",
      "epoch: 10 step: 340, loss is 0.001186940586194396\n",
      "epoch: 10 step: 341, loss is 0.00017220221343450248\n",
      "epoch: 10 step: 342, loss is 0.0003761455009225756\n",
      "epoch: 10 step: 343, loss is 0.0004943018429912627\n",
      "epoch: 10 step: 344, loss is 0.0045325844548642635\n",
      "epoch: 10 step: 345, loss is 0.0001072446902981028\n",
      "epoch: 10 step: 346, loss is 0.00894143432378769\n",
      "epoch: 10 step: 347, loss is 1.2665320355154108e-05\n",
      "epoch: 10 step: 348, loss is 0.0025385799817740917\n",
      "epoch: 10 step: 349, loss is 0.020498204976320267\n",
      "epoch: 10 step: 350, loss is 0.010499224998056889\n",
      "epoch: 10 step: 351, loss is 4.405579238664359e-05\n",
      "epoch: 10 step: 352, loss is 0.0002004859270527959\n",
      "epoch: 10 step: 353, loss is 0.004967028740793467\n",
      "epoch: 10 step: 354, loss is 0.003421179251745343\n",
      "epoch: 10 step: 355, loss is 0.005071106366813183\n",
      "epoch: 10 step: 356, loss is 0.10854033380746841\n",
      "epoch: 10 step: 357, loss is 0.12065555155277252\n",
      "epoch: 10 step: 358, loss is 0.00011898052616743371\n",
      "epoch: 10 step: 359, loss is 0.0016996857011690736\n",
      "epoch: 10 step: 360, loss is 0.007959898561239243\n",
      "epoch: 10 step: 361, loss is 0.00026387677644379437\n",
      "epoch: 10 step: 362, loss is 0.004146090243011713\n",
      "epoch: 10 step: 363, loss is 7.675454253330827e-05\n",
      "epoch: 10 step: 364, loss is 0.011958219110965729\n",
      "epoch: 10 step: 365, loss is 0.0013377347495406866\n",
      "epoch: 10 step: 366, loss is 9.048949868883938e-05\n",
      "epoch: 10 step: 367, loss is 0.00022028126113582402\n",
      "epoch: 10 step: 368, loss is 0.0001744197797961533\n",
      "epoch: 10 step: 369, loss is 0.008559085428714752\n",
      "epoch: 10 step: 370, loss is 0.00020318050519563258\n",
      "epoch: 10 step: 371, loss is 0.00129948859103024\n",
      "epoch: 10 step: 372, loss is 0.0035426029935479164\n",
      "epoch: 10 step: 373, loss is 0.0022936200257390738\n",
      "epoch: 10 step: 374, loss is 0.0025939405895769596\n",
      "epoch: 10 step: 375, loss is 0.07305961102247238\n",
      "epoch: 10 step: 376, loss is 0.02163429744541645\n",
      "epoch: 10 step: 377, loss is 0.00477411225438118\n",
      "epoch: 10 step: 378, loss is 0.02088998630642891\n",
      "epoch: 10 step: 379, loss is 0.002931744558736682\n",
      "epoch: 10 step: 380, loss is 0.03995129093527794\n",
      "epoch: 10 step: 381, loss is 0.00023093103663995862\n",
      "epoch: 10 step: 382, loss is 0.0013193697668612003\n",
      "epoch: 10 step: 383, loss is 0.0007671095663681626\n",
      "epoch: 10 step: 384, loss is 0.05692072585225105\n",
      "epoch: 10 step: 385, loss is 0.004532489459961653\n",
      "epoch: 10 step: 386, loss is 0.003340922063216567\n",
      "epoch: 10 step: 387, loss is 0.0009725040290504694\n",
      "epoch: 10 step: 388, loss is 0.0033475132659077644\n",
      "epoch: 10 step: 389, loss is 7.838447345420718e-05\n",
      "epoch: 10 step: 390, loss is 0.00909167155623436\n",
      "epoch: 10 step: 391, loss is 0.006707023363560438\n",
      "epoch: 10 step: 392, loss is 0.006121802143752575\n",
      "epoch: 10 step: 393, loss is 0.07378381490707397\n",
      "epoch: 10 step: 394, loss is 0.00046665355330333114\n",
      "epoch: 10 step: 395, loss is 0.002188276033848524\n",
      "epoch: 10 step: 396, loss is 0.004604063928127289\n",
      "epoch: 10 step: 397, loss is 0.001808994566090405\n",
      "epoch: 10 step: 398, loss is 0.0033411995973438025\n",
      "epoch: 10 step: 399, loss is 0.004100538790225983\n",
      "epoch: 10 step: 400, loss is 0.0012627341784536839\n",
      "epoch: 10 step: 401, loss is 0.12239250540733337\n",
      "epoch: 10 step: 402, loss is 0.004193768836557865\n",
      "epoch: 10 step: 403, loss is 0.00011464858107501641\n",
      "epoch: 10 step: 404, loss is 0.012264061719179153\n",
      "epoch: 10 step: 405, loss is 0.006291481666266918\n",
      "epoch: 10 step: 406, loss is 0.0002939817786682397\n",
      "epoch: 10 step: 407, loss is 0.0006812562933191657\n",
      "epoch: 10 step: 408, loss is 0.00011878932127729058\n",
      "epoch: 10 step: 409, loss is 0.00025528401602059603\n",
      "epoch: 10 step: 410, loss is 0.013574543409049511\n",
      "epoch: 10 step: 411, loss is 4.395979431137675e-06\n",
      "epoch: 10 step: 412, loss is 0.005582489538937807\n",
      "epoch: 10 step: 413, loss is 3.274812115705572e-05\n",
      "epoch: 10 step: 414, loss is 0.0006388036999851465\n",
      "epoch: 10 step: 415, loss is 0.000191064304090105\n",
      "epoch: 10 step: 416, loss is 0.00010618374653859064\n",
      "epoch: 10 step: 417, loss is 0.0001946920237969607\n",
      "epoch: 10 step: 418, loss is 0.0007281245198100805\n",
      "epoch: 10 step: 419, loss is 0.0001708053023321554\n",
      "epoch: 10 step: 420, loss is 0.0001712181983748451\n",
      "epoch: 10 step: 421, loss is 3.683704198920168e-05\n",
      "epoch: 10 step: 422, loss is 1.8315047782380134e-05\n",
      "epoch: 10 step: 423, loss is 0.045427046716213226\n",
      "epoch: 10 step: 424, loss is 0.005193762481212616\n",
      "epoch: 10 step: 425, loss is 0.004589390009641647\n",
      "epoch: 10 step: 426, loss is 9.993819548981264e-05\n",
      "epoch: 10 step: 427, loss is 2.163144927180838e-05\n",
      "epoch: 10 step: 428, loss is 0.00012927928764838725\n",
      "epoch: 10 step: 429, loss is 0.004402192309498787\n",
      "epoch: 10 step: 430, loss is 0.0019228503806516528\n",
      "epoch: 10 step: 431, loss is 2.5000434106914327e-05\n",
      "epoch: 10 step: 432, loss is 0.0008132731891237199\n",
      "epoch: 10 step: 433, loss is 0.08224895596504211\n",
      "epoch: 10 step: 434, loss is 0.042867742478847504\n",
      "epoch: 10 step: 435, loss is 0.0006503926706500351\n",
      "epoch: 10 step: 436, loss is 7.930099673103541e-05\n",
      "epoch: 10 step: 437, loss is 0.036868222057819366\n",
      "epoch: 10 step: 438, loss is 0.00320567493326962\n",
      "epoch: 10 step: 439, loss is 0.00029373663710430264\n",
      "epoch: 10 step: 440, loss is 4.1766659705899656e-05\n",
      "epoch: 10 step: 441, loss is 7.226674642879516e-05\n",
      "epoch: 10 step: 442, loss is 0.0007186440634541214\n",
      "epoch: 10 step: 443, loss is 4.595243080984801e-05\n",
      "epoch: 10 step: 444, loss is 0.0002608674403745681\n",
      "epoch: 10 step: 445, loss is 0.01247791200876236\n",
      "epoch: 10 step: 446, loss is 0.0023510402534157038\n",
      "epoch: 10 step: 447, loss is 1.4463887964666355e-05\n",
      "epoch: 10 step: 448, loss is 0.007897637784481049\n",
      "epoch: 10 step: 449, loss is 0.00041295294067822397\n",
      "epoch: 10 step: 450, loss is 0.0007109440630301833\n",
      "epoch: 10 step: 451, loss is 0.0002695046423468739\n",
      "epoch: 10 step: 452, loss is 0.017429104074835777\n",
      "epoch: 10 step: 453, loss is 0.0016853277338668704\n",
      "epoch: 10 step: 454, loss is 0.0007146265124902129\n",
      "epoch: 10 step: 455, loss is 0.002050284529104829\n",
      "epoch: 10 step: 456, loss is 0.00015162103227339685\n",
      "epoch: 10 step: 457, loss is 0.06139436736702919\n",
      "epoch: 10 step: 458, loss is 0.0005803350359201431\n",
      "epoch: 10 step: 459, loss is 0.000387825071811676\n",
      "epoch: 10 step: 460, loss is 8.214974513975903e-05\n",
      "epoch: 10 step: 461, loss is 0.00039984809700399637\n",
      "epoch: 10 step: 462, loss is 5.080017581349239e-05\n",
      "epoch: 10 step: 463, loss is 0.0013013845309615135\n",
      "epoch: 10 step: 464, loss is 0.016102785244584084\n",
      "epoch: 10 step: 465, loss is 0.00023388881527353078\n",
      "epoch: 10 step: 466, loss is 3.151638111376087e-06\n",
      "epoch: 10 step: 467, loss is 0.014912509359419346\n",
      "epoch: 10 step: 468, loss is 0.00041887760744430125\n",
      "epoch: 10 step: 469, loss is 0.00015085314225871116\n",
      "epoch: 10 step: 470, loss is 0.07321873307228088\n",
      "epoch: 10 step: 471, loss is 4.991473542759195e-05\n",
      "epoch: 10 step: 472, loss is 0.004161553457379341\n",
      "epoch: 10 step: 473, loss is 0.006580134388059378\n",
      "epoch: 10 step: 474, loss is 3.0227582101360895e-05\n",
      "epoch: 10 step: 475, loss is 0.031979553401470184\n",
      "epoch: 10 step: 476, loss is 0.012408794835209846\n",
      "epoch: 10 step: 477, loss is 0.0002128196065314114\n",
      "epoch: 10 step: 478, loss is 0.035256277769804\n",
      "epoch: 10 step: 479, loss is 2.3134248294809368e-06\n",
      "epoch: 10 step: 480, loss is 0.027250569313764572\n",
      "epoch: 10 step: 481, loss is 0.012354632839560509\n",
      "epoch: 10 step: 482, loss is 0.04757814481854439\n",
      "epoch: 10 step: 483, loss is 0.05682346969842911\n",
      "epoch: 10 step: 484, loss is 0.13195231556892395\n",
      "epoch: 10 step: 485, loss is 0.1093086525797844\n",
      "epoch: 10 step: 486, loss is 0.0003776617522817105\n",
      "epoch: 10 step: 487, loss is 0.0006456368719227612\n",
      "epoch: 10 step: 488, loss is 0.051954109221696854\n",
      "epoch: 10 step: 489, loss is 0.0005597794661298394\n",
      "epoch: 10 step: 490, loss is 0.0008103480795398355\n",
      "epoch: 10 step: 491, loss is 0.0005549697671085596\n",
      "epoch: 10 step: 492, loss is 0.011599056422710419\n",
      "epoch: 10 step: 493, loss is 0.003991610370576382\n",
      "epoch: 10 step: 494, loss is 0.0026798234321177006\n",
      "epoch: 10 step: 495, loss is 2.722904537222348e-05\n",
      "epoch: 10 step: 496, loss is 0.007693111430853605\n",
      "epoch: 10 step: 497, loss is 0.012271571904420853\n",
      "epoch: 10 step: 498, loss is 0.13682901859283447\n",
      "epoch: 10 step: 499, loss is 0.016785962507128716\n",
      "epoch: 10 step: 500, loss is 0.014467468485236168\n",
      "epoch: 10 step: 501, loss is 0.0003563260252121836\n",
      "epoch: 10 step: 502, loss is 0.00019559901556931436\n",
      "epoch: 10 step: 503, loss is 0.0005808096611872315\n",
      "epoch: 10 step: 504, loss is 0.0002097840333590284\n",
      "epoch: 10 step: 505, loss is 0.00022536050528287888\n",
      "epoch: 10 step: 506, loss is 7.826113869668916e-05\n",
      "epoch: 10 step: 507, loss is 0.00011334962619002908\n",
      "epoch: 10 step: 508, loss is 0.0008229475351981819\n",
      "epoch: 10 step: 509, loss is 0.0003490324306767434\n",
      "epoch: 10 step: 510, loss is 0.00023722139303572476\n",
      "epoch: 10 step: 511, loss is 0.0024439021944999695\n",
      "epoch: 10 step: 512, loss is 0.04780751094222069\n",
      "epoch: 10 step: 513, loss is 0.0014367795083671808\n",
      "epoch: 10 step: 514, loss is 0.0008521274430677295\n",
      "epoch: 10 step: 515, loss is 0.00017419179494027048\n",
      "epoch: 10 step: 516, loss is 0.004414374008774757\n",
      "epoch: 10 step: 517, loss is 0.00019470420375000685\n",
      "epoch: 10 step: 518, loss is 0.0004381385224405676\n",
      "epoch: 10 step: 519, loss is 0.007078384980559349\n",
      "epoch: 10 step: 520, loss is 0.0005196189158596098\n",
      "epoch: 10 step: 521, loss is 0.002080754144117236\n",
      "epoch: 10 step: 522, loss is 0.0022026340011507273\n",
      "epoch: 10 step: 523, loss is 0.00035704157198779285\n",
      "epoch: 10 step: 524, loss is 0.0009721683454699814\n",
      "epoch: 10 step: 525, loss is 0.0012255302863195539\n",
      "epoch: 10 step: 526, loss is 0.006491425912827253\n",
      "epoch: 10 step: 527, loss is 0.07154285162687302\n",
      "epoch: 10 step: 528, loss is 0.11816143989562988\n",
      "epoch: 10 step: 529, loss is 0.18858219683170319\n",
      "epoch: 10 step: 530, loss is 0.0035966546274721622\n",
      "epoch: 10 step: 531, loss is 0.029779206961393356\n",
      "epoch: 10 step: 532, loss is 0.0015488485805690289\n",
      "epoch: 10 step: 533, loss is 0.021974671632051468\n",
      "epoch: 10 step: 534, loss is 0.009859862737357616\n",
      "epoch: 10 step: 535, loss is 0.04366877302527428\n",
      "epoch: 10 step: 536, loss is 0.003844051156193018\n",
      "epoch: 10 step: 537, loss is 0.00020890038285870105\n",
      "epoch: 10 step: 538, loss is 3.274835034972057e-05\n",
      "epoch: 10 step: 539, loss is 0.026147503405809402\n",
      "epoch: 10 step: 540, loss is 0.000611787021625787\n",
      "epoch: 10 step: 541, loss is 0.005029723979532719\n",
      "epoch: 10 step: 542, loss is 9.304899140261114e-05\n",
      "epoch: 10 step: 543, loss is 0.006187063176184893\n",
      "epoch: 10 step: 544, loss is 0.012233438901603222\n",
      "epoch: 10 step: 545, loss is 0.00048535867244936526\n",
      "epoch: 10 step: 546, loss is 0.0016298427945002913\n",
      "epoch: 10 step: 547, loss is 0.0011524544097483158\n",
      "epoch: 10 step: 548, loss is 0.0009979287860915065\n",
      "epoch: 10 step: 549, loss is 0.005117653403431177\n",
      "epoch: 10 step: 550, loss is 0.037765972316265106\n",
      "epoch: 10 step: 551, loss is 0.0003860672004520893\n",
      "epoch: 10 step: 552, loss is 0.0017894315533339977\n",
      "epoch: 10 step: 553, loss is 0.04175933077931404\n",
      "epoch: 10 step: 554, loss is 0.0006814371445216238\n",
      "epoch: 10 step: 555, loss is 0.00022066259407438338\n",
      "epoch: 10 step: 556, loss is 0.0003521857433952391\n",
      "epoch: 10 step: 557, loss is 0.08301082998514175\n",
      "epoch: 10 step: 558, loss is 0.009181304834783077\n",
      "epoch: 10 step: 559, loss is 0.010466438718140125\n",
      "epoch: 10 step: 560, loss is 0.006604067515581846\n",
      "epoch: 10 step: 561, loss is 0.0005541883874684572\n",
      "epoch: 10 step: 562, loss is 0.001574284746311605\n",
      "epoch: 10 step: 563, loss is 4.741286102216691e-05\n",
      "epoch: 10 step: 564, loss is 0.05947566404938698\n",
      "epoch: 10 step: 565, loss is 0.006380806677043438\n",
      "epoch: 10 step: 566, loss is 0.00022749138588551432\n",
      "epoch: 10 step: 567, loss is 0.0001664269220782444\n",
      "epoch: 10 step: 568, loss is 9.089298691833392e-05\n",
      "epoch: 10 step: 569, loss is 0.015468842349946499\n",
      "epoch: 10 step: 570, loss is 0.0010525311809033155\n",
      "epoch: 10 step: 571, loss is 0.0006548712844960392\n",
      "epoch: 10 step: 572, loss is 0.00563434325158596\n",
      "epoch: 10 step: 573, loss is 0.00038081605453044176\n",
      "epoch: 10 step: 574, loss is 0.00023103707644622773\n",
      "epoch: 10 step: 575, loss is 0.00024631049018353224\n",
      "epoch: 10 step: 576, loss is 0.000561266322620213\n",
      "epoch: 10 step: 577, loss is 0.0016262925928458571\n",
      "epoch: 10 step: 578, loss is 0.009779505431652069\n",
      "epoch: 10 step: 579, loss is 3.8280937587842345e-05\n",
      "epoch: 10 step: 580, loss is 0.0002836048952303827\n",
      "epoch: 10 step: 581, loss is 0.008564513176679611\n",
      "epoch: 10 step: 582, loss is 0.007039896212518215\n",
      "epoch: 10 step: 583, loss is 0.0003043257165700197\n",
      "epoch: 10 step: 584, loss is 0.00025597400963306427\n",
      "epoch: 10 step: 585, loss is 5.412922746472759e-06\n",
      "epoch: 10 step: 586, loss is 0.15681962668895721\n",
      "epoch: 10 step: 587, loss is 0.0010447537060827017\n",
      "epoch: 10 step: 588, loss is 4.206844459986314e-05\n",
      "epoch: 10 step: 589, loss is 0.00415347283706069\n",
      "epoch: 10 step: 590, loss is 0.0025992689188569784\n",
      "epoch: 10 step: 591, loss is 0.004275284707546234\n",
      "epoch: 10 step: 592, loss is 0.018966853618621826\n",
      "epoch: 10 step: 593, loss is 0.004980100784450769\n",
      "epoch: 10 step: 594, loss is 0.05864255130290985\n",
      "epoch: 10 step: 595, loss is 0.03866799548268318\n",
      "epoch: 10 step: 596, loss is 0.0014607025077566504\n",
      "epoch: 10 step: 597, loss is 0.0012534987181425095\n",
      "epoch: 10 step: 598, loss is 0.0005752175347879529\n",
      "epoch: 10 step: 599, loss is 0.0006869564531370997\n",
      "epoch: 10 step: 600, loss is 0.005415403284132481\n",
      "epoch: 10 step: 601, loss is 0.0002413109177723527\n",
      "epoch: 10 step: 602, loss is 0.0004775717679876834\n",
      "epoch: 10 step: 603, loss is 2.932329152827151e-05\n",
      "epoch: 10 step: 604, loss is 0.00014461753016803414\n",
      "epoch: 10 step: 605, loss is 0.0013816214632242918\n",
      "epoch: 10 step: 606, loss is 0.06940355151891708\n",
      "epoch: 10 step: 607, loss is 0.12337521463632584\n",
      "epoch: 10 step: 608, loss is 0.0036152719985693693\n",
      "epoch: 10 step: 609, loss is 0.0003317123919259757\n",
      "epoch: 10 step: 610, loss is 0.004579248372465372\n",
      "epoch: 10 step: 611, loss is 0.0005269108805805445\n",
      "epoch: 10 step: 612, loss is 0.019053518772125244\n",
      "epoch: 10 step: 613, loss is 0.00020415741892065853\n",
      "epoch: 10 step: 614, loss is 0.0020102225244045258\n",
      "epoch: 10 step: 615, loss is 0.0008375644683837891\n",
      "epoch: 10 step: 616, loss is 0.1284492462873459\n",
      "epoch: 10 step: 617, loss is 0.00046143040526658297\n",
      "epoch: 10 step: 618, loss is 0.1235571950674057\n",
      "epoch: 10 step: 619, loss is 0.20559100806713104\n",
      "epoch: 10 step: 620, loss is 0.014307532459497452\n",
      "epoch: 10 step: 621, loss is 0.007750817574560642\n",
      "epoch: 10 step: 622, loss is 0.012770644389092922\n",
      "epoch: 10 step: 623, loss is 0.006273136008530855\n",
      "epoch: 10 step: 624, loss is 0.000632218667306006\n",
      "epoch: 10 step: 625, loss is 0.10105529427528381\n",
      "epoch: 10 step: 626, loss is 0.00021634962467942387\n",
      "epoch: 10 step: 627, loss is 0.03077821619808674\n",
      "epoch: 10 step: 628, loss is 0.0009025153121910989\n",
      "epoch: 10 step: 629, loss is 0.0002484732249286026\n",
      "epoch: 10 step: 630, loss is 0.002525811782106757\n",
      "epoch: 10 step: 631, loss is 0.0024404723662883043\n",
      "epoch: 10 step: 632, loss is 5.184139445191249e-05\n",
      "epoch: 10 step: 633, loss is 0.0003243345709051937\n",
      "epoch: 10 step: 634, loss is 0.05512392148375511\n",
      "epoch: 10 step: 635, loss is 0.000849180098157376\n",
      "epoch: 10 step: 636, loss is 0.00023597230028826743\n",
      "epoch: 10 step: 637, loss is 0.00037259707460179925\n",
      "epoch: 10 step: 638, loss is 0.0007525917608290911\n",
      "epoch: 10 step: 639, loss is 0.0007027966203168035\n",
      "epoch: 10 step: 640, loss is 0.00036305515095591545\n",
      "epoch: 10 step: 641, loss is 0.006794275715947151\n",
      "epoch: 10 step: 642, loss is 0.004977778531610966\n",
      "epoch: 10 step: 643, loss is 8.868860459188e-05\n",
      "epoch: 10 step: 644, loss is 0.0010330763179808855\n",
      "epoch: 10 step: 645, loss is 0.00016484626394230872\n",
      "epoch: 10 step: 646, loss is 0.002376492600888014\n",
      "epoch: 10 step: 647, loss is 0.004657089710235596\n",
      "epoch: 10 step: 648, loss is 0.0022530423011630774\n",
      "epoch: 10 step: 649, loss is 0.0007697261171415448\n",
      "epoch: 10 step: 650, loss is 0.0019626179710030556\n",
      "epoch: 10 step: 651, loss is 0.005214204546064138\n",
      "epoch: 10 step: 652, loss is 0.0041912454180419445\n",
      "epoch: 10 step: 653, loss is 0.006177375093102455\n",
      "epoch: 10 step: 654, loss is 0.0005928574246354401\n",
      "epoch: 10 step: 655, loss is 0.010309898294508457\n",
      "epoch: 10 step: 656, loss is 0.006678263656795025\n",
      "epoch: 10 step: 657, loss is 0.0009179223561659455\n",
      "epoch: 10 step: 658, loss is 0.0005620454903692007\n",
      "epoch: 10 step: 659, loss is 0.011639376170933247\n",
      "epoch: 10 step: 660, loss is 0.000377735763322562\n",
      "epoch: 10 step: 661, loss is 0.00021775055211037397\n",
      "epoch: 10 step: 662, loss is 0.0003211138246115297\n",
      "epoch: 10 step: 663, loss is 0.0009491356904618442\n",
      "epoch: 10 step: 664, loss is 0.0012235484318807721\n",
      "epoch: 10 step: 665, loss is 0.02404208481311798\n",
      "epoch: 10 step: 666, loss is 0.010981741361320019\n",
      "epoch: 10 step: 667, loss is 0.00943237729370594\n",
      "epoch: 10 step: 668, loss is 0.0036735672038048506\n",
      "epoch: 10 step: 669, loss is 0.0005594879039563239\n",
      "epoch: 10 step: 670, loss is 0.0002421060053166002\n",
      "epoch: 10 step: 671, loss is 0.0010078948689624667\n",
      "epoch: 10 step: 672, loss is 0.003963083494454622\n",
      "epoch: 10 step: 673, loss is 0.0004406931111589074\n",
      "epoch: 10 step: 674, loss is 0.0004128286673221737\n",
      "epoch: 10 step: 675, loss is 0.0006394744268618524\n",
      "epoch: 10 step: 676, loss is 0.010006195865571499\n",
      "epoch: 10 step: 677, loss is 0.0005061609554104507\n",
      "epoch: 10 step: 678, loss is 0.004074172582477331\n",
      "epoch: 10 step: 679, loss is 0.06955508142709732\n",
      "epoch: 10 step: 680, loss is 0.0008676985162310302\n",
      "epoch: 10 step: 681, loss is 0.0016434586141258478\n",
      "epoch: 10 step: 682, loss is 0.000122207697131671\n",
      "epoch: 10 step: 683, loss is 0.004150399938225746\n",
      "epoch: 10 step: 684, loss is 0.0011495309881865978\n",
      "epoch: 10 step: 685, loss is 0.00035947622382082045\n",
      "epoch: 10 step: 686, loss is 0.01792687550187111\n",
      "epoch: 10 step: 687, loss is 0.0007670554332435131\n",
      "epoch: 10 step: 688, loss is 0.0007255795644596219\n",
      "epoch: 10 step: 689, loss is 0.012351369485259056\n",
      "epoch: 10 step: 690, loss is 6.228961865417659e-05\n",
      "epoch: 10 step: 691, loss is 0.000583173125050962\n",
      "epoch: 10 step: 692, loss is 0.007451766170561314\n",
      "epoch: 10 step: 693, loss is 0.001702877227216959\n",
      "epoch: 10 step: 694, loss is 0.0007197643863037229\n",
      "epoch: 10 step: 695, loss is 0.0026528730522841215\n",
      "epoch: 10 step: 696, loss is 0.029132351279258728\n",
      "epoch: 10 step: 697, loss is 0.0004216950910631567\n",
      "epoch: 10 step: 698, loss is 0.013246110640466213\n",
      "epoch: 10 step: 699, loss is 0.00014239901793189347\n",
      "epoch: 10 step: 700, loss is 0.0073166899383068085\n",
      "epoch: 10 step: 701, loss is 0.0017978010000661016\n",
      "epoch: 10 step: 702, loss is 0.002743468852713704\n",
      "epoch: 10 step: 703, loss is 0.003276320407167077\n",
      "epoch: 10 step: 704, loss is 0.008267584256827831\n",
      "epoch: 10 step: 705, loss is 0.029370300471782684\n",
      "epoch: 10 step: 706, loss is 0.014624428004026413\n",
      "epoch: 10 step: 707, loss is 0.0010315421968698502\n",
      "epoch: 10 step: 708, loss is 0.0017083733109757304\n",
      "epoch: 10 step: 709, loss is 2.211358878412284e-05\n",
      "epoch: 10 step: 710, loss is 0.14214113354682922\n",
      "epoch: 10 step: 711, loss is 0.24888992309570312\n",
      "epoch: 10 step: 712, loss is 5.491273896041093e-06\n",
      "epoch: 10 step: 713, loss is 3.936020584660582e-05\n",
      "epoch: 10 step: 714, loss is 0.000551002798601985\n",
      "epoch: 10 step: 715, loss is 0.002067473717033863\n",
      "epoch: 10 step: 716, loss is 0.00035293123801238835\n",
      "epoch: 10 step: 717, loss is 0.0013483139919117093\n",
      "epoch: 10 step: 718, loss is 0.0003792197094298899\n",
      "epoch: 10 step: 719, loss is 0.005907426122575998\n",
      "epoch: 10 step: 720, loss is 0.0003417292609810829\n",
      "epoch: 10 step: 721, loss is 1.6897205568966456e-05\n",
      "epoch: 10 step: 722, loss is 0.004148128442466259\n",
      "epoch: 10 step: 723, loss is 7.044453377602622e-05\n",
      "epoch: 10 step: 724, loss is 0.00511449109762907\n",
      "epoch: 10 step: 725, loss is 0.0049738893285393715\n",
      "epoch: 10 step: 726, loss is 0.0002289963886141777\n",
      "epoch: 10 step: 727, loss is 0.051096946001052856\n",
      "epoch: 10 step: 728, loss is 9.371913620270789e-05\n",
      "epoch: 10 step: 729, loss is 0.005265944171696901\n",
      "epoch: 10 step: 730, loss is 0.00024800412938930094\n",
      "epoch: 10 step: 731, loss is 0.0021164019126445055\n",
      "epoch: 10 step: 732, loss is 0.019891010597348213\n",
      "epoch: 10 step: 733, loss is 0.002753669396042824\n",
      "epoch: 10 step: 734, loss is 0.00044734106631949544\n",
      "epoch: 10 step: 735, loss is 0.00045495928497985005\n",
      "epoch: 10 step: 736, loss is 0.0004409347602631897\n",
      "epoch: 10 step: 737, loss is 0.001108322525396943\n",
      "epoch: 10 step: 738, loss is 0.0009296191856265068\n",
      "epoch: 10 step: 739, loss is 0.0013345535844564438\n",
      "epoch: 10 step: 740, loss is 2.7820617106044665e-05\n",
      "epoch: 10 step: 741, loss is 0.015393316745758057\n",
      "epoch: 10 step: 742, loss is 0.0005440064123831689\n",
      "epoch: 10 step: 743, loss is 0.0031287020538002253\n",
      "epoch: 10 step: 744, loss is 0.003055987413972616\n",
      "epoch: 10 step: 745, loss is 0.0009240246145054698\n",
      "epoch: 10 step: 746, loss is 4.415877629071474e-05\n",
      "epoch: 10 step: 747, loss is 0.02362016960978508\n",
      "epoch: 10 step: 748, loss is 0.16889461874961853\n",
      "epoch: 10 step: 749, loss is 0.00016723046428523958\n",
      "epoch: 10 step: 750, loss is 0.0005839826771989465\n",
      "epoch: 10 step: 751, loss is 0.023121803998947144\n",
      "epoch: 10 step: 752, loss is 0.015439363196492195\n",
      "epoch: 10 step: 753, loss is 0.003385823220014572\n",
      "epoch: 10 step: 754, loss is 0.027380628511309624\n",
      "epoch: 10 step: 755, loss is 5.148412583366735e-06\n",
      "epoch: 10 step: 756, loss is 0.0020825220271945\n",
      "epoch: 10 step: 757, loss is 0.00033004075521603227\n",
      "epoch: 10 step: 758, loss is 0.011024512350559235\n",
      "epoch: 10 step: 759, loss is 0.0015197895700111985\n",
      "epoch: 10 step: 760, loss is 4.842955604544841e-06\n",
      "epoch: 10 step: 761, loss is 0.00023974318173713982\n",
      "epoch: 10 step: 762, loss is 0.005279235541820526\n",
      "epoch: 10 step: 763, loss is 0.009543489664793015\n",
      "epoch: 10 step: 764, loss is 0.0010208816966041923\n",
      "epoch: 10 step: 765, loss is 0.002835172228515148\n",
      "epoch: 10 step: 766, loss is 0.005162657704204321\n",
      "epoch: 10 step: 767, loss is 0.00016482037608511746\n",
      "epoch: 10 step: 768, loss is 4.719013668363914e-05\n",
      "epoch: 10 step: 769, loss is 0.0009937769500538707\n",
      "epoch: 10 step: 770, loss is 4.919655839330517e-05\n",
      "epoch: 10 step: 771, loss is 0.016963165253400803\n",
      "epoch: 10 step: 772, loss is 0.0009751997422426939\n",
      "epoch: 10 step: 773, loss is 0.009091388434171677\n",
      "epoch: 10 step: 774, loss is 0.0006531933322548866\n",
      "epoch: 10 step: 775, loss is 0.011576962657272816\n",
      "epoch: 10 step: 776, loss is 1.7924379790201783e-05\n",
      "epoch: 10 step: 777, loss is 0.20532384514808655\n",
      "epoch: 10 step: 778, loss is 0.00014701898908242583\n",
      "epoch: 10 step: 779, loss is 0.009235563687980175\n",
      "epoch: 10 step: 780, loss is 4.077946869074367e-05\n",
      "epoch: 10 step: 781, loss is 0.07355282455682755\n",
      "epoch: 10 step: 782, loss is 0.0015917971031740308\n",
      "epoch: 10 step: 783, loss is 0.00046822737203910947\n",
      "epoch: 10 step: 784, loss is 0.0006222078227438033\n",
      "epoch: 10 step: 785, loss is 8.266363147413358e-05\n",
      "epoch: 10 step: 786, loss is 6.57061900710687e-05\n",
      "epoch: 10 step: 787, loss is 0.0006531720864586532\n",
      "epoch: 10 step: 788, loss is 0.0006789265898987651\n",
      "epoch: 10 step: 789, loss is 5.100114503875375e-05\n",
      "epoch: 10 step: 790, loss is 3.403192386031151e-05\n",
      "epoch: 10 step: 791, loss is 0.00037686616997234523\n",
      "epoch: 10 step: 792, loss is 0.0004976614727638662\n",
      "epoch: 10 step: 793, loss is 0.008305292576551437\n",
      "epoch: 10 step: 794, loss is 0.0020789429545402527\n",
      "epoch: 10 step: 795, loss is 0.0008306624367833138\n",
      "epoch: 10 step: 796, loss is 0.00045602081809192896\n",
      "epoch: 10 step: 797, loss is 0.005014903377741575\n",
      "epoch: 10 step: 798, loss is 0.0009552945266477764\n",
      "epoch: 10 step: 799, loss is 0.0013210116885602474\n",
      "epoch: 10 step: 800, loss is 0.00022349602659232914\n",
      "epoch: 10 step: 801, loss is 0.0022499433252960443\n",
      "epoch: 10 step: 802, loss is 0.3246617317199707\n",
      "epoch: 10 step: 803, loss is 0.0025961576029658318\n",
      "epoch: 10 step: 804, loss is 0.0010791908716782928\n",
      "epoch: 10 step: 805, loss is 0.00024776652571745217\n",
      "epoch: 10 step: 806, loss is 0.0069488040171563625\n",
      "epoch: 10 step: 807, loss is 5.288060492603108e-05\n",
      "epoch: 10 step: 808, loss is 0.0014893633779138327\n",
      "epoch: 10 step: 809, loss is 0.0006399457342922688\n",
      "epoch: 10 step: 810, loss is 0.00014285527868196368\n",
      "epoch: 10 step: 811, loss is 1.8965098206535913e-05\n",
      "epoch: 10 step: 812, loss is 0.02133357524871826\n",
      "epoch: 10 step: 813, loss is 9.317854710388929e-05\n",
      "epoch: 10 step: 814, loss is 0.15073512494564056\n",
      "epoch: 10 step: 815, loss is 0.003588220803067088\n",
      "epoch: 10 step: 816, loss is 0.01074108574539423\n",
      "epoch: 10 step: 817, loss is 0.000774094311054796\n",
      "epoch: 10 step: 818, loss is 0.006201711017638445\n",
      "epoch: 10 step: 819, loss is 0.05048610642552376\n",
      "epoch: 10 step: 820, loss is 2.407268220849801e-05\n",
      "epoch: 10 step: 821, loss is 0.0028629619628190994\n",
      "epoch: 10 step: 822, loss is 0.00156945432536304\n",
      "epoch: 10 step: 823, loss is 0.00047977035865187645\n",
      "epoch: 10 step: 824, loss is 0.012601925060153008\n",
      "epoch: 10 step: 825, loss is 0.0028468079399317503\n",
      "epoch: 10 step: 826, loss is 0.0004472215659916401\n",
      "epoch: 10 step: 827, loss is 0.0035510477609932423\n",
      "epoch: 10 step: 828, loss is 0.0013027624227106571\n",
      "epoch: 10 step: 829, loss is 0.032879386097192764\n",
      "epoch: 10 step: 830, loss is 0.0017541309352964163\n",
      "epoch: 10 step: 831, loss is 0.0023833345621824265\n",
      "epoch: 10 step: 832, loss is 0.03168938308954239\n",
      "epoch: 10 step: 833, loss is 0.06007746607065201\n",
      "epoch: 10 step: 834, loss is 0.00032245402690023184\n",
      "epoch: 10 step: 835, loss is 8.494204666931182e-05\n",
      "epoch: 10 step: 836, loss is 0.00045718037290498614\n",
      "epoch: 10 step: 837, loss is 0.00039671570993959904\n",
      "epoch: 10 step: 838, loss is 0.0010122798848897219\n",
      "epoch: 10 step: 839, loss is 0.0035032618325203657\n",
      "epoch: 10 step: 840, loss is 0.07501834630966187\n",
      "epoch: 10 step: 841, loss is 7.398096204269677e-05\n",
      "epoch: 10 step: 842, loss is 0.0006325922440737486\n",
      "epoch: 10 step: 843, loss is 0.012826373800635338\n",
      "epoch: 10 step: 844, loss is 0.002822721377015114\n",
      "epoch: 10 step: 845, loss is 0.00035119077074341476\n",
      "epoch: 10 step: 846, loss is 0.01238329242914915\n",
      "epoch: 10 step: 847, loss is 0.004544919356703758\n",
      "epoch: 10 step: 848, loss is 0.00042857261723838747\n",
      "epoch: 10 step: 849, loss is 0.0007912354194559157\n",
      "epoch: 10 step: 850, loss is 0.00011723476927727461\n",
      "epoch: 10 step: 851, loss is 0.0016096975887194276\n",
      "epoch: 10 step: 852, loss is 0.002695322735235095\n",
      "epoch: 10 step: 853, loss is 0.001203678548336029\n",
      "epoch: 10 step: 854, loss is 1.0341977031202987e-05\n",
      "epoch: 10 step: 855, loss is 0.0017156954854726791\n",
      "epoch: 10 step: 856, loss is 0.040641359984874725\n",
      "epoch: 10 step: 857, loss is 0.0008378172060474753\n",
      "epoch: 10 step: 858, loss is 0.014721859246492386\n",
      "epoch: 10 step: 859, loss is 0.0011796802282333374\n",
      "epoch: 10 step: 860, loss is 0.0013888463145121932\n",
      "epoch: 10 step: 861, loss is 9.926411439664662e-05\n",
      "epoch: 10 step: 862, loss is 0.001206614775583148\n",
      "epoch: 10 step: 863, loss is 0.002279391046613455\n",
      "epoch: 10 step: 864, loss is 0.016103189438581467\n",
      "epoch: 10 step: 865, loss is 0.04237591102719307\n",
      "epoch: 10 step: 866, loss is 0.0016414924757555127\n",
      "epoch: 10 step: 867, loss is 0.00020379932539071888\n",
      "epoch: 10 step: 868, loss is 0.010796800255775452\n",
      "epoch: 10 step: 869, loss is 0.037210337817668915\n",
      "epoch: 10 step: 870, loss is 0.08177050948143005\n",
      "epoch: 10 step: 871, loss is 0.00894900131970644\n",
      "epoch: 10 step: 872, loss is 0.01996697299182415\n",
      "epoch: 10 step: 873, loss is 0.09003185480833054\n",
      "epoch: 10 step: 874, loss is 0.00017563698929734528\n",
      "epoch: 10 step: 875, loss is 0.0004395289288368076\n",
      "epoch: 10 step: 876, loss is 4.302387242205441e-05\n",
      "epoch: 10 step: 877, loss is 0.0009536871802993119\n",
      "epoch: 10 step: 878, loss is 0.08239716291427612\n",
      "epoch: 10 step: 879, loss is 0.006547559518367052\n",
      "epoch: 10 step: 880, loss is 0.0002853299956768751\n",
      "epoch: 10 step: 881, loss is 0.008818542584776878\n",
      "epoch: 10 step: 882, loss is 0.007368814200162888\n",
      "epoch: 10 step: 883, loss is 0.0014290388207882643\n",
      "epoch: 10 step: 884, loss is 0.007069089915603399\n",
      "epoch: 10 step: 885, loss is 0.0002462831325829029\n",
      "epoch: 10 step: 886, loss is 0.011335205286741257\n",
      "epoch: 10 step: 887, loss is 0.0026657048147171736\n",
      "epoch: 10 step: 888, loss is 0.0010140291415154934\n",
      "epoch: 10 step: 889, loss is 0.004359482321888208\n",
      "epoch: 10 step: 890, loss is 0.00030238452018238604\n",
      "epoch: 10 step: 891, loss is 2.3311360564548522e-05\n",
      "epoch: 10 step: 892, loss is 0.1638101488351822\n",
      "epoch: 10 step: 893, loss is 0.00027217151364311576\n",
      "epoch: 10 step: 894, loss is 0.00014857239148113877\n",
      "epoch: 10 step: 895, loss is 0.0008944308501668274\n",
      "epoch: 10 step: 896, loss is 0.000958191929385066\n",
      "epoch: 10 step: 897, loss is 0.04716230183839798\n",
      "epoch: 10 step: 898, loss is 0.0023262074682861567\n",
      "epoch: 10 step: 899, loss is 0.002257118234410882\n",
      "epoch: 10 step: 900, loss is 0.0008041951223276556\n",
      "epoch: 10 step: 901, loss is 0.0325099416077137\n",
      "epoch: 10 step: 902, loss is 0.010381508618593216\n",
      "epoch: 10 step: 903, loss is 0.0020139641128480434\n",
      "epoch: 10 step: 904, loss is 0.12193304300308228\n",
      "epoch: 10 step: 905, loss is 1.933144812937826e-05\n",
      "epoch: 10 step: 906, loss is 0.03866363689303398\n",
      "epoch: 10 step: 907, loss is 0.09752321988344193\n",
      "epoch: 10 step: 908, loss is 0.030795741826295853\n",
      "epoch: 10 step: 909, loss is 0.010705968365073204\n",
      "epoch: 10 step: 910, loss is 0.009731772355735302\n",
      "epoch: 10 step: 911, loss is 0.001438621780835092\n",
      "epoch: 10 step: 912, loss is 0.00020222818420734257\n",
      "epoch: 10 step: 913, loss is 0.0002635023556649685\n",
      "epoch: 10 step: 914, loss is 0.0004013651341665536\n",
      "epoch: 10 step: 915, loss is 0.0014052896294742823\n",
      "epoch: 10 step: 916, loss is 0.0020703324116766453\n",
      "epoch: 10 step: 917, loss is 1.823589627747424e-05\n",
      "epoch: 10 step: 918, loss is 0.00139801565092057\n",
      "epoch: 10 step: 919, loss is 0.009034530259668827\n",
      "epoch: 10 step: 920, loss is 0.0004781206080224365\n",
      "epoch: 10 step: 921, loss is 0.12158814817667007\n",
      "epoch: 10 step: 922, loss is 0.002501371083781123\n",
      "epoch: 10 step: 923, loss is 0.002861842978745699\n",
      "epoch: 10 step: 924, loss is 0.04321269318461418\n",
      "epoch: 10 step: 925, loss is 0.0013386213686317205\n",
      "epoch: 10 step: 926, loss is 0.04079323634505272\n",
      "epoch: 10 step: 927, loss is 0.0026433442253619432\n",
      "epoch: 10 step: 928, loss is 0.002146438229829073\n",
      "epoch: 10 step: 929, loss is 0.00036537472624331713\n",
      "epoch: 10 step: 930, loss is 0.014384460635483265\n",
      "epoch: 10 step: 931, loss is 0.0036513381637632847\n",
      "epoch: 10 step: 932, loss is 0.0005482210544869304\n",
      "epoch: 10 step: 933, loss is 0.00046472722897306085\n",
      "epoch: 10 step: 934, loss is 0.015891144052147865\n",
      "epoch: 10 step: 935, loss is 0.03948666900396347\n",
      "epoch: 10 step: 936, loss is 0.0016394039848819375\n",
      "epoch: 10 step: 937, loss is 0.0043904464691877365\n",
      "epoch: 10 step: 938, loss is 0.00043571527930907905\n",
      "epoch: 10 step: 939, loss is 0.002009736141189933\n",
      "epoch: 10 step: 940, loss is 0.006177434232085943\n",
      "epoch: 10 step: 941, loss is 0.0017406685510650277\n",
      "epoch: 10 step: 942, loss is 0.0190531425178051\n",
      "epoch: 10 step: 943, loss is 0.0017681153258308768\n",
      "epoch: 10 step: 944, loss is 0.0015225186944007874\n",
      "epoch: 10 step: 945, loss is 0.007297467906028032\n",
      "epoch: 10 step: 946, loss is 8.02285285317339e-05\n",
      "epoch: 10 step: 947, loss is 0.003673241473734379\n",
      "epoch: 10 step: 948, loss is 0.05012090131640434\n",
      "epoch: 10 step: 949, loss is 0.00089037767611444\n",
      "epoch: 10 step: 950, loss is 0.0018067843047901988\n",
      "epoch: 10 step: 951, loss is 3.024235411430709e-05\n",
      "epoch: 10 step: 952, loss is 2.8891026886412874e-05\n",
      "epoch: 10 step: 953, loss is 0.04980519413948059\n",
      "epoch: 10 step: 954, loss is 0.001698962296359241\n",
      "epoch: 10 step: 955, loss is 0.019315404817461967\n",
      "epoch: 10 step: 956, loss is 0.001545579289086163\n",
      "epoch: 10 step: 957, loss is 0.05693379417061806\n",
      "epoch: 10 step: 958, loss is 7.59068425395526e-05\n",
      "epoch: 10 step: 959, loss is 0.0002852598554454744\n",
      "epoch: 10 step: 960, loss is 0.0016086117830127478\n",
      "epoch: 10 step: 961, loss is 0.00014339409244712442\n",
      "epoch: 10 step: 962, loss is 0.0009598869364708662\n",
      "epoch: 10 step: 963, loss is 0.00857604667544365\n",
      "epoch: 10 step: 964, loss is 0.000209653124329634\n",
      "epoch: 10 step: 965, loss is 0.0007159474189393222\n",
      "epoch: 10 step: 966, loss is 0.00017822839436121285\n",
      "epoch: 10 step: 967, loss is 0.0005326251848600805\n",
      "epoch: 10 step: 968, loss is 0.008918160572648048\n",
      "epoch: 10 step: 969, loss is 0.0004354591656010598\n",
      "epoch: 10 step: 970, loss is 7.603403355460614e-05\n",
      "epoch: 10 step: 971, loss is 0.0514536015689373\n",
      "epoch: 10 step: 972, loss is 0.005726640112698078\n",
      "epoch: 10 step: 973, loss is 8.291161066154018e-05\n",
      "epoch: 10 step: 974, loss is 0.006439291872084141\n",
      "epoch: 10 step: 975, loss is 0.0071440148167312145\n",
      "epoch: 10 step: 976, loss is 6.650698196608573e-05\n",
      "epoch: 10 step: 977, loss is 0.0005076962406747043\n",
      "epoch: 10 step: 978, loss is 0.03501181676983833\n",
      "epoch: 10 step: 979, loss is 0.0009719961672089994\n",
      "epoch: 10 step: 980, loss is 0.011951352469623089\n",
      "epoch: 10 step: 981, loss is 0.0002793547755572945\n",
      "epoch: 10 step: 982, loss is 0.020695608109235764\n",
      "epoch: 10 step: 983, loss is 0.0008467594743706286\n",
      "epoch: 10 step: 984, loss is 0.0011366272810846567\n",
      "epoch: 10 step: 985, loss is 0.006612367462366819\n",
      "epoch: 10 step: 986, loss is 0.0003923959448002279\n",
      "epoch: 10 step: 987, loss is 2.445505560899619e-05\n",
      "epoch: 10 step: 988, loss is 0.03504158928990364\n",
      "epoch: 10 step: 989, loss is 0.0005384594551287591\n",
      "epoch: 10 step: 990, loss is 0.002229473553597927\n",
      "epoch: 10 step: 991, loss is 0.003124476643279195\n",
      "epoch: 10 step: 992, loss is 0.0002241901820525527\n",
      "epoch: 10 step: 993, loss is 0.0003816202515736222\n",
      "epoch: 10 step: 994, loss is 0.02541350945830345\n",
      "epoch: 10 step: 995, loss is 0.0019919814076274633\n",
      "epoch: 10 step: 996, loss is 0.0015047733904793859\n",
      "epoch: 10 step: 997, loss is 1.1910201465070713e-05\n",
      "epoch: 10 step: 998, loss is 0.0009747037547640502\n",
      "epoch: 10 step: 999, loss is 0.0019343749154359102\n",
      "epoch: 10 step: 1000, loss is 0.09601797163486481\n",
      "epoch: 10 step: 1001, loss is 0.00013516709441319108\n",
      "epoch: 10 step: 1002, loss is 0.015236151404678822\n",
      "epoch: 10 step: 1003, loss is 0.0002384801337029785\n",
      "epoch: 10 step: 1004, loss is 0.0006527837831526995\n",
      "epoch: 10 step: 1005, loss is 0.00010055215534521267\n",
      "epoch: 10 step: 1006, loss is 4.5968161430209875e-05\n",
      "epoch: 10 step: 1007, loss is 0.004287370480597019\n",
      "epoch: 10 step: 1008, loss is 0.002001479733735323\n",
      "epoch: 10 step: 1009, loss is 0.05741328001022339\n",
      "epoch: 10 step: 1010, loss is 0.013682381249964237\n",
      "epoch: 10 step: 1011, loss is 0.0010827806545421481\n",
      "epoch: 10 step: 1012, loss is 8.17383115645498e-05\n",
      "epoch: 10 step: 1013, loss is 0.013008602894842625\n",
      "epoch: 10 step: 1014, loss is 0.00017053511692211032\n",
      "epoch: 10 step: 1015, loss is 0.001235530129633844\n",
      "epoch: 10 step: 1016, loss is 0.0007551466696895659\n",
      "epoch: 10 step: 1017, loss is 0.046816710382699966\n",
      "epoch: 10 step: 1018, loss is 0.013105765916407108\n",
      "epoch: 10 step: 1019, loss is 0.004281220026314259\n",
      "epoch: 10 step: 1020, loss is 0.005434506572782993\n",
      "epoch: 10 step: 1021, loss is 0.019314685836434364\n",
      "epoch: 10 step: 1022, loss is 0.0002747792750597\n",
      "epoch: 10 step: 1023, loss is 0.0018406284507364035\n",
      "epoch: 10 step: 1024, loss is 0.0008321345085278153\n",
      "epoch: 10 step: 1025, loss is 0.0001175489742308855\n",
      "epoch: 10 step: 1026, loss is 0.005033903755247593\n",
      "epoch: 10 step: 1027, loss is 0.001387304626405239\n",
      "epoch: 10 step: 1028, loss is 0.010866496711969376\n",
      "epoch: 10 step: 1029, loss is 0.014799567870795727\n",
      "epoch: 10 step: 1030, loss is 0.003409594064578414\n",
      "epoch: 10 step: 1031, loss is 0.0008492222405038774\n",
      "epoch: 10 step: 1032, loss is 0.0031078243628144264\n",
      "epoch: 10 step: 1033, loss is 0.007638937793672085\n",
      "epoch: 10 step: 1034, loss is 0.0004248918849043548\n",
      "epoch: 10 step: 1035, loss is 2.8842903702752665e-05\n",
      "epoch: 10 step: 1036, loss is 0.0004030652344226837\n",
      "epoch: 10 step: 1037, loss is 0.0008959559490904212\n",
      "epoch: 10 step: 1038, loss is 0.00098803685978055\n",
      "epoch: 10 step: 1039, loss is 0.0001980313245439902\n",
      "epoch: 10 step: 1040, loss is 0.0003581116034183651\n",
      "epoch: 10 step: 1041, loss is 0.004374606069177389\n",
      "epoch: 10 step: 1042, loss is 0.03505369648337364\n",
      "epoch: 10 step: 1043, loss is 0.0025571249425411224\n",
      "epoch: 10 step: 1044, loss is 0.0019962668884545565\n",
      "epoch: 10 step: 1045, loss is 0.00021131373068783432\n",
      "epoch: 10 step: 1046, loss is 0.00117110600695014\n",
      "epoch: 10 step: 1047, loss is 2.4914616005844437e-05\n",
      "epoch: 10 step: 1048, loss is 0.0011282266350463033\n",
      "epoch: 10 step: 1049, loss is 0.04500674083828926\n",
      "epoch: 10 step: 1050, loss is 0.0022216900251805782\n",
      "epoch: 10 step: 1051, loss is 0.0035497606731951237\n",
      "epoch: 10 step: 1052, loss is 0.0002471689658705145\n",
      "epoch: 10 step: 1053, loss is 0.024309124797582626\n",
      "epoch: 10 step: 1054, loss is 0.00043262538383714855\n",
      "epoch: 10 step: 1055, loss is 0.15554511547088623\n",
      "epoch: 10 step: 1056, loss is 0.16623181104660034\n",
      "epoch: 10 step: 1057, loss is 0.0013828444061800838\n",
      "epoch: 10 step: 1058, loss is 1.3235380720288958e-05\n",
      "epoch: 10 step: 1059, loss is 0.004775546491146088\n",
      "epoch: 10 step: 1060, loss is 0.0011783767258748412\n",
      "epoch: 10 step: 1061, loss is 0.09349556267261505\n",
      "epoch: 10 step: 1062, loss is 0.0006532497354783118\n",
      "epoch: 10 step: 1063, loss is 0.00069176044780761\n",
      "epoch: 10 step: 1064, loss is 8.382670057471842e-05\n",
      "epoch: 10 step: 1065, loss is 0.0020476263016462326\n",
      "epoch: 10 step: 1066, loss is 0.00335987051948905\n",
      "epoch: 10 step: 1067, loss is 1.9864606656483375e-05\n",
      "epoch: 10 step: 1068, loss is 0.0003621061041485518\n",
      "epoch: 10 step: 1069, loss is 0.01038815826177597\n",
      "epoch: 10 step: 1070, loss is 0.00019842626352328807\n",
      "epoch: 10 step: 1071, loss is 0.00011952979548368603\n",
      "epoch: 10 step: 1072, loss is 0.0012691884767264128\n",
      "epoch: 10 step: 1073, loss is 0.0001169398965430446\n",
      "epoch: 10 step: 1074, loss is 0.03636222705245018\n",
      "epoch: 10 step: 1075, loss is 0.0004747344646602869\n",
      "epoch: 10 step: 1076, loss is 8.742086356505752e-05\n",
      "epoch: 10 step: 1077, loss is 0.019285481423139572\n",
      "epoch: 10 step: 1078, loss is 0.0027124136686325073\n",
      "epoch: 10 step: 1079, loss is 0.00030631289700977504\n",
      "epoch: 10 step: 1080, loss is 0.0001388105156365782\n",
      "epoch: 10 step: 1081, loss is 0.17535662651062012\n",
      "epoch: 10 step: 1082, loss is 0.007424709852784872\n",
      "epoch: 10 step: 1083, loss is 0.014719626866281033\n",
      "epoch: 10 step: 1084, loss is 0.0019349970389157534\n",
      "epoch: 10 step: 1085, loss is 0.008716655895113945\n",
      "epoch: 10 step: 1086, loss is 0.0029972379561513662\n",
      "epoch: 10 step: 1087, loss is 0.0005533157382160425\n",
      "epoch: 10 step: 1088, loss is 0.13493016362190247\n",
      "epoch: 10 step: 1089, loss is 0.0371408648788929\n",
      "epoch: 10 step: 1090, loss is 0.006346848793327808\n",
      "epoch: 10 step: 1091, loss is 0.0684693306684494\n",
      "epoch: 10 step: 1092, loss is 8.989130583358929e-05\n",
      "epoch: 10 step: 1093, loss is 0.05873825401067734\n",
      "epoch: 10 step: 1094, loss is 0.0004751280357595533\n",
      "epoch: 10 step: 1095, loss is 0.00023844285169616342\n",
      "epoch: 10 step: 1096, loss is 0.1458989977836609\n",
      "epoch: 10 step: 1097, loss is 0.01669922098517418\n",
      "epoch: 10 step: 1098, loss is 0.09502562135457993\n",
      "epoch: 10 step: 1099, loss is 0.0006034215330146253\n",
      "epoch: 10 step: 1100, loss is 0.08792997151613235\n",
      "epoch: 10 step: 1101, loss is 0.00015286543930415064\n",
      "epoch: 10 step: 1102, loss is 0.0342632494866848\n",
      "epoch: 10 step: 1103, loss is 0.0024101054295897484\n",
      "epoch: 10 step: 1104, loss is 0.12351492792367935\n",
      "epoch: 10 step: 1105, loss is 6.470958851423347e-06\n",
      "epoch: 10 step: 1106, loss is 6.15437875239877e-06\n",
      "epoch: 10 step: 1107, loss is 0.0019115549512207508\n",
      "epoch: 10 step: 1108, loss is 6.960185419302434e-05\n",
      "epoch: 10 step: 1109, loss is 0.0015903871972113848\n",
      "epoch: 10 step: 1110, loss is 0.0002654974814504385\n",
      "epoch: 10 step: 1111, loss is 0.001048517762683332\n",
      "epoch: 10 step: 1112, loss is 0.00012602026981767267\n",
      "epoch: 10 step: 1113, loss is 1.5983170669642277e-05\n",
      "epoch: 10 step: 1114, loss is 0.002324309665709734\n",
      "epoch: 10 step: 1115, loss is 0.002229411853477359\n",
      "epoch: 10 step: 1116, loss is 0.00016993095050565898\n",
      "epoch: 10 step: 1117, loss is 0.005397774279117584\n",
      "epoch: 10 step: 1118, loss is 0.018075963482260704\n",
      "epoch: 10 step: 1119, loss is 0.00039956948603503406\n",
      "epoch: 10 step: 1120, loss is 0.055952902883291245\n",
      "epoch: 10 step: 1121, loss is 0.0002676875446923077\n",
      "epoch: 10 step: 1122, loss is 0.0005805537803098559\n",
      "epoch: 10 step: 1123, loss is 0.0027848363388329744\n",
      "epoch: 10 step: 1124, loss is 0.0005672484403476119\n",
      "epoch: 10 step: 1125, loss is 0.0007896375609561801\n",
      "epoch: 10 step: 1126, loss is 0.0009502051980234683\n",
      "epoch: 10 step: 1127, loss is 0.04113755002617836\n",
      "epoch: 10 step: 1128, loss is 8.859177796693984e-06\n",
      "epoch: 10 step: 1129, loss is 0.010896192863583565\n",
      "epoch: 10 step: 1130, loss is 0.00022449898824561387\n",
      "epoch: 10 step: 1131, loss is 0.07805951684713364\n",
      "epoch: 10 step: 1132, loss is 0.18403948843479156\n",
      "epoch: 10 step: 1133, loss is 0.0026839319616556168\n",
      "epoch: 10 step: 1134, loss is 0.020045557990670204\n",
      "epoch: 10 step: 1135, loss is 0.01768304780125618\n",
      "epoch: 10 step: 1136, loss is 0.001097582746297121\n",
      "epoch: 10 step: 1137, loss is 0.001207925146445632\n",
      "epoch: 10 step: 1138, loss is 0.0001055058601195924\n",
      "epoch: 10 step: 1139, loss is 0.03802037239074707\n",
      "epoch: 10 step: 1140, loss is 0.000529626733623445\n",
      "epoch: 10 step: 1141, loss is 0.0030448823235929012\n",
      "epoch: 10 step: 1142, loss is 0.001515734358690679\n",
      "epoch: 10 step: 1143, loss is 2.249032331747003e-05\n",
      "epoch: 10 step: 1144, loss is 0.0009507519425824285\n",
      "epoch: 10 step: 1145, loss is 0.0015829172916710377\n",
      "epoch: 10 step: 1146, loss is 0.00046752451453357935\n",
      "epoch: 10 step: 1147, loss is 0.0001468971895519644\n",
      "epoch: 10 step: 1148, loss is 0.03354255482554436\n",
      "epoch: 10 step: 1149, loss is 0.00020031913300044835\n",
      "epoch: 10 step: 1150, loss is 0.03032783418893814\n",
      "epoch: 10 step: 1151, loss is 0.00029717845609411597\n",
      "epoch: 10 step: 1152, loss is 0.06975195556879044\n",
      "epoch: 10 step: 1153, loss is 0.0044830902479588985\n",
      "epoch: 10 step: 1154, loss is 0.04211669787764549\n",
      "epoch: 10 step: 1155, loss is 0.033003754913806915\n",
      "epoch: 10 step: 1156, loss is 0.0001879557967185974\n",
      "epoch: 10 step: 1157, loss is 0.06505311280488968\n",
      "epoch: 10 step: 1158, loss is 0.0003121858462691307\n",
      "epoch: 10 step: 1159, loss is 0.23558346927165985\n",
      "epoch: 10 step: 1160, loss is 0.009673296473920345\n",
      "epoch: 10 step: 1161, loss is 0.00012878718553110957\n",
      "epoch: 10 step: 1162, loss is 0.004197986796498299\n",
      "epoch: 10 step: 1163, loss is 5.5636948673054576e-05\n",
      "epoch: 10 step: 1164, loss is 0.0002821566304191947\n",
      "epoch: 10 step: 1165, loss is 8.196113049052656e-05\n",
      "epoch: 10 step: 1166, loss is 0.00011745384836103767\n",
      "epoch: 10 step: 1167, loss is 0.0031707927118986845\n",
      "epoch: 10 step: 1168, loss is 0.00019264017464593053\n",
      "epoch: 10 step: 1169, loss is 0.004159773234277964\n",
      "epoch: 10 step: 1170, loss is 0.07110650837421417\n",
      "epoch: 10 step: 1171, loss is 0.000605589768383652\n",
      "epoch: 10 step: 1172, loss is 0.0012132066767662764\n",
      "epoch: 10 step: 1173, loss is 0.004951493348926306\n",
      "epoch: 10 step: 1174, loss is 0.08021076023578644\n",
      "epoch: 10 step: 1175, loss is 0.00028754080994986\n",
      "epoch: 10 step: 1176, loss is 0.03250868618488312\n",
      "epoch: 10 step: 1177, loss is 0.0029272965621203184\n",
      "epoch: 10 step: 1178, loss is 0.00011342841753503308\n",
      "epoch: 10 step: 1179, loss is 0.005461756605654955\n",
      "epoch: 10 step: 1180, loss is 0.011380945332348347\n",
      "epoch: 10 step: 1181, loss is 0.010832528583705425\n",
      "epoch: 10 step: 1182, loss is 0.0035320818424224854\n",
      "epoch: 10 step: 1183, loss is 0.005814120639115572\n",
      "epoch: 10 step: 1184, loss is 0.041879162192344666\n",
      "epoch: 10 step: 1185, loss is 0.0020911460742354393\n",
      "epoch: 10 step: 1186, loss is 0.00013991488958708942\n",
      "epoch: 10 step: 1187, loss is 8.9341338025406e-05\n",
      "epoch: 10 step: 1188, loss is 0.0007107898127287626\n",
      "epoch: 10 step: 1189, loss is 0.11292500793933868\n",
      "epoch: 10 step: 1190, loss is 0.31284549832344055\n",
      "epoch: 10 step: 1191, loss is 0.0010213948553428054\n",
      "epoch: 10 step: 1192, loss is 0.0015994067071005702\n",
      "epoch: 10 step: 1193, loss is 0.003990714438259602\n",
      "epoch: 10 step: 1194, loss is 0.00019387737847864628\n",
      "epoch: 10 step: 1195, loss is 0.11636759340763092\n",
      "epoch: 10 step: 1196, loss is 0.002349869580939412\n",
      "epoch: 10 step: 1197, loss is 0.04162061959505081\n",
      "epoch: 10 step: 1198, loss is 0.002413657959550619\n",
      "epoch: 10 step: 1199, loss is 0.04665396362543106\n",
      "epoch: 10 step: 1200, loss is 0.11755328625440598\n",
      "epoch: 10 step: 1201, loss is 0.0011361180804669857\n",
      "epoch: 10 step: 1202, loss is 0.0008291922276839614\n",
      "epoch: 10 step: 1203, loss is 0.00010817287693498656\n",
      "epoch: 10 step: 1204, loss is 0.000112917237856891\n",
      "epoch: 10 step: 1205, loss is 0.0005096605746075511\n",
      "epoch: 10 step: 1206, loss is 0.0012681973166763783\n",
      "epoch: 10 step: 1207, loss is 0.0007743096794001758\n",
      "epoch: 10 step: 1208, loss is 0.00433859508484602\n",
      "epoch: 10 step: 1209, loss is 0.003941941540688276\n",
      "epoch: 10 step: 1210, loss is 0.0019157987553626299\n",
      "epoch: 10 step: 1211, loss is 0.0011022044345736504\n",
      "epoch: 10 step: 1212, loss is 0.01157538965344429\n",
      "epoch: 10 step: 1213, loss is 0.09237927943468094\n",
      "epoch: 10 step: 1214, loss is 0.0018502762541174889\n",
      "epoch: 10 step: 1215, loss is 0.0068319677375257015\n",
      "epoch: 10 step: 1216, loss is 0.008569253608584404\n",
      "epoch: 10 step: 1217, loss is 0.07826686650514603\n",
      "epoch: 10 step: 1218, loss is 0.007361583411693573\n",
      "epoch: 10 step: 1219, loss is 0.030240755528211594\n",
      "epoch: 10 step: 1220, loss is 0.005696069914847612\n",
      "epoch: 10 step: 1221, loss is 0.009472327306866646\n",
      "epoch: 10 step: 1222, loss is 0.004405511077493429\n",
      "epoch: 10 step: 1223, loss is 0.007856440730392933\n",
      "epoch: 10 step: 1224, loss is 0.003936886787414551\n",
      "epoch: 10 step: 1225, loss is 0.01521136611700058\n",
      "epoch: 10 step: 1226, loss is 0.00015223778609652072\n",
      "epoch: 10 step: 1227, loss is 0.0005197555292397738\n",
      "epoch: 10 step: 1228, loss is 0.010240632109344006\n",
      "epoch: 10 step: 1229, loss is 0.0023796623572707176\n",
      "epoch: 10 step: 1230, loss is 0.05819832906126976\n",
      "epoch: 10 step: 1231, loss is 0.000657073687762022\n",
      "epoch: 10 step: 1232, loss is 4.029744741274044e-05\n",
      "epoch: 10 step: 1233, loss is 6.28737107035704e-05\n",
      "epoch: 10 step: 1234, loss is 0.051785752177238464\n",
      "epoch: 10 step: 1235, loss is 0.003410852514207363\n",
      "epoch: 10 step: 1236, loss is 5.158384738024324e-05\n",
      "epoch: 10 step: 1237, loss is 0.002012432785704732\n",
      "epoch: 10 step: 1238, loss is 0.04028069227933884\n",
      "epoch: 10 step: 1239, loss is 0.00021181497140787542\n",
      "epoch: 10 step: 1240, loss is 0.0015774799976497889\n",
      "epoch: 10 step: 1241, loss is 0.3255232274532318\n",
      "epoch: 10 step: 1242, loss is 0.027346059679985046\n",
      "epoch: 10 step: 1243, loss is 0.004734409973025322\n",
      "epoch: 10 step: 1244, loss is 0.004647310823202133\n",
      "epoch: 10 step: 1245, loss is 5.806214176118374e-05\n",
      "epoch: 10 step: 1246, loss is 0.0037808106280863285\n",
      "epoch: 10 step: 1247, loss is 0.0003228132554795593\n",
      "epoch: 10 step: 1248, loss is 0.0002249454555567354\n",
      "epoch: 10 step: 1249, loss is 0.005136509891599417\n",
      "epoch: 10 step: 1250, loss is 0.04991794377565384\n",
      "epoch: 10 step: 1251, loss is 0.06095752492547035\n",
      "epoch: 10 step: 1252, loss is 0.01662733592092991\n",
      "epoch: 10 step: 1253, loss is 0.0029581242706626654\n",
      "epoch: 10 step: 1254, loss is 0.0015335320495069027\n",
      "epoch: 10 step: 1255, loss is 0.001115457620471716\n",
      "epoch: 10 step: 1256, loss is 0.18840554356575012\n",
      "epoch: 10 step: 1257, loss is 0.000287839793600142\n",
      "epoch: 10 step: 1258, loss is 0.15617811679840088\n",
      "epoch: 10 step: 1259, loss is 0.2023780792951584\n",
      "epoch: 10 step: 1260, loss is 0.09227430820465088\n",
      "epoch: 10 step: 1261, loss is 0.04432064667344093\n",
      "epoch: 10 step: 1262, loss is 0.03397958353161812\n",
      "epoch: 10 step: 1263, loss is 0.01527745183557272\n",
      "epoch: 10 step: 1264, loss is 0.00033472987706772983\n",
      "epoch: 10 step: 1265, loss is 0.001897108624689281\n",
      "epoch: 10 step: 1266, loss is 0.015533401630818844\n",
      "epoch: 10 step: 1267, loss is 0.002310022944584489\n",
      "epoch: 10 step: 1268, loss is 0.1691717505455017\n",
      "epoch: 10 step: 1269, loss is 0.001412948127835989\n",
      "epoch: 10 step: 1270, loss is 0.00032484810799360275\n",
      "epoch: 10 step: 1271, loss is 0.011471430771052837\n",
      "epoch: 10 step: 1272, loss is 0.0017901624087244272\n",
      "epoch: 10 step: 1273, loss is 0.013352947309613228\n",
      "epoch: 10 step: 1274, loss is 0.0012875109678134322\n",
      "epoch: 10 step: 1275, loss is 0.0014029835583642125\n",
      "epoch: 10 step: 1276, loss is 0.00246965023688972\n",
      "epoch: 10 step: 1277, loss is 0.024828389286994934\n",
      "epoch: 10 step: 1278, loss is 0.0006772720953449607\n",
      "epoch: 10 step: 1279, loss is 0.009241132996976376\n",
      "epoch: 10 step: 1280, loss is 0.0897221788764\n",
      "epoch: 10 step: 1281, loss is 0.00047512922901660204\n",
      "epoch: 10 step: 1282, loss is 0.07613424956798553\n",
      "epoch: 10 step: 1283, loss is 0.006643800996243954\n",
      "epoch: 10 step: 1284, loss is 5.398571374826133e-05\n",
      "epoch: 10 step: 1285, loss is 0.0019227315206080675\n",
      "epoch: 10 step: 1286, loss is 0.13481149077415466\n",
      "epoch: 10 step: 1287, loss is 0.0020285779610276222\n",
      "epoch: 10 step: 1288, loss is 0.0006094909040257335\n",
      "epoch: 10 step: 1289, loss is 0.0004911854630336165\n",
      "epoch: 10 step: 1290, loss is 0.0006170962005853653\n",
      "epoch: 10 step: 1291, loss is 0.0018883167067542672\n",
      "epoch: 10 step: 1292, loss is 0.0010540223447605968\n",
      "epoch: 10 step: 1293, loss is 0.016282441094517708\n",
      "epoch: 10 step: 1294, loss is 0.001123201334849\n",
      "epoch: 10 step: 1295, loss is 0.01642744429409504\n",
      "epoch: 10 step: 1296, loss is 0.0023237939458340406\n",
      "epoch: 10 step: 1297, loss is 0.044547416269779205\n",
      "epoch: 10 step: 1298, loss is 0.00717746838927269\n",
      "epoch: 10 step: 1299, loss is 0.0005112886428833008\n",
      "epoch: 10 step: 1300, loss is 0.059038881212472916\n",
      "epoch: 10 step: 1301, loss is 0.003285027574747801\n",
      "epoch: 10 step: 1302, loss is 0.004021084867417812\n",
      "epoch: 10 step: 1303, loss is 0.049128271639347076\n",
      "epoch: 10 step: 1304, loss is 0.0004891529097221792\n",
      "epoch: 10 step: 1305, loss is 0.0151123171672225\n",
      "epoch: 10 step: 1306, loss is 0.039950888603925705\n",
      "epoch: 10 step: 1307, loss is 0.0016944239614531398\n",
      "epoch: 10 step: 1308, loss is 0.003111537080258131\n",
      "epoch: 10 step: 1309, loss is 0.00046658399514853954\n",
      "epoch: 10 step: 1310, loss is 0.011896084994077682\n",
      "epoch: 10 step: 1311, loss is 0.008670767769217491\n",
      "epoch: 10 step: 1312, loss is 0.0007871963898651302\n",
      "epoch: 10 step: 1313, loss is 0.00022640744282398373\n",
      "epoch: 10 step: 1314, loss is 0.0007415757863782346\n",
      "epoch: 10 step: 1315, loss is 0.0001481773069826886\n",
      "epoch: 10 step: 1316, loss is 0.00023705311468802392\n",
      "epoch: 10 step: 1317, loss is 0.0032282606698572636\n",
      "epoch: 10 step: 1318, loss is 0.0010036126477643847\n",
      "epoch: 10 step: 1319, loss is 0.0018027357291430235\n",
      "epoch: 10 step: 1320, loss is 0.019096875563263893\n",
      "epoch: 10 step: 1321, loss is 0.006135042756795883\n",
      "epoch: 10 step: 1322, loss is 0.21382731199264526\n",
      "epoch: 10 step: 1323, loss is 0.0009574353462085128\n",
      "epoch: 10 step: 1324, loss is 0.0010326704941689968\n",
      "epoch: 10 step: 1325, loss is 0.030875304713845253\n",
      "epoch: 10 step: 1326, loss is 0.03566165268421173\n",
      "epoch: 10 step: 1327, loss is 2.6293138944311067e-05\n",
      "epoch: 10 step: 1328, loss is 0.002057423582300544\n",
      "epoch: 10 step: 1329, loss is 0.11159926652908325\n",
      "epoch: 10 step: 1330, loss is 0.0013928593834862113\n",
      "epoch: 10 step: 1331, loss is 0.001531933550722897\n",
      "epoch: 10 step: 1332, loss is 0.0005365572869777679\n",
      "epoch: 10 step: 1333, loss is 0.00023128111206460744\n",
      "epoch: 10 step: 1334, loss is 0.00030038674594834447\n",
      "epoch: 10 step: 1335, loss is 0.026174169033765793\n",
      "epoch: 10 step: 1336, loss is 7.667606405448169e-05\n",
      "epoch: 10 step: 1337, loss is 0.00036067559267394245\n",
      "epoch: 10 step: 1338, loss is 0.0009920636657625437\n",
      "epoch: 10 step: 1339, loss is 0.014561225660145283\n",
      "epoch: 10 step: 1340, loss is 0.0007460406050086021\n",
      "epoch: 10 step: 1341, loss is 0.005621925927698612\n",
      "epoch: 10 step: 1342, loss is 0.018573381006717682\n",
      "epoch: 10 step: 1343, loss is 0.0025623529218137264\n",
      "epoch: 10 step: 1344, loss is 0.0004999496741220355\n",
      "epoch: 10 step: 1345, loss is 0.0007834658026695251\n",
      "epoch: 10 step: 1346, loss is 0.04366603493690491\n",
      "epoch: 10 step: 1347, loss is 0.008226616308093071\n",
      "epoch: 10 step: 1348, loss is 8.393296593567356e-05\n",
      "epoch: 10 step: 1349, loss is 0.000177720095962286\n",
      "epoch: 10 step: 1350, loss is 0.0003783264837693423\n",
      "epoch: 10 step: 1351, loss is 0.016053298488259315\n",
      "epoch: 10 step: 1352, loss is 0.0028003717307001352\n",
      "epoch: 10 step: 1353, loss is 0.08251935243606567\n",
      "epoch: 10 step: 1354, loss is 2.4448892872896977e-05\n",
      "epoch: 10 step: 1355, loss is 0.0003056567220482975\n",
      "epoch: 10 step: 1356, loss is 0.0031134856399148703\n",
      "epoch: 10 step: 1357, loss is 0.055231012403964996\n",
      "epoch: 10 step: 1358, loss is 0.024883832782506943\n",
      "epoch: 10 step: 1359, loss is 0.0017601440194994211\n",
      "epoch: 10 step: 1360, loss is 0.03868220001459122\n",
      "epoch: 10 step: 1361, loss is 0.0011597597040235996\n",
      "epoch: 10 step: 1362, loss is 0.0030045611783862114\n",
      "epoch: 10 step: 1363, loss is 0.008150591515004635\n",
      "epoch: 10 step: 1364, loss is 0.00010957971244351938\n",
      "epoch: 10 step: 1365, loss is 0.012444784864783287\n",
      "epoch: 10 step: 1366, loss is 6.835556996520609e-05\n",
      "epoch: 10 step: 1367, loss is 0.0007721474394202232\n",
      "epoch: 10 step: 1368, loss is 0.012085068970918655\n",
      "epoch: 10 step: 1369, loss is 0.0022810297086834908\n",
      "epoch: 10 step: 1370, loss is 0.009561260230839252\n",
      "epoch: 10 step: 1371, loss is 0.006887522991746664\n",
      "epoch: 10 step: 1372, loss is 4.812583210878074e-05\n",
      "epoch: 10 step: 1373, loss is 0.0038180777337402105\n",
      "epoch: 10 step: 1374, loss is 0.009785807691514492\n",
      "epoch: 10 step: 1375, loss is 0.025839824229478836\n",
      "epoch: 10 step: 1376, loss is 0.0009142471244558692\n",
      "epoch: 10 step: 1377, loss is 0.00774716679006815\n",
      "epoch: 10 step: 1378, loss is 0.005871020257472992\n",
      "epoch: 10 step: 1379, loss is 0.01479946170002222\n",
      "epoch: 10 step: 1380, loss is 0.0016481260536238551\n",
      "epoch: 10 step: 1381, loss is 0.0065057286992669106\n",
      "epoch: 10 step: 1382, loss is 0.0005086053861305118\n",
      "epoch: 10 step: 1383, loss is 0.0009069759398698807\n",
      "epoch: 10 step: 1384, loss is 0.009902812540531158\n",
      "epoch: 10 step: 1385, loss is 0.0001500806538388133\n",
      "epoch: 10 step: 1386, loss is 0.0017387692350894213\n",
      "epoch: 10 step: 1387, loss is 0.00544854486361146\n",
      "epoch: 10 step: 1388, loss is 0.05645119026303291\n",
      "epoch: 10 step: 1389, loss is 5.8021130826091394e-05\n",
      "epoch: 10 step: 1390, loss is 0.0005846109706908464\n",
      "epoch: 10 step: 1391, loss is 0.06813155859708786\n",
      "epoch: 10 step: 1392, loss is 0.0005017565563321114\n",
      "epoch: 10 step: 1393, loss is 0.051783327013254166\n",
      "epoch: 10 step: 1394, loss is 0.000127436185721308\n",
      "epoch: 10 step: 1395, loss is 0.002570450771600008\n",
      "epoch: 10 step: 1396, loss is 7.835299766156822e-05\n",
      "epoch: 10 step: 1397, loss is 0.00034649312146939337\n",
      "epoch: 10 step: 1398, loss is 0.01628827303647995\n",
      "epoch: 10 step: 1399, loss is 0.0006934925331734121\n",
      "epoch: 10 step: 1400, loss is 0.0006974316202104092\n",
      "epoch: 10 step: 1401, loss is 0.0014905269490554929\n",
      "epoch: 10 step: 1402, loss is 3.0379282179637812e-05\n",
      "epoch: 10 step: 1403, loss is 0.007574144285172224\n",
      "epoch: 10 step: 1404, loss is 0.0019127337727695704\n",
      "epoch: 10 step: 1405, loss is 0.016869058832526207\n",
      "epoch: 10 step: 1406, loss is 0.0008353109587915242\n",
      "epoch: 10 step: 1407, loss is 0.0027202966157346964\n",
      "epoch: 10 step: 1408, loss is 0.004238588735461235\n",
      "epoch: 10 step: 1409, loss is 0.007381908595561981\n",
      "epoch: 10 step: 1410, loss is 0.0015141530893743038\n",
      "epoch: 10 step: 1411, loss is 0.001373404636979103\n",
      "epoch: 10 step: 1412, loss is 0.0016014088178053498\n",
      "epoch: 10 step: 1413, loss is 0.005425532814115286\n",
      "epoch: 10 step: 1414, loss is 0.0014847001293674111\n",
      "epoch: 10 step: 1415, loss is 0.0007597829098813236\n",
      "epoch: 10 step: 1416, loss is 0.00027838567621074617\n",
      "epoch: 10 step: 1417, loss is 0.0019696105737239122\n",
      "epoch: 10 step: 1418, loss is 0.00013140215014573187\n",
      "epoch: 10 step: 1419, loss is 0.00012218351184856147\n",
      "epoch: 10 step: 1420, loss is 0.0010525549296289682\n",
      "epoch: 10 step: 1421, loss is 4.5069289626553655e-05\n",
      "epoch: 10 step: 1422, loss is 0.0033601780887693167\n",
      "epoch: 10 step: 1423, loss is 0.18532750010490417\n",
      "epoch: 10 step: 1424, loss is 0.0013523076195269823\n",
      "epoch: 10 step: 1425, loss is 7.488865230698138e-05\n",
      "epoch: 10 step: 1426, loss is 1.1094221008534078e-05\n",
      "epoch: 10 step: 1427, loss is 0.017744986340403557\n",
      "epoch: 10 step: 1428, loss is 0.10688025504350662\n",
      "epoch: 10 step: 1429, loss is 0.08026644587516785\n",
      "epoch: 10 step: 1430, loss is 0.010432668030261993\n",
      "epoch: 10 step: 1431, loss is 9.746941941557452e-05\n",
      "epoch: 10 step: 1432, loss is 0.00041633754153735936\n",
      "epoch: 10 step: 1433, loss is 0.0016349611105397344\n",
      "epoch: 10 step: 1434, loss is 5.622929893434048e-05\n",
      "epoch: 10 step: 1435, loss is 0.0014518201351165771\n",
      "epoch: 10 step: 1436, loss is 0.042701900005340576\n",
      "epoch: 10 step: 1437, loss is 0.00036671903217211366\n",
      "epoch: 10 step: 1438, loss is 0.0034761005081236362\n",
      "epoch: 10 step: 1439, loss is 0.07505354285240173\n",
      "epoch: 10 step: 1440, loss is 0.0003233395691495389\n",
      "epoch: 10 step: 1441, loss is 0.0010538337519392371\n",
      "epoch: 10 step: 1442, loss is 0.002147071762010455\n",
      "epoch: 10 step: 1443, loss is 0.0978562980890274\n",
      "epoch: 10 step: 1444, loss is 0.0017471001483500004\n",
      "epoch: 10 step: 1445, loss is 0.007930916734039783\n",
      "epoch: 10 step: 1446, loss is 0.00576586090028286\n",
      "epoch: 10 step: 1447, loss is 4.3700827518478036e-05\n",
      "epoch: 10 step: 1448, loss is 0.0009052761015482247\n",
      "epoch: 10 step: 1449, loss is 0.0004295207909308374\n",
      "epoch: 10 step: 1450, loss is 0.002879723673686385\n",
      "epoch: 10 step: 1451, loss is 0.006270566023886204\n",
      "epoch: 10 step: 1452, loss is 0.0032571135088801384\n",
      "epoch: 10 step: 1453, loss is 0.0010963856475427747\n",
      "epoch: 10 step: 1454, loss is 0.0003924729535356164\n",
      "epoch: 10 step: 1455, loss is 0.00018368235032539815\n",
      "epoch: 10 step: 1456, loss is 0.06237174943089485\n",
      "epoch: 10 step: 1457, loss is 0.024187622591853142\n",
      "epoch: 10 step: 1458, loss is 0.013736315071582794\n",
      "epoch: 10 step: 1459, loss is 0.019905345514416695\n",
      "epoch: 10 step: 1460, loss is 0.0011177042033523321\n",
      "epoch: 10 step: 1461, loss is 0.0006324209389276803\n",
      "epoch: 10 step: 1462, loss is 0.00020271331595722586\n",
      "epoch: 10 step: 1463, loss is 0.0006702953833155334\n",
      "epoch: 10 step: 1464, loss is 0.0025565349496901035\n",
      "epoch: 10 step: 1465, loss is 0.0002275372389703989\n",
      "epoch: 10 step: 1466, loss is 0.005898799281567335\n",
      "epoch: 10 step: 1467, loss is 0.001035351655445993\n",
      "epoch: 10 step: 1468, loss is 0.0012267748825252056\n",
      "epoch: 10 step: 1469, loss is 0.00015245024405885488\n",
      "epoch: 10 step: 1470, loss is 6.718982331221923e-05\n",
      "epoch: 10 step: 1471, loss is 0.002269922522827983\n",
      "epoch: 10 step: 1472, loss is 0.0009119905298575759\n",
      "epoch: 10 step: 1473, loss is 0.0011069921310991049\n",
      "epoch: 10 step: 1474, loss is 0.0019181271782144904\n",
      "epoch: 10 step: 1475, loss is 3.941406248486601e-06\n",
      "epoch: 10 step: 1476, loss is 0.009009589441120625\n",
      "epoch: 10 step: 1477, loss is 0.000820307875983417\n",
      "epoch: 10 step: 1478, loss is 0.025678884238004684\n",
      "epoch: 10 step: 1479, loss is 0.0006960217142477632\n",
      "epoch: 10 step: 1480, loss is 6.009277785778977e-05\n",
      "epoch: 10 step: 1481, loss is 0.002273616613820195\n",
      "epoch: 10 step: 1482, loss is 0.0001983004913199693\n",
      "epoch: 10 step: 1483, loss is 0.0015295989578589797\n",
      "epoch: 10 step: 1484, loss is 0.0017220757436007261\n",
      "epoch: 10 step: 1485, loss is 0.0053363218903541565\n",
      "epoch: 10 step: 1486, loss is 0.00024136988213285804\n",
      "epoch: 10 step: 1487, loss is 0.00342765380628407\n",
      "epoch: 10 step: 1488, loss is 0.02192501910030842\n",
      "epoch: 10 step: 1489, loss is 0.0001806000800570473\n",
      "epoch: 10 step: 1490, loss is 0.00022896451991982758\n",
      "epoch: 10 step: 1491, loss is 0.0648689717054367\n",
      "epoch: 10 step: 1492, loss is 0.005322306416928768\n",
      "epoch: 10 step: 1493, loss is 0.0001206169108627364\n",
      "epoch: 10 step: 1494, loss is 0.006092736031860113\n",
      "epoch: 10 step: 1495, loss is 0.001611318439245224\n",
      "epoch: 10 step: 1496, loss is 2.2814674593973905e-05\n",
      "epoch: 10 step: 1497, loss is 1.3775238585367333e-05\n",
      "epoch: 10 step: 1498, loss is 1.130318378272932e-05\n",
      "epoch: 10 step: 1499, loss is 0.00042798335198313\n",
      "epoch: 10 step: 1500, loss is 0.034844618290662766\n",
      "epoch: 10 step: 1501, loss is 7.583684055134654e-05\n",
      "epoch: 10 step: 1502, loss is 8.920268010115251e-05\n",
      "epoch: 10 step: 1503, loss is 0.0060633947141468525\n",
      "epoch: 10 step: 1504, loss is 0.00010149385343538597\n",
      "epoch: 10 step: 1505, loss is 0.0035965577699244022\n",
      "epoch: 10 step: 1506, loss is 8.778683695709333e-05\n",
      "epoch: 10 step: 1507, loss is 0.0017354298615828156\n",
      "epoch: 10 step: 1508, loss is 0.0021025575697422028\n",
      "epoch: 10 step: 1509, loss is 2.5833878680714406e-05\n",
      "epoch: 10 step: 1510, loss is 0.0027315407060086727\n",
      "epoch: 10 step: 1511, loss is 0.00028986952384002507\n",
      "epoch: 10 step: 1512, loss is 9.145343210548162e-05\n",
      "epoch: 10 step: 1513, loss is 0.0007882161880843341\n",
      "epoch: 10 step: 1514, loss is 0.00031388716888614\n",
      "epoch: 10 step: 1515, loss is 4.8710549890529364e-05\n",
      "epoch: 10 step: 1516, loss is 0.0003021368756890297\n",
      "epoch: 10 step: 1517, loss is 0.00012212945148348808\n",
      "epoch: 10 step: 1518, loss is 0.03503312170505524\n",
      "epoch: 10 step: 1519, loss is 0.0007835248252376914\n",
      "epoch: 10 step: 1520, loss is 0.017023181542754173\n",
      "epoch: 10 step: 1521, loss is 6.81562814861536e-05\n",
      "epoch: 10 step: 1522, loss is 0.017837898805737495\n",
      "epoch: 10 step: 1523, loss is 0.00020407991542015225\n",
      "epoch: 10 step: 1524, loss is 0.222469300031662\n",
      "epoch: 10 step: 1525, loss is 0.01205611601471901\n",
      "epoch: 10 step: 1526, loss is 0.0004204906872473657\n",
      "epoch: 10 step: 1527, loss is 0.08596649765968323\n",
      "epoch: 10 step: 1528, loss is 0.0008370288996957242\n",
      "epoch: 10 step: 1529, loss is 0.009354726411402225\n",
      "epoch: 10 step: 1530, loss is 0.0002409316221019253\n",
      "epoch: 10 step: 1531, loss is 0.0006035030819475651\n",
      "epoch: 10 step: 1532, loss is 0.0023067176807671785\n",
      "epoch: 10 step: 1533, loss is 0.003405773313716054\n",
      "epoch: 10 step: 1534, loss is 0.04720490798354149\n",
      "epoch: 10 step: 1535, loss is 0.0015969598898664117\n",
      "epoch: 10 step: 1536, loss is 0.022637173533439636\n",
      "epoch: 10 step: 1537, loss is 0.006838658824563026\n",
      "epoch: 10 step: 1538, loss is 6.99159936630167e-05\n",
      "epoch: 10 step: 1539, loss is 0.00346072087995708\n",
      "epoch: 10 step: 1540, loss is 0.0006032709497958422\n",
      "epoch: 10 step: 1541, loss is 0.07196374237537384\n",
      "epoch: 10 step: 1542, loss is 0.0014695220161229372\n",
      "epoch: 10 step: 1543, loss is 0.00046610168647021055\n",
      "epoch: 10 step: 1544, loss is 6.023910827934742e-06\n",
      "epoch: 10 step: 1545, loss is 0.0004219799884594977\n",
      "epoch: 10 step: 1546, loss is 4.866842573392205e-05\n",
      "epoch: 10 step: 1547, loss is 0.12317828834056854\n",
      "epoch: 10 step: 1548, loss is 0.0005318755866028368\n",
      "epoch: 10 step: 1549, loss is 0.018581457436084747\n",
      "epoch: 10 step: 1550, loss is 0.00019186720601283014\n",
      "epoch: 10 step: 1551, loss is 0.04402046278119087\n",
      "epoch: 10 step: 1552, loss is 0.00024209664843510836\n",
      "epoch: 10 step: 1553, loss is 2.1252220903988928e-05\n",
      "epoch: 10 step: 1554, loss is 0.011371389031410217\n",
      "epoch: 10 step: 1555, loss is 0.00018202359206043184\n",
      "epoch: 10 step: 1556, loss is 0.004276698920875788\n",
      "epoch: 10 step: 1557, loss is 4.092129165655933e-05\n",
      "epoch: 10 step: 1558, loss is 0.00989000778645277\n",
      "epoch: 10 step: 1559, loss is 0.001980549655854702\n",
      "epoch: 10 step: 1560, loss is 0.04363178461790085\n",
      "epoch: 10 step: 1561, loss is 0.01277952641248703\n",
      "epoch: 10 step: 1562, loss is 0.0031770113855600357\n",
      "epoch: 10 step: 1563, loss is 0.048521388322114944\n",
      "epoch: 10 step: 1564, loss is 0.0005393276223912835\n",
      "epoch: 10 step: 1565, loss is 0.01091643888503313\n",
      "epoch: 10 step: 1566, loss is 0.001151644391939044\n",
      "epoch: 10 step: 1567, loss is 0.0001908692647702992\n",
      "epoch: 10 step: 1568, loss is 0.0002131753572029993\n",
      "epoch: 10 step: 1569, loss is 0.010683825239539146\n",
      "epoch: 10 step: 1570, loss is 0.0010078592458739877\n",
      "epoch: 10 step: 1571, loss is 0.003962720278650522\n",
      "epoch: 10 step: 1572, loss is 0.0017671887762844563\n",
      "epoch: 10 step: 1573, loss is 0.0006342452252283692\n",
      "epoch: 10 step: 1574, loss is 6.224500975804403e-05\n",
      "epoch: 10 step: 1575, loss is 0.006732797250151634\n",
      "epoch: 10 step: 1576, loss is 0.0028675401117652655\n",
      "epoch: 10 step: 1577, loss is 0.0004323629545979202\n",
      "epoch: 10 step: 1578, loss is 0.15206678211688995\n",
      "epoch: 10 step: 1579, loss is 8.444187551504001e-05\n",
      "epoch: 10 step: 1580, loss is 0.00016740008140914142\n",
      "epoch: 10 step: 1581, loss is 0.0013949146959930658\n",
      "epoch: 10 step: 1582, loss is 0.002886820351704955\n",
      "epoch: 10 step: 1583, loss is 0.0004270023200660944\n",
      "epoch: 10 step: 1584, loss is 0.1506941169500351\n",
      "epoch: 10 step: 1585, loss is 0.02359098382294178\n",
      "epoch: 10 step: 1586, loss is 8.837198402034119e-05\n",
      "epoch: 10 step: 1587, loss is 0.0007838098099455237\n",
      "epoch: 10 step: 1588, loss is 0.002604043809697032\n",
      "epoch: 10 step: 1589, loss is 0.0006991568952798843\n",
      "epoch: 10 step: 1590, loss is 0.0020406809635460377\n",
      "epoch: 10 step: 1591, loss is 0.00021269275748636574\n",
      "epoch: 10 step: 1592, loss is 0.041225362569093704\n",
      "epoch: 10 step: 1593, loss is 0.02499610371887684\n",
      "epoch: 10 step: 1594, loss is 0.0016931552672758698\n",
      "epoch: 10 step: 1595, loss is 0.0030357837677001953\n",
      "epoch: 10 step: 1596, loss is 0.0003401935682632029\n",
      "epoch: 10 step: 1597, loss is 0.016341812908649445\n",
      "epoch: 10 step: 1598, loss is 0.07283009588718414\n",
      "epoch: 10 step: 1599, loss is 0.006850512698292732\n",
      "epoch: 10 step: 1600, loss is 4.447113678907044e-05\n",
      "epoch: 10 step: 1601, loss is 0.001902333227917552\n",
      "epoch: 10 step: 1602, loss is 0.0007366575882770121\n",
      "epoch: 10 step: 1603, loss is 0.0006307358853518963\n",
      "epoch: 10 step: 1604, loss is 0.0005062299314886332\n",
      "epoch: 10 step: 1605, loss is 0.0001228106557391584\n",
      "epoch: 10 step: 1606, loss is 0.034697484225034714\n",
      "epoch: 10 step: 1607, loss is 0.01568436250090599\n",
      "epoch: 10 step: 1608, loss is 0.0003979075700044632\n",
      "epoch: 10 step: 1609, loss is 0.0006583795184269547\n",
      "epoch: 10 step: 1610, loss is 0.14848993718624115\n",
      "epoch: 10 step: 1611, loss is 0.2604731619358063\n",
      "epoch: 10 step: 1612, loss is 0.0002035814250120893\n",
      "epoch: 10 step: 1613, loss is 0.06918155401945114\n",
      "epoch: 10 step: 1614, loss is 0.0012955444399267435\n",
      "epoch: 10 step: 1615, loss is 6.359990948112682e-05\n",
      "epoch: 10 step: 1616, loss is 0.0006978671881370246\n",
      "epoch: 10 step: 1617, loss is 0.05217897891998291\n",
      "epoch: 10 step: 1618, loss is 0.0001147026487160474\n",
      "epoch: 10 step: 1619, loss is 0.015156530775129795\n",
      "epoch: 10 step: 1620, loss is 0.04275736212730408\n",
      "epoch: 10 step: 1621, loss is 0.0009701125673018396\n",
      "epoch: 10 step: 1622, loss is 0.0017821446526795626\n",
      "epoch: 10 step: 1623, loss is 4.2144343751715496e-05\n",
      "epoch: 10 step: 1624, loss is 0.004456487484276295\n",
      "epoch: 10 step: 1625, loss is 0.00014523681602440774\n",
      "epoch: 10 step: 1626, loss is 0.000978596042841673\n",
      "epoch: 10 step: 1627, loss is 0.037530384957790375\n",
      "epoch: 10 step: 1628, loss is 0.21088556945323944\n",
      "epoch: 10 step: 1629, loss is 0.05211291089653969\n",
      "epoch: 10 step: 1630, loss is 0.023844916373491287\n",
      "epoch: 10 step: 1631, loss is 0.0009467843919992447\n",
      "epoch: 10 step: 1632, loss is 0.0010578660294413567\n",
      "epoch: 10 step: 1633, loss is 0.00030992896063253284\n",
      "epoch: 10 step: 1634, loss is 0.0010247541358694434\n",
      "epoch: 10 step: 1635, loss is 0.0032359459437429905\n",
      "epoch: 10 step: 1636, loss is 0.00151271210052073\n",
      "epoch: 10 step: 1637, loss is 0.011136023327708244\n",
      "epoch: 10 step: 1638, loss is 0.000117568823043257\n",
      "epoch: 10 step: 1639, loss is 0.0009426238830201328\n",
      "epoch: 10 step: 1640, loss is 0.004606056492775679\n",
      "epoch: 10 step: 1641, loss is 0.07363481819629669\n",
      "epoch: 10 step: 1642, loss is 0.0008585485629737377\n",
      "epoch: 10 step: 1643, loss is 0.0012762632686644793\n",
      "epoch: 10 step: 1644, loss is 0.00043971207924187183\n",
      "epoch: 10 step: 1645, loss is 3.180977364536375e-05\n",
      "epoch: 10 step: 1646, loss is 0.0856250524520874\n",
      "epoch: 10 step: 1647, loss is 0.009736609645187855\n",
      "epoch: 10 step: 1648, loss is 0.13317573070526123\n",
      "epoch: 10 step: 1649, loss is 0.0006604199879802763\n",
      "epoch: 10 step: 1650, loss is 0.0015950602246448398\n",
      "epoch: 10 step: 1651, loss is 0.00036346015986055136\n",
      "epoch: 10 step: 1652, loss is 0.006292409263551235\n",
      "epoch: 10 step: 1653, loss is 0.06614980101585388\n",
      "epoch: 10 step: 1654, loss is 0.00016929519188124686\n",
      "epoch: 10 step: 1655, loss is 0.16157729923725128\n",
      "epoch: 10 step: 1656, loss is 0.00016367768694180995\n",
      "epoch: 10 step: 1657, loss is 0.031952887773513794\n",
      "epoch: 10 step: 1658, loss is 0.0009890288347378373\n",
      "epoch: 10 step: 1659, loss is 0.04715612903237343\n",
      "epoch: 10 step: 1660, loss is 0.00014431112504098564\n",
      "epoch: 10 step: 1661, loss is 0.22523938119411469\n",
      "epoch: 10 step: 1662, loss is 6.388914334820583e-05\n",
      "epoch: 10 step: 1663, loss is 0.00011871744209202006\n",
      "epoch: 10 step: 1664, loss is 8.351709402631968e-05\n",
      "epoch: 10 step: 1665, loss is 0.011995729990303516\n",
      "epoch: 10 step: 1666, loss is 0.00017289718380197883\n",
      "epoch: 10 step: 1667, loss is 0.00030309916473925114\n",
      "epoch: 10 step: 1668, loss is 0.006099002435803413\n",
      "epoch: 10 step: 1669, loss is 0.0002995984978042543\n",
      "epoch: 10 step: 1670, loss is 0.017794707790017128\n",
      "epoch: 10 step: 1671, loss is 0.0014648438664153218\n",
      "epoch: 10 step: 1672, loss is 0.0010613816557452083\n",
      "epoch: 10 step: 1673, loss is 0.0049802022986114025\n",
      "epoch: 10 step: 1674, loss is 0.00017445320554543287\n",
      "epoch: 10 step: 1675, loss is 0.0006839215056970716\n",
      "epoch: 10 step: 1676, loss is 0.001801014645025134\n",
      "epoch: 10 step: 1677, loss is 1.663785042183008e-05\n",
      "epoch: 10 step: 1678, loss is 0.04404643177986145\n",
      "epoch: 10 step: 1679, loss is 0.0023072517942637205\n",
      "epoch: 10 step: 1680, loss is 0.016424745321273804\n",
      "epoch: 10 step: 1681, loss is 0.0012085263151675463\n",
      "epoch: 10 step: 1682, loss is 0.0006897282437421381\n",
      "epoch: 10 step: 1683, loss is 0.041281845420598984\n",
      "epoch: 10 step: 1684, loss is 0.005074997432529926\n",
      "epoch: 10 step: 1685, loss is 0.0009953771950677037\n",
      "epoch: 10 step: 1686, loss is 0.0897383987903595\n",
      "epoch: 10 step: 1687, loss is 6.188271072460338e-05\n",
      "epoch: 10 step: 1688, loss is 0.000894940341822803\n",
      "epoch: 10 step: 1689, loss is 0.021432770416140556\n",
      "epoch: 10 step: 1690, loss is 0.000983656384050846\n",
      "epoch: 10 step: 1691, loss is 0.12396582961082458\n",
      "epoch: 10 step: 1692, loss is 0.0011805641697719693\n",
      "epoch: 10 step: 1693, loss is 0.011776636354625225\n",
      "epoch: 10 step: 1694, loss is 0.036817628890275955\n",
      "epoch: 10 step: 1695, loss is 0.0001210349437315017\n",
      "epoch: 10 step: 1696, loss is 0.047218553721904755\n",
      "epoch: 10 step: 1697, loss is 0.0003510559326969087\n",
      "epoch: 10 step: 1698, loss is 0.0015307065332308412\n",
      "epoch: 10 step: 1699, loss is 0.0008050542674027383\n",
      "epoch: 10 step: 1700, loss is 1.4311772247310728e-05\n",
      "epoch: 10 step: 1701, loss is 0.008837261237204075\n",
      "epoch: 10 step: 1702, loss is 0.012609124183654785\n",
      "epoch: 10 step: 1703, loss is 1.1023643310181797e-05\n",
      "epoch: 10 step: 1704, loss is 0.0009411724749952555\n",
      "epoch: 10 step: 1705, loss is 0.2173786461353302\n",
      "epoch: 10 step: 1706, loss is 5.907649756409228e-05\n",
      "epoch: 10 step: 1707, loss is 0.01418968103826046\n",
      "epoch: 10 step: 1708, loss is 0.0033457023091614246\n",
      "epoch: 10 step: 1709, loss is 0.021860415115952492\n",
      "epoch: 10 step: 1710, loss is 0.0001455190358683467\n",
      "epoch: 10 step: 1711, loss is 0.0002068948233500123\n",
      "epoch: 10 step: 1712, loss is 0.02082901820540428\n",
      "epoch: 10 step: 1713, loss is 0.002222612267360091\n",
      "epoch: 10 step: 1714, loss is 5.452407640404999e-05\n",
      "epoch: 10 step: 1715, loss is 0.02422960475087166\n",
      "epoch: 10 step: 1716, loss is 0.00025393147370778024\n",
      "epoch: 10 step: 1717, loss is 0.0001470460119890049\n",
      "epoch: 10 step: 1718, loss is 0.05083639919757843\n",
      "epoch: 10 step: 1719, loss is 0.0026209650095552206\n",
      "epoch: 10 step: 1720, loss is 0.0002067501627607271\n",
      "epoch: 10 step: 1721, loss is 0.00034767924807965755\n",
      "epoch: 10 step: 1722, loss is 3.9397178625222296e-05\n",
      "epoch: 10 step: 1723, loss is 0.009896287694573402\n",
      "epoch: 10 step: 1724, loss is 0.0026380817871540785\n",
      "epoch: 10 step: 1725, loss is 0.003958593122661114\n",
      "epoch: 10 step: 1726, loss is 6.720743112964556e-05\n",
      "epoch: 10 step: 1727, loss is 0.008019291795790195\n",
      "epoch: 10 step: 1728, loss is 0.05298672989010811\n",
      "epoch: 10 step: 1729, loss is 0.0028847188223153353\n",
      "epoch: 10 step: 1730, loss is 0.11190100014209747\n",
      "epoch: 10 step: 1731, loss is 0.010019786655902863\n",
      "epoch: 10 step: 1732, loss is 7.674160588067025e-05\n",
      "epoch: 10 step: 1733, loss is 0.00048544880701228976\n",
      "epoch: 10 step: 1734, loss is 9.5565032097511e-05\n",
      "epoch: 10 step: 1735, loss is 0.00015105083002708852\n",
      "epoch: 10 step: 1736, loss is 0.006920644547790289\n",
      "epoch: 10 step: 1737, loss is 0.014003317803144455\n",
      "epoch: 10 step: 1738, loss is 0.0007706739706918597\n",
      "epoch: 10 step: 1739, loss is 0.06759832799434662\n",
      "epoch: 10 step: 1740, loss is 1.521330705145374e-05\n",
      "epoch: 10 step: 1741, loss is 0.00138413580134511\n",
      "epoch: 10 step: 1742, loss is 0.00042435561772435904\n",
      "epoch: 10 step: 1743, loss is 0.14327892661094666\n",
      "epoch: 10 step: 1744, loss is 0.009974404238164425\n",
      "epoch: 10 step: 1745, loss is 0.016031010076403618\n",
      "epoch: 10 step: 1746, loss is 0.00034912294358946383\n",
      "epoch: 10 step: 1747, loss is 0.003674308303743601\n",
      "epoch: 10 step: 1748, loss is 0.00027073005912825465\n",
      "epoch: 10 step: 1749, loss is 0.00047863717190921307\n",
      "epoch: 10 step: 1750, loss is 0.003981714602559805\n",
      "epoch: 10 step: 1751, loss is 0.0017471383325755596\n",
      "epoch: 10 step: 1752, loss is 0.003218767000362277\n",
      "epoch: 10 step: 1753, loss is 0.1274396926164627\n",
      "epoch: 10 step: 1754, loss is 0.005421190522611141\n",
      "epoch: 10 step: 1755, loss is 0.0004172910121269524\n",
      "epoch: 10 step: 1756, loss is 0.01576274074614048\n",
      "epoch: 10 step: 1757, loss is 0.0006992570706643164\n",
      "epoch: 10 step: 1758, loss is 0.0027979754377156496\n",
      "epoch: 10 step: 1759, loss is 0.003148834453895688\n",
      "epoch: 10 step: 1760, loss is 0.0036184496711939573\n",
      "epoch: 10 step: 1761, loss is 0.09772796928882599\n",
      "epoch: 10 step: 1762, loss is 0.0012527829967439175\n",
      "epoch: 10 step: 1763, loss is 0.03008485771715641\n",
      "epoch: 10 step: 1764, loss is 0.001462218933738768\n",
      "epoch: 10 step: 1765, loss is 4.685922613134608e-05\n",
      "epoch: 10 step: 1766, loss is 0.0002516538370400667\n",
      "epoch: 10 step: 1767, loss is 0.00013417321315500885\n",
      "epoch: 10 step: 1768, loss is 0.052058473229408264\n",
      "epoch: 10 step: 1769, loss is 0.0038938242942094803\n",
      "epoch: 10 step: 1770, loss is 0.022496962919831276\n",
      "epoch: 10 step: 1771, loss is 0.012708223424851894\n",
      "epoch: 10 step: 1772, loss is 0.058719735592603683\n",
      "epoch: 10 step: 1773, loss is 0.01485227420926094\n",
      "epoch: 10 step: 1774, loss is 3.3302814699709415e-05\n",
      "epoch: 10 step: 1775, loss is 0.0003964688803534955\n",
      "epoch: 10 step: 1776, loss is 0.11029434204101562\n",
      "epoch: 10 step: 1777, loss is 0.0002273623540531844\n",
      "epoch: 10 step: 1778, loss is 0.00016944111848715693\n",
      "epoch: 10 step: 1779, loss is 0.00012757252261508256\n",
      "epoch: 10 step: 1780, loss is 0.010969433933496475\n",
      "epoch: 10 step: 1781, loss is 0.001971230376511812\n",
      "epoch: 10 step: 1782, loss is 0.021798567846417427\n",
      "epoch: 10 step: 1783, loss is 0.00011672997788991779\n",
      "epoch: 10 step: 1784, loss is 0.00043837600969709456\n",
      "epoch: 10 step: 1785, loss is 0.00030227104434743524\n",
      "epoch: 10 step: 1786, loss is 0.0007840977632440627\n",
      "epoch: 10 step: 1787, loss is 0.0004304239701014012\n",
      "epoch: 10 step: 1788, loss is 0.0010954971658065915\n",
      "epoch: 10 step: 1789, loss is 0.0017343087820336223\n",
      "epoch: 10 step: 1790, loss is 0.013994180597364902\n",
      "epoch: 10 step: 1791, loss is 0.006327344570308924\n",
      "epoch: 10 step: 1792, loss is 0.0016490918351337314\n",
      "epoch: 10 step: 1793, loss is 0.0008438030490651727\n",
      "epoch: 10 step: 1794, loss is 0.00010656940867193043\n",
      "epoch: 10 step: 1795, loss is 0.0007300059660337865\n",
      "epoch: 10 step: 1796, loss is 0.08831741660833359\n",
      "epoch: 10 step: 1797, loss is 0.0004491143045015633\n",
      "epoch: 10 step: 1798, loss is 0.14063601195812225\n",
      "epoch: 10 step: 1799, loss is 0.0036908076144754887\n",
      "epoch: 10 step: 1800, loss is 0.037541721016168594\n",
      "epoch: 10 step: 1801, loss is 0.18352682888507843\n",
      "epoch: 10 step: 1802, loss is 0.07085416465997696\n",
      "epoch: 10 step: 1803, loss is 0.00494250375777483\n",
      "epoch: 10 step: 1804, loss is 0.0012960225576534867\n",
      "epoch: 10 step: 1805, loss is 0.008797625079751015\n",
      "epoch: 10 step: 1806, loss is 0.0002879996900446713\n",
      "epoch: 10 step: 1807, loss is 0.0011324420338496566\n",
      "epoch: 10 step: 1808, loss is 0.01781480759382248\n",
      "epoch: 10 step: 1809, loss is 0.0010180511744692922\n",
      "epoch: 10 step: 1810, loss is 0.00036636783624999225\n",
      "epoch: 10 step: 1811, loss is 0.0008135418756864965\n",
      "epoch: 10 step: 1812, loss is 0.09094803780317307\n",
      "epoch: 10 step: 1813, loss is 0.005472356453537941\n",
      "epoch: 10 step: 1814, loss is 0.08277960866689682\n",
      "epoch: 10 step: 1815, loss is 0.0009750587050803006\n",
      "epoch: 10 step: 1816, loss is 0.04396520555019379\n",
      "epoch: 10 step: 1817, loss is 0.0005892324261367321\n",
      "epoch: 10 step: 1818, loss is 0.09997188299894333\n",
      "epoch: 10 step: 1819, loss is 9.168305041384883e-06\n",
      "epoch: 10 step: 1820, loss is 0.013381809927523136\n",
      "epoch: 10 step: 1821, loss is 0.01968776062130928\n",
      "epoch: 10 step: 1822, loss is 0.06972725689411163\n",
      "epoch: 10 step: 1823, loss is 0.0006506620557047427\n",
      "epoch: 10 step: 1824, loss is 0.00016613111074548215\n",
      "epoch: 10 step: 1825, loss is 0.16324324905872345\n",
      "epoch: 10 step: 1826, loss is 0.018737096339464188\n",
      "epoch: 10 step: 1827, loss is 0.043060969561338425\n",
      "epoch: 10 step: 1828, loss is 0.00013809962547384202\n",
      "epoch: 10 step: 1829, loss is 0.0014588338090106845\n",
      "epoch: 10 step: 1830, loss is 0.16553129255771637\n",
      "epoch: 10 step: 1831, loss is 0.00013922469224780798\n",
      "epoch: 10 step: 1832, loss is 0.1693694293498993\n",
      "epoch: 10 step: 1833, loss is 0.0007203189888969064\n",
      "epoch: 10 step: 1834, loss is 0.026792749762535095\n",
      "epoch: 10 step: 1835, loss is 0.001355303917080164\n",
      "epoch: 10 step: 1836, loss is 0.028988489881157875\n",
      "epoch: 10 step: 1837, loss is 0.002165788784623146\n",
      "epoch: 10 step: 1838, loss is 0.09146647900342941\n",
      "epoch: 10 step: 1839, loss is 0.0006623872905038297\n",
      "epoch: 10 step: 1840, loss is 0.04308190196752548\n",
      "epoch: 10 step: 1841, loss is 0.00016898050671443343\n",
      "epoch: 10 step: 1842, loss is 0.014541301876306534\n",
      "epoch: 10 step: 1843, loss is 0.0027401314582675695\n",
      "epoch: 10 step: 1844, loss is 0.00017480326641816646\n",
      "epoch: 10 step: 1845, loss is 0.0015111146494746208\n",
      "epoch: 10 step: 1846, loss is 0.050461627542972565\n",
      "epoch: 10 step: 1847, loss is 0.0017790631391108036\n",
      "epoch: 10 step: 1848, loss is 0.010193854570388794\n",
      "epoch: 10 step: 1849, loss is 0.02886432781815529\n",
      "epoch: 10 step: 1850, loss is 0.003580818884074688\n",
      "epoch: 10 step: 1851, loss is 0.0012053794926032424\n",
      "epoch: 10 step: 1852, loss is 0.001531751244328916\n",
      "epoch: 10 step: 1853, loss is 0.07553349435329437\n",
      "epoch: 10 step: 1854, loss is 0.0214089248329401\n",
      "epoch: 10 step: 1855, loss is 0.11576160788536072\n",
      "epoch: 10 step: 1856, loss is 0.00134811841417104\n",
      "epoch: 10 step: 1857, loss is 0.0006884749163873494\n",
      "epoch: 10 step: 1858, loss is 0.06625238060951233\n",
      "epoch: 10 step: 1859, loss is 0.00012327141303103417\n",
      "epoch: 10 step: 1860, loss is 0.0011022754479199648\n",
      "epoch: 10 step: 1861, loss is 0.0019296924583613873\n",
      "epoch: 10 step: 1862, loss is 0.0005460969405248761\n",
      "epoch: 10 step: 1863, loss is 0.000195440516108647\n",
      "epoch: 10 step: 1864, loss is 0.01468033716082573\n",
      "epoch: 10 step: 1865, loss is 0.00030419902759604156\n",
      "epoch: 10 step: 1866, loss is 1.0776218914543279e-05\n",
      "epoch: 10 step: 1867, loss is 0.0025283838622272015\n",
      "epoch: 10 step: 1868, loss is 0.001447353744879365\n",
      "epoch: 10 step: 1869, loss is 4.732269371743314e-05\n",
      "epoch: 10 step: 1870, loss is 0.00017372146248817444\n",
      "epoch: 10 step: 1871, loss is 0.0022410491947084665\n",
      "epoch: 10 step: 1872, loss is 0.0007182256667874753\n",
      "epoch: 10 step: 1873, loss is 0.0020545232109725475\n",
      "epoch: 10 step: 1874, loss is 0.00013815310376230627\n",
      "epoch: 10 step: 1875, loss is 0.00422803545370698\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T07:04:19.977477Z",
     "start_time": "2025-04-03T07:04:19.761673Z"
    }
   },
   "cell_type": "code",
   "source": [
    "steps = step_loss[\"step\"]\n",
    "loss_value = step_loss[\"loss_value\"]\n",
    "steps = list(map(int, steps))\n",
    "loss_value = list(map(float, loss_value))\n",
    "plt.plot(steps, loss_value, color=\"red\")\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"Loss_value\")\n",
    "plt.title(\"Loss function value change chart\")\n",
    "plt.show()"
   ],
   "id": "4f9a2ab3e4faa63f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T07:04:47.420532Z",
     "start_time": "2025-04-03T07:04:45.743658Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def test_net(network, model, mnist_path):\n",
    "    \"\"\"Define the evaluation method.\"\"\"\n",
    "    print(\"============== Starting Testing ==============\")\n",
    "    # load the saved model for evaluation\n",
    "    param_dict = load_checkpoint(\"checkpoint_lenet-10_1875.ckpt\")\n",
    "    # load parameter to the network\n",
    "    load_param_into_net(network, param_dict)\n",
    "    # load testing dataset\n",
    "    ds_eval = create_dataset(os.path.join(mnist_path, \"test\"))\n",
    "    acc = model.eval(ds_eval, dataset_sink_mode=True)\n",
    "    print(\"============== Accuracy:{} ==============\".format(acc))\n",
    "\n",
    "test_net(network, model, mnist_path)"
   ],
   "id": "2a1e5a42b213c5a2",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] ME(30262:132595847530048,MainProcess):2025-04-03-15:04:45.759.000 [mindspore/dataset/core/validator_helpers.py:744] 'Resize' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Resize' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(30262:132595847530048,MainProcess):2025-04-03-15:04:45.761.000 [mindspore/dataset/core/validator_helpers.py:744] 'Rescale' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Rescale' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(30262:132595847530048,MainProcess):2025-04-03-15:04:45.761.000 [mindspore/dataset/core/validator_helpers.py:744] 'Rescale' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Rescale' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(30262:132595847530048,MainProcess):2025-04-03-15:04:45.762.000 [mindspore/dataset/core/validator_helpers.py:744] 'HWC2CHW' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'HWC2CHW' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(30262:132595847530048,MainProcess):2025-04-03-15:04:45.762.000 [mindspore/dataset/core/validator_helpers.py:744] 'TypeCast' from mindspore.dataset.transforms.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'TypeCast' from mindspore.dataset.transforms instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== Starting Testing ==============\n",
      "============== Accuracy:{'Accuracy': 0.9888822115384616} ==============\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T07:05:57.540847Z",
     "start_time": "2025-04-03T07:05:46.743509Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def acc_model_info(network, model, mnist_path, model_numbers, epoch_size):\n",
    "    \"\"\"Define the plot info method\"\"\"\n",
    "    step_list = []\n",
    "    acc_list = []\n",
    "    for i in range(1, epoch_size +1):\n",
    "        # load the saved model for evaluation\n",
    "        #加载同一个模型得到的模型训练步数变化，精度随之变化\n",
    "        param_dict = load_checkpoint(\"checkpoint_lenet-10_1875.ckpt\")\n",
    "        #加载不同一个模型得到的模型训练步数变化，精度随之变化\n",
    "        #param_dict = load_checkpoint(\"checkpoint_lenet-{}_1875.ckpt\".format(str(i)))\n",
    "\n",
    "        # load parameter to the network\n",
    "        load_param_into_net(network, param_dict)\n",
    "        # load testing dataset\n",
    "    for i in range(1, model_numbers +1):\n",
    "        ds_eval = create_dataset(os.path.join(mnist_path, \"test\"))\n",
    "        acc = model.eval(ds_eval, dataset_sink_mode=True)\n",
    "        acc_list.append(acc['Accuracy'])\n",
    "        step_list.append(i*125)\n",
    "    return step_list,acc_list\n",
    "\n",
    "# Draw line chart according to training steps and model accuracy\n",
    "l1,l2 = acc_model_info(network, model, mnist_path, 15, 10)\n",
    "plt.xlabel(\"Model of Steps\")\n",
    "plt.ylabel(\"Model accuracy\")\n",
    "plt.title(\"Model accuracy variation chart\")\n",
    "plt.plot(l1, l2, 'red')\n",
    "plt.show()"
   ],
   "id": "7eabb7aefb2571dd",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] ME(30262:132595847530048,MainProcess):2025-04-03-15:05:46.810.000 [mindspore/dataset/core/validator_helpers.py:744] 'Resize' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Resize' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(30262:132595847530048,MainProcess):2025-04-03-15:05:46.811.000 [mindspore/dataset/core/validator_helpers.py:744] 'Rescale' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Rescale' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(30262:132595847530048,MainProcess):2025-04-03-15:05:46.812.000 [mindspore/dataset/core/validator_helpers.py:744] 'Rescale' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Rescale' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(30262:132595847530048,MainProcess):2025-04-03-15:05:46.812.000 [mindspore/dataset/core/validator_helpers.py:744] 'HWC2CHW' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'HWC2CHW' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(30262:132595847530048,MainProcess):2025-04-03-15:05:46.813.000 [mindspore/dataset/core/validator_helpers.py:744] 'TypeCast' from mindspore.dataset.transforms.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'TypeCast' from mindspore.dataset.transforms instead.\n",
      "[WARNING] ME(30262:132595847530048,MainProcess):2025-04-03-15:05:48.680.00 [mindspore/dataset/core/validator_helpers.py:744] 'Resize' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Resize' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(30262:132595847530048,MainProcess):2025-04-03-15:05:48.700.00 [mindspore/dataset/core/validator_helpers.py:744] 'Rescale' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Rescale' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(30262:132595847530048,MainProcess):2025-04-03-15:05:48.700.00 [mindspore/dataset/core/validator_helpers.py:744] 'Rescale' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Rescale' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(30262:132595847530048,MainProcess):2025-04-03-15:05:48.720.00 [mindspore/dataset/core/validator_helpers.py:744] 'HWC2CHW' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'HWC2CHW' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(30262:132595847530048,MainProcess):2025-04-03-15:05:48.730.00 [mindspore/dataset/core/validator_helpers.py:744] 'TypeCast' from mindspore.dataset.transforms.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'TypeCast' from mindspore.dataset.transforms instead.\n",
      "[WARNING] ME(30262:132595847530048,MainProcess):2025-04-03-15:05:49.270.00 [mindspore/dataset/core/validator_helpers.py:744] 'Resize' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Resize' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(30262:132595847530048,MainProcess):2025-04-03-15:05:49.280.00 [mindspore/dataset/core/validator_helpers.py:744] 'Rescale' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Rescale' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(30262:132595847530048,MainProcess):2025-04-03-15:05:49.290.00 [mindspore/dataset/core/validator_helpers.py:744] 'Rescale' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Rescale' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(30262:132595847530048,MainProcess):2025-04-03-15:05:49.300.00 [mindspore/dataset/core/validator_helpers.py:744] 'HWC2CHW' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'HWC2CHW' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(30262:132595847530048,MainProcess):2025-04-03-15:05:49.300.00 [mindspore/dataset/core/validator_helpers.py:744] 'TypeCast' from mindspore.dataset.transforms.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'TypeCast' from mindspore.dataset.transforms instead.\n",
      "[WARNING] ME(30262:132595847530048,MainProcess):2025-04-03-15:05:50.310.000 [mindspore/dataset/core/validator_helpers.py:744] 'Resize' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Resize' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(30262:132595847530048,MainProcess):2025-04-03-15:05:50.312.000 [mindspore/dataset/core/validator_helpers.py:744] 'Rescale' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Rescale' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(30262:132595847530048,MainProcess):2025-04-03-15:05:50.313.000 [mindspore/dataset/core/validator_helpers.py:744] 'Rescale' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Rescale' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(30262:132595847530048,MainProcess):2025-04-03-15:05:50.314.000 [mindspore/dataset/core/validator_helpers.py:744] 'HWC2CHW' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'HWC2CHW' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(30262:132595847530048,MainProcess):2025-04-03-15:05:50.314.000 [mindspore/dataset/core/validator_helpers.py:744] 'TypeCast' from mindspore.dataset.transforms.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'TypeCast' from mindspore.dataset.transforms instead.\n",
      "[WARNING] ME(30262:132595847530048,MainProcess):2025-04-03-15:05:51.323.000 [mindspore/dataset/core/validator_helpers.py:744] 'Resize' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Resize' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(30262:132595847530048,MainProcess):2025-04-03-15:05:51.324.000 [mindspore/dataset/core/validator_helpers.py:744] 'Rescale' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Rescale' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(30262:132595847530048,MainProcess):2025-04-03-15:05:51.324.000 [mindspore/dataset/core/validator_helpers.py:744] 'Rescale' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Rescale' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(30262:132595847530048,MainProcess):2025-04-03-15:05:51.325.000 [mindspore/dataset/core/validator_helpers.py:744] 'HWC2CHW' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'HWC2CHW' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(30262:132595847530048,MainProcess):2025-04-03-15:05:51.326.000 [mindspore/dataset/core/validator_helpers.py:744] 'TypeCast' from mindspore.dataset.transforms.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'TypeCast' from mindspore.dataset.transforms instead.\n",
      "[WARNING] ME(30262:132595847530048,MainProcess):2025-04-03-15:05:52.176.000 [mindspore/dataset/core/validator_helpers.py:744] 'Resize' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Resize' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(30262:132595847530048,MainProcess):2025-04-03-15:05:52.177.000 [mindspore/dataset/core/validator_helpers.py:744] 'Rescale' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Rescale' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(30262:132595847530048,MainProcess):2025-04-03-15:05:52.178.000 [mindspore/dataset/core/validator_helpers.py:744] 'Rescale' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Rescale' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(30262:132595847530048,MainProcess):2025-04-03-15:05:52.178.000 [mindspore/dataset/core/validator_helpers.py:744] 'HWC2CHW' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'HWC2CHW' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(30262:132595847530048,MainProcess):2025-04-03-15:05:52.179.000 [mindspore/dataset/core/validator_helpers.py:744] 'TypeCast' from mindspore.dataset.transforms.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'TypeCast' from mindspore.dataset.transforms instead.\n",
      "[WARNING] ME(30262:132595847530048,MainProcess):2025-04-03-15:05:52.788.000 [mindspore/dataset/core/validator_helpers.py:744] 'Resize' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Resize' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(30262:132595847530048,MainProcess):2025-04-03-15:05:52.789.000 [mindspore/dataset/core/validator_helpers.py:744] 'Rescale' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Rescale' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(30262:132595847530048,MainProcess):2025-04-03-15:05:52.789.000 [mindspore/dataset/core/validator_helpers.py:744] 'Rescale' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Rescale' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(30262:132595847530048,MainProcess):2025-04-03-15:05:52.790.000 [mindspore/dataset/core/validator_helpers.py:744] 'HWC2CHW' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'HWC2CHW' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(30262:132595847530048,MainProcess):2025-04-03-15:05:52.790.000 [mindspore/dataset/core/validator_helpers.py:744] 'TypeCast' from mindspore.dataset.transforms.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'TypeCast' from mindspore.dataset.transforms instead.\n",
      "[WARNING] ME(30262:132595847530048,MainProcess):2025-04-03-15:05:53.288.000 [mindspore/dataset/core/validator_helpers.py:744] 'Resize' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Resize' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(30262:132595847530048,MainProcess):2025-04-03-15:05:53.289.000 [mindspore/dataset/core/validator_helpers.py:744] 'Rescale' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Rescale' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(30262:132595847530048,MainProcess):2025-04-03-15:05:53.289.000 [mindspore/dataset/core/validator_helpers.py:744] 'Rescale' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Rescale' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(30262:132595847530048,MainProcess):2025-04-03-15:05:53.290.000 [mindspore/dataset/core/validator_helpers.py:744] 'HWC2CHW' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'HWC2CHW' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(30262:132595847530048,MainProcess):2025-04-03-15:05:53.290.000 [mindspore/dataset/core/validator_helpers.py:744] 'TypeCast' from mindspore.dataset.transforms.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'TypeCast' from mindspore.dataset.transforms instead.\n",
      "[WARNING] ME(30262:132595847530048,MainProcess):2025-04-03-15:05:53.795.000 [mindspore/dataset/core/validator_helpers.py:744] 'Resize' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Resize' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(30262:132595847530048,MainProcess):2025-04-03-15:05:53.796.000 [mindspore/dataset/core/validator_helpers.py:744] 'Rescale' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Rescale' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(30262:132595847530048,MainProcess):2025-04-03-15:05:53.797.000 [mindspore/dataset/core/validator_helpers.py:744] 'Rescale' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Rescale' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(30262:132595847530048,MainProcess):2025-04-03-15:05:53.797.000 [mindspore/dataset/core/validator_helpers.py:744] 'HWC2CHW' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'HWC2CHW' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(30262:132595847530048,MainProcess):2025-04-03-15:05:53.798.000 [mindspore/dataset/core/validator_helpers.py:744] 'TypeCast' from mindspore.dataset.transforms.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'TypeCast' from mindspore.dataset.transforms instead.\n",
      "[WARNING] ME(30262:132595847530048,MainProcess):2025-04-03-15:05:54.305.000 [mindspore/dataset/core/validator_helpers.py:744] 'Resize' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Resize' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(30262:132595847530048,MainProcess):2025-04-03-15:05:54.306.000 [mindspore/dataset/core/validator_helpers.py:744] 'Rescale' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Rescale' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(30262:132595847530048,MainProcess):2025-04-03-15:05:54.306.000 [mindspore/dataset/core/validator_helpers.py:744] 'Rescale' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Rescale' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(30262:132595847530048,MainProcess):2025-04-03-15:05:54.306.000 [mindspore/dataset/core/validator_helpers.py:744] 'HWC2CHW' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'HWC2CHW' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(30262:132595847530048,MainProcess):2025-04-03-15:05:54.307.000 [mindspore/dataset/core/validator_helpers.py:744] 'TypeCast' from mindspore.dataset.transforms.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'TypeCast' from mindspore.dataset.transforms instead.\n",
      "[WARNING] ME(30262:132595847530048,MainProcess):2025-04-03-15:05:54.817.000 [mindspore/dataset/core/validator_helpers.py:744] 'Resize' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Resize' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(30262:132595847530048,MainProcess):2025-04-03-15:05:54.817.000 [mindspore/dataset/core/validator_helpers.py:744] 'Rescale' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Rescale' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(30262:132595847530048,MainProcess):2025-04-03-15:05:54.818.000 [mindspore/dataset/core/validator_helpers.py:744] 'Rescale' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Rescale' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(30262:132595847530048,MainProcess):2025-04-03-15:05:54.818.000 [mindspore/dataset/core/validator_helpers.py:744] 'HWC2CHW' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'HWC2CHW' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(30262:132595847530048,MainProcess):2025-04-03-15:05:54.818.000 [mindspore/dataset/core/validator_helpers.py:744] 'TypeCast' from mindspore.dataset.transforms.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'TypeCast' from mindspore.dataset.transforms instead.\n",
      "[WARNING] ME(30262:132595847530048,MainProcess):2025-04-03-15:05:55.341.000 [mindspore/dataset/core/validator_helpers.py:744] 'Resize' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Resize' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(30262:132595847530048,MainProcess):2025-04-03-15:05:55.342.000 [mindspore/dataset/core/validator_helpers.py:744] 'Rescale' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Rescale' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(30262:132595847530048,MainProcess):2025-04-03-15:05:55.343.000 [mindspore/dataset/core/validator_helpers.py:744] 'Rescale' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Rescale' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(30262:132595847530048,MainProcess):2025-04-03-15:05:55.343.000 [mindspore/dataset/core/validator_helpers.py:744] 'HWC2CHW' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'HWC2CHW' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(30262:132595847530048,MainProcess):2025-04-03-15:05:55.344.000 [mindspore/dataset/core/validator_helpers.py:744] 'TypeCast' from mindspore.dataset.transforms.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'TypeCast' from mindspore.dataset.transforms instead.\n",
      "[WARNING] ME(30262:132595847530048,MainProcess):2025-04-03-15:05:55.852.000 [mindspore/dataset/core/validator_helpers.py:744] 'Resize' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Resize' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(30262:132595847530048,MainProcess):2025-04-03-15:05:55.853.000 [mindspore/dataset/core/validator_helpers.py:744] 'Rescale' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Rescale' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(30262:132595847530048,MainProcess):2025-04-03-15:05:55.854.000 [mindspore/dataset/core/validator_helpers.py:744] 'Rescale' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Rescale' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(30262:132595847530048,MainProcess):2025-04-03-15:05:55.854.000 [mindspore/dataset/core/validator_helpers.py:744] 'HWC2CHW' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'HWC2CHW' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(30262:132595847530048,MainProcess):2025-04-03-15:05:55.854.000 [mindspore/dataset/core/validator_helpers.py:744] 'TypeCast' from mindspore.dataset.transforms.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'TypeCast' from mindspore.dataset.transforms instead.\n",
      "[WARNING] ME(30262:132595847530048,MainProcess):2025-04-03-15:05:56.386.000 [mindspore/dataset/core/validator_helpers.py:744] 'Resize' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Resize' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(30262:132595847530048,MainProcess):2025-04-03-15:05:56.387.000 [mindspore/dataset/core/validator_helpers.py:744] 'Rescale' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Rescale' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(30262:132595847530048,MainProcess):2025-04-03-15:05:56.387.000 [mindspore/dataset/core/validator_helpers.py:744] 'Rescale' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Rescale' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(30262:132595847530048,MainProcess):2025-04-03-15:05:56.388.000 [mindspore/dataset/core/validator_helpers.py:744] 'HWC2CHW' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'HWC2CHW' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(30262:132595847530048,MainProcess):2025-04-03-15:05:56.388.000 [mindspore/dataset/core/validator_helpers.py:744] 'TypeCast' from mindspore.dataset.transforms.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'TypeCast' from mindspore.dataset.transforms instead.\n",
      "[WARNING] ME(30262:132595847530048,MainProcess):2025-04-03-15:05:56.921.000 [mindspore/dataset/core/validator_helpers.py:744] 'Resize' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Resize' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(30262:132595847530048,MainProcess):2025-04-03-15:05:56.922.000 [mindspore/dataset/core/validator_helpers.py:744] 'Rescale' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Rescale' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(30262:132595847530048,MainProcess):2025-04-03-15:05:56.923.000 [mindspore/dataset/core/validator_helpers.py:744] 'Rescale' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Rescale' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(30262:132595847530048,MainProcess):2025-04-03-15:05:56.923.000 [mindspore/dataset/core/validator_helpers.py:744] 'HWC2CHW' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'HWC2CHW' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(30262:132595847530048,MainProcess):2025-04-03-15:05:56.923.000 [mindspore/dataset/core/validator_helpers.py:744] 'TypeCast' from mindspore.dataset.transforms.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'TypeCast' from mindspore.dataset.transforms instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T07:06:36.033207Z",
     "start_time": "2025-04-03T07:06:34.157879Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ds_test = create_dataset(test_data_path).create_dict_iterator()\n",
    "data = ds_test.__next__()\n",
    "images = data[\"image\"].asnumpy()\n",
    "labels = data[\"label\"].asnumpy() # The subscript of data picture is the standard for us to judge whether it is correct or not\n",
    "\n",
    "output =model.predict(Tensor(data['image']))\n",
    "# The predict function returns the probability of 0-9 numbers corresponding to each picture\n",
    "prb = output.asnumpy()\n",
    "pred = np.argmax(output.asnumpy(), axis=1)\n",
    "err_num = []\n",
    "index = 1\n",
    "for i in range(len(labels)):\n",
    "    plt.subplot(4, 8, i+1)\n",
    "    color = 'blue' if pred[i] == labels[i] else 'red'\n",
    "    plt.title(\"pre:{}\".format(pred[i]), color=color)\n",
    "    plt.imshow(np.squeeze(images[i]))\n",
    "    plt.axis(\"off\")\n",
    "    if color == 'red':\n",
    "        index = 0\n",
    "        # Print out the wrong data identified by the current group\n",
    "        print(\"Row {}, column {} is incorrectly identified as {}, the correct value should be {}\".format(int(i/8)+1, i%8+1, pred[i], labels[i]), '\\n')\n",
    "if index:\n",
    "    print(\"All the figures in this group are predicted correctly！\")\n",
    "print(pred, \"<--Predicted figures\") # Print the numbers recognized by each group of pictures\n",
    "print(labels, \"<--The right number\") # Print the subscript corresponding to each group of pictures\n",
    "plt.show()"
   ],
   "id": "d849a4e84cf68362",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] ME(30262:132595847530048,MainProcess):2025-04-03-15:06:34.165.000 [mindspore/dataset/core/validator_helpers.py:744] 'Resize' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Resize' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(30262:132595847530048,MainProcess):2025-04-03-15:06:34.166.000 [mindspore/dataset/core/validator_helpers.py:744] 'Rescale' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Rescale' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(30262:132595847530048,MainProcess):2025-04-03-15:06:34.167.000 [mindspore/dataset/core/validator_helpers.py:744] 'Rescale' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Rescale' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(30262:132595847530048,MainProcess):2025-04-03-15:06:34.167.000 [mindspore/dataset/core/validator_helpers.py:744] 'HWC2CHW' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'HWC2CHW' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(30262:132595847530048,MainProcess):2025-04-03-15:06:34.168.000 [mindspore/dataset/core/validator_helpers.py:744] 'TypeCast' from mindspore.dataset.transforms.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'TypeCast' from mindspore.dataset.transforms instead.\n",
      "[ERROR] CORE(30262,789860640640,python):2025-04-03-15:06:34.551.313 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_30262/2100403872.py]\n",
      "[WARNING] CORE(30262,789860640640,python):2025-04-03-15:06:34.551.367 [mindspore/core/utils/info.cc:121] ToString] The file '/tmp/ipykernel_30262/2100403872.py' may not exists.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row 3, column 7 is incorrectly identified as 4, the correct value should be 9 \n",
      "\n",
      "[1 7 7 4 9 1 4 4 2 7 8 2 8 4 8 4 9 2 5 0 6 6 4 9 7 2 5 6 8 1 7 9] <--Predicted figures\n",
      "[1 7 7 4 9 1 4 4 2 7 8 2 8 4 8 4 9 2 5 0 6 6 9 9 7 2 5 6 8 1 7 9] <--The right number\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 32 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T07:07:11.038961Z",
     "start_time": "2025-04-03T07:07:10.791718Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# define the pie drawing function of probability analysis\n",
    "def plot_pie(prbs):\n",
    "    dict1 = {}\n",
    "    # Remove the negative number and build the dictionary dict1. The key is the number and the value is the probability value\n",
    "    for i in range(10):\n",
    "        if prbs[i] > 0:\n",
    "            dict1[str(i)] = prbs[i]\n",
    "\n",
    "    label_list = dict1.keys()    # Label of each part\n",
    "    size = dict1.values()    # Size of each part\n",
    "    colors = [\"red\", \"green\", \"pink\", \"blue\", \"purple\", \"orange\", \"gray\"] # Building a round cake pigment Library\n",
    "    color = colors[: len(size)]# Color of each part\n",
    "    plt.pie(size, colors=color, labels=label_list, labeldistance=1.1, autopct=\"%1.1f%%\", shadow=False, startangle=90, pctdistance=0.6)\n",
    "    plt.axis(\"equal\")    # Set the scale size of x-axis and y-axis to be equal\n",
    "    plt.legend()\n",
    "    plt.title(\"Image classification\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "for i in range(2):\n",
    "    print(\"Figure {} probability of corresponding numbers [0-9]:\\n\".format(i+1), prb[i])\n",
    "    plot_pie(prb[i])"
   ],
   "id": "db6fb49e2fb93fbd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Figure 1 probability of corresponding numbers [0-9]:\n",
      " [-4.619632   11.049773   -1.058803   -3.6434796   0.7165255  -0.71282876\n",
      "  0.74135077  1.3543155  -1.8832588  -1.9515005 ]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Figure 2 probability of corresponding numbers [0-9]:\n",
      " [-2.7222958   6.2951527  -1.4805734  -3.677535   -0.82885605 -3.2688353\n",
      " -6.4579587  10.616985   -1.9450471   3.5981624 ]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 25
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
