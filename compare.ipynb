{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d0e50e9-1c81-4a48-9e4b-8dc89a6f01ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "实验 1: MSELoss vs SoftmaxCrossEntropyWithLogits\n",
      "Applied OneHot encoding to labels for path: ./data/train\n",
      "Applied OneHot encoding to labels for path: ./data/test\n",
      "开始训练: Loss=MSELoss, Batch Size=32, Activation=relu, Dropout=False, ApplyOneHot=True\n",
      "epoch: 1 step: 1000, loss is 0.09021913260221481\n",
      "epoch: 2 step: 125, loss is 0.08975284546613693\n",
      "epoch: 2 step: 1125, loss is 0.08704525232315063\n",
      "epoch: 3 step: 250, loss is 0.026349369436502457\n",
      "epoch: 3 step: 1250, loss is 0.019763758406043053\n",
      "epoch: 4 step: 375, loss is 0.011463145725429058\n",
      "epoch: 4 step: 1375, loss is 0.012111770920455456\n",
      "epoch: 5 step: 500, loss is 0.008468328975141048\n",
      "epoch: 5 step: 1500, loss is 0.009978756308555603\n",
      "epoch: 6 step: 625, loss is 0.009144991636276245\n",
      "epoch: 6 step: 1625, loss is 0.006385263986885548\n",
      "epoch: 7 step: 750, loss is 0.0055742026306688786\n",
      "epoch: 7 step: 1750, loss is 0.0029047676362097263\n",
      "epoch: 8 step: 875, loss is 0.001899060676805675\n",
      "epoch: 8 step: 1875, loss is 0.0028625493869185448\n",
      "epoch: 9 step: 1000, loss is 0.0049676476046442986\n",
      "epoch: 10 step: 125, loss is 0.002295218873769045\n",
      "epoch: 10 step: 1125, loss is 0.0021290164440870285\n",
      "测试结果: {'Accuracy': 0.9883814102564102}\n",
      "----------------------------------------\n",
      "开始训练: Loss=SoftmaxCrossEntropyWithLogits, Batch Size=32, Activation=relu, Dropout=False, ApplyOneHot=False\n",
      "epoch: 1 step: 1000, loss is 0.1855834275484085\n",
      "epoch: 2 step: 125, loss is 0.3498673141002655\n",
      "epoch: 2 step: 1125, loss is 0.0025506329257041216\n",
      "epoch: 3 step: 250, loss is 0.010172358714044094\n",
      "epoch: 3 step: 1250, loss is 0.14076033234596252\n",
      "epoch: 4 step: 375, loss is 0.003732281504198909\n",
      "epoch: 4 step: 1375, loss is 0.002579912543296814\n",
      "epoch: 5 step: 500, loss is 6.178614421514794e-05\n",
      "epoch: 5 step: 1500, loss is 0.010062449611723423\n",
      "epoch: 6 step: 625, loss is 0.000805762130767107\n",
      "epoch: 6 step: 1625, loss is 0.04325958341360092\n",
      "epoch: 7 step: 750, loss is 0.08380312472581863\n",
      "epoch: 7 step: 1750, loss is 0.003865532809868455\n",
      "epoch: 8 step: 875, loss is 0.36931324005126953\n",
      "epoch: 8 step: 1875, loss is 0.0006352621130645275\n",
      "epoch: 9 step: 1000, loss is 3.5372278944123536e-05\n",
      "epoch: 10 step: 125, loss is 0.001033168868161738\n",
      "epoch: 10 step: 1125, loss is 0.0004206321609672159\n",
      "测试结果: {'Accuracy': 0.9892828525641025}\n",
      "----------------------------------------\n",
      "实验 2: Batch Size 32 vs 1\n",
      "开始训练: Loss=SoftmaxCrossEntropyWithLogits, Batch Size=32, Activation=relu, Dropout=False, ApplyOneHot=False\n",
      "epoch: 1 step: 1000, loss is 0.32896143198013306\n",
      "epoch: 2 step: 125, loss is 0.1754622906446457\n",
      "epoch: 2 step: 1125, loss is 0.16643810272216797\n",
      "epoch: 3 step: 250, loss is 0.02119339630007744\n",
      "epoch: 3 step: 1250, loss is 0.003751239739358425\n",
      "epoch: 4 step: 375, loss is 0.0888080969452858\n",
      "epoch: 4 step: 1375, loss is 0.08650031685829163\n",
      "epoch: 5 step: 500, loss is 0.002204909920692444\n",
      "epoch: 5 step: 1500, loss is 0.013647767715156078\n",
      "epoch: 6 step: 625, loss is 0.0875900462269783\n",
      "epoch: 6 step: 1625, loss is 0.03562793508172035\n",
      "epoch: 7 step: 750, loss is 0.0003600385971367359\n",
      "epoch: 7 step: 1750, loss is 0.02475930191576481\n",
      "epoch: 8 step: 875, loss is 0.00020012649474665523\n",
      "epoch: 8 step: 1875, loss is 0.0024626776576042175\n",
      "epoch: 9 step: 1000, loss is 0.0012415101518854499\n",
      "epoch: 10 step: 125, loss is 0.00014116548118181527\n",
      "epoch: 10 step: 1125, loss is 0.012683176435530186\n",
      "测试结果: {'Accuracy': 0.9898838141025641}\n",
      "----------------------------------------\n",
      "开始训练: Loss=SoftmaxCrossEntropyWithLogits, Batch Size=1, Activation=relu, Dropout=False, ApplyOneHot=False\n",
      "epoch: 1 step: 1000, loss is 2.2121329307556152\n",
      "epoch: 1 step: 2000, loss is 2.231372117996216\n",
      "epoch: 1 step: 3000, loss is 2.0134150981903076\n",
      "epoch: 1 step: 4000, loss is 2.3822996616363525\n",
      "epoch: 1 step: 5000, loss is 2.324969530105591\n",
      "epoch: 1 step: 6000, loss is 2.299053907394409\n",
      "epoch: 1 step: 7000, loss is 2.6335461139678955\n",
      "epoch: 1 step: 8000, loss is 2.2532291412353516\n",
      "epoch: 1 step: 9000, loss is 2.1647276878356934\n",
      "epoch: 1 step: 10000, loss is 2.3317911624908447\n",
      "epoch: 1 step: 11000, loss is 2.3392484188079834\n",
      "epoch: 1 step: 12000, loss is 2.070085287094116\n",
      "epoch: 1 step: 13000, loss is 2.65395188331604\n",
      "epoch: 1 step: 14000, loss is 1.9388808012008667\n",
      "epoch: 1 step: 15000, loss is 2.599302053451538\n",
      "epoch: 1 step: 16000, loss is 2.1181564331054688\n",
      "epoch: 1 step: 17000, loss is 2.526174783706665\n",
      "epoch: 1 step: 18000, loss is 2.3570148944854736\n",
      "epoch: 1 step: 19000, loss is 2.6131701469421387\n",
      "epoch: 1 step: 20000, loss is 2.1091461181640625\n",
      "epoch: 1 step: 21000, loss is 2.1447722911834717\n",
      "epoch: 1 step: 22000, loss is 2.09285569190979\n",
      "epoch: 1 step: 23000, loss is 2.435985803604126\n",
      "epoch: 1 step: 24000, loss is 2.496063470840454\n",
      "epoch: 1 step: 25000, loss is 2.6092324256896973\n",
      "epoch: 1 step: 26000, loss is 2.3811733722686768\n",
      "epoch: 1 step: 27000, loss is 1.8283058404922485\n",
      "epoch: 1 step: 28000, loss is 2.428516149520874\n",
      "epoch: 1 step: 29000, loss is 2.182429552078247\n",
      "epoch: 1 step: 30000, loss is 2.2573137283325195\n",
      "epoch: 1 step: 31000, loss is 2.2405567169189453\n",
      "epoch: 1 step: 32000, loss is 2.963054895401001\n",
      "epoch: 1 step: 33000, loss is 2.505095958709717\n",
      "epoch: 1 step: 34000, loss is 2.3304367065429688\n",
      "epoch: 1 step: 35000, loss is 2.3014893531799316\n",
      "epoch: 1 step: 36000, loss is 2.227133274078369\n",
      "epoch: 1 step: 37000, loss is 2.4298954010009766\n",
      "epoch: 1 step: 38000, loss is 1.7510024309158325\n",
      "epoch: 1 step: 39000, loss is 2.420619487762451\n",
      "epoch: 1 step: 40000, loss is 2.295851707458496\n",
      "epoch: 1 step: 41000, loss is 2.1738414764404297\n",
      "epoch: 1 step: 42000, loss is 2.233980178833008\n",
      "epoch: 1 step: 43000, loss is 2.5067427158355713\n",
      "epoch: 1 step: 44000, loss is 2.4658985137939453\n",
      "epoch: 1 step: 45000, loss is 2.479855537414551\n",
      "epoch: 1 step: 46000, loss is 2.1394882202148438\n",
      "epoch: 1 step: 47000, loss is 2.2894508838653564\n",
      "epoch: 1 step: 48000, loss is 2.330389976501465\n",
      "epoch: 1 step: 49000, loss is 2.6392459869384766\n",
      "epoch: 1 step: 50000, loss is 2.183105707168579\n",
      "epoch: 1 step: 51000, loss is 2.389939785003662\n",
      "epoch: 1 step: 52000, loss is 2.522148609161377\n",
      "epoch: 1 step: 53000, loss is 2.248471260070801\n",
      "epoch: 1 step: 54000, loss is 2.073380947113037\n",
      "epoch: 1 step: 55000, loss is 2.434300422668457\n",
      "epoch: 1 step: 56000, loss is 2.2495346069335938\n",
      "epoch: 1 step: 57000, loss is 2.558910369873047\n",
      "epoch: 1 step: 58000, loss is 1.965580940246582\n",
      "epoch: 1 step: 59000, loss is 2.0650219917297363\n",
      "epoch: 1 step: 60000, loss is 2.1542224884033203\n",
      "epoch: 2 step: 1000, loss is 1.9713674783706665\n",
      "epoch: 2 step: 2000, loss is 2.504127264022827\n",
      "epoch: 2 step: 3000, loss is 2.2488954067230225\n",
      "epoch: 2 step: 4000, loss is 2.248507261276245\n",
      "epoch: 2 step: 5000, loss is 2.2418758869171143\n",
      "epoch: 2 step: 6000, loss is 2.1497156620025635\n",
      "epoch: 2 step: 7000, loss is 2.801729202270508\n",
      "epoch: 2 step: 8000, loss is 2.3382418155670166\n",
      "epoch: 2 step: 9000, loss is 2.507288694381714\n",
      "epoch: 2 step: 10000, loss is 2.3146395683288574\n",
      "epoch: 2 step: 11000, loss is 2.298497438430786\n",
      "epoch: 2 step: 12000, loss is 2.4022419452667236\n",
      "epoch: 2 step: 13000, loss is 2.3685169219970703\n",
      "epoch: 2 step: 14000, loss is 1.8844901323318481\n",
      "epoch: 2 step: 15000, loss is 2.4531514644622803\n",
      "epoch: 2 step: 16000, loss is 2.053877353668213\n",
      "epoch: 2 step: 17000, loss is 2.409105062484741\n",
      "epoch: 2 step: 18000, loss is 2.40071177482605\n",
      "epoch: 2 step: 19000, loss is 2.051248073577881\n",
      "epoch: 2 step: 20000, loss is 2.144101858139038\n",
      "epoch: 2 step: 21000, loss is 2.49760103225708\n",
      "epoch: 2 step: 22000, loss is 2.1879568099975586\n",
      "epoch: 2 step: 23000, loss is 2.2076799869537354\n",
      "epoch: 2 step: 24000, loss is 2.841224193572998\n",
      "epoch: 2 step: 25000, loss is 1.8928600549697876\n",
      "epoch: 2 step: 26000, loss is 2.434633731842041\n",
      "epoch: 2 step: 27000, loss is 2.156851291656494\n",
      "epoch: 2 step: 28000, loss is 2.291048049926758\n",
      "epoch: 2 step: 29000, loss is 1.9304862022399902\n",
      "epoch: 2 step: 30000, loss is 2.310007333755493\n",
      "epoch: 2 step: 31000, loss is 2.4727466106414795\n",
      "epoch: 2 step: 32000, loss is 1.9753667116165161\n",
      "epoch: 2 step: 33000, loss is 2.264911651611328\n",
      "epoch: 2 step: 34000, loss is 2.321927785873413\n",
      "epoch: 2 step: 35000, loss is 2.1995787620544434\n",
      "epoch: 2 step: 36000, loss is 2.163339853286743\n",
      "epoch: 2 step: 37000, loss is 2.8156960010528564\n",
      "epoch: 2 step: 38000, loss is 2.07270884513855\n",
      "epoch: 2 step: 39000, loss is 2.0321011543273926\n",
      "epoch: 2 step: 40000, loss is 2.4532363414764404\n",
      "epoch: 2 step: 41000, loss is 2.087397336959839\n",
      "epoch: 2 step: 42000, loss is 2.2593696117401123\n",
      "epoch: 2 step: 43000, loss is 2.5523831844329834\n",
      "epoch: 2 step: 44000, loss is 2.682544708251953\n",
      "epoch: 2 step: 45000, loss is 2.3776447772979736\n",
      "epoch: 2 step: 46000, loss is 2.4663212299346924\n",
      "epoch: 2 step: 47000, loss is 2.4486117362976074\n",
      "epoch: 2 step: 48000, loss is 2.325106382369995\n",
      "epoch: 2 step: 49000, loss is 2.6161437034606934\n",
      "epoch: 2 step: 50000, loss is 2.369013786315918\n",
      "epoch: 2 step: 51000, loss is 2.2437126636505127\n",
      "epoch: 2 step: 52000, loss is 2.0595099925994873\n",
      "epoch: 2 step: 53000, loss is 2.155803680419922\n",
      "epoch: 2 step: 54000, loss is 2.0101795196533203\n",
      "epoch: 2 step: 55000, loss is 2.203310251235962\n",
      "epoch: 2 step: 56000, loss is 2.731456756591797\n",
      "epoch: 2 step: 57000, loss is 2.495185375213623\n",
      "epoch: 2 step: 58000, loss is 2.282914638519287\n",
      "epoch: 2 step: 59000, loss is 2.081881046295166\n",
      "epoch: 2 step: 60000, loss is 2.4577009677886963\n",
      "epoch: 3 step: 1000, loss is 2.35982608795166\n",
      "epoch: 3 step: 2000, loss is 2.3437087535858154\n",
      "epoch: 3 step: 3000, loss is 2.405660390853882\n",
      "epoch: 3 step: 4000, loss is 2.294224500656128\n",
      "epoch: 3 step: 5000, loss is 2.532848834991455\n",
      "epoch: 3 step: 6000, loss is 2.3782997131347656\n",
      "epoch: 3 step: 7000, loss is 2.2704625129699707\n",
      "epoch: 3 step: 8000, loss is 2.2676589488983154\n",
      "epoch: 3 step: 9000, loss is 2.529604911804199\n",
      "epoch: 3 step: 10000, loss is 2.2915666103363037\n",
      "epoch: 3 step: 11000, loss is 2.285370349884033\n",
      "epoch: 3 step: 12000, loss is 2.356229782104492\n",
      "epoch: 3 step: 13000, loss is 2.2419238090515137\n",
      "epoch: 3 step: 14000, loss is 2.7093992233276367\n",
      "epoch: 3 step: 15000, loss is 2.6136844158172607\n",
      "epoch: 3 step: 16000, loss is 2.6275391578674316\n",
      "epoch: 3 step: 17000, loss is 2.450129508972168\n",
      "epoch: 3 step: 18000, loss is 2.3006319999694824\n",
      "epoch: 3 step: 19000, loss is 2.6455814838409424\n",
      "epoch: 3 step: 20000, loss is 2.487802267074585\n",
      "epoch: 3 step: 21000, loss is 2.3290817737579346\n",
      "epoch: 3 step: 22000, loss is 2.421630620956421\n",
      "epoch: 3 step: 23000, loss is 2.279712200164795\n",
      "epoch: 3 step: 24000, loss is 2.5656797885894775\n",
      "epoch: 3 step: 25000, loss is 2.184173583984375\n",
      "epoch: 3 step: 26000, loss is 2.318866014480591\n",
      "epoch: 3 step: 27000, loss is 2.2532126903533936\n",
      "epoch: 3 step: 28000, loss is 1.8526462316513062\n",
      "epoch: 3 step: 29000, loss is 2.258920669555664\n",
      "epoch: 3 step: 30000, loss is 2.4544198513031006\n",
      "epoch: 3 step: 31000, loss is 2.176136016845703\n",
      "epoch: 3 step: 32000, loss is 2.71744441986084\n",
      "epoch: 3 step: 33000, loss is 2.0195183753967285\n",
      "epoch: 3 step: 34000, loss is 1.9881939888000488\n",
      "epoch: 3 step: 35000, loss is 2.14426851272583\n",
      "epoch: 3 step: 36000, loss is 2.456747055053711\n",
      "epoch: 3 step: 37000, loss is 2.3122804164886475\n",
      "epoch: 3 step: 38000, loss is 2.243089199066162\n",
      "epoch: 3 step: 39000, loss is 2.228363037109375\n",
      "epoch: 3 step: 40000, loss is 2.1506972312927246\n",
      "epoch: 3 step: 41000, loss is 2.205707550048828\n",
      "epoch: 3 step: 42000, loss is 2.5565333366394043\n",
      "epoch: 3 step: 43000, loss is 2.427950143814087\n",
      "epoch: 3 step: 44000, loss is 2.2355377674102783\n",
      "epoch: 3 step: 45000, loss is 2.207275152206421\n",
      "epoch: 3 step: 46000, loss is 2.6794662475585938\n",
      "epoch: 3 step: 47000, loss is 2.143446683883667\n",
      "epoch: 3 step: 48000, loss is 2.141603946685791\n",
      "epoch: 3 step: 49000, loss is 1.9124621152877808\n",
      "epoch: 3 step: 50000, loss is 1.772034764289856\n",
      "epoch: 3 step: 51000, loss is 2.514965057373047\n",
      "epoch: 3 step: 52000, loss is 1.893166422843933\n",
      "epoch: 3 step: 53000, loss is 2.302881956100464\n",
      "epoch: 3 step: 54000, loss is 2.778447151184082\n",
      "epoch: 3 step: 55000, loss is 2.5071492195129395\n",
      "epoch: 3 step: 56000, loss is 2.531298875808716\n",
      "epoch: 3 step: 57000, loss is 2.563262701034546\n",
      "epoch: 3 step: 58000, loss is 2.2000629901885986\n",
      "epoch: 3 step: 59000, loss is 2.207435369491577\n",
      "epoch: 3 step: 60000, loss is 2.106518507003784\n",
      "epoch: 4 step: 1000, loss is 2.3913135528564453\n",
      "epoch: 4 step: 2000, loss is 2.027050495147705\n",
      "epoch: 4 step: 3000, loss is 2.485776424407959\n",
      "epoch: 4 step: 4000, loss is 2.4068470001220703\n",
      "epoch: 4 step: 5000, loss is 2.3176374435424805\n",
      "epoch: 4 step: 6000, loss is 2.4670650959014893\n",
      "epoch: 4 step: 7000, loss is 2.3420772552490234\n",
      "epoch: 4 step: 8000, loss is 2.5762109756469727\n",
      "epoch: 4 step: 9000, loss is 2.395522356033325\n",
      "epoch: 4 step: 10000, loss is 2.718698501586914\n",
      "epoch: 4 step: 11000, loss is 2.416297674179077\n",
      "epoch: 4 step: 12000, loss is 2.172750234603882\n",
      "epoch: 4 step: 13000, loss is 2.387965679168701\n",
      "epoch: 4 step: 14000, loss is 2.1102020740509033\n",
      "epoch: 4 step: 15000, loss is 2.471937656402588\n",
      "epoch: 4 step: 16000, loss is 2.1767001152038574\n",
      "epoch: 4 step: 17000, loss is 2.0380451679229736\n",
      "epoch: 4 step: 18000, loss is 1.7209763526916504\n",
      "epoch: 4 step: 19000, loss is 2.1126794815063477\n",
      "epoch: 4 step: 20000, loss is 2.1544108390808105\n",
      "epoch: 4 step: 21000, loss is 1.9269118309020996\n",
      "epoch: 4 step: 22000, loss is 2.452134370803833\n",
      "epoch: 4 step: 23000, loss is 2.352705478668213\n",
      "epoch: 4 step: 24000, loss is 2.1346819400787354\n",
      "epoch: 4 step: 25000, loss is 2.026324510574341\n",
      "epoch: 4 step: 26000, loss is 1.9485071897506714\n",
      "epoch: 4 step: 27000, loss is 2.3187990188598633\n",
      "epoch: 4 step: 28000, loss is 2.207437753677368\n",
      "epoch: 4 step: 29000, loss is 2.324477195739746\n",
      "epoch: 4 step: 30000, loss is 2.177156448364258\n",
      "epoch: 4 step: 31000, loss is 2.5370359420776367\n",
      "epoch: 4 step: 32000, loss is 2.435783624649048\n",
      "epoch: 4 step: 33000, loss is 2.372357130050659\n",
      "epoch: 4 step: 34000, loss is 2.167102098464966\n",
      "epoch: 4 step: 35000, loss is 2.2279052734375\n",
      "epoch: 4 step: 36000, loss is 2.1399571895599365\n",
      "epoch: 4 step: 37000, loss is 2.5710606575012207\n",
      "epoch: 4 step: 38000, loss is 2.303971529006958\n",
      "epoch: 4 step: 39000, loss is 1.8640908002853394\n",
      "epoch: 4 step: 40000, loss is 2.2410988807678223\n",
      "epoch: 4 step: 41000, loss is 2.1833338737487793\n",
      "epoch: 4 step: 42000, loss is 2.428401470184326\n",
      "epoch: 4 step: 43000, loss is 2.4253413677215576\n",
      "epoch: 4 step: 44000, loss is 2.3500165939331055\n",
      "epoch: 4 step: 45000, loss is 2.3765976428985596\n",
      "epoch: 4 step: 46000, loss is 2.6203153133392334\n",
      "epoch: 4 step: 47000, loss is 2.402501106262207\n",
      "epoch: 4 step: 48000, loss is 2.0813636779785156\n",
      "epoch: 4 step: 49000, loss is 2.898613691329956\n",
      "epoch: 4 step: 50000, loss is 2.2601473331451416\n",
      "epoch: 4 step: 51000, loss is 2.446639060974121\n",
      "epoch: 4 step: 52000, loss is 2.3396778106689453\n",
      "epoch: 4 step: 53000, loss is 2.2207727432250977\n",
      "epoch: 4 step: 54000, loss is 2.6184639930725098\n",
      "epoch: 4 step: 55000, loss is 2.1986911296844482\n",
      "epoch: 4 step: 56000, loss is 2.305952548980713\n",
      "epoch: 4 step: 57000, loss is 2.1372764110565186\n",
      "epoch: 4 step: 58000, loss is 2.386416435241699\n",
      "epoch: 4 step: 59000, loss is 2.1941497325897217\n",
      "epoch: 4 step: 60000, loss is 2.3907837867736816\n",
      "epoch: 5 step: 1000, loss is 2.551478385925293\n",
      "epoch: 5 step: 2000, loss is 2.191819190979004\n",
      "epoch: 5 step: 3000, loss is 2.4778425693511963\n",
      "epoch: 5 step: 4000, loss is 2.294854164123535\n",
      "epoch: 5 step: 5000, loss is 2.002631902694702\n",
      "epoch: 5 step: 6000, loss is 2.5550622940063477\n",
      "epoch: 5 step: 7000, loss is 2.4722228050231934\n",
      "epoch: 5 step: 8000, loss is 2.2607734203338623\n",
      "epoch: 5 step: 9000, loss is 2.290856122970581\n",
      "epoch: 5 step: 10000, loss is 2.5211217403411865\n",
      "epoch: 5 step: 11000, loss is 2.385528564453125\n",
      "epoch: 5 step: 12000, loss is 1.9243378639221191\n",
      "epoch: 5 step: 13000, loss is 1.9636585712432861\n",
      "epoch: 5 step: 14000, loss is 2.328580856323242\n",
      "epoch: 5 step: 15000, loss is 2.206371307373047\n",
      "epoch: 5 step: 16000, loss is 2.21182918548584\n",
      "epoch: 5 step: 17000, loss is 2.2523937225341797\n",
      "epoch: 5 step: 18000, loss is 2.3042068481445312\n",
      "epoch: 5 step: 19000, loss is 2.4642724990844727\n",
      "epoch: 5 step: 20000, loss is 2.477231025695801\n",
      "epoch: 5 step: 21000, loss is 2.3318898677825928\n",
      "epoch: 5 step: 22000, loss is 2.2462892532348633\n",
      "epoch: 5 step: 23000, loss is 2.3593814373016357\n",
      "epoch: 5 step: 24000, loss is 2.5518882274627686\n",
      "epoch: 5 step: 25000, loss is 2.4037113189697266\n",
      "epoch: 5 step: 26000, loss is 2.597264528274536\n",
      "epoch: 5 step: 27000, loss is 2.606689214706421\n",
      "epoch: 5 step: 28000, loss is 2.1123275756835938\n",
      "epoch: 5 step: 29000, loss is 2.2861294746398926\n",
      "epoch: 5 step: 30000, loss is 2.3569469451904297\n",
      "epoch: 5 step: 31000, loss is 2.422584295272827\n",
      "epoch: 5 step: 32000, loss is 2.2838666439056396\n",
      "epoch: 5 step: 33000, loss is 2.536675453186035\n",
      "epoch: 5 step: 34000, loss is 2.318732738494873\n",
      "epoch: 5 step: 35000, loss is 2.741647243499756\n",
      "epoch: 5 step: 36000, loss is 2.266151189804077\n",
      "epoch: 5 step: 37000, loss is 1.9973362684249878\n",
      "epoch: 5 step: 38000, loss is 2.2594001293182373\n",
      "epoch: 5 step: 39000, loss is 2.5023019313812256\n",
      "epoch: 5 step: 40000, loss is 2.0840156078338623\n",
      "epoch: 5 step: 41000, loss is 2.217639684677124\n",
      "epoch: 5 step: 42000, loss is 2.1867709159851074\n",
      "epoch: 5 step: 43000, loss is 2.5106008052825928\n",
      "epoch: 5 step: 44000, loss is 2.122633934020996\n",
      "epoch: 5 step: 45000, loss is 2.388549327850342\n",
      "epoch: 5 step: 46000, loss is 2.3117549419403076\n",
      "epoch: 5 step: 47000, loss is 2.513202667236328\n",
      "epoch: 5 step: 48000, loss is 2.3064584732055664\n",
      "epoch: 5 step: 49000, loss is 2.2943239212036133\n",
      "epoch: 5 step: 50000, loss is 2.321719169616699\n",
      "epoch: 5 step: 51000, loss is 2.2221460342407227\n",
      "epoch: 5 step: 52000, loss is 2.0400664806365967\n",
      "epoch: 5 step: 53000, loss is 2.236483335494995\n",
      "epoch: 5 step: 54000, loss is 2.807971954345703\n",
      "epoch: 5 step: 55000, loss is 2.418705940246582\n",
      "epoch: 5 step: 56000, loss is 2.377249002456665\n",
      "epoch: 5 step: 57000, loss is 2.3825631141662598\n",
      "epoch: 5 step: 58000, loss is 2.121894598007202\n",
      "epoch: 5 step: 59000, loss is 2.2708663940429688\n",
      "epoch: 5 step: 60000, loss is 2.4802916049957275\n",
      "epoch: 6 step: 1000, loss is 2.3840675354003906\n",
      "epoch: 6 step: 2000, loss is 2.2711455821990967\n",
      "epoch: 6 step: 3000, loss is 2.1378653049468994\n",
      "epoch: 6 step: 4000, loss is 2.3369266986846924\n",
      "epoch: 6 step: 5000, loss is 2.4787540435791016\n",
      "epoch: 6 step: 6000, loss is 2.6165614128112793\n",
      "epoch: 6 step: 7000, loss is 2.3470141887664795\n",
      "epoch: 6 step: 8000, loss is 2.3953118324279785\n",
      "epoch: 6 step: 9000, loss is 2.5530357360839844\n",
      "epoch: 6 step: 10000, loss is 2.452467441558838\n",
      "epoch: 6 step: 11000, loss is 2.4346020221710205\n",
      "epoch: 6 step: 12000, loss is 2.371217727661133\n",
      "epoch: 6 step: 13000, loss is 2.2105770111083984\n",
      "epoch: 6 step: 14000, loss is 2.370166778564453\n",
      "epoch: 6 step: 15000, loss is 2.446394681930542\n",
      "epoch: 6 step: 16000, loss is 2.3598792552948\n",
      "epoch: 6 step: 17000, loss is 2.4984254837036133\n",
      "epoch: 6 step: 18000, loss is 2.173147201538086\n",
      "epoch: 6 step: 19000, loss is 2.413771629333496\n",
      "epoch: 6 step: 20000, loss is 2.520639419555664\n",
      "epoch: 6 step: 21000, loss is 2.2991600036621094\n",
      "epoch: 6 step: 22000, loss is 2.2905311584472656\n",
      "epoch: 6 step: 23000, loss is 2.3451988697052\n",
      "epoch: 6 step: 24000, loss is 2.3063015937805176\n",
      "epoch: 6 step: 25000, loss is 2.08852481842041\n",
      "epoch: 6 step: 26000, loss is 2.3872077465057373\n",
      "epoch: 6 step: 27000, loss is 2.4692625999450684\n",
      "epoch: 6 step: 28000, loss is 2.2239878177642822\n",
      "epoch: 6 step: 29000, loss is 2.0859930515289307\n",
      "epoch: 6 step: 30000, loss is 2.508653402328491\n",
      "epoch: 6 step: 31000, loss is 2.255697250366211\n",
      "epoch: 6 step: 32000, loss is 2.4090347290039062\n",
      "epoch: 6 step: 33000, loss is 2.044034004211426\n",
      "epoch: 6 step: 34000, loss is 2.09594988822937\n",
      "epoch: 6 step: 35000, loss is 2.4834110736846924\n",
      "epoch: 6 step: 36000, loss is 2.364293336868286\n",
      "epoch: 6 step: 37000, loss is 2.094756841659546\n",
      "epoch: 6 step: 38000, loss is 2.4858851432800293\n",
      "epoch: 6 step: 39000, loss is 2.2693045139312744\n",
      "epoch: 6 step: 40000, loss is 2.1648969650268555\n",
      "epoch: 6 step: 41000, loss is 2.5029284954071045\n",
      "epoch: 6 step: 42000, loss is 2.3939223289489746\n",
      "epoch: 6 step: 43000, loss is 2.0185604095458984\n",
      "epoch: 6 step: 44000, loss is 2.269564628601074\n",
      "epoch: 6 step: 45000, loss is 2.4578194618225098\n",
      "epoch: 6 step: 46000, loss is 2.1289989948272705\n",
      "epoch: 6 step: 47000, loss is 2.4039132595062256\n",
      "epoch: 6 step: 48000, loss is 2.690451145172119\n",
      "epoch: 6 step: 49000, loss is 2.28656005859375\n",
      "epoch: 6 step: 50000, loss is 2.24288272857666\n",
      "epoch: 6 step: 51000, loss is 2.468186855316162\n",
      "epoch: 6 step: 52000, loss is 2.372769832611084\n",
      "epoch: 6 step: 53000, loss is 2.653688430786133\n",
      "epoch: 6 step: 54000, loss is 2.1728413105010986\n",
      "epoch: 6 step: 55000, loss is 2.253980875015259\n",
      "epoch: 6 step: 56000, loss is 2.6727471351623535\n",
      "epoch: 6 step: 57000, loss is 1.9468748569488525\n",
      "epoch: 6 step: 58000, loss is 2.3109188079833984\n",
      "epoch: 6 step: 59000, loss is 2.263155460357666\n",
      "epoch: 6 step: 60000, loss is 2.1640102863311768\n",
      "epoch: 7 step: 1000, loss is 2.6342153549194336\n",
      "epoch: 7 step: 2000, loss is 2.3013908863067627\n",
      "epoch: 7 step: 3000, loss is 2.7697737216949463\n",
      "epoch: 7 step: 4000, loss is 2.3116374015808105\n",
      "epoch: 7 step: 5000, loss is 2.592801809310913\n",
      "epoch: 7 step: 6000, loss is 2.29237699508667\n",
      "epoch: 7 step: 7000, loss is 2.0629169940948486\n",
      "epoch: 7 step: 8000, loss is 2.491481065750122\n",
      "epoch: 7 step: 9000, loss is 2.3898019790649414\n",
      "epoch: 7 step: 10000, loss is 2.4046685695648193\n",
      "epoch: 7 step: 11000, loss is 2.1116528511047363\n",
      "epoch: 7 step: 12000, loss is 2.2298877239227295\n",
      "epoch: 7 step: 13000, loss is 2.244786024093628\n",
      "epoch: 7 step: 14000, loss is 2.030946731567383\n",
      "epoch: 7 step: 15000, loss is 2.4845309257507324\n",
      "epoch: 7 step: 16000, loss is 2.2042236328125\n",
      "epoch: 7 step: 17000, loss is 2.3036022186279297\n",
      "epoch: 7 step: 18000, loss is 2.3645131587982178\n",
      "epoch: 7 step: 19000, loss is 2.5729706287384033\n",
      "epoch: 7 step: 20000, loss is 2.2776334285736084\n",
      "epoch: 7 step: 21000, loss is 2.705256223678589\n",
      "epoch: 7 step: 22000, loss is 2.820946216583252\n",
      "epoch: 7 step: 23000, loss is 2.2512168884277344\n",
      "epoch: 7 step: 24000, loss is 2.6279358863830566\n",
      "epoch: 7 step: 25000, loss is 2.3333206176757812\n",
      "epoch: 7 step: 26000, loss is 2.444880723953247\n",
      "epoch: 7 step: 27000, loss is 2.2399024963378906\n",
      "epoch: 7 step: 28000, loss is 2.2343835830688477\n",
      "epoch: 7 step: 29000, loss is 2.334714412689209\n",
      "epoch: 7 step: 30000, loss is 2.3481287956237793\n",
      "epoch: 7 step: 31000, loss is 2.123384475708008\n",
      "epoch: 7 step: 32000, loss is 2.334744691848755\n",
      "epoch: 7 step: 33000, loss is 1.9980247020721436\n",
      "epoch: 7 step: 34000, loss is 2.3620412349700928\n",
      "epoch: 7 step: 35000, loss is 2.3946125507354736\n",
      "epoch: 7 step: 36000, loss is 2.306525945663452\n",
      "epoch: 7 step: 37000, loss is 2.2970125675201416\n",
      "epoch: 7 step: 38000, loss is 2.5982398986816406\n",
      "epoch: 7 step: 39000, loss is 2.0865955352783203\n",
      "epoch: 7 step: 40000, loss is 2.1433591842651367\n",
      "epoch: 7 step: 41000, loss is 2.447124719619751\n",
      "epoch: 7 step: 42000, loss is 1.948370337486267\n",
      "epoch: 7 step: 43000, loss is 2.3484060764312744\n",
      "epoch: 7 step: 44000, loss is 2.2508842945098877\n",
      "epoch: 7 step: 45000, loss is 2.900517702102661\n",
      "epoch: 7 step: 46000, loss is 2.191298007965088\n",
      "epoch: 7 step: 47000, loss is 2.5984625816345215\n",
      "epoch: 7 step: 48000, loss is 2.3023335933685303\n",
      "epoch: 7 step: 49000, loss is 1.9647152423858643\n",
      "epoch: 7 step: 50000, loss is 2.701044797897339\n",
      "epoch: 7 step: 51000, loss is 2.4150590896606445\n",
      "epoch: 7 step: 52000, loss is 2.300490617752075\n",
      "epoch: 7 step: 53000, loss is 2.298691511154175\n",
      "epoch: 7 step: 54000, loss is 2.1270761489868164\n",
      "epoch: 7 step: 55000, loss is 2.5012593269348145\n",
      "epoch: 7 step: 56000, loss is 2.4620304107666016\n",
      "epoch: 7 step: 57000, loss is 2.1709096431732178\n",
      "epoch: 7 step: 58000, loss is 2.379211902618408\n",
      "epoch: 7 step: 59000, loss is 2.3605525493621826\n",
      "epoch: 7 step: 60000, loss is 2.203880548477173\n",
      "epoch: 8 step: 1000, loss is 2.086092710494995\n",
      "epoch: 8 step: 2000, loss is 2.820890426635742\n",
      "epoch: 8 step: 3000, loss is 2.0630202293395996\n",
      "epoch: 8 step: 4000, loss is 2.4173316955566406\n",
      "epoch: 8 step: 5000, loss is 2.1744091510772705\n",
      "epoch: 8 step: 6000, loss is 2.673190116882324\n",
      "epoch: 8 step: 7000, loss is 2.2058956623077393\n",
      "epoch: 8 step: 8000, loss is 2.4749441146850586\n",
      "epoch: 8 step: 9000, loss is 2.309840440750122\n",
      "epoch: 8 step: 10000, loss is 2.479569435119629\n",
      "epoch: 8 step: 11000, loss is 2.612123727798462\n",
      "epoch: 8 step: 12000, loss is 2.135944128036499\n",
      "epoch: 8 step: 13000, loss is 2.1502373218536377\n",
      "epoch: 8 step: 14000, loss is 2.252267360687256\n",
      "epoch: 8 step: 15000, loss is 2.2420425415039062\n",
      "epoch: 8 step: 16000, loss is 2.0960168838500977\n",
      "epoch: 8 step: 17000, loss is 2.4560413360595703\n",
      "epoch: 8 step: 18000, loss is 2.6774721145629883\n",
      "epoch: 8 step: 19000, loss is 2.376845121383667\n",
      "epoch: 8 step: 20000, loss is 2.5665416717529297\n",
      "epoch: 8 step: 21000, loss is 2.3999338150024414\n",
      "epoch: 8 step: 22000, loss is 2.074214458465576\n",
      "epoch: 8 step: 23000, loss is 2.737649440765381\n",
      "epoch: 8 step: 24000, loss is 2.0848312377929688\n",
      "epoch: 8 step: 25000, loss is 2.3222053050994873\n",
      "epoch: 8 step: 26000, loss is 2.3355226516723633\n",
      "epoch: 8 step: 27000, loss is 2.1479201316833496\n",
      "epoch: 8 step: 28000, loss is 2.6408894062042236\n",
      "epoch: 8 step: 29000, loss is 1.943050503730774\n",
      "epoch: 8 step: 30000, loss is 2.17181134223938\n",
      "epoch: 8 step: 31000, loss is 2.700065851211548\n",
      "epoch: 8 step: 32000, loss is 2.0271711349487305\n",
      "epoch: 8 step: 33000, loss is 2.2463176250457764\n",
      "epoch: 8 step: 34000, loss is 2.236151695251465\n",
      "epoch: 8 step: 35000, loss is 2.2151379585266113\n",
      "epoch: 8 step: 36000, loss is 1.9976199865341187\n",
      "epoch: 8 step: 37000, loss is 2.329843521118164\n",
      "epoch: 8 step: 38000, loss is 2.2450954914093018\n",
      "epoch: 8 step: 39000, loss is 2.579662322998047\n",
      "epoch: 8 step: 40000, loss is 2.4508917331695557\n",
      "epoch: 8 step: 41000, loss is 2.363739490509033\n",
      "epoch: 8 step: 42000, loss is 2.5437450408935547\n",
      "epoch: 8 step: 43000, loss is 2.6094112396240234\n",
      "epoch: 8 step: 44000, loss is 2.3818068504333496\n",
      "epoch: 8 step: 45000, loss is 2.372767448425293\n",
      "epoch: 8 step: 46000, loss is 2.4328317642211914\n",
      "epoch: 8 step: 47000, loss is 2.241811752319336\n",
      "epoch: 8 step: 48000, loss is 2.597099781036377\n",
      "epoch: 8 step: 49000, loss is 2.325995683670044\n",
      "epoch: 8 step: 50000, loss is 2.6809518337249756\n",
      "epoch: 8 step: 51000, loss is 2.2608561515808105\n",
      "epoch: 8 step: 52000, loss is 2.2840707302093506\n",
      "epoch: 8 step: 53000, loss is 2.407076597213745\n",
      "epoch: 8 step: 54000, loss is 2.2812721729278564\n",
      "epoch: 8 step: 55000, loss is 2.281949520111084\n",
      "epoch: 8 step: 56000, loss is 2.7320311069488525\n",
      "epoch: 8 step: 57000, loss is 2.462707996368408\n",
      "epoch: 8 step: 58000, loss is 2.200354814529419\n",
      "epoch: 8 step: 59000, loss is 2.165792942047119\n",
      "epoch: 8 step: 60000, loss is 2.295597553253174\n",
      "epoch: 9 step: 1000, loss is 2.3186113834381104\n",
      "epoch: 9 step: 2000, loss is 2.263929843902588\n",
      "epoch: 9 step: 3000, loss is 2.3365612030029297\n",
      "epoch: 9 step: 4000, loss is 2.6590309143066406\n",
      "epoch: 9 step: 5000, loss is 2.6260476112365723\n",
      "epoch: 9 step: 6000, loss is 2.3190317153930664\n",
      "epoch: 9 step: 7000, loss is 2.0742900371551514\n",
      "epoch: 9 step: 8000, loss is 2.2380824089050293\n",
      "epoch: 9 step: 9000, loss is 2.507094383239746\n",
      "epoch: 9 step: 10000, loss is 2.143758773803711\n",
      "epoch: 9 step: 11000, loss is 2.2687318325042725\n",
      "epoch: 9 step: 12000, loss is 1.9545162916183472\n",
      "epoch: 9 step: 13000, loss is 2.93708872795105\n",
      "epoch: 9 step: 14000, loss is 2.384855031967163\n",
      "epoch: 9 step: 15000, loss is 2.404132843017578\n",
      "epoch: 9 step: 16000, loss is 2.2682907581329346\n",
      "epoch: 9 step: 17000, loss is 2.1807501316070557\n",
      "epoch: 9 step: 18000, loss is 2.141282796859741\n",
      "epoch: 9 step: 19000, loss is 2.5274524688720703\n",
      "epoch: 9 step: 20000, loss is 2.0032851696014404\n",
      "epoch: 9 step: 21000, loss is 1.9275541305541992\n",
      "epoch: 9 step: 22000, loss is 2.2557928562164307\n",
      "epoch: 9 step: 23000, loss is 2.49845814704895\n",
      "epoch: 9 step: 24000, loss is 2.8107728958129883\n",
      "epoch: 9 step: 25000, loss is 2.370880365371704\n",
      "epoch: 9 step: 26000, loss is 2.4728171825408936\n",
      "epoch: 9 step: 27000, loss is 2.531946897506714\n",
      "epoch: 9 step: 28000, loss is 2.38295316696167\n",
      "epoch: 9 step: 29000, loss is 2.3487308025360107\n",
      "epoch: 9 step: 30000, loss is 1.8524596691131592\n",
      "epoch: 9 step: 31000, loss is 2.5938258171081543\n",
      "epoch: 9 step: 32000, loss is 2.3015103340148926\n",
      "epoch: 9 step: 33000, loss is 2.11376690864563\n",
      "epoch: 9 step: 34000, loss is 2.038072109222412\n",
      "epoch: 9 step: 35000, loss is 2.3081672191619873\n",
      "epoch: 9 step: 36000, loss is 2.093755006790161\n",
      "epoch: 9 step: 37000, loss is 2.0295305252075195\n",
      "epoch: 9 step: 38000, loss is 2.399104595184326\n",
      "epoch: 9 step: 39000, loss is 2.256904125213623\n",
      "epoch: 9 step: 40000, loss is 2.3093414306640625\n",
      "epoch: 9 step: 41000, loss is 1.8954691886901855\n",
      "epoch: 9 step: 42000, loss is 2.6311285495758057\n",
      "epoch: 9 step: 43000, loss is 2.2864034175872803\n",
      "epoch: 9 step: 44000, loss is 2.325317621231079\n",
      "epoch: 9 step: 45000, loss is 2.443498134613037\n",
      "epoch: 9 step: 46000, loss is 1.8976662158966064\n",
      "epoch: 9 step: 47000, loss is 2.0117099285125732\n",
      "epoch: 9 step: 48000, loss is 2.2681477069854736\n",
      "epoch: 9 step: 49000, loss is 2.439310073852539\n",
      "epoch: 9 step: 50000, loss is 2.246572971343994\n",
      "epoch: 9 step: 51000, loss is 2.6488664150238037\n",
      "epoch: 9 step: 52000, loss is 2.2047555446624756\n",
      "epoch: 9 step: 53000, loss is 2.6592535972595215\n",
      "epoch: 9 step: 54000, loss is 2.631004571914673\n",
      "epoch: 9 step: 55000, loss is 2.167416572570801\n",
      "epoch: 9 step: 56000, loss is 2.430999279022217\n",
      "epoch: 9 step: 57000, loss is 2.2986111640930176\n",
      "epoch: 9 step: 58000, loss is 2.588390827178955\n",
      "epoch: 9 step: 59000, loss is 2.68522047996521\n",
      "epoch: 9 step: 60000, loss is 2.1190073490142822\n",
      "epoch: 10 step: 1000, loss is 2.6190946102142334\n",
      "epoch: 10 step: 2000, loss is 2.45794415473938\n",
      "epoch: 10 step: 3000, loss is 2.5362863540649414\n",
      "epoch: 10 step: 4000, loss is 2.3427417278289795\n",
      "epoch: 10 step: 5000, loss is 2.3817026615142822\n",
      "epoch: 10 step: 6000, loss is 2.2873663902282715\n",
      "epoch: 10 step: 7000, loss is 2.283179521560669\n",
      "epoch: 10 step: 8000, loss is 2.312215566635132\n",
      "epoch: 10 step: 9000, loss is 2.0133602619171143\n",
      "epoch: 10 step: 10000, loss is 2.356159210205078\n",
      "epoch: 10 step: 11000, loss is 2.6038973331451416\n",
      "epoch: 10 step: 12000, loss is 2.5312700271606445\n",
      "epoch: 10 step: 13000, loss is 2.196047306060791\n",
      "epoch: 10 step: 14000, loss is 2.3538639545440674\n",
      "epoch: 10 step: 15000, loss is 2.688438892364502\n",
      "epoch: 10 step: 16000, loss is 2.40309476852417\n",
      "epoch: 10 step: 17000, loss is 2.50325083732605\n",
      "epoch: 10 step: 18000, loss is 2.085026741027832\n",
      "epoch: 10 step: 19000, loss is 2.5149893760681152\n",
      "epoch: 10 step: 20000, loss is 2.157886505126953\n",
      "epoch: 10 step: 21000, loss is 2.3625845909118652\n",
      "epoch: 10 step: 22000, loss is 2.3674328327178955\n",
      "epoch: 10 step: 23000, loss is 2.0374181270599365\n",
      "epoch: 10 step: 24000, loss is 2.510253667831421\n",
      "epoch: 10 step: 25000, loss is 1.975170612335205\n",
      "epoch: 10 step: 26000, loss is 2.523651123046875\n",
      "epoch: 10 step: 27000, loss is 2.0984725952148438\n",
      "epoch: 10 step: 28000, loss is 2.30830979347229\n",
      "epoch: 10 step: 29000, loss is 2.3496792316436768\n",
      "epoch: 10 step: 30000, loss is 2.168764591217041\n",
      "epoch: 10 step: 31000, loss is 2.1377108097076416\n",
      "epoch: 10 step: 32000, loss is 2.3864402770996094\n",
      "epoch: 10 step: 33000, loss is 2.1584317684173584\n",
      "epoch: 10 step: 34000, loss is 2.2716500759124756\n",
      "epoch: 10 step: 35000, loss is 2.3832931518554688\n",
      "epoch: 10 step: 36000, loss is 2.1292779445648193\n",
      "epoch: 10 step: 37000, loss is 2.304908275604248\n",
      "epoch: 10 step: 38000, loss is 2.077014446258545\n",
      "epoch: 10 step: 39000, loss is 2.307661771774292\n",
      "epoch: 10 step: 40000, loss is 2.3176369667053223\n",
      "epoch: 10 step: 41000, loss is 2.53914737701416\n",
      "epoch: 10 step: 42000, loss is 2.571668863296509\n",
      "epoch: 10 step: 43000, loss is 2.2784976959228516\n",
      "epoch: 10 step: 44000, loss is 2.0095467567443848\n",
      "epoch: 10 step: 45000, loss is 1.9888083934783936\n",
      "epoch: 10 step: 46000, loss is 2.507117986679077\n",
      "epoch: 10 step: 47000, loss is 2.515671968460083\n",
      "epoch: 10 step: 48000, loss is 2.3790643215179443\n",
      "epoch: 10 step: 49000, loss is 2.6000277996063232\n",
      "epoch: 10 step: 50000, loss is 2.3543126583099365\n",
      "epoch: 10 step: 51000, loss is 1.9633815288543701\n",
      "epoch: 10 step: 52000, loss is 2.4596900939941406\n",
      "epoch: 10 step: 53000, loss is 2.310760259628296\n",
      "epoch: 10 step: 54000, loss is 2.3112285137176514\n",
      "epoch: 10 step: 55000, loss is 2.262722969055176\n",
      "epoch: 10 step: 56000, loss is 2.243213653564453\n",
      "epoch: 10 step: 57000, loss is 2.075961112976074\n",
      "epoch: 10 step: 58000, loss is 2.4172403812408447\n",
      "epoch: 10 step: 59000, loss is 2.0972557067871094\n",
      "epoch: 10 step: 60000, loss is 2.533829927444458\n",
      "测试结果: {'Accuracy': 0.1135}\n",
      "----------------------------------------\n",
      "实验 3: ReLU vs Sigmoid\n",
      "开始训练: Loss=SoftmaxCrossEntropyWithLogits, Batch Size=32, Activation=relu, Dropout=False, ApplyOneHot=False\n",
      "epoch: 1 step: 1000, loss is 0.3523212671279907\n",
      "epoch: 2 step: 125, loss is 0.1961401104927063\n",
      "epoch: 2 step: 1125, loss is 0.12807108461856842\n",
      "epoch: 3 step: 250, loss is 0.031719405204057693\n",
      "epoch: 3 step: 1250, loss is 0.010960814543068409\n",
      "epoch: 4 step: 375, loss is 0.0005482630804181099\n",
      "epoch: 4 step: 1375, loss is 0.0008696952718310058\n",
      "epoch: 5 step: 500, loss is 0.0009578882018104196\n",
      "epoch: 5 step: 1500, loss is 0.0016676652012392879\n",
      "epoch: 6 step: 625, loss is 0.001165522146038711\n",
      "epoch: 6 step: 1625, loss is 0.0005272446433082223\n",
      "epoch: 7 step: 750, loss is 0.011412999592721462\n",
      "epoch: 7 step: 1750, loss is 0.0007014804286882281\n",
      "epoch: 8 step: 875, loss is 0.006218667607754469\n",
      "epoch: 8 step: 1875, loss is 0.0004822130431421101\n",
      "epoch: 9 step: 1000, loss is 0.02127940021455288\n",
      "epoch: 10 step: 125, loss is 9.989070531446487e-05\n",
      "epoch: 10 step: 1125, loss is 0.03054293803870678\n",
      "测试结果: {'Accuracy': 0.9910857371794872}\n",
      "----------------------------------------\n",
      "开始训练: Loss=SoftmaxCrossEntropyWithLogits, Batch Size=32, Activation=sigmoid, Dropout=False, ApplyOneHot=False\n",
      "epoch: 1 step: 1000, loss is 2.311690092086792\n",
      "epoch: 2 step: 125, loss is 2.3152146339416504\n",
      "epoch: 2 step: 1125, loss is 2.3059699535369873\n",
      "epoch: 3 step: 250, loss is 2.320448160171509\n",
      "epoch: 3 step: 1250, loss is 2.296147584915161\n",
      "epoch: 4 step: 375, loss is 2.3013627529144287\n",
      "epoch: 4 step: 1375, loss is 2.3082611560821533\n",
      "epoch: 5 step: 500, loss is 2.2941033840179443\n",
      "epoch: 5 step: 1500, loss is 2.3028392791748047\n",
      "epoch: 6 step: 625, loss is 2.3165838718414307\n",
      "epoch: 6 step: 1625, loss is 2.298177719116211\n",
      "epoch: 7 step: 750, loss is 2.293386459350586\n",
      "epoch: 7 step: 1750, loss is 2.3054332733154297\n",
      "epoch: 8 step: 875, loss is 2.2938523292541504\n",
      "epoch: 8 step: 1875, loss is 2.0938878059387207\n",
      "epoch: 9 step: 1000, loss is 1.8500664234161377\n",
      "epoch: 10 step: 125, loss is 1.8994171619415283\n",
      "epoch: 10 step: 1125, loss is 1.5759806632995605\n",
      "测试结果: {'Accuracy': 0.41095753205128205}\n",
      "----------------------------------------\n",
      "实验 4: Dropout vs No Dropout\n",
      "开始训练: Loss=SoftmaxCrossEntropyWithLogits, Batch Size=32, Activation=relu, Dropout=True, ApplyOneHot=False\n",
      "epoch: 1 step: 1000, loss is 2.270078420639038\n",
      "epoch: 2 step: 125, loss is 0.2260025590658188\n",
      "epoch: 2 step: 1125, loss is 0.029402343556284904\n",
      "epoch: 3 step: 250, loss is 0.11840032786130905\n",
      "epoch: 3 step: 1250, loss is 0.0024827721063047647\n",
      "epoch: 4 step: 375, loss is 0.0085010826587677\n",
      "epoch: 4 step: 1375, loss is 0.1330501139163971\n",
      "epoch: 5 step: 500, loss is 0.13011744618415833\n",
      "epoch: 5 step: 1500, loss is 0.06806791573762894\n",
      "epoch: 6 step: 625, loss is 0.014402786269783974\n",
      "epoch: 6 step: 1625, loss is 0.08558141440153122\n",
      "epoch: 7 step: 750, loss is 0.03524364158511162\n",
      "epoch: 7 step: 1750, loss is 0.07875371724367142\n",
      "epoch: 8 step: 875, loss is 0.005513584241271019\n",
      "epoch: 8 step: 1875, loss is 0.03791690990328789\n",
      "epoch: 9 step: 1000, loss is 0.0033175575081259012\n",
      "epoch: 10 step: 125, loss is 0.0026921764947474003\n",
      "epoch: 10 step: 1125, loss is 0.023611262440681458\n",
      "测试结果: {'Accuracy': 0.9904847756410257}\n",
      "----------------------------------------\n",
      "开始训练: Loss=SoftmaxCrossEntropyWithLogits, Batch Size=32, Activation=relu, Dropout=False, ApplyOneHot=False\n",
      "epoch: 1 step: 1000, loss is 0.33576005697250366\n",
      "epoch: 2 step: 125, loss is 0.10579296946525574\n",
      "epoch: 2 step: 1125, loss is 0.06988658756017685\n",
      "epoch: 3 step: 250, loss is 0.012924104928970337\n",
      "epoch: 3 step: 1250, loss is 0.010069147683680058\n",
      "epoch: 4 step: 375, loss is 0.011492202989757061\n",
      "epoch: 4 step: 1375, loss is 0.007924100384116173\n",
      "epoch: 5 step: 500, loss is 0.007902640849351883\n",
      "epoch: 5 step: 1500, loss is 0.00031458039302378893\n",
      "epoch: 6 step: 625, loss is 0.000551474979147315\n",
      "epoch: 6 step: 1625, loss is 0.00013264466542750597\n",
      "epoch: 7 step: 750, loss is 0.00044836843153461814\n",
      "epoch: 7 step: 1750, loss is 0.04856175556778908\n",
      "epoch: 8 step: 875, loss is 0.0037180085200816393\n",
      "epoch: 8 step: 1875, loss is 0.03651953116059303\n",
      "epoch: 9 step: 1000, loss is 0.0004419208562467247\n",
      "epoch: 10 step: 125, loss is 0.0005039519164711237\n",
      "epoch: 10 step: 1125, loss is 0.00029584389994852245\n",
      "测试结果: {'Accuracy': 0.9881810897435898}\n",
      "----------------------------------------\n",
      "所有实验完成。\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import mindspore.ops as ops\n",
    "import numpy as np\n",
    "import mindspore.dataset as ds\n",
    "import mindspore.nn as nn\n",
    "from mindspore import context, Tensor\n",
    "from mindspore.train import Model\n",
    "from mindspore.common import dtype as mstype\n",
    "from mindspore.nn.loss import MSELoss, SoftmaxCrossEntropyWithLogits\n",
    "from mindspore.nn.metrics import Accuracy\n",
    "from mindspore.train.callback import LossMonitor, Callback\n",
    "from mindspore.train.callback import ModelCheckpoint, CheckpointConfig\n",
    "from mindspore.common.initializer import TruncatedNormal\n",
    "\n",
    "context.set_context(mode=context.GRAPH_MODE, device_target=\"GPU\") # 或者 \"Ascend\", \"CPU\"\n",
    "\n",
    "# 数据集处理函数 - 修改后\n",
    "def create_dataset(data_path, batch_size=32, repeat_size=1, num_parallel_workers=1, apply_one_hot=False): # 增加 apply_one_hot 参数\n",
    "    mnist_ds = ds.MnistDataset(data_path)\n",
    "    resize_height, resize_width = 32, 32\n",
    "    rescale = 1.0 / 255.0\n",
    "    shift = 0.0\n",
    "    rescale_nml = 1 / 0.3081\n",
    "    shift_nml = -1 * 0.1307 / 0.3081\n",
    "\n",
    "    # 数据增强操作\n",
    "    resize_op = ds.vision.c_transforms.Resize((resize_height, resize_width))\n",
    "    rescale_op = ds.vision.c_transforms.Rescale(rescale, shift)\n",
    "    rescale_nml_op = ds.vision.c_transforms.Rescale(rescale_nml, shift_nml)\n",
    "    hwc2chw_op = ds.vision.c_transforms.HWC2CHW()\n",
    "    type_cast_op_label = ds.transforms.c_transforms.TypeCast(mstype.int32) # 原始标签类型转换\n",
    "\n",
    "    # 对标签应用 TypeCast\n",
    "    mnist_ds = mnist_ds.map(input_columns=\"label\", operations=type_cast_op_label, num_parallel_workers=num_parallel_workers)\n",
    "\n",
    "    # 如果需要，应用 OneHot 编码 (在 batch 之前)\n",
    "    if apply_one_hot:\n",
    "        depth = 10  # 类别数\n",
    "        on_value = Tensor(1.0, mstype.float32)\n",
    "        off_value = Tensor(0.0, mstype.float32)\n",
    "        one_hot_op = ds.transforms.c_transforms.OneHot(num_classes=depth) # 使用 dataset 内置 OneHot\n",
    "        # 注意：内置 OneHot 输出默认是 int32，MSELoss 需要 float32，所以后面加一个 TypeCast\n",
    "        type_cast_op_onehot = ds.transforms.c_transforms.TypeCast(mstype.float32)\n",
    "        mnist_ds = mnist_ds.map(input_columns=\"label\", operations=[one_hot_op, type_cast_op_onehot], num_parallel_workers=num_parallel_workers)\n",
    "        print(f\"Applied OneHot encoding to labels for path: {data_path}\")\n",
    "\n",
    "    # 对图像应用变换\n",
    "    mnist_ds = mnist_ds.map(input_columns=\"image\", operations=[resize_op, rescale_op, rescale_nml_op, hwc2chw_op],\n",
    "                            num_parallel_workers=num_parallel_workers)\n",
    "\n",
    "    # Shuffle, Batch, Repeat\n",
    "    mnist_ds = mnist_ds.shuffle(buffer_size=10000)\n",
    "    mnist_ds = mnist_ds.batch(batch_size, drop_remainder=True)\n",
    "    mnist_ds = mnist_ds.repeat(repeat_size)\n",
    "    return mnist_ds\n",
    "\n",
    "# 定义LeNet5网络 (保持不变)\n",
    "class LeNet5(nn.Cell):\n",
    "    def __init__(self, activation=\"relu\", use_dropout=False):\n",
    "        super(LeNet5, self).__init__()\n",
    "        self.use_dropout = use_dropout\n",
    "        self.activation = nn.ReLU() if activation == \"relu\" else nn.Sigmoid()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5, pad_mode=\"valid\", weight_init=TruncatedNormal(0.02))\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5, pad_mode=\"valid\", weight_init=TruncatedNormal(0.02))\n",
    "        self.fc1 = nn.Dense(16 * 5 * 5, 120, weight_init=TruncatedNormal(0.02))\n",
    "        self.fc2 = nn.Dense(120, 84, weight_init=TruncatedNormal(0.02))\n",
    "        self.fc3 = nn.Dense(84, 10, weight_init=TruncatedNormal(0.02))\n",
    "        self.max_pool2d = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.dropout = nn.Dropout(keep_prob=0.5) # MindSpore >= 1.6 推荐使用 p=0.5\n",
    "\n",
    "    def construct(self, x):\n",
    "        x = self.max_pool2d(self.activation(self.conv1(x)))\n",
    "        x = self.max_pool2d(self.activation(self.conv2(x)))\n",
    "        x = self.flatten(x)\n",
    "        x = self.activation(self.fc1(x))\n",
    "        if self.use_dropout:\n",
    "            x = self.dropout(x)\n",
    "        x = self.activation(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# 自定义训练函数 - 修改后\n",
    "def train_model(loss_fn, batch_size, activation, use_dropout, epoch_size=10):\n",
    "    # 根据损失函数决定是否在创建数据集时应用 OneHot\n",
    "    apply_one_hot_for_loss = isinstance(loss_fn, MSELoss)\n",
    "\n",
    "    # 创建数据集，传入 apply_one_hot 参数\n",
    "    train_dataset = create_dataset(\"./data/train\", batch_size=batch_size, apply_one_hot=apply_one_hot_for_loss)\n",
    "    test_dataset = create_dataset(\"./data/test\", batch_size=batch_size, apply_one_hot=apply_one_hot_for_loss)\n",
    "\n",
    "    network = LeNet5(activation=activation, use_dropout=use_dropout)\n",
    "    optimizer = nn.Momentum(network.trainable_params(), learning_rate=0.01, momentum=0.9)\n",
    "\n",
    "    # SoftmaxCrossEntropyWithLogits 的 sparse 参数需要根据标签是否是 OneHot 来设置\n",
    "    # 如果标签已经是 OneHot (MSELoss情况)，sparse=False\n",
    "    # 如果标签是原始类别索引 (CrossEntropy情况)，sparse=True\n",
    "    if isinstance(loss_fn, SoftmaxCrossEntropyWithLogits):\n",
    "        loss_fn = SoftmaxCrossEntropyWithLogits(sparse=not apply_one_hot_for_loss, reduction=\"mean\")\n",
    "\n",
    "    model = Model(network, loss_fn, optimizer, metrics={\"Accuracy\": Accuracy()})\n",
    "\n",
    "    # 移除 train_model 内部的 OneHot 逻辑\n",
    "\n",
    "    print(f\"开始训练: Loss={loss_fn.__class__.__name__}, Batch Size={batch_size}, Activation={activation}, Dropout={use_dropout}, ApplyOneHot={apply_one_hot_for_loss}\")\n",
    "    model.train(epoch_size, train_dataset, callbacks=[LossMonitor(1000)], dataset_sink_mode=False)\n",
    "\n",
    "    # 评估时，Accuracy metric 会自动处理 OneHot 或非 OneHot 标签\n",
    "    acc = model.eval(test_dataset)\n",
    "    print(f\"测试结果: {acc}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "# --- 对比实验 ---\n",
    "# 1. 平方差损失 vs 交叉熵损失\n",
    "print(\"实验 1: MSELoss vs SoftmaxCrossEntropyWithLogits\")\n",
    "train_model(MSELoss(), batch_size=32, activation=\"relu\", use_dropout=False)\n",
    "train_model(SoftmaxCrossEntropyWithLogits(), batch_size=32, activation=\"relu\", use_dropout=False) \n",
    "\n",
    "# 2. Mini-batch vs No batch (Batch Size 1)\n",
    "print(\"实验 2: Batch Size 32 vs 1\")\n",
    "train_model(SoftmaxCrossEntropyWithLogits(), batch_size=32, activation=\"relu\", use_dropout=False)\n",
    "train_model(SoftmaxCrossEntropyWithLogits(), batch_size=1, activation=\"relu\", use_dropout=False) \n",
    "\n",
    "# 3. ReLU vs Sigmoid\n",
    "print(\"实验 3: ReLU vs Sigmoid\")\n",
    "train_model(SoftmaxCrossEntropyWithLogits(), batch_size=32, activation=\"relu\", use_dropout=False)\n",
    "train_model(SoftmaxCrossEntropyWithLogits(), batch_size=32, activation=\"sigmoid\", use_dropout=False)\n",
    "\n",
    "# 4. 有Dropout vs 无Dropout\n",
    "print(\"实验 4: Dropout vs No Dropout\")\n",
    "train_model(SoftmaxCrossEntropyWithLogits(), batch_size=32, activation=\"relu\", use_dropout=True)\n",
    "train_model(SoftmaxCrossEntropyWithLogits(), batch_size=32, activation=\"relu\", use_dropout=False)\n",
    "\n",
    "print(\"所有实验完成。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371c94bb-7b8b-4d7a-9e43-e3e894b8c1f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MindSpore",
   "language": "python",
   "name": "mindspore"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
